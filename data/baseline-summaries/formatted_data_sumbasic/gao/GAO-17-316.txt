the u.s. government plans to spend approximately $35 billion on foreign assistance in 2017 to improve the lives and health of millions living in poverty , support democracy , enhance global security , and achieve other u.s. foreign policy goals .

for u.s. agencies that provide foreign assistance , evaluations are essential to assess and help improve program results .

preparing and disseminating high - quality evaluations helps agencies and their implementing partners assess their program results , adjust program designs , and make evidence - based decisions about the use of their resources .

both the 2010 gpra modernization act and the 2010 presidential policy directive on global development policy called for an increased focus on evaluations of agency programs .

in addition , in july 2016 , the foreign aid transparency and accountability act of 2016 required the president to set forth guidelines for the establishment of measurable goals , performance metrics , and monitoring and evaluation ( m&e ) plans for u.s. foreign assistance within 18 months of its enactment .

in recent years , foreign assistance agencies have adopted or updated their guidance on evaluations .

however , prior gao work has identified challenges in the design , implementation , and dissemination of evaluations of individual foreign assistance programs .

we were asked to review u.s. agencies' evaluation of foreign assistance programs .

focusing on evaluations completed in fiscal year 2015 by the six agencies that administer the largest amounts of u.s. foreign assistance — the department of defense ( dod ) , the department of health and human services ( hhs ) , the millennium challenge corporation ( mcc ) , the department of state ( state ) , the u.s. department of agriculture ( usda ) , and the u.s. agency for international development ( usaid ) — this report examines ( 1 ) the extent to which foreign assistance program evaluations met key evaluation quality criteria , ( 2 ) the costs of the agencies' evaluations and factors that affect these costs , and ( 3 ) the extent to which the agencies ensure the dissemination of evaluation reports within the agency and to the public .

to identify the six agencies that administer the largest amounts of foreign assistance , we reviewed obligations data that the agencies reported to usaid's u.s. overseas loans and grants database for fiscal years 2008 through 2012 .

to identify evaluations completed in fiscal year 2015 , we requested that each agency provide a list of all foreign aid evaluation reports completed in that year .

we did not separately review agency files to identify if agencies had additional evaluations beyond those listed in the registries .

we performed an initial review of the evaluation lists and documents provided by agencies and excluded some documents from our review because they were incomplete , were not evaluation reports , or were not completed in fiscal year 2015 .

to address our first objective , we reviewed all state , dod , and mcc evaluation reports completed in fiscal year 2015 .

we reviewed representative samples of usaid , usda , and hhs evaluation reports to create estimates about the population of all evaluation reports at the sampled agencies .

we reviewed the selected evaluations against eight criteria for high - quality evaluations related to the appropriateness of design , data collection methods , and analysis and the extent of support for conclusions and any recommendations .

we developed these criteria on the basis of a review of federal , international , and evaluation organization guidance and our prior reports ( see app .

ii for the criteria and data by agency for our review ) .

we assessed each agency's evaluations as “generally,” “partially,” or “not at all” meeting each criterion ; we also rated some evaluations as providing insufficient information to make an assessment .

in addition to assessing evaluation quality , we also collected information about the characteristics of each evaluation , such as the location of the study and its methodology .

to address our second objective , we reviewed contract documents , invoices , and related documents to determine the cumulative cost of final evaluations conducted by an outside evaluator .

we defined the cumulative costs as the cost of conducting the final evaluation and any related activities that informed the final evaluation , such as a midterm or baseline evaluation .

in some cases , we were unable to determine an evaluation's precise cost if it was procured under a contract that covered additional activities .

in these cases , we approximated the cost on the basis of estimates provided by the agency or contractor .

we did not determine the cost of evaluations prepared by agency staff because agencies did not separately track these costs .

to identify factors that affect the costs of foreign aid evaluations , we analyzed the cost of mcc , state , usda , and usaid evaluations in relation to the data we collected on these evaluations' quality and other characteristics .

we report only limited data on the cost of dod's gt&e and hhs's pepfar evaluations because the evaluation contracts or implementing partner agreements did not separately track evaluation costs , and we concluded that the available estimates were too limited to include in our statistical analysis .

to address our third objective , we identified leading practices for the dissemination of evaluation findings .

we identified these leading practices using federal guidance that encourages the timely public posting of agency information on a searchable website , as well as plans and additional efforts to actively disseminate agency information .

in addition to the federal guidance , we also used the american evaluation association's ( aea ) an evaluation roadmap for a more effective government ( aea roadmap ) , as well as some other nonfederal sources that also cite timely public posting , dissemination planning , and additional active efforts to disseminate results as important communication tools for evaluations .

we then reviewed each agency's evaluation policies to identify their requirements for dissemination of evaluation reports and interviewed cognizant officials .

we compared agency policies and practices with the leading practices we identified .

we reviewed agency websites to determine whether evaluation reports were posted online and examined each agency website to determine whether it provided a search engine that could be used to locate evaluations .

see appendix i for a more detailed discussion of our scope and methodology .

we conducted this performance audit from october 2015 to march 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the six agencies whose evaluations we reviewed focus on foreign assistance to varying degrees .

dod , hhs , and usda provide foreign assistance as part of their larger portfolios of programs , while mcc , state , and usaid focus exclusively on foreign affairs or foreign assistance .

dod's gt&e program provides training , equipment , and small - scale military construction activities to partner nations to build their capacity and enable them to conduct counterterrorism operations or to support ongoing allied or coalition military or stability operations that benefit the national security interests of the united states .

hhs's cdc implements a portion of the president's emergency plan for aids relief ( pepfar ) programs under the direction of state's office of the u.s .

global aids coordinator and health diplomacy .

mcc , a u.s. government corporation , provides aid to developing countries that have demonstrated a commitment to ruling justly , encouraging economic freedom , and investing in people .

mcc supplies this assistance to eligible countries primarily through 5-year compacts with the goal of reducing poverty by stimulating economic growth .

state , the lead u.s. foreign affairs agency , implements programs that provide , for example , counternarcotics assistance ; refugee assistance ; and support for democracy , governance , and human rights .

usaid , the lead u.s. foreign assistance agency , implements programs intended to both further america's interests and improve lives in the developing world .

usaid's broad portfolio includes programs that address democracy and human rights , water and sanitation , food security , education , poverty , the environment , global health , and other areas .

usda's foreign agricultural service ( fas ) administers two nonemergency food aid programs: ( 1 ) the food for progress program supports agricultural value chain development , expanding revenue and production capacity , and increasing incomes in food - insecure countries ; ( 2 ) the mcgovern - dole international food for education and child nutrition program supports education and nutrition for schoolchildren , particularly girls , expectant mothers , and infants .

each of the six agencies has adopted evaluation guidance for the programs included in our review .

dod's november 2012 section 1206 assessment handbook serves as a guide to evaluation planners and implementers for conducting evaluations of dod's gt&e programs as required by federal law .

the fiscal year 2012 national defense authorization act ( ndaa ) required dod , no later than 90 days after the end of each fiscal year , to submit to congress a report including an assessment of the effectiveness of gt&e programs conducted that fiscal year in building the capacity of the recipient foreign country .

the fiscal year 2015 ndaa maintained this requirement through 2020 .

dod did not have agency - wide evaluation guidance for security cooperation at the time we performed our review but issued such guidance in january 2017 .

for pepfar programs , including those implemented by hhs , state's office of the u.s .

global aids coordinator and health diplomacy issued the pepfar evaluation standards of practice in january 2014 and an updated version ( version 2 ) in september 2015 .

mcc's may 2012 policy for monitoring and evaluation of compacts and threshold programs requires that compact m&e plans identify and describe the evaluations that will be conducted , key evaluation questions and methodologies , and data collection strategies .

state issued its current evaluation policy and an additional guidance document for evaluations in january 2015 and issued a revised and updated version of the guidance in january 2016 .

usaid lays out its evaluation policies in its automated directives system ( ads ) .

usaid issued a fully revised ads 201 , addressing evaluation guidance , planning , and implementation , in september 2016 .

usda's fas evaluations are guided by its may 2013 monitoring and evaluation policy , which requires both interim and final program evaluations .

with the exception of hhs , the agencies we selected for our review generally rely on outside contractors to conduct evaluations .

dod , mcc , state , and usaid directly contract for third - party evaluation services .

the hhs pepfar evaluations we reviewed were prepared ( 1 ) by cdc staff using existing program data ; ( 2 ) by an implementing partner as part of the partner's cooperative agreement ; or ( 3 ) in one instance , under a separate agreement .

usda implementing partners procured the usda evaluations whose costs we reviewed .

the six agencies track evaluation costs to varying extents .

dod , mcc , and state procured the evaluations we reviewed through centrally managed contracts , and cost information for these evaluations was available through the program or agency evaluation office .

hhs's pepfar , usda , and usaid evaluations were often procured and managed at the country , mission , or implementing partner level .

cost information was not centrally available and could be obtained only from each mission or implementing partner .

foreign assistance evaluations may vary in type , timing , and method .

two common types of evaluation are the following: performance evaluations assess the extent to which a program is operating as was intended or the extent to which it achieves its outcome - oriented objectives .

performance evaluations often judge program effectiveness against criteria , such as progress against baselines , whether program goals were met , or whether expected targets were met .

net impact evaluations assess the net effect of a program by comparing program outcomes with an estimate of what would have happened in the program's absence .

net impact evaluations use a variety of experimental and quasi - experimental designs , including randomized methods in which participants are assigned to separate control or treatment groups to isolate the program's effect .

net impact evaluations have more complex methodologies than the other evaluation types .

agencies may conduct evaluations during or after the completion of a program .

interim or midterm evaluations are conducted while a program is in progress , and final evaluations are conducted after the program ends .

baseline evaluations are also sometimes conducted before a program begins as a basis for determining any effects of the program .

evaluations may use one or more methods to produce their results .

for example , evaluations may use random or nonrandom sampling from the target population to select cases for inclusion in the study .

evaluations may also use one or more methods to collect data on the chosen indicators and measures — for example , structured or unstructured interviews , focus groups , surveys , direct observations , or collection and analysis of existing data .

each of these methods has potential benefits and limitations that an evaluator must consider in assessing the evaluation's evidence as a basis for its conclusions and recommendations .

overall , about three quarters of all 2015 foreign aid evaluations from the six agencies we reviewed generally or partially met the quality criteria we identified .

the remaining evaluations did not meet one or more of these criteria or provided insufficient information .

while we generally found that evaluations met quality criteria related to design , implementation , and conclusions , we more often found limitations in implementation — including sampling methods , data collection , and analysis .

in addition , we found that the independence of evaluators was not always clearly evident .

while the quality of evaluations varied by agency , we identified shortcomings at all six of the selected agencies that could limit evaluation reliability and usefulness .

by reviewing policies of federal agencies , international organizations , and evaluation organizations , and our prior reporting , we identified common characteristics of high - quality evaluations , from which we developed eight criteria for assessing evaluation quality .

these quality criteria are associated with the ( 1 ) design , ( 2 ) implementation , and ( 3 ) conclusions of an evaluation , as follows .

 ( see app .

i for a full description of how we developed our evaluation criteria. ) .

evaluation questions are aligned with program goals .

performance indicators are appropriate for measuring progress against program goals .

design is appropriate for answering the evaluation questions .

target population and sampling method are appropriate , given the scope and nature of the evaluation questions .

data collection is appropriate for answering the evaluation questions .

data analysis is appropriate to answer the evaluation questions .

conclusions are supported by the available evidence .

recommendations and lessons learned are justified by the available evidence .

based on an assessment of agency evaluations against these criteria , we rated 73 percent of all evaluations as high quality ( 26 percent ) or acceptable quality ( 47 percent ) , because they generally or partially met all applicable quality criteria .

we rated the remaining 27 percent as lower quality because they either did not meet or did not provide sufficient information related to at least one applicable criterion .

these evaluations may not provide sufficiently reliable evidence to inform agency program and budget decisions .

overall , we encountered more instances when evaluations did not provide sufficient information about a certain criterion than instances when evaluations did not meet a quality criterion at all .

table 1 summarizes our observations about the quality of evaluations at the six selected agencies in our review .

the quality of evaluations varied by the type of quality criterion we applied .

as figure 1 shows , many evaluations generally met the criteria related to the appropriateness of evaluation design , implementation , and conclusions .

however , overall , evaluations generally met fewer criteria related to implementation , reflecting limitations in the way evidence was collected or analyzed .

a relatively high percentage of evaluations generally met each of the criteria we used to assess the alignment of the study questions with the program goals , the appropriateness of the evaluation design for the study questions , and the use of indicators for measuring progress .

alignment of questions with program goals .

evaluation questions generally aligned with one or more of the evaluated program's goals in more than 90 percent of the evaluations .

thus , evaluations were designed to provide useful information about program results .

appropriate evaluation design for the study questions .

about 80 percent of the evaluations used a design that was generally appropriate for the study questions , and the remainder of the designs was at least partially appropriate .

appropriate use of indicators .

indicators for measuring progress were generally appropriate in about 80 percent of the evaluations .

as a result , successes or failures identified by these evaluations are likely to be directly relevant to assessing achievement of the evaluated programs' goals .

we found more limitations in the implementation of evaluations than in their design .

on average , about 60 percent of evaluations generally met each of the criteria related to this aspect of quality — sampling , data collection , and analysis .

limitations we identified revealed that conducting evaluations overseas can pose challenges for evaluators .

for example , travel to remote areas with safety and security concerns may limit an evaluator's ability to conduct appropriate sampling and collect primary data for the study .

also , insufficient local resources to implement certain methodologies , such as implementing survey instruments , or a lack of local administrative data on the study population may constitute additional obstacles to sampling and data collection .

about 40 percent of evaluations had limitations in , or provided insufficient information about , their sampling methodology .

if an evaluation does not clearly describe how sampling was conducted , it may raise questions about the quality of the evidence , including concerns regarding selection bias of respondents , sufficiency of the sample size for the findings , and the relevance of the evaluation's findings and conclusions for the entire study population .

sampling methods were particularly problematic or unclear in evaluations that used nonrandom sampling .

for evaluations that relied primarily or exclusively on testimonial evidence , the method for selecting participants for interviews or focus group discussions was sometimes inappropriate or unclear .

for example , one evaluation we reviewed relied largely on interviews but did not describe the process used for selecting participants , and it indicated that the available list of potential participants was incomplete and inaccurate .

several evaluations provided insufficient information about the target population , other than identifying them as program beneficiaries , and included no discussion of how participants were selected for interviews , focus groups , or surveys .

limitations in data collection methods about 40 percent of the evaluations had limitations in , or provided insufficient information about , their data collection methods .

we identified a number of deficiencies in the data collection process , including a lack of documentation of data collection instruments ( dci ) , such as questionnaires or structured interview protocols .

in cases where evidence was gathered through a dci , some evaluations were unclear about how the instrument was designed and administered .

for example , an evaluation of a program intended to increase access to mobile technologies and improve mothers' health used a survey to gather data .

however , the evaluation did not provide sufficient details about the survey , such as the questionnaire itself or the sampling strategy , for the reader to be able to determine the validity and reliability of the data collected .

in addition , about half of the evaluations did not collect baseline data from which to calculate change after the program was implemented , and about half generally did not set targets , which makes an assessment of progress toward meeting the goals of the program difficult .

for example , an evaluation of two community training programs in an asian country identified the study question but did not collect baseline data and did not establish targets .

although some baseline data may have been gathered by the implementing partner , such information was not used for comparative purposes , making it impossible to assess the net effects of the program .

further , an estimated 60 percent of the evaluations used data collection procedures that only partially ensured the reliability of the data , or there was not sufficient information to assess data reliability .

for example , an evaluation of a small business program in latin america acknowledged numerous data quality problems , including serious attrition among the group used as a comparison group to program participants .

such limitations raise questions about the strength of the conclusions .

about 40 percent of evaluations did not demonstrate that they had conducted appropriate data analysis .

these evaluations often did not specify the analysis methods for each question , such as how interview responses were analyzed .

for example , an evaluation of a program serving women living with hiv analyzed data through a content analysis but did not clearly explain the categories created for the analysis or the numbers of individual responses that fell into each category .

the lack of clarity in the analysis makes it difficult for the reader to determine whether the findings from this program have broader applicability .

several evaluations relied on focus group discussions but analyzed and reported the percentages of informants expressing the stated views in ways that did not appropriately account for the potential influence of other focus group members on informants' responses .

evaluations that used some quantitative data analysis also had certain shortcomings .

for example , an evaluation reported a statistically significant change from baseline but did not include a discussion of the type of statistical test that supported this result .

finally , while about 90 percent of the evaluations assessed processes such as program implementation , about half of those evaluations did not establish any criteria , such as evaluation plans , budgets , timeframes , and targets .

without such benchmarks , it is difficult to define what constituted success for the evaluated program .

the majority of evaluations generally met each of our criteria related to conclusions .

these evaluations considered the strengths and limitations of the available evidence from the evaluation's design and implementation and included conclusions that were generally supported and recommendations that were generally justified .

conclusions supported by the available evidence .

about 70 percent of the evaluations had conclusions that were generally supported by the evidence , and nearly all of the evaluations had conclusions that were partially supported .

this indicates that these evaluations did not reach beyond what was supported by the evidence and justified given the limitations .

recommendations and lessons learned justified by the available evidence .

about 75 percent of evaluations with recommendations included evidence that generally supported the recommendations , and all evaluations with recommendations included evidence that at least partially supported them .

this indicates that the collected evidence justified the follow - up steps the evaluations recommended .

our analysis found that , in addition to meeting the eight criteria to varying extents , the evaluations did not always provide documentation of the evaluator's independence and whether there were any potential conflicts of interest .

in instances where an evaluation was not conducted by a third party , a statement about conflicts of interest may be especially important to forestall any potential concerns about the evaluator's impartiality .

in all , about 80 percent of the agency evaluations documented that they were conducted by third - party evaluators , while about 13 percent were not conducted by a third - party evaluator , and another 6 percent did not indicate whether they were performed by a third - party evaluator .

about 70 percent of hhs evaluations , about 30 percent of state evaluations , and about 40 percent of usaid evaluations included a conflict - of - interest statement , while no dod , usda , or mcc evaluations included such a statement .

if an evaluation does not address the independence of the evaluation organization and of individual evaluators , questions could arise about the objectivity and reliability of the evaluation's findings .

as table 2 shows , the extent to which the evaluations met each quality criterion varied among the six agencies we reviewed .

while our assessments revealed strengths in each agency's evaluations , all six agencies' evaluations also showed shortcomings in quality that could limit the agencies' ability to ensure the effectiveness of foreign assistance based on evaluation results .

each applicable quality criterion was generally met by a majority of hhs , mcc , and usaid evaluations .

however , evaluations for all three agencies scored generally lower on the criteria related to evaluation implementation — that is , the appropriateness of the target population and sampling , data collection , and data analysis .

while most hhs , mcc , and usaid evaluations used generally appropriate sampling methods , our analysis showed that overall about half of the evaluations did not use appropriate nonrandom sampling techniques .

in addition , we estimate that overall only about half of the three agencies' evaluations generally used data collection methods that ensured data reliability , and only about 10 to 20 percent of usaid and hhs evaluations generally specified the key assumptions of the data analysis methods used .

dod's gt&e program evaluations' study questions met the first quality criterion — aligning with the program's goals — but overall did not generally meet the other criteria .

for example , we identified weaknesses in the implementation of the evaluations' designs in terms of target population and sampling , data collection , and analysis .

in particular , some evaluations did not describe the target population and did not discuss the methods the evaluators used for their selection of the equipment items they observed or the persons they interviewed .

in addition , we found limited discussion about how the data were summarized and analyzed , incomplete baseline metrics , and a lack of targets .

without systematic selection of equipment to observe or respondents to interview , it is difficult to know whether the selections were justifiable and selected in a way that supports the intervention's objectives and the conclusions drawn .

because of these implementation weaknesses as well as a lack of discussion of study limitations , we determined that the dod gt&e program evaluations provided only partial support for their conclusions .

state and usda evaluations each met more than one quality criterion about half the time or less .

about half or fewer of state evaluations generally met four criteria: appropriate indicators , appropriate target population and sampling , appropriate data collection , and appropriate data analysis .

regarding the appropriateness of chosen indicators , less than a third of state evaluations had indicators with baselines or established criteria such as plans or budgets , and almost none of the evaluations had indicators with targets against which progress could be assessed .

in addition , about 80 percent of state evaluations used a data collection process that did not generally ensure the reliability of the data , and about half of the state evaluations generally did not specify data analysis methods for each question and the key assumptions used in the analysis .

state officials noted that their programs are often implemented rapidly in response to specific events , making it difficult to design an evaluation for the program and to gather baseline data .

we estimate that overall about half of usda evaluations had generally appropriate target population and sampling , generally appropriate data analysis , or support for conclusions .

most foreign aid evaluations we reviewed cost less than $200,000 , but costs ranged widely and varied by agency and type .

we identified costs for mcc , state , usaid , and usda final evaluations but could not obtain specific cost information for dod's gt&e and hhs's pepfar evaluations because these programs used procurement methods for their evaluations that did not separately track evaluation costs .

evaluation costs were related to the evaluation's methodology and location , and higher - cost evaluations tended to meet more evaluation quality criteria , though we also identified lower - cost evaluations that met all quality criteria .

costs for the majority of the foreign aid evaluations whose costs we reviewed were less than $200,000 , but the costs ranged widely and varied by type of evaluation and agency .

of the 76 mcc , state , usaid , and usda evaluations , 48 cost less than $200,000 while 6 cost more than $900,000 .

the costs of net impact evaluations ranged from approximately $36,100 to $2.2 million , with a median of $117,500 .

the costs of performance evaluations ranged from $9,600 to $902,100 , with a median cost of $169,600 .

while the median cost was higher for the performance evaluations than the net impact evaluations , the net impact evaluations had a higher average cost than the performance evaluations ; five of the six evaluations that cost more than $900,000 were net impact evaluations .

net impact evaluations that used randomized controlled trials were the most expensive evaluations in our sample , with a median cost of $926,600 compared with $154,700 for all other evaluations .

figure 2 shows the range of costs for the net impact and performance evaluations whose costs we reviewed .

of the four agencies' evaluations whose costs we reviewed , mcc's evaluations had the highest median cost , at $268,900 , and usda's evaluations had the lowest median cost , at $87,900 ( see table 3 ) .

seven of 12 mcc evaluations cost over $200,000 , including 3 net impact evaluations that cost over $900,000 .

in contrast , 8 of the 10 usda evaluations were performance evaluations that cost less than $200,000 .

most state evaluations were performance evaluations , which were generally more expensive than performance evaluations at the other agencies .

usaid costs for impact and performance evaluations both ranged widely , and usaid's net impact evaluations had a lower median cost than its performance evaluations .

costs for dod's gt&e evaluations and hhs's pepfar evaluations were not specifically identifiable because they were not separately tracked by the agencies , contractors , or implementing partners .

the contract for the dod gt&e evaluations included many activities in addition to the evaluations and was not structured to show the cost of each activity .

additionally , according to dod officials , neither dod nor the contractor separately tracked the evaluation costs in their financial records .

however , on the basis of the contractor's estimate of contract time spent on 20 gt&e evaluations in fiscal years 2012 through 2015 ( including the four evaluations in our sample ) , we estimated the total cost of these evaluations at approximately $1.1 million — an average of approximately $56,300 per evaluation .

according to dod officials , actual costs likely varied across evaluations due to differences in the size of the evaluation teams , the foreign country in which the evaluation took place , and the amount of time each team spent abroad .

hhs's pepfar programs typically conduct evaluations as part of larger cooperative agreements that are not structured to specify evaluation costs .

we reviewed cost information for 10 hhs evaluations .

we identified a specific cost — $15,400 — for only one evaluation , which was conducted under a cooperative agreement specifically for the evaluation ; the remaining nine evaluations were conducted as part of cooperative agreements that did not specify evaluation costs .

using budget documents and informed estimates from cdc staff and implementing partners , we estimated that the costs of these nine evaluations ranged from $25,100 to $356,200 .

additionally , 11 of the 34 hhs evaluations in our sample had no external costs because they were conducted solely by hhs staff using existing datasets .

cdc officials stated that cdc intends to track evaluation costs in the future .

for example , according to hhs , upon continuation of a cooperative agreement , an implementing partner will be required to report on progress on its evaluation and performance monitoring plan , as well as on expenditures to date and plans and budgets for the following year .

our analysis found that data collection methods , frequency of data collection , evaluation duration , and evaluation location all affect evaluations' cost .

for example , evaluations that collected data by surveying program beneficiaries had a median cost of $202,500 — approximately $74,000 higher than the median cost of those that did not — and evaluations that collected data repeatedly over time had a median cost of $194,500 — approximately $44,500 higher than those that did not .

evaluations that took longer to perform also tended to be more expensive .

other factors that might influence costs include unstable locations and evaluations conducted at multiple sites .

for example , a performance evaluation conducted in an unstable country cost $365,700 for 78 days of work , and a performance evaluation that conducted data collection in 12 countries cost $902,100 .

in addition , conducting an evaluation in multiple sites within the same country might increase evaluation costs .

for example , a performance evaluation conducted in eight cities and seven states in india cost $407,500 , including the cost of a midterm evaluation that was also conducted in multiple cities .

these costs greatly exceeded the median costs for all evaluations .

our analysis found that high - quality evaluations tend to be more expensive , but well - designed lower - cost evaluations also met the criteria we identified for a high - quality evaluation .

overall , as table 4 shows , the median cost of high - quality evaluations ( i.e. , evaluations that met all quality criteria ) was $137,800 more than the median cost of acceptable - quality evaluations ( i.e. , evaluations that partially or generally met all quality criteria ) and $208,600 more than the median cost of lower - quality evaluations ( i.e. , evaluations that did not meet , or provided insufficient information for , one or more quality criteria ) .

high - quality evaluations also tended to include factors associated with higher evaluation costs .

for example , the most expensive evaluation in our sample cost $2.2 million and generally met all quality criteria .

this net impact evaluation assessed multiple civil society and governance programs in an african country using different methodologies , including a randomized controlled trial , over 4 years and conducted two rounds of surveys of program beneficiaries .

another high - quality evaluation cost $1.4 million and took almost 4 years to complete ; this evaluation conducted three surveys of program beneficiaries and used a quasi - experimental methodology to assess the net impacts of energy - efficient stoves in asia .

however , some lower - cost evaluations also met all of the quality criteria .

of the 15 high - quality evaluations for which we identified costs , 4 cost less than $150,000 .

we assessed dod's , hhs's , mcc's , state's , usaid's , and usda's use of six dissemination practices that federal , aea , and other guidance indicate agencies should generally use to ensure effective dissemination of evaluations .

we found that the agencies varied in their performance of these practices for the fiscal year 2015 evaluations we reviewed ( see table 5 ) .

all except usda generally made nonsensitive evaluations publicly available online .

these nonsensitive evaluations could generally be located with the agencies' website search engines .

however , some agencies' evaluations were not posted in a timely manner .

each of the agencies posted its sensitive evaluations internally for access by internal users .

only usaid included dissemination plans in most nonsensitive evaluations to help ensure their dissemination to potential users of the evaluation , but most of the other agencies now require such plans to be prepared for future evaluations .

in addition to publicly posting the report , all of the agencies used other means to actively disseminate evaluation findings .

following these practices can help agencies ensure that their evaluation reports are accessible , timely , and useful to decision makers and other stakeholders .

every agency with nonsensitive evaluations requires public , online posting of nonsensitive evaluation documents , and all except usda publicly posted all of the nonsensitive evaluations we reviewed on publicly accessible websites .

making evaluation reports publically available on their websites helps agencies share evaluation findings with partners , program beneficiaries , and the wider public and facilitates the incorporation of evaluation findings into program management decisions .

we examined the agencies' dissemination of 193 evaluations .

the agencies did not require 22 evaluations to be publicly posted due to their sensitivity — all 4 dod evaluations as well as 17 evaluations from state and 1 from usaid .

of the remaining 171 nonsensitive evaluations , we found that more than three - quarters ( 133 ) were publicly posted .

usda did not publicly post any of its 38 nonsensitive evaluations .

according to usda officials , the department is in the process of developing procedures for making these nonsensitive evaluations public , which would include reviewing the documents to ensure that they did not contain , for example , personally identifiable or proprietary information .

without posting all nonsensitive evaluations online , agencies cannot ensure that the evaluations' findings reach intended audiences and are available to inform future program design or budget decisions .

most of the nonsensitive , publicly posted evaluations we reviewed could be located with a search engine on the agencies' websites .

providing a search engine that potential evaluation users can employ to find the evaluation reports ensures that users can locate the information they seek , in a format that matches their expectations .

websites at three of the four agencies with publicly posted evaluations — mcc , state , and usaid — have search engines that enable users to find each specific evaluation .

the pepfar website , which hosts evaluations of pepfar programs implemented by cdc and other agencies , has these evaluations listed in a spreadsheet locatable on the site .

many evaluations of pepfar programs implemented by cdc can also be found using a search engine at a separate website , called “cdc stacks.” according to cdc , almost all of the evaluations that we reviewed were posted on the cdc stacks website .

the agency reported that it is in the process of adding the remaining cdc evaluations from fiscal year 2015 to this website .

some of the nonsensitive evaluations we reviewed were not posted on the agencies' websites within required timeframes .

making evaluation reports accessible in a timely manner ensures that interested parties can access the findings of these evaluations in time to incorporate them into program management decisions .

mcc and hhs did not post some evaluations within the timeframes they require , limiting stakeholders' ability to make optimal use of the evaluation findings .

we found that mcc did not post 10 of its 16 evaluations , as mcc requires , within 6 months after mcc received them , and it did not post 8 of these 10 evaluations until a year or more after mcc received them .

according to mcc officials , the agency's internal evaluation quality review process for evaluations , in which the agency reviews the document before releasing it to the public , has been a major factor in these delays .

mcc officials reported that for some of the evaluations — for instance , those written in a language other than english — this process took significantly longer than usual .

hhs did not post 11 hhs evaluations in the timeframe required by the agency .

it did not post 6 of these 11 evaluations online within 90 days as required by pepfar .

pepfar guidance requires that evaluations be posted within 90 days of completion , while hhs requires that evaluations be publicly posted within a year of their completion .

one hhs official stated that the delay in the posting of these six evaluations was due to the conflicting policies .

however , the remaining five evaluations were also not posted within the year as required by hhs / cdc .

these five evaluations have since been posted online .

since evaluated conditions may change over time , not posting evaluations online within the required timeframe limits internal and external stakeholders' access to current , actionable information .

in comments on a draft of this report , cdc noted that , as of december 2016 , cdc is providing guidance that all evaluations be posted online within 90 days , as required by pepfar .

cdc published this guidance in january 2017 .

of the three agencies with sensitive evaluations — dod , state , and usaid — all have websites to make these evaluations available to internal stakeholders .

while sensitive evaluations are not required to be made available to the public on an agency's website , disseminating sensitive evaluation findings to the appropriate audience will facilitate their use .

dod , state , and usaid all reported that they have internal websites that can be used to post sensitive evaluations .

in addition , state updated its policy for 2015 to require that state officials post a nonsensitive summary of sensitive evaluations on state's public website .

while usda does not currently publicly post its evaluations , usda reported that it makes these evaluations internally available through its grant management system .

hhs and mcc did not have sensitive evaluations in fiscal year 2015 .

usaid requires the development of dissemination plans and included evidence of such planning in the majority of the evaluations we reviewed , and all of the other agencies except state now require such plans for nonsensitive evaluations .

dissemination planning identifies potential users of an evaluation and describes an approach to providing users with the evaluation results .

such planning can help agencies ensure that evaluation reports are disseminated effectively among the six agencies , only usaid required the development of dissemination plans for fiscal year 2015 evaluations and included evidence of such planning in the majority of the evaluations whose dissemination we reviewed .

of the 62 usaid evaluations , 44 included evidence that dissemination planning had been completed .

hhs , mcc , state , and usda did not require dissemination plans for their evaluations completed in fiscal year 2015 .

agency officials at hhs and mcc provided evidence that dissemination planning took place for at least one of their respective evaluations we reviewed , but this dissemination planning was not required by agency policy , and therefore this planning was ad hoc .

dod plans for evaluation dissemination by identifying potential users of the evaluation and sending e - mails to these internal and congressional stakeholders after the evaluations are completed .

hhs , mcc , and usda guidance now require dissemination plans for future evaluations .

state officials reported that , as of november 2016 , state was planning to revise its policy to require the use of dissemination plans for evaluations but had not instituted this requirement .

without dissemination planning , state cannot ensure that its evaluations are disseminated as effectively as possible to potential users .

in addition to posting evaluations online , each of the six agencies reported disseminating evaluation findings through other means .

our prior work has shown that taking such additional steps to actively disseminate evaluation reports — for example , briefing stakeholders on evaluation findings and distributing the evaluations to interested stakeholders via e - mail — facilitates dissemination of evaluation report findings and encourages their use .

agency officials reported using various means besides web posting to disseminate evaluation findings .

for example , officials at all six agencies reported using briefings to share evaluation findings with various stakeholders within and outside of the agency .

additionally , hhs , state , mcc , and usaid officials reported that they shared evaluation results with interested parties at various professional conferences .

usaid officials stated that the agency also disseminates evaluation findings by posting its evaluations on partner websites , creating video companions to evaluation reports to provide to stakeholders , and posting syntheses of evaluation findings on the agency's website .

foreign assistance evaluations can be challenging to implement , but they are an essential tool for guiding agency decision making and allocation of resources .

agencies' foreign assistance evaluations assess a wide variety of programs around the world , using many different designs and methodologies , and the wide range of evaluation costs reflects this diverse context .

however , regardless of the location , design , or cost , an evaluation should provide sufficient and reliable evidence to support its findings .

a high - quality evaluation helps agencies and stakeholders identify successful programs to expand or pitfalls to avoid .

evaluations that do not meet all quality criteria that we identified may not provide sufficiently reliable evidence to inform these decisions .

in addition , for evaluations to inform decision making , stakeholders must be able to find them .

while foreign assistance agencies have generally made their evaluations available online in a timely manner , several agencies can take additional steps to ensure that stakeholders have improved access to these evaluations to make better - informed decisions about future program design and implementation .

a growing body of high - quality , broadly disseminated evaluations can help the united states continuously improve its foreign assistance programs and thereby support democracy , enhance security , reduce poverty and suffering , and achieve other u.s. foreign policy goals .

to improve the reliability and usefulness of program evaluations for agency program and budget decisions , we recommend that the chief executive officer of mcc , the administrator of usaid , the secretary of agriculture , the secretary of defense , the secretary of state , and the secretary of health and human services ( in cooperation with state's office of the u.s .

global aids coordinator and health diplomacy ) each develop a plan for improving the quality of evaluations for the programs included in our review , focusing on areas where our analysis has shown the largest areas for potential improvement .

to better ensure that the evaluation findings reach their intended audiences and are available to facilitate incorporating lessons learned into future program design or budget decisions , we recommend that the secretary of health and human services direct the centers for disease control and prevention to update its guidance and practices on the posting of evaluations to require pepfar evaluations to be posted within the timeframe required by pepfar guidance ; the chief executive officer of mcc adjust mcc evaluation practices to make evaluation reports available within the timeframe required by mcc guidance ; the secretary of state amend state's evaluation policy to require the completion of dissemination plans for all agency evaluations ; and the secretary of agriculture implement guidance and procedures for making fas evaluations available online and searchable on a single website that can be accessed by the general public .

we provided a draft of this report to dod , state , hhs , mcc , usaid and usda for review and comment .

dod , state , hhs , mcc , usaid , and usda provided official comments , which are reproduced in appendixes iii through viii with , where relevant , our responses .

dod , hhs , and usaid also provided technical comments , which we incorporated as appropriate .

the following summarizes dod , state's , hhs's , mcc's , usaid's , and usda's official comments and our responses .

dod stated that it partially concurred with our recommendation , noting that in many cases , certain methodologies are not well suited for security assistance evaluation .

dod observed that , for example , it would be unethical to establish randomized control groups for security assistance evaluations and that foreign military organizations may be unwilling to provide dod significant access to some military units solely for the purpose of the evaluation .

we recognize that certain methodologies are not appropriate in every context , and we did not advocate the use of randomized control groups in the dod evaluations we reviewed .

our main concerns about the dod evaluations focused on implementation of the methods used .

in particular , we found limitations in sampling methods including descriptions of the target population , data collection methods , and data analysis .

we adjusted pertinent wording in our report to clarify these points .

state concurred with our recommendations and noted that its forthcoming program design and performance management policy for programs , projects , and processes , recently published program design and performance management toolkit , and updated policy guidance will constitute a plan for improvement .

we will monitor the implementation of this plan to verify that state takes appropriate steps to address our recommendation .

hhs concurred with our recommendation that it update guidance and practices on the posting of pepfar evaluations and stated that cdc guidance now requires evaluation reports to be posted on a publically accessible website within 90 days of the evaluation's completion .

hhs did not comment on our recommendation that it develop a plan for improving the quality of evaluations .

mcc stated that it welcomed our findings and recommendations for improvement but noted that it could not agree or disagree with our quality assessments because we did not provide data on our determinations for individual evaluations .

in response to our observation that mcc evaluations did not contain conflict - of - interest statements , mcc noted that it has required independent third - party evaluation of all its projects since 2009 and that , in 2013 , it standardized the language in its evaluation contracts to explicitly establish the independent role of evaluators .

while these are positive steps , we believe that including in mcc's published evaluations explicit statements about the evaluators' independence and any potential conflicts of interest would bolster the evaluations' credibility and usefulness .

with regard to the timeliness of public access to its evaluations , mcc indicated that when it established its internal review process for evaluations in 2013 , it did not anticipate the length of time that would be required to finalize evaluation reports .

mcc noted that its forthcoming revised policy on monitoring and evaluation states that “mcc expects to make each interim and final evaluation report publicly available as soon as practical after receiving the draft report.” however , the revised policy does not establish a target time frame for completing internal reviews of the reports .

establishing such a time frame could help mcc ensure that evaluation reports are published in a timely fashion that maximizes their usefulness .

usaid stated that it has established a plan to improve the quality of evaluations , including an update and clarification of the requirements and quality standards for evaluations .

usaid also stated that it plans to provide additional training and other capacity - building efforts to help ensure that staff have the necessary skills to manage evaluations .

we will monitor implementation of this plan to verify that usaid takes appropriate steps to address our recommendation .

usda agreed with our recommendations .

to address the recommendations , usda stated that it would update its guidance on reviewing evaluation terms of reference to include a section on quality that specifically focuses on the areas where the gao analysis has shown the largest areas for potential improvement .

usda further stated that fas will continue its current efforts to make nonsensitive evaluations publicly available online and will make them searchable as well .

we are sending copies of this report to the appropriate congressional committees and to the secretaries of agriculture , defense , state , and health and human services ; the chief executive officer of the millennium challenge corporation ; and the administrator of the u.s. agency for international development .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have questions about this report , please contact me at ( 202 ) 512-6991 , or farbj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix ix .

in response to congressional requests , we examined ( 1 ) the extent to which foreign assistance program evaluations met key evaluation quality criteria ; ( 2 ) the costs of the evaluations , as well as factors that affect these costs ; and ( 3 ) the extent to which the agencies ensure the dissemination of evaluation reports within the agency and to the public .

to address our objectives , we identified the six major agencies administering the most foreign assistance on the basis of obligations reported to the u.s. agency for international development's ( usaid ) u.s. overseas loans and grants database for fiscal years 2008 through 2012 .

the six agencies we identified are usaid , the department of state ( state ) , the millennium challenge corporation ( mcc ) , the department of health and human services ( hhs ) , the u.s. department of agriculture ( usda ) and the department of defense ( dod ) .

for the three agencies that are not focused exclusively on foreign aid or foreign affairs ( hhs , usda , and dod ) , we limited our scope to selected programs .

for hhs and usda , we selected programs that account for the vast majority of foreign assistance program dollars that the agency implemented .

at hhs we examined evaluations of the president's emergency plan for aids relief ( pepfar ) programs implemented by hhs's centers for disease control and prevention ( cdc ) .

at usda we examined evaluations for the food for progress and mcgovern - dole food assistance programs , implemented by the foreign agricultural service ( fas ) .

at dod we examined evaluations prepared for the global train and equip ( gt&e ) program .

while our previous review of agency evaluation policies did not identify dod - wide evaluation policies , we did identify gt&e as having relevant policies to guide its evaluations .

to identify evaluations completed in fiscal year 2015 , the most recently completed fiscal year as we undertook our review , we requested that each agency provide a list of all foreign aid evaluation reports completed in that year .

we did not separately review agency files to identify if agencies had additional evaluations beyond those listed in the registries .

to assess the reliability of the agency evaluation lists , we reviewed the documents provided to ensure that each was a completed evaluation and to confirm that the date of the document fell within our specified timeframe .

if necessary , we followed up with agency officials to clarify the date or status of the document .

based on their responses , we removed documents that were not evaluations or fell outside of our timeframe .

we also did not review evaluation reports that were not written in english .

we determined that the data in the evaluation lists were sufficiently reliable for the purposes of this engagement .

in all , we identified a study population of 361 evaluations: 4 dod evaluations , 51 hhs evaluations , 17 mcc evaluations , 28 state evaluations , 221 usaid evaluations , and 40 usda evaluations .

we examined the evaluations themselves and any appendices that the agency provided which were directly referred to in the evaluations .

we did not consider evaluation plans and protocols , underlying documents and other work papers as evidence that the planned design was implemented .

similarly , we did not consider contracts with third - party evaluators or evaluation organizations as evidence that the evaluator had maintained independence .

instead we required statements in the reports or methodological appendices that steps and procedures were actually taken and that no threats to independence have been identified .

from the study population of fiscal year 2015 evaluations , we reviewed all dod , mcc , and state evaluations ; all usaid net impact evaluations ; and a sample of hhs , usda , and usaid performance evaluations .

we randomly selected a probability sample from the study population of hhs , usda , and usaid performance evaluations .

with this probability sample , each member of the study population had a nonzero probability of being included , and that probability could be computed for any member .

for usaid , we included all net impact evaluations in the sample because net impact evaluations constituted less than 20 percent of all the evaluations provided , and if we had not included them all , we would not have been able to comment on this type of evaluation .

based on the review of the evaluation documents after the initial screening , an additional 16 evaluations were found not to be within our scope , and we substituted for these evaluations when possible .

for example , we excluded documents that did not evaluate a specific program , were monitoring or grant reports , or were plans for an evaluation rather than an evaluation report .

we included two substitute hhs and two substitute usda evaluations to replace those that were found to not be in scope and also reviewed additional usda evaluations .

however , because we had initially included all mcc , state , and usaid net impact evaluations , there were no additional evaluations available to substitute if those were excluded .

the original sample and the final respondents across the six agencies can be found in table 6 .

each sample selection was subsequently weighted in the analysis to represent the evaluations in the population that were not selected .

we reviewed the full population of dod , mcc , and state evaluations ; therefore , our results from the quality review of these evaluations do not have an associated margin of error .

the results from our review of the hhs , usda , and usaid evaluations are reported with an associated margin of error .

because we followed a probability procedure based on random selections , our sample is only one of a large number of samples that we might have drawn .

since each sample could have provided different estimates , we express our confidence in the precision of our particular sample's results as a 95-percent confidence interval .

this is the interval that would contain the actual population value for 95 percent of the samples we could have drawn .

all percentage estimates for aggregated results from our review have margins of error at the 95 percent confidence level of plus or minus 8 percentage points or less , unless otherwise noted , and all percentage estimates for individual agencies from our review have margins of error at the 95 percent confidence level of plus or minus 11 percentage points or less , unless otherwise noted .

to assess the extent to which the results of foreign assistance program evaluations are supported by their evidence and whether they assess if programs have met their goals , we assessed the sample of agency fiscal year 2015 evaluation reports against quality criteria we identified .

we identified these criteria based on our review and analysis of evaluation guidance from agencies included in our review ( including any agency internal evaluation review checklists ) , international organizations , evaluation organizations , and prior gao reporting .

these criteria include necessary high - level elements in designing , implementing and reporting on evaluations that could serve as standards across different agencies and evaluation types .

prior to undertaking our quality review , these criteria were discussed and reviewed within the engagement team , as well as by other gao staff with experience in program evaluation and methodologies .

we incorporated the identified criteria into a standardized data collection instrument ( dci ) in order to consistently review the sampled evaluation reports .

the dci contained evaluative questions against which to assess evaluation quality as well as descriptive questions to gather information about the evaluations , such as its location and methodology .

the high - level criteria each included subquestions about elements the reviewer should consider in making his or her overall decision .

the evaluation quality criteria were judged on a four - part scale for most of the judgmental questions: generally addressed: the evaluation mostly addressed the key element ( s ) of the criterion but did not have to completely address all elements in the subquestions ; partially addressed: the evaluation had one or more clear area ( s ) for improvement on the criterion ; not at all addressed: the evaluation did not show that steps were taken to address the criterion ; and insufficient information: reviewers could not make a determination due to a lack of information in the evaluation and any other associated materials .

if a criterion was partially or not at all addressed , or if there was insufficient information in the evaluation to assess the criterion , we considered it a deficiency .

we did not consider study protocols or design documents that indicated plans for a particular evaluation step as sufficient evidence that such a step was performed unless the evaluation report also provided evidence that it had .

the descriptive questions in the dci about evaluation types and methodology were based on prior gao work and asked about designs that examined net impacts of interventions , outcomes of interventions , and processes .

the questions on net impact evaluations asked about the type of design using four categories: ( 1 ) randomized controlled trials or groups , ( 2 ) comparison groups , ( 3 ) time series that would allow for trends to be determined pre - and post - intervention , and ( 4 ) quasi - experimental statistical modelling techniques .

the questions on outcome and process evaluations asked whether baselines and targets had been established for the outcomes and whether criteria had been established to assess processes .

from these types of evaluation , we created two broad categories to use in our analysis of evaluation cost and quality: net impact evaluations , and performance evaluations .

the net impact category included all four impact design types , while the performance evaluation category included outcome and process evaluations as well as a few evaluations that did not fall within the outcome and process categories .

the performance evaluations included some that had established targets or baselines and others that had not , while the process evaluations included some that had established criteria for assessment and others that had not .

we noted some overlap between the net impact , performance , and process categories .

for example , net impact evaluations often considered outcomes or processes , and performance evaluations often considered both outcomes and processes .

this overlap was a key reason we decided to develop and analyze two broad types of evaluation categories rather than attempt to develop more refined types .

a key assumption underlying our analysis was that different types of evaluations were appropriate for different types of study questions .

the evaluative questions in the dci were relative , rather than absolute , with respect to the study questions and were intended to be applicable across evaluation types .

we did not assess the study questions in terms of their scope or rigor .

we instead took the study questions as given , thereby giving every evaluation an equal chance to receive a high score if its design and implementation were appropriate for the study questions .

evaluation reviewers were instructed to consider design , implementation , and reporting in terms of the study questions the evaluations set out to answer rather than against an absolute standard .

in this way , we determined that it would be as possible for a qualitative midterm evaluation that considered program implementation to achieve high scores as it would for a final net impact evaluation that considered effects attributable to the program .

the main criteria questions in the dci asked about the appropriateness of the design , implementation , and conclusions in light of the study objectives .

we did not determine a single definition of appropriateness because we recognized that it is dependent on the study objectives and data collection conditions .

for example , the standards for appropriateness of a final net impact evaluation of a pilot health care program that seeks to establish whether the intervention is achieving positive outcomes are different from the standards for a mid - term performance evaluation of a well - established water and sanitation program supported by a solid evidence base and focused on whether the program was implemented as planned .

given the variation in agency goals and programs , evaluation types , and evaluation timing , we determined that we would rely on expert professional judgment rather than attempt to use a single definition of appropriateness for every situation .

while we designed our dci to apply broadly across agencies and evaluation types , differences in agency evaluation practices and areas of responsibility may limit comparisons between the agencies .

for example , the target audience of an evaluation may determine whether it includes certain reporting elements .

hhs's pepfar evaluations are generally produced for dissemination in research publications and journals , while usaid evaluations evaluate a wide range of programs and are generally directed to an audience of program officials and managers .

in addition , the evaluations we reviewed assessed a wide range of foreign assistance programs with varying characteristics .

these characteristics include the nature of the foreign assistance intervention , the type of program responsible for the intervention , whether the program was designed to be evaluated , and the timing of the evaluation .

for example , some evaluations consider ongoing development assistance in areas such as education or health , while others consider emergency responses to humanitarian crises .

agency officials noted that it could be harder to ensure quality in an evaluation of a program that had to respond quickly to a crisis and therefore did not have the opportunity to plan for an evaluation .

they also noted that if an evaluation is not started until after the program has begun , there may not be any baseline data available .

the evaluation review consisted of multiple reviews by a team of gao staff with experience and familiarity with research methods as well as with reviewing studies and evaluations across a wide range of subject areas and disciplines .

after completing his or her initial review , the first reviewer notified the second reviewer that the evaluation report was available for his or her review .

the second reviews were not independent ; as the second reviewer saw the decisions made by the first reviewer and could review the first reviewer's notes on sources and justifications for his or her decisions .

the second reviewers read the evaluation and indicated whether he or she agreed with the first reviewers' decisions or whether he or she proposed another decision .

the first and second reviewers subsequently met to reconcile any differences .

after the reconciliations were completed , a supervisor then reviewed the work of the two reviewers for internal consistency and completeness according to a standard protocol but did not re - review the evaluation documents .

the supervisor related the identified issues as needed to the first and second reviewers , who addressed them before the supervisor recorded the review as final .

we took several steps to ensure consistency among the reviewers .

we conducted two pretests of the dci on sample evaluations .

the first pretest included members of the engagement team as well as gao staff with experience in the design of survey instruments or in the review of foreign assistance evaluations .

the second pretest included members of the evaluation review team .

after each round of pretests , we made appropriate revisions to the dci .

to help ensure consistency of interpretation , we created a guidance document where reviewers recorded questions about certain decision rules to follow in specific instances .

answers to the questions were then posted after discussion among the review and engagement team members .

additionally , the engagement and review team held regular weekly meetings to discuss any methodological issues that arose and preliminary tabulations of the review data .

to analyze the responses to the dci , we examined how evaluations' quality varied by the eight quality criteria , by agency , by timing of the evaluation relative to the stage of program implementation ( midterm or interim vs. final ) , and type of evaluation ( net impact vs. performance ) .

while there were some differences between the final and midterm evaluations , these were not statistically significant .

however , a higher percentage of evaluations that attempted to assess net impacts were of high quality than those that did not attempt to assess net impacts .

we determined that these differences were due primarily to specific weaknesses in the implementation of the design of the evaluations .

for example , performance evaluations used nonrandom sampling more often than net impact evaluations , which used at least some random sampling .

while our dci and subsequent analysis treated both methods of sampling equally , we focused our assessment on the extent to which each method had been appropriate for the study questions .

we found that the performance evaluations' nonrandom sampling was carried out appropriately less often than the random sampling typically used in net impact evaluations .

in the body of our report , therefore , we focus on the specific weaknesses that we found , such as the one regarding nonrandom sampling , rather than differences at the level of the two broad evaluation types .

our analysis found a reasonably high degree of overlap between several approaches that we considered for categorizing the evaluations .

for example , we categorized the evaluations into three groups: high quality , acceptable quality , and lower quality based on the number of quality criteria that were generally or partially met for each evaluation , as well as instances when quality areas were either not met or there was insufficient information to determine if a certain criterion was met by a particular evaluation .

those evaluations that fell into the lowest and middle categories based on these three categories also generally fell into the lowest and middle categories using another approach that we examined , as table 7 shows .

this comparison also shows that some evaluations in the highest category had a relatively higher number of criteria generally met , while some in the middle category had a relatively lower number of criteria generally met .

to determine the cost of foreign aid evaluations , we reviewed contracts , invoices , and related documents to determine the cumulative cost of final evaluations that were conducted by an outside evaluator .

we defined the cumulative cost as the cost of the final evaluation and any related activities that informed the evaluation's findings , such as a separate data collection effort or a midterm or baseline evaluation .

we did not determine the cost of evaluations that had a midterm evaluation but not a final evaluation because midterm evaluation costs do not reflect the total cumulative cost of evaluating a program .

state and usaid officials noted that some of their midterm evaluations may ultimately be a program's only evaluation .

we did not review the state and usaid fiscal year 2015 midterm evaluations to determine if the agency intends to conduct an additional , final evaluation or include these midterms in our cost analysis .

table 8 shows the total number of evaluations in gao's quality sample , the number of those evaluations whose costs we reviewed , and the number of evaluations whose costs we reviewed that we included in the statistical analysis by agency .

we used contracts and invoices to determine the cost of 42 of the 76 mcc , state , usaid , and usda final evaluations .

to determine an evaluation's cost , we used either the final invoiced amount or the contract's total obligations .

for each evaluation , we also read the statement of work to determine if the contract covered only the evaluation in our sample or if it covered additional activities as well .

in some cases , while the contract covered additional activities , the evaluation's cost was clearly identifiable in a separate line item or invoice .

we identified the evaluation start and end dates using the period of performance in the contract or statement of work .

in some cases we determined the period of performance using dates in the evaluation report if the contract's period of performance covered a broader time period than the evaluation in our sample .

for six usaid evaluations and nine state department evaluations , we determined the evaluation cost using data from the federal procurement data system – next generation ( fpds - ng ) .

fpds - ng provides a contract's obligations , start and end dates , and other descriptive data .

in each case , we confirmed that the contract covered only the evaluation in our sample by reviewing the statement of work or by confirming with agency officials .

we used total obligations to determine the evaluation's cost and also used the date signed and end date listed in fpds - ng to determine the period of performance .

to assess the reliability of the fpds - ng data , we ( 1 ) reviewed related documentation , ( 2 ) traced to or from source documents , and ( 3 ) confirmed fpds - ng data with knowledgeable agency officials .

we determined that the fpds - ng data were sufficiently reliable for the purposes of this engagement .

for mcc , state , and usaid evaluations without clearly identifiable cost information from contract documents or fpds - ng , we estimated the cost based on budget documents or cost estimates provided by the agency or contractor , where available .

we relied on budgets or cost estimates to determine the cost of 5 mcc evaluations , 4 state department evaluations , and 10 usaid evaluations .

we excluded one mcc evaluation from the cost sample because mcc provided a wide range for the estimated cost , and we concluded that this range was not sufficiently reliable to report .

we could not determine the cost of one state department evaluation and one usaid evaluation that were each procured under large agreements that did not separately track evaluation costs .

additionally , usaid officials did not provide cost information for one evaluation .

we report only limited data on the cost of dod's gt&e and hhs's pepfar evaluations because the evaluation contracts or implementing partner agreements did not separately track evaluation costs , and we concluded that the available estimates were too limited to include in our statistical analysis .

to estimate the cost of dod's gt&e evaluations , we reviewed the associated contract and invoices , which included the evaluations as well as additional services .

since the contract and related documents did not contain a separate line item for the evaluations , we requested a cost estimate from agency officials and the contractor .

the contractor was able to provide only the broad estimate that we include in our report with appropriate caveats but which we concluded was not sufficiently reliable to include in our statistical analysis .

the costs of the hhs pepfar evaluations were also not separately tracked by the agency and implementing partners .

evaluation costs were instead estimated by hhs country teams or implementing partners based on their review of previous years' financial records , budgets , or cooperative agreements .

because of the volume of records involved , we judgmentally selected a subsample of 10 hhs evaluations to review the cost estimates provided by hhs officials .

to review these estimates , we traced the estimates to source documentation and spoke with knowledgeable agency officials to understand the methodology used to prepare the source estimates .

because of the uncertainty of these cost estimates , we include them in our report with appropriate caveats but concluded that they were not sufficiently reliable to include in our statistical analysis .

to determine the factors that are associated with the costs of foreign aid evaluations , we analyzed the costs of mcc , state , usda , and usaid evaluations in relation to the data that we collected on these evaluations' quality scores , duration , and other characteristics .

we then produced summary statistics showing the cost differences of various characteristics .

for example , we compared the average cost of evaluations with a survey to those without surveys .

we conducted difference - in - means tests to determine if any of the characteristics were statistically significant at the 95-percent confidence level and reported characteristics that were significantly related to costs .

we also reviewed the evaluations to obtain insights into other likely cost factors , such as unstable locations and the number of sites , for which systematic data were not available for difference - in - means tests .

we included location among the characteristics we considered after observing that evaluations that were more costly than others that were of the same type , or required the same performance period to complete , tended to be conducted in unstable or multiple locations .

to assess our third objective , we identified leading practices for the dissemination of evaluation findings .

we identified these leading practices using federal guidance , including the president's open government directive and office of management and budget ( omb ) guidance , which encourages or requires the timely public posting of agency information on a searchable website , as well as plans and additional efforts to actively disseminate agency information .

in addition to the federal guidance , we also used the american evaluation association's ( aea ) an evaluation roadmap for a more effective government ( aea roadmap ) ; the organization for economic co - operation and development , development assistance committee's ( oecd dac ) quality standards for development evaluation and evaluating development activities: 12 lessons from the oecd dac ; and hhs and the general services administration's ( gsa ) research - based web design and usability guidelines that cite timely public posting , dissemination planning , and additional active efforts to disseminate results as important communication tools for evaluations .

we used these sources to identify six practices that agencies should use in order to successfully disseminate the results of foreign aid evaluations .

we reviewed the dissemination of all evaluations from fiscal year 2015 for five of the agencies and a sample of usaid evaluations .

in total , we examined the dissemination of 193 evaluations: 4 at dod , 49 at hhs , 16 at mcc , 23 at state , 63 at usaid , and 38 at usda .

to assess the availability and timeliness of the dissemination of evaluation reports , we reviewed agency policies and websites and interviewed agency officials .

we reviewed agency evaluation websites to determine if the evaluation reports in agency evaluation lists had been publicly posted .

if an evaluation report had not been posted , we followed up with agency officials regarding the reasons it had not been .

we also reviewed the evaluation reports to ensure the documents contained the information necessary for a user to determine if the findings were valid .

for example , we reviewed evaluations to ensure that any related annexes had been included when the document had been posted .

we examined each agency website to determine whether it provided a search engine that could be used to locate evaluations .

we also checked whether the search engine included additional search filters such as the year the evaluation was completed or its location .

to assess timeliness , we reviewed agency policies and guidance to determine how soon it required evaluation reports to be posted after the completion of the report .

we compared the date an evaluation was considered complete by the agency to the date that it was posted online to determine whether it had been posted within the timeframe required by the agency .

to determine whether sensitive evaluations were made available to identified stakeholders via an internal digital system , we reviewed agency lists of sensitive evaluations and interviewed agency officials about agency processes for making sensitive evaluations available internally .

we also received an in - person demonstration of the internal posting of usaid's sensitive evaluations and documentation of the internal systems that dod and usda use to post evaluations .

to assess agency dissemination planning and its use of additional means for dissemination , we interviewed agency officials and reviewed agency policies , practices , and evaluation documents .

to determine if the agency required dissemination planning , we reviewed the dissemination requirements in its evaluation guidance .

if the agency required dissemination plans , we reviewed its evaluation reports , contracts , and related documents to determine if they included an identification of the potential users of an evaluation and a description of the approach that will provide users with the evaluation results .

if the agency guidance did not require dissemination plans , we asked the agencies if dissemination planning had occurred without the policy in place .

if such ad hoc planning had occurred , we asked that the agencies provide examples .

we also provided written questions to agency officials regarding additional agency practices for disseminating evaluations other than posting the evaluation online .

if agency officials identified additional means of dissemination , we reviewed additional documentary evidence that evaluation findings had been disseminated using these means .

we conducted this performance audit from october 2015 to march 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

agencies varied in the extent to which they met the applicable quality criteria for evaluations that we identified .

tables 9 through 26 below provide further detail on the characteristics and quality of the design , implementation , and conclusions of fiscal year 2015 evaluations we reviewed summarized for all six agencies and then individually for ( 1 ) the president's emergency plan for aids relief ( pepfar ) programs implemented by the centers for disease control and prevention ( cdc ) of the department of health and human services ( hhs ) , ( 2 ) the millennium challenge corporation ( mcc ) , ( 3 ) the department of state ( state ) , ( 4 ) the u.s. agency for international development ( usaid ) , and ( 5 ) the u.s. department of agriculture's ( usda ) foreign agricultural service's food aid programs .

dod partially concurs with our recommendation and notes that in many cases certain methodologies are not well suited for security assistance evaluation .

dod observed that , for example , it would be unethical for dod to establish a randomized control group for security assistance evaluation and that some foreign military organizations may be unwilling to provide significant access to military units solely for the purpose of an evaluation .

we recognize that certain methodologies are not appropriate in every context , and we do not advocate the use of randomized control groups in the evaluations we reviewed for dod .

our main concerns about the dod evaluations focus on implementation of the methods used .

in particular , we found limitations in sampling methods , including descriptions of the target population ; data collection methods ; and data analysis .

we adjusted pertinent wording in our report to clarify these points .

1 .

mcc notes that it has required independent third - party evaluation of all its projects since 2009 and that , in 2013 , it standardized the language in its independent evaluation contracts to explicitly define an independent role for evaluators .

while these are positive steps , we believe that including in mcc's published evaluations explicit statements about the evaluators' independence and any potential conflicts of interest would bolster the evaluations' credibility and the usefulness .

2 .

mcc states that it had not anticipated the length of time required by the review process for all evaluations that it implemented beginning in 2013 .

mcc notes that its forthcoming revised policy on monitoring and evaluation will state that “mcc expects to make each interim and final evaluation report publicly available as soon as practical after receiving the draft report.” this revised guidance does not set a specific time frame for the reviews .

while agency review efforts may help ensure quality , a specific target for the length of time for the reviews would provide a metric for assessing whether reports are being published in a timely fashion that maximizes their usefulness .

in addition to the contact named above , james michels , assistant director ; thomas beall , miranda berry , anthony costulas , gergana danailova - trainor , martin de alteriis , neil doherty , mark dowling , laurie ekstrand , justin fisher , georgette hagans , kay halpern , reid lowe , luann moy , barry seltser , stephanie shipman , michael simon , douglas sloane , and gregory wilmoth made key contributions to this report .

