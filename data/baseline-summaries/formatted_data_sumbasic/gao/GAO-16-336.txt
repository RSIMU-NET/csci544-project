the department of defense ( dod ) is one of the largest and most complex organizations in the world .

to meet its mission to protect the security of our nation and to deter war , it relies heavily on the use of information technology ( it ) to support our warfighters .

in this regard , according to dod's it investment portfolio for fiscal year 2015 , the department spent approximately $30 billion for it investments .

of this amount , approximately $3.7 billion was spent on major automated information system ( mais ) programs , which are intended to help the department sustain its key operations .

these include communications , business , and command and control systems that provide department and component officials with access to information to organize , plan , direct , and monitor mission operations .

dod it investments that fall within one of the following categories are designated as a mais program when: ( 1 ) program costs in any single year exceed $40 million , ( 2 ) total program acquisition costs exceed $165 million , or ( 3 ) total life - cycle costs exceed $520 million .

the secretary of defense can also use discretion to designate a program as a mais if it does not meet these cost thresholds .

in addition , mais programs must comply with certain annual and quarterly reporting requirements identified in statute .

the national defense authorization act for fiscal year 2012 includes a provision that we select , assess , and report on dod mais programs annually through march 2018 .

in addition , senate report 113-176 accompanying s. 2410 includes a provision that we evaluate dod's implementation of statutory reporting requirements for mais programs experiencing a critical change .

a critical change must be declared if the program has experienced , among other things , a schedule delay of 1 year or more , a full life - cycle cost increase of 25 percent or more over the original estimate , or a change that will undermine the system's ability to perform as intended .

the carl levin and howard p. “buck” mckeon national defense authorization act for fiscal year 2015 mandated that agencies , including dod , enhance transparency and risk management of their major it investments through the reporting of performance information on the federal it dashboard , a website that allows stakeholders and the public to view details on the effectiveness of government it programs .

our objectives for this review were to: ( 1 ) evaluate dod's implementation of statutory reporting requirements for mais programs experiencing a critical change ; ( 2 ) describe the extent to which selected mais programs have changed their planned cost and schedule estimates , and met performance targets ; ( 3 ) assess the extent to which selected mais programs have used key it acquisition best practices , including requirements and risk management ; and ( 4 ) determine the extent to which mais programs are represented on the federal it dashboard .

to accomplish the first objective , we identified 18 of 39 mais programs that experienced a critical change and compared their status reports submitted to congress to statutory reporting requirements to determine whether gaps exist .

we also interviewed dod officials responsible for the quality of the data and assessed their procedures for maintaining its accuracy and completeness for mais programs that experienced a critical change between december 2008 and june 2014 , as well as factors impacting the timelines in delivering these critical change reports to congress and the processes used to ensure that appropriated funds were not being obligated on major contracts if prohibited under law .

to address the second and third objectives , we selected 3 programs by identifying them from the mais population of 39 programs that met several criteria , such as programs with a baseline that could be used as a reference point for evaluating cost and schedule characteristics .

we selected the army's tactical mission command ( tmc ) , the navy's common aviation command and control system ( cac2s ) , and the air force's defense enterprise accounting and management system ( deams ) .

to determine the extent that selected program estimates changed , we compared their best or objective cost and schedule estimates established in the first acquisition baseline estimate ( where available ) to the latest total life - cycle estimates .

to determine whether technical performance targets were met , we compared each program's system performance targets against actual performance data , and reviewed the results of assessments conducted on the systems .

we then aggregated and summarized the results of our analyses across the programs .

to determine the extent that best practices were used , we identified key risk management and requirements management practices from the software engineering institute's capability maturity model® integration for acquisition and assessed each program against these and other criteria .

we analyzed documents , such as risk register logs and risk management plans , and compared them to each program's processes and practices .

we interviewed program officials to obtain additional information on the processes and practices used .

to address the fourth objective , we compared all 39 mais programs to the it dashboard website to identify reporting gaps .

we also evaluated the process the department used to determine and submit status and performance information to the website .

we interviewed officials from the dod office of the chief information officer ( cio ) , office of the under secretary of defense for acquisition , technology , and logistics ( at&l ) , and the office of management and budget ( omb ) to obtain their perspectives on the quality of reporting of dod mais programs .

we conducted this performance audit from april 2015 to march 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

see appendix i for a more detailed discussion of our objectives , scope , and methodology .

dod's organizational structure includes the office of the secretary of defense , the joint chiefs of staff , the military departments , numerous defense agencies and field activities , and various unified combatant commands that contribute to the oversight of dod's acquisition programs .

figure 1 provides a simplified depiction of dod's organizational structure .

the under secretary of defense for at&l serves as the defense acquisition executive and the under secretary has responsibility for oversight of mais acquisition programs .

at&l has policy and procedural authority for the defense acquisition system , which establishes the steps that dod programs generally take as dod plans , acquires , deploys , operates , and maintains its it systems ( discussed in more detail following this section ) .

additionally , at&l is the principal acquisition official of the department and is the acquisition advisor to the secretary of defense .

at&l's authority includes directing the military services and defense agencies on acquisition matters and making milestone decisions for mais programs .

at&l can delegate decision authority for mais programs to a component head who may further delegate the authority to the component acquisition executive .

dod's cio is the principal staff assistant and senior it advisor to the secretary of defense .

this role includes overseeing many national security and defense business systems and managing information resources .

the cio coordinates with at&l to develop and maintain a process for assessing and managing the risks related to the department's it acquisitions , including mais programs .

the department of defense instruction 5000.02 establishes policy for the management of all dod acquisition programs .

in january 2015 , dod updated these guidelines which outline the framework for mais programs .

this framework consists of six models for acquiring and deploying a program , including two hybrid models that each describe how a program may be structured based on the type of product being acquired ( eg , software - intensive programs and hardware - intensive programs ) .

a generic acquisition model that shows all of the program life - cycle phases and key decision points is shown in figure 2 and described following the figure .

materiel solution analysis: refine the initial system solution ( concept ) and create a strategy for acquiring the solution .

a decision — referred to as milestone a — is made at the end of this phase to authorize entry into the technology maturation and risk reduction phase .

technology maturation and risk: determine the preferred technology solution and validate that it is affordable , satisfies program requirements , and has acceptable technical risk .

a decision — referred to as milestone b — is made at the end of this phase to authorize entry of the program into the engineering and manufacturing development phase and award development contracts .

an acquisition program baseline is first established at the milestone b decision point or at program initiation , whichever occurs later .

a program's first acquisition program baseline contains the original life - cycle cost estimate ( which includes acquisition and operations and maintenance costs ) , the schedule estimate ( which consists of major milestones and decision points ) , and performance parameters that were approved for that program by the milestone decision authority .

the first acquisition program baseline is established after the program has refined user requirements and identified the most appropriate technology solution that demonstrates that it can meet users' needs .

engineering and manufacturing development: develop a system and demonstrate through testing that the system meets all program requirements .

a decision — referred to as milestone c — is made during this phase to authorize entry of the system into the production and deployment phase or into limited deployment in support of operational testing .

production and deployment: achieve an operational capability that meets program requirements , as verified through independent operational tests and evaluation , and implement the system at all applicable locations .

operations and support: operationally sustain the system in the most cost - effective manner over its life cycle .

mais programs enable dod to organize , plan , direct and monitor important mission operations .

as previously mentioned , mais programs must comply with certain annual and quarterly reporting requirements identified in statute .

each calendar year , dod must submit to congress budget justification documents on each mais program , including information on cost , schedule , and performance .

specifically , these programs must report , among other things , on the development and implementation schedules and total acquisition and full life - cycle cost estimates and provide a summary of the key performance parameters for each program .

dod must also provide a summary of any significant changes to information previously provided for each program .

moreover , on a quarterly basis , the program manager for each mais program is required to provide the senior dod official responsible for the program a written report that identifies any variance in the program's cost , schedule , or performance .

depending on the determination after reviewing the variances identified in the quarterly report , the senior dod official responsible for the program must notify the congressional defense committees of any programs that have experienced either a significant or critical change .

during our review , mais programs were required to comply with the following reporting requirements: significant change .

a significant change must be declared if a program experienced a schedule delay of more than 6 months but less than a year ; estimated total acquisition or full life - cycle cost for the program has increased by at least 15 percent but less than 25 percent ; or there has been a significant adverse change in the expected performance of the system .

if such an event occurs , the senior dod official responsible for the program must notify the congressional defense committees in writing no later than 45 days after receiving the quarterly report from the program manager .

critical change .

a critical change must be declared if a program failed to achieve a full deployment decision within 5 years after the milestone a decision or , if there was no milestone a decision , the date when the preferred alternative was selected for the program ; experienced a schedule delay of 1 year or more ; experienced an estimated total acquisition or full life - cycle cost increase of 25 percent or more over the original estimate ; or experienced a change in the expected performance of the system that will undermine the ability of the system to perform as intended .

if such an event occurs , the senior dod official responsible for the program must carry out an evaluation and submit a critical change report to the congressional defense committees no later than 60 days after receiving the quarterly report .

since the december 19 , 2014 , enactment of the carl levin and howard p. “buck” mckeon national defense authorization act for fiscal year 2015 , mais programs are now required to declare a significant change — instead of a critical change — if they fail to achieve a full deployment decision within 5 years after the milestone a decision , the date when the preferred alternative was selected for the program ( excluding any time during which program activity is delayed as a result of a bid protest ) .

more recently , the national defense authorization act for fiscal year 2016 directed the secretary of defense to issue guidance for mais programs to establish an acquisition baseline within 2 years after program initiation .

this statute provides a response to a recommendation we made in our last annual report on mais programs .

in particular , we found that these programs spent , on average , more than 5 years and $450 million prior to establishing baselines .

we noted that programs that have not established baselines were subject to less oversight and could not be measured against cost , schedule , and performance targets .

also , the propensity to carry out mais programs for multiple years prior to committing to baselines is inconsistent with incremental and rapid development as called for in federal law and gao's it management best practices .

accordingly , we recommended that these programs be baselined within 2 years ; for which dod partially concurred .

we maintained that establishing baselines within 2 years would improve outcomes and increase accountability .

dod's cio , along with other agencies , must report on the progress of its it investments , including mais programs , on a public website known as the it dashboard .

omb established this website in june 2009 to improve the transparency and oversight of agencies' investments .

the dashboard visually displays federal agencies' cost , schedule , and performance data for over 700 major federal investments at 26 federal agencies .

it also includes a risk rating that is to be performed by agency cios .

according to omb , these data are intended to provide a near - real - time perspective on the performance of these investments .

the public display of agency data is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold federal agencies accountable for their progress and results .

in august 2011 , omb issued guidance that stated , among other things , that agency cio's shall be held accountable for the performance of it investments .

the dashboard presents performance ratings for individual investments using metrics that omb has defined — cost , schedule , and cio evaluation .

if omb or the agency cio determine the reported data is not timely or reliable , the cio must notify omb and establish within 30 days of this determination an improvement program and the progress the agency is making .

according to omb , the addition of cio names and photos on the website is intended to highlight this accountability and link the dashboard's reporting on investment performance .

in order to enhance transparency and improve risk management of federal it acquisitions , congress codified the dashboard reporting process through key provisions , known as the federal information technology acquisition reform provisions in the carl levin and howard p. “buck” mckeon national defense authorization act for fiscal year 2015 .

entities such as the project management institute , the software engineering institute at carnegie mellon university , and gao have developed and identified best practices to help guide organizations to effectively plan and manage their acquisitions of major it systems , such as mais programs .

our prior reviews have shown that properly applying such practices can significantly increase the likelihood of delivering promised system capabilities on time and within budget .

these practices include , but are not limited to: requirements management: requirements establish what the system is to do , how well it is to do it , and how it is to interact with other systems .

appropriate requirements development involves eliciting and developing customer and stakeholder requirements , and analyzing them to ensure that they will meet users' needs and expectations .

it also consists of validating requirements as the system is being developed to ensure that the final systems to be deployed will perform as intended in an operational environment .

risk management: a process for anticipating problems and taking appropriate steps to mitigate risks and minimize their impact on program commitments .

it involves identifying and documenting risks , categorizing them based on their estimated impact , prioritizing them , developing risk mitigation strategies , and tracking progress in executing the strategies .

according to statute , for programs that declare a critical change , the report that is submitted to congress must include a written certification stating that: the automated information system or it investment to be acquired is essential to the national security or to the efficient management of the dod ; there is no alternative to the system or it investment which will provide equal or greater capability at less cost ; the new estimates of the costs , schedule , and performance parameters have been determined , with the concurrence of the director of cost assessment and program evaluation , to be reasonable ; and the management structure for the program is adequate to manage and control program costs .

all 18 mais critical change reports in our review contained the required elements .

in addition , this report must be prepared and submitted to congress no later than 60 days after the senior dod official responsible for the program receives the quarterly report from the program manager that leads to the determination that a critical change event has occurred .

programs that do not submit the report to congress within the 60-day period are statutorily prohibited from obligating appropriated funds for any major contract until the date that congress receives the report .

further , if a mais program violates the statutory prohibition against obligations , it will also violate the antideficiency act .

this act prohibits an officer or employee of the united states government from making or authorizing an expenditure or obligation in excess of or in advance of available appropriations .

the antideficiency act also requires that an appropriation must be available for an agency to incur an obligation .

thus , if dod incurs an obligation against an appropriation that is not legally available , the department has violated the act .

violating the antideficiency act would require the secretary of defense to immediately report to the president and congress all relevant facts and a statement of actions taken .

of the 18 mais programs experiencing a critical change , most exceeded the 60-day reporting requirement , several by a substantial amount .

specifically , 16 exceeded the 60-day reporting requirement and 10 of those programs took over 100 days to report .

of the 10 programs , 5 of the programs took over 200 days to report .

two programs — teleport gen i / ii and general fund enterprise business systems — delivered their reports to congress within the 60-day requirement .

figure 3 shows the extent that programs met or exceeded the 60-day reporting requirement .

officials from several programs provided various reasons for why they delayed submitting their critical change reports .

for example: mission planning systems #2 submitted a report to the office of the secretary of defense on time but it took 29 days to transmit the package to congress and it was 3 days late .

cac2s program was late because of the need to conduct an independent assessment that was directed by dod .

the expeditionary combat support system program had delays due to changes that were made to the size and complexity of its originally scoped effort and contracting process that , in turn , required additional updates .

the update triggered a process to re - evaluate the revised strategy .

according to a dod at&l official , 60 days is too short to perform a program evaluation and achieve all the coordination necessary for an important communication with congress .

the official also said the addition of the office of cost assessment and program evaluation requirement to review and approve the reports , as mandated by the weapon systems acquisition reform act of 2009 , consumes much of the 60-day allotment time .

a cost assessment and program evaluation official noted that reviews often exceed the 60-day period because , most notably , the significant amount of time needed to collect and develop comprehensive information used to determine a program's cost .

the official added that dod is working to strengthen the data collection efforts to improve the ability of the office of cost assessment and program evaluation to complete its evaluation such as reviewing the basis for the revised cost and schedule estimates .

however , the official noted that there has been no overall evaluation or study on the cause for delays .

while it may be possible that 60 days is too short of a time frame for submitting reports , without understanding the cause for the delays dod is not in a position to state what time frame would be feasible .

further , the fact that two programs were able to submit reports in a timely manner suggests that 60 days is achievable .

until dod ascertains the cause for the delays and implements corrective actions , the reports may continue to be delivered in a manner which may impact the timeliness of information considered by congress in making oversight and funding decisions for mais programs .

this may also affect budget and other strategic decisions on how and what programs to prioritize .

further , dod does not have a mechanism to monitor and ensure that mais programs with late reports were restricted as required by law from obligating funds on major contracts prior to congress receiving the report .

this mechanism is especially important because of the potential for violating the antideficiency act .

although dod states in its guidance that program managers should not obligate any funds during the entire period in which the report is being prepared , dod does not currently have a way to monitor this .

according to a dod at&l official , dod is not required by statute , regulation , or guidance to collect the information for monitoring purposes .

however , our guidance on internal controls for federal agencies states that agency management should establish a baseline to monitor the current state of a control system .

once established , management should monitor the agencies' internal control system through ongoing monitoring and separate evaluations dod does not have a management internal control to monitor the system and evaluate whether programs are complying with the dod guidance .

instead , dod relies on the programs to act in accordance with the law .

this official said the need to obligate funds on major contracts should be a driver for programs to submit their reports to congress as expeditiously as possible .

however , with so many programs submitting the critical change reports well after the 60-day period , there is a risk that programs could potentially violate the prohibition on obligations , and thus , in addition , the antideficiency act .

the extent to which the three selected mais programs in our study experienced changes in their cost and schedule estimates and met performance targets varied .

specifically , the army and air force programs experienced slight changes in their cost and schedule estimates , while the navy program experienced more significant changes .

in addition , only one program , air force's deams , did not fully meet its technical performance targets .

table 1 provides a status of the cost , schedule changes , and the results of technical performance targets for the programs .

see appendix ii for the detailed profiles of each program .

as of january 2016 , the latest life - cycle cost estimate for the army's tmc program had increased about 19 percent from the program's february 2008 acquisition program baseline estimate ( from approximately $1.97 billion up to $2.34 billion ) .

program officials attributed the cost increase to a breach in the research and development testing and evaluation cost estimation that was reported to congress .

the program's estimated program development cost increased by 45 percent over the original acquisition program baseline due to program scope changes derived from the realignment of certain missions , such as the endorsement of the command post of the future as a foundation for mission command .

as of january 2016 , tmc program experienced a 3-month slippage in its full deployment date , currently scheduled for december 2018 .

the slippage was within the program's pre - established threshold allowance to account for minor changes in schedule and , program officials stated that this slippage was considered to be a low risk that the program has accepted .

program officials stated that , although the command post of the future product is 95 percent fielded and is on schedule to reach full deployment by december 2018 , continued support of the command post computing environment is needed beyond fiscal year 2019 .

as of january 2016 , the tmc program met all three of its key performance targets , which include supporting net - centric military operations , disseminating orders with future army and joint command and control systems , and displaying unified information on subject matters .

as of october 2015 , the latest life - cycle cost estimate for navy's cac2s increment 1 program had increased about 477 percent from its first acquisition program baseline estimate ( from approximately $347 million up to $2 billion ) .

as previously reported , factors attributed to the program's early developmental challenges contributed to an increase in its cost estimate , which include program scope growth and restructuring .

according to program documentation , despite the program's initial challenges in its cost estimates due to an increase in its operations and support expenditures , the cac2s program has demonstrated gradual improvement as it reported a cost avoidance of $54.4 million for implementing the dod better buying power initiatives that befitted from competitive market forces that drove down cost .

as of october 2015 , the program's latest life - cycle estimate relative to its november 2010 production acquisition program baseline cost estimate had decreased by about 19 percent ( from approximately $2.46 billion down to $2 billion ) .

as of october 2015 , compared to its first acquisition program baseline schedule , the program experienced a 13 year and 9 month slippage in its full deployment date — currently scheduled for march 2022 .

as previously reported , factors that attributed to the prior schedule slippage included the addition of new requirements and program restructuring .

however , program officials stated that the program has been executing in accordance within its approved schedule .

as of october 2015 , the estimated milestone c phase 2 was delayed by 6 months from the program's production acquisition program baseline but achieved it milestone approved within the program's pre - established schedule threshold .

program officials attributed this delay , in part , to administrative factors , which included the review and approval process .

cac2s successfully achieved milestone c phase 2 approval in february 2015 but the acquisition decision memorandum had not been signed until march 2015 .

as of october 2015 , program officials reported that , during performance testing , it was meeting both of its key performance targets related to net - ready and data fusion .

as of october 2015 , the latest life - cycle cost estimate for the air force's deams program had increased about 9 percent from its first february 2012 acquisition program baseline estimate ( from approximately $1.43 billion up to $1.56 billion ) .

program officials attributed the cost increase , in part , to program scope growth and the addition of software upgrade enhancements .

specifically , as of october 2015 , the program's life - cycle cost estimate incorporated additional infrastructure maintenance costs throughout the life - cycle that added performance monitoring and additional deployment support .

also , according to program officials , the program brought forward increment 2 requirements and a second oracle software upgrade in year 2021 .

deams experienced a 6 month slip in its milestone c but was within its threshold , a predefined point where programs that exceed it are at increased risk .

however , it did experience a 1 year slip in its full deployment decision date — currently scheduled for february 2016 .

program officials attributed this slippage due to findings identified deams's initial operational test and evaluation report .

while the program had been established since august 2003 , a full deployment date had not been determined .

as of september 2015 , program officials expect full deployment to be reached by october 2016 .

as of october 2015 , deams program officials reported that the program did not meet all of its nine key performance targets .

specifically , deams did not meet five performance targets: balance with treasury , accurate balance of available funds , timely reporting , period - end processing , and net - ready .

for example , the two operational assessments that were conducted from 2012 to 2014 identified significant weaknesses in three measures of effectiveness and suitability .

further , the initial operational test and evaluation report identified system performance issues within the deams program , which included change management issues , transaction backlogs , and ineffective reporting tools .

subsequently , the air force operational test and evaluation center provided 29 recommendations for the air force to implement to support the successful fielding of deams increment 1 , 17 of which were documented as being completed , while corrective action for the remaining 12 are still underway .

however , according to program documentation , deams must demonstrate measureable improvement by the full deployment decision date of february 2016 , in order to avert future schedule delays in its fielding deployment .

according to the software engineering institute's capability maturity model integration for acquisition , an appropriate requirements management involves establishing an agreed - upon set of requirements , ensuring traceability between requirements and work products , and managing any changes to the requirements in collaboration with stakeholders .

likewise , an effective risk management process identifies potential problems before they occur , so that risk - handling activities may be planned and invoked , as needed , across the life of the project in order to mitigate the potential for adverse impacts .

table 2 provides key practices used to comprehensively manage requirements and risk .

all three selected programs implemented it acquisition best practices for risk management , but requirements management best practices were not consistently implemented by the programs .

table 3 provides a summary of the extent to which requirements and risk management best practices were implemented by each program .

the army implemented all risk management best practices for the tmc program .

for example , the risk management plan , dated may 2014 , identified risk sources to include , among other things , unclear system requirements , immature technology , and an unstable organizational environment .

in addition , program officials analyzed , categorized , and controlled risks using a probability and impact model that considered the risks ( from very low to very high ) and the potential consequences .

the program also used a risk radar tool to track and monitor risks .

these are reviewed weekly during staff meetings and updated monthly .

further , tmc's risk management plan indicated that contingency plans are invoked whenever adjustments to cost , schedule , or performance are required .

in taking these and other actions , the tmc program had established and utilized the key risk management practices .

doing so should better position the program to mitigate adverse impacts from potential problems before they occur .

the army had implemented three requirements management best practices for the program , but did not fully implement two: the practice of managing requirements changes and ensuring that work products are in alignment with requirements .

for example , one key practice that the program implemented included the maintenance of the bidirectional traceability tool among requirement .

specifically , program officials utilized a traceability tool used to generate a requirements matrix to track all of its program elements to the requirements .

regarding managing requirements changes , while program officials tracked requirements changes in a database , requirements changes were not always available at the stakeholder level to evaluate the impact and determine the status of requirements changes for all elements of the program .

even though tmc was in the production phase , it relied solely on the functions within its requirements database rather than a requirements management document .

this left the program without a formal mechanism to track and ensure project plans , activities , and work products are consistent with defined requirements .

without such a document , the program does not have a formal mechanism to track and ensure that project plans , activities , and work products are consistent with defined requirements .

the navy implemented all risk management best practices for the cac2s program .

for example , the risk management plan assessed risks in terms of their probability and consequence of occurrence .

the program also identified and documented risks and had the supporting documentation that included risk and issue register logs , detailed reports , the integrated master schedules , and risk assessments .

in addition , the risk management plan established the strategy that included the processes to guide risk mitigation efforts at the lowest , appropriate level .

the program's risk registers , which included risk mitigation steps , were provided by program officials to demonstrate that risk mitigation plans had been implemented for each risk .

in taking these and other actions , the program had established and utilized effective risk management practices .

the navy implemented three requirements management best practices for the program but did not implement two .

for example , one key practice implemented was managing requirements changes .

specifically , the program office demonstrated the ability to effectively manage changes to requirements as they evolved during the project .

program officials did this by documenting the alignment of requirements to its respective requirements changes , maintaining a history of requirements changes with rationale explaining the change request within its configuration documentation , and publishing requirements data using a database tool .

however , the program did not fully implement the practice of maintaining traceability among requirements and work products to ensure that work products were in alignment .

further , with regard to the program's bidirectional traceability tool , according to the july 2014 traceability tool , 25 specifications and 13 capabilities did not map to its respective work products .

according to program officials , as of november 2015 , 11 of the 25 specifications had been mapped , but due to an oversight , mapping of these specifications were not associated , while the remaining 14 specifications did not map to its respective capability production documentation to demonstrate completeness .

according to program officials , 4 capabilities had been recently mapped but the remaining 9 capabilities listed in the capability production documentation did not map the traceability to the respective requirements .

according to program officials , the mapping discrepancy of 7 capabilities was attributed to unfunded , obsolete , and programmatic requirements .

furthermore , after notifying the program of the gaps we identified , officials stated that they would take action to ensure mapping of 14 specifications and 2 capabilities to their respective requirements work products would be addressed .

regarding the alignment of requirements , the program's requirements management plan had not been updated since may 2009 and software specifications and capabilities were not consistently maintained .

according to program officials , the current requirements management plan had been previously reviewed and was determined to be suitable for the purpose of implementing requirements management best practices .

according to the capability maturity model® integration for acquisition , without a clear linkage between requirements all the lower - level requirements and capabilities , the program may not be effectively managing development efforts in accordance with the most recent requirements .

until work products are updated , the program cannot provide assurance that its requirements are aligned with the most updated work products and is at - risk of potential cost and schedule consequences .

the air force implemented all seven risk management best practices for the deams program .

to the program's credit , deams demonstrated great strides in improving its risk management best practices .

since our prior mais review , the program has made improvements such as monitoring the status of each risk periodically , and ensuring that risk reports were up to date , which included the status of actions to mitigate risks .

other key practices include defining parameters to analyze and categorize risks , documenting risk , and developing risk mitigation plans in accordance with the risk management strategy , among other areas .

in taking these and other actions , the deams program had established and utilized effective risk management practices .

the air force had implemented four requirements management best practices , but did not fully implement one requirements management practice , the practice of developing an understanding with providers on the meaning of requirements .

for example , the air force ensured that project plans and work products were aligned with the most recent requirements .

specifically , the program maintained consistent documentation and oversight of work products , which included an up - to - date requirements management plan , system specifications , and capabilities documentation .

however , while the program had established an adjudication process by which requirements were reviewed and approved , and implemented a test methodology to validate requirements prior to production installation , the function to ensure accountability was not working properly .

as such , program officials did not determine whether key requirements were validated during system integration testing prior to deploying software into production , which was released with unresolved issues .

program officials subsequently resolved the issues without any negative impacts .

the program office attributed the issue of not fully validating requirements to environmental issues that were considered to be acceptable risks and , subsequently , scheduled production installation in november 2014 .

nevertheless , according to the software engineering institute's capability maturity model® integration for acquisition , requirements should be analyzed to ensure that established criteria are met so that proper control functions are in place .

pursuant to its statutory responsibility to analyze , track , and evaluate risks , omb requires agency cios to provide cost , schedule , and risk information for all major it investments on the it dashboard .

in addition , the it dashboard shows cio names and photographs who are responsible for investments to increase accountability for it acquisitions .

as of october 2015 , 27 of the 39 dod mais programs were listed on the it dashboard .

according to dod officials , and in accordance with omb policy , 8 mais programs that have not been funded in the president's budget submission are not reported to the dashboard , as appropriate .

in addition , 4 mais programs have been designated by dod as containing national security - sensitive information and were therefore classified and not subject to being reported on the dashboard .

according to dod cio and at&l officials , the 8 unfunded mais programs will be reported to the dashboard after the president's 2017 budget submission has been finalized .

however , the organization responsible for supervising mais acquisition programs — at&l — is not represented on the dashboard .

instead , the dashboard publicly shows dod's cio as the responsible party , pursuant to omb's direction , but is not accurate .

at&l has oversight responsibility for the acquisition performance of mais programs .

in this regard , not only does at&l supervise department acquisitions and establish its acquisition policies , but as the milestone decision authority for mais programs , the under secretary or his designee , has overall responsibility for each program .

by contrast , the cio is not involved in managing the performance of the mais programs but is responsible for submitting the rating to the dashboard .

officials from dod's office of the cio and omb's office of e - government and information technology told us that they were aware of this inconsistency on the dashboard but did not think it was a significant issue .

further , the dod officials stated that since the cio is involved in the rating process the representation of their office on the dashboard is sufficient .

nonetheless , since only the dod cio is represented on the dashboard , the public and other users may be unaware that at&l has overall oversight for the acquisition performance of mais programs , minimizing the intended accountability the dashboard is to provide .

since mais programs account for billions of dollars of dod's it budget , it is important that the required critical change reports are timely so congress has the necessary information to make budgetary and oversight decisions .

while the reports contained the required elements , many were not submitted in a timely manner , potentially hampering congress' ability to make informed decisions .

further , all three selected programs implemented it acquisition best practices for risk management and implemented most practices for requirements management .

while this is a significant achievement , improvements can be made in managing requirements .

among other things , programs were operating without a current requirements management plan that was considered to be acceptable risks .

managing requirements effectively is especially necessary since mais programs are intended to help the department sustain its key operations .

finally , there is a lack of accountability for at&l on the it dashboard .

while omb intended for accountability by requiring that major investments show agency cios as responsible , it did not consider that dod's at&l is the responsible party for oversight of the acquisition performance of mais programs .

since the dashboard does not reflect that at&l has such responsibility , there is decreased public accountability .

to help improve the management of mais programs , we are making six recommendations that: the secretary of defense examine the mais critical change reporting process to identify root causes for delays and implement corrective actions for the timely delivery of critical change reports .

the secretary of defense develop a mechanism for monitoring whether mais programs with late reports are restricted from obligating funds and in turn ensuring compliance with the antideficiency act .

the secretary of the army direct the tmc program manager to develop a requirements management plan to document and manage its requirements process .

the secretary of the navy direct the cac2s program manager to identify weaknesses in the requirements traceability process and take corrective actions to manage the traceability of requirements to the respective lower - level requirements , and periodically evaluate work products , including the requirements management plan , and update them in accordance with the requirements guidance .

the secretary of the air force direct the deams program manager to address weaknesses in its controls for ensuring that all software requirements are tested and validated before deployment of new software releases .

director of omb instruct the federal cio to add the under secretary of defense for at&l as a responsible party to dod's mais entries on the federal it dashboard website , alongside the cio , to publicly disclose the responsible party for the acquisition performance management of mais programs .

we provided a draft of this report to dod and omb .

we received written comments from dod's acting principal deputy assistant secretary of defense for acquisition , which are reprinted in appendix iii .

in its comments , the department concurred with all five recommendations to improve oversight , it acquisition practices , and tools used to manage mais programs .

in e - mail comments , an official from omb's audit liaison group stated that omb's office of e - government and information technology does not agree with the recommendation to add at&l to the it dashboard as a responsible party for mais programs but would work with dod to address it .

the official did not provide a rationale for this position or explain how omb would work with dod .

nonetheless , we continue to believe there is a lack of transparency and accountability for at&l on the it dashboard .

the it dashboard publicly shows dod's cio as the responsible party , pursuant to omb's direction .

however , at&l has oversight responsibility for the acquisition performance of mais programs .

in this regard , not only does at&l supervise department acquisitions and establish its acquisition policies , but as the milestone decision authority for mais programs , the under secretary or his designee , has overall responsibility for each program .

the dod cio is not involved in managing the performance of the programs and is only responsible for submitting the rating to the dashboard .

we believe that adding at&l to the it dashboard would increase public accountability and leadership transparency for the acquisition management of mais programs .

we are sending copies of this report to the appropriate congressional committees ; the secretary of defense ; the secretary of the air force , the secretary of the army , the secretary of the navy , the office of management and budget , and other interested parties .

this report also is available at no charge on the gao website at http: / / www.gao.gov .

should you or your staffs have any questions on information discussed in this report , please contact me at ( 202 ) 512-4456 or chac@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

the national defense authorization act for fiscal year 2012 includes a provision that we select , assess , and report on selected department of defense ( dod ) major automated information system ( mais ) programs annually through march 2018 .

in addition , senate report 113-176 accompanying s. 2410 includes a provision that we evaluate dod's implementation of statutory reporting requirements for mais programs experiencing a critical change .

our objectives for this report were to ( 1 ) evaluate dod's implementation of statutory reporting requirements for mais programs experiencing a critical change ; ( 2 ) describe the extent to which selected mais programs have changed their planned cost and schedule estimates , and met performance targets ; ( 3 ) assess the extent to which selected mais programs have used key it acquisition best practices , including risk management ; and ( 4 ) determine the extent to which mais programs are accurately represented on the federal it dashboard .

to evaluate dod's implementation of statutory reporting requirements for mais programs experiencing a critical change — a schedule delay of 1 year or more , a full life - cycle cost increase of 25 percent or more over the original estimate , or a change that will undermine the system's ability to perform as intended — we collected and analyzed information about critical changes from december 2008 to june 2014 and their corresponding reports .

we assessed whether dod had met reporting requirements by reviewing mais critical change reports and supporting documentation to determine if they included required written certifications stating that: the automated information system or it investment to be acquired is essential to the national security or to the efficient management of the dod ; there is no alternative to the system or it investment which will provide equal or greater capability at less cost ; the new estimates of the costs , schedule , and performance parameters have been determined , with the concurrence of the director of cost assessment and program evaluation , to be reasonable ; and the management structure for the program is adequate to manage and control program costs .

we did not look at the quality of the assessments and estimate done .

we reviewed the reports to determine if the particular elements were included .

we then summarized the number of elements addressed by program .

in order to determine if the critical change reports met or exceeded the 60-day requirement , we collected all mais reports and compared the dates within them .

we subtracted the date in which the reports were delivered to congress via letter from the date in which the program manager provided the quarterly report to the senior dod official responsible for the program in order to determine the number of days that had lapsed .

additionally , we rearranged the programs by number of days ( from greatest to least ) it took to deliver the critical change reports .

we noted how many programs took more than 100 days , and 200 days to deliver the reports .

we also interviewed dod officials in order to ask their opinion on the length of time it takes to deliver a critical change report to congress .

we interviewed dod officials responsible for the quality of the data and assessed their procedures for maintaining its accuracy and completeness .

in addition , we examined the data for outliers or others extraordinary items .

based on these procedures , we have concluded that these data are sufficiently reliable for our purposes .

additionally , in order to determine whether dod is tracking the obligation of funds during a program's critical change , we interviewed dod officials in order to determine what processes and tools they had in place to ensure that funds were not being obligated .

to address the second and third objectives , we used dod's official list of 39 mais programs , as of february 25 , 2015 , to establish the basis for selecting the mais programs that were used to assess objectives two and three .

we used the criteria below to select three mais programs .

any programs that were used in the prior two reviews should be excluded .

any programs that are fully deployed or cancelled should be eliminated from consideration .

the program should not be new to the mais list ; otherwise , there may not be sufficient acquisition activity and documentation to evaluate .

the program must have a baseline in order to have a reference point for evaluating cost and schedule performance characteristics .

the program cannot be a national security agency program .

we included one program from each military department — army , air force , and navy in order to diversify the portfolio .

thus , we excluded any dod - wide programs .

we selected programs that had the lowest relative ratings on the federal it dashboard as of april 2015 .

we preferred that the program be complex ( eg , integration across domains , global , critical to battle operations , etc .

 ) , rather than an upgrade .

we preferred to select programs with funding profiles that are significant when compared to the rest of the portfolio .

we considered issues identified from credible sources of information , such as defense acquisition management information retrieval online resources , it dashboard ratings , etc .

we filtered the original list of mais programs using the criteria above .

based on this filtering , we chose the following systems: the air force's defense enterprise accounting and management system - increment 1 ( deams increment 1 ) , the army's tactical mission command ( tmc ) , and the navy's common aviation command and control system increment 1 ( cac2s increment 1 ) .

to address the second objective , we analyzed and compared each selected program's first acquisition program baseline cost estimate to the latest life - cycle estimate to determine the extent to which planned program costs had changed .

similarly , to determine the extent to which these programs changed their planned schedule estimates , we compared each program's first acquisition program baseline schedule to the latest schedule .

we relied on the thresholds established by statute to describe the amount of any deviation ( i.e. , significant or critical ) that each program's latest life - cycle cost and schedule estimates experienced from the first acquisition program baseline .

to determine whether the selected programs met their performance targets , we compared program and system performance targets against actual performance data in test reports and program management briefings .

we reviewed the results of operational assessments and program evaluations conducted on the systems .

we also reviewed additional information on each program's cost , schedule , and performance , including program documentation , such as dod's mais annual and quarterly reports ; information from the office of management and budget's ( omb ) it dashboard ; acquisition program baselines ; monthly status briefings ; system test reports ; and our prior reports .

we also interviewed program officials from each of the selected mais programs to obtain additional information on cost , schedule , and performance .

we provided our assessments to the program management offices of each selected program for comment .

we aggregated and summarized the results of these analyses across the programs , as well as developed individual profiles for each program ( see appendix ii ) .

to address the third objective , we analyzed each selected program's it acquisition documentation and compared it to key requirements management and risk management best practices — including software engineering institute's capability maturity model® integration for acquisition ( cmmi - acq ) practices — to determine the extent to which the programs were implementing these practices .

in particular , the key requirements management best practices we reviewed were: develop an understanding with the requirements providers on the meaning of the requirements , obtain commitment to requirements from project participants , manage changes to requirements as they evolve during the project , maintain bidirectional traceability among requirements and work , and ensure that project plans and work products remain aligned with requirements .

specifically , we analyzed program requirements documentation , including requirements management plans , requirements traceability matrices , requirements change forms , technical performance assessments , and requirements board meeting minutes .

additionally , we interviewed program officials to obtain additional information about their requirements management practices .

additionally , we reviewed the following key risk management best practices: determine risk sources and categories ; define parameters used to analyze and categorize risks and to control the risk management effort ; establish and maintain the strategy to be used for risk management ; identify and document risks ; evaluate and categorize each identified risk using defined risk categories and parameters , and determine its relative priority ; develop a risk mitigation plan in accordance with the risk management monitor the status of each risk periodically and implement the risk mitigation plan as appropriate .

specifically , we analyzed program risk documentation , including monthly risk logs and reports , risk - level assignments , risk management plans , risk mitigation plans , and risk board meeting minutes .

additionally , we interviewed program officials to obtain additional information about their risks and risk management practices .

to address the fourth objective , we used dod's official list of mais programs , as of february 25 , 2015 .

these programs were used as basis to determine whether programs were reported to the federal it dashboard .

to do so , we: exported the it portfolio program data reported to the federal it dashboard by dod in fiscal year 2015 , and compared it to the 39 programs on dod's official list of mais programs .

for those programs that were found to not be reported on the dashboard , we met with agency officials from the office of the chief information officer ( ocio ) and the under secretary of defense ( acquisition , technology , and logistics ) ( at&l ) to determine the reasons for not reporting .

we also interviewed ocio officials to obtain information on the processes used by the ocio when reviewing programs for it dashboard updates .

further , we interviewed officials at omb to obtain their views on representing the cio as the sole party responsible for programs reported to the it dashboard was accurate .

specifically , we discussed dod's unique structure that at&l is responsible for the acquisition performance of mais programs but is a separate organization from the ocio .

we conducted this performance audit from april 2015 to march 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

this section contains profiles of the three selected major automated information system ( mais ) programs for which we determined whether they had changed their planned cost and schedule estimates and met performance measures .

each profile presents data on the program's purpose and status , its latest cost and schedule estimates compared to the first acquisition program baseline ( where established ) , as well as system performance data .

the first page of each two - page profile contains a description of the program's purpose and a figure that provides a comparison of the program's first acquisition program baseline to the program's latest schedule .

the years depicted on the figure represent calendar years , and the milestones represent the program's best estimates of dates for those milestones .

the program's start represents the date that program officials reported that they first started work on the program .

the first page also provides ( 1 ) essential program details , such as the name of the prime contractor , the total number of active contractors — which includes the prime contractor — and any other contractors ( and in some cases subcontractors ) supporting the program ; ( 2 ) program costs ( in then - year dollars ) , comparing the program's latest life - cycle cost estimate ( separated into acquisition and operations and maintenance costs ) to its first acquisition program baseline ( subsequent acquisition program baselines that may have been established are not identified ) , ( 3 ) locations to which the system will be deployed ; and ( 4 ) a summary of the cost , schedule , and performance of each program , which is further discussed on the second page of the profile .

the symbols , denoted by arrows or filled circles , included in the summary box on the first page of each profile and in the headings on the second page represent whether a program's cost estimate had increased ( ^ ) , decreased ( v ) , or stayed within ( i.e. , not to exceed threshold ) planned cost estimate ( i.e. , (  ) and whether the program's schedule estimate had slipped ( > ) ,been accelerated to meet milestones earlier than planned ( < ) , or stayed within ( i.e. , not to exceed threshold ) planned schedule estimate (  ) of meeting milestones .

the second page of each profile provides detailed information on each program's status , costs , schedule , and performance .

in the status section , we discuss recent and upcoming milestones and events for each program .

in the cost section , we identify the extent to which the program's life - cycle cost estimate has changed from its first acquisition program baseline , as well as the causes for any changes identified .

in the schedule section , we discuss the extent to which the program's schedule has changed from its first acquisition program baseline , and the causes for any schedule changes identified .

in the performance section , we identify the extent to which each program has met its established measures , as well as discuss the results of system performance tests .

these performance ratings represent a point - in - time assessment as reported by the program .

system performance targets were rated as “met” when ( 1 ) system tests were passed with no deficiencies or limitations , ( 2 ) the program fully met all of its key performance parameters , or ( 3 ) a program had addressed all deficiencies or limitations that were identified during system tests .

system performance was rated as “not fully met” when a program either ( 1 ) did not fully pass system testing and was still in the process of addressing the deficiencies or limitations identified during system testing or ( 2 ) did not pass system testing and subsequently removed the problematic functionality from the system in order to pass subsequent system tests , instead of fixing the problematic functionality and keeping it in the planned release of the system .

army tactical mission command ( tmc ) tmc is a suite of products — comprised of hardware and software equipment and elements — that are intended to provide the army commanders and their staff with mission command capabilities , such as real - time situational awareness and a user - defined common operational picture .

tmc products are fielded worldwide and are intended to support decision - making , planning , rehearsal , and execution management .

tmc is now engaged in transitioning to the web - based command post computing environment .

one key element of this new environment — known as tactical applications — is aimed to minimize administrative burdens on the user , and simplify the overall mission command collaborative experience .

all of the products included in tmc are post - development and in production .

the program continues to field equipment and perform a technical refresh of the hardware and software in the field .

the program is also working to resolve sustainment metric issues and updating their life - cycle sustainment plan .

exceeded planned cost estimate ( ^ ) tmc's planned total life - cycle cost estimate has increased by 19 percent from the program's first acquisition program baseline estimate of approximately $1.97 billion .

specifically , as of january 2016 , the life - cycle cost estimate was approximately $2.34 billion .

program officials reported that the increased costs were attributed to research and development testing and evaluation cost estimate breach that was reported to congress .

the army's tmc program estimated program development cost increased by 45 percent over the original estimate due to program scope changes derived from the realignment of command post of the future as a foundation for mission command collapse , the integration of personalized assistant that learns , and the incorporation of future force requirements .

stayed within planned schedule estimate (  ) as of january 2016 , the program had experienced a 3 month slippage in its full deployment date compared to its first acquisition program baseline of september 2018 .

the slippage was within the pre - established threshold allowance to account for minor shifts in program schedule .

program officials stated that , although the command post of the future product is 95 percent fielded and is on schedule to reach full deployment by december 2018 , continued support of the command post computing environment is needed beyond fiscal year 2019 .

program officials considered the slippage to be of low risk and will continue to operate within the planned schedule estimate .

as of january 2016 , the tmc program met all three of its key performance parameters , including net - centric military operations , disseminate orders with future army and joint c2 systems , and displaying unified information on subject matters , such as friendly and enemy forces .

navy common aviation command and control system ( cac2s ) increment 1 cac2s is an integrated and coordinated modernization effort for the equipment of the marine air command and control system and is intended to provide enhanced capability for three defense centers to support aviation employment in joint , combined , and coalition operations .

cac2s provides the tactical situational display , information management , sensor and data link interface , and operational facilities for planning and execution of marine aviation missions within the marine air ground task force .

it is intended to replace existing aviation command and control equipment from 12 legacy systems .

cac2s increment 1 will eliminate the air command and control systems and will capability for aviation combat direction and air defense functions by providing a single networked system .

cac2s increment 1 , which comprises of two phases , is currently in post - milestone c where the phase 2 system is now in development .

cac2s's current work consists of developmental testing and the production of the limited deployment units in preparation for the march 2016 initial operational test and evaluation .

to help achieve its goals of a successful initial operation test and evaluation , a phase 2 milestone c decision was authorized in february 2015 for cac2s to procure four limited deployment units .

as of october 2015 , cac2s had delivered all four limited units to support current developmental testing .

production of the limited deployments units are on schedule and planning activities are underway .

regarding the developmental testing , program officials anticipate favorable test results after subsequent software enhancements had been made to address software concerns identified in prior developmental testing .

exceeded planned cost estimate ( ^ ) as of october 2015 , cac2s's life cycle cost estimate was $2 billion , which was about a 477 percent increase from its first acquisition program baseline estimate of $347 million established in august 2000 .

as previously reported , factors that attributed to the cost increase were early challenges in estimating costs due to program scope growth and restructuring .

according to program documentation , operations and support expenditures of approximately $1.6 billion for the production of milestone c had been carried over into the program's total life - cycle cost estimate .

however , since our previous report , program documentation indicated that improvements to the cost position are being made .

specifically , the milestone c service cost position , dated february 2015 , produced a cost avoidance of $54.4 million compared to its 2010 cost assessment .

as of october 2015 , the program's latest life - cycle estimate relative to its november 2010 production acquisition program baseline cost estimate had decreased about 19 percent .

program officials attributed this decrease due to the program embracing the dod better buying power initiatives that benefitted from competitive market forces that drove down cost .

exceeded planned schedule estimate ( > ) as of october 2015 , cac2s increment 1's estimated full deployment date was march 2022 , which represented a 13 year and 9 month schedule slip from the program's first acquisition program baseline schedule estimate .

as previously reported , factors that attributed to the schedule delay included the addition of new requirements and program restructure .

program officials stated that subsequent to our prior report , the program has been executing in accordance within its approved schedule .

as of october 2015 , the estimated milestone c phase 2 was delayed by 6 months from the program's production acquisition program baseline schedule but , as stated above , achieved its milestone .

program officials attributed this delay , in part , to administrative factors , which included the review and approval process of getting signature approval .

cac2s successfully achieved milestone c phase 2 approval in february 2015 but the acquisition decision memorandum had not been signed until march 2015 .

as of october 2015 , cac2s program documentation reported that it was meeting both of its key performance parameters related to net - ready and data fusion .

program officials stated that , during testing of the program's key performance parameters , its net - ready and data fusion performance targets were both met , while many attributes for the data fusion key performance parameter were consistently above the threshold for being met .

air force defense enterprise accounting and management system ( deams ) increment 1 the deams increment 1 program is intended to provide the air force with the entire spectrum of financial management capabilities , including collections ; commitments and obligations ; cost accounting ; general ledger ; funds control ; receipts and acceptance ; accounts payable and disbursement ; billing ; and financial reporting deams is also intended to be a key component of dod's solution for achieving fully - auditable financial statements by september 30 , 2017 , as required by the national defense authorization act for fiscal year 2010 .

as of november 2015 , the deams program is working to achieve full deployment decision by february 2016 .

in august 2015 , the initial operational test and evaluation report , conducted by the air force operational test and evaluation center , indicated a number of findings requiring remediation prior to the february 2016 full deployment decision .

nevertheless , deams was granted a limited deployment decision , but the program experienced a significant change as a result of breaching the full deployment decision threshold for a timing issue only .

deams current work efforts consist of deployment to new users and remaining 35 sites , capability development for deployment , training new users , and resolving initial operational test and evaluation findings .

exceeded planned cost estimate ( ^ ) as of october 2015 , deams's latest life - cycle cost estimate was about $1.56 billion , which was about a 9 percent increase from its first acquisition program baseline estimate of approximately $1.43 billion — established in february 2012 .

program officials attributed this increase , in part , to program scope growth due to addition of requirements from increment 2 and the addition of a second oracle software upgrade projected for 2021 .

exceeded planned schedule estimate ( > ) deams experienced a 6 month slippage in its milestone c but successfully attained milestone c approval within the established threshold .

program officials did not provide a rationale for factors that attributed to this delay but maintained that the program operated within the threshold requirements .

deams also experienced a 1 year slippage in its full deployment decision date — currently scheduled for february 2016 .

program officials attributed this slippage due to findings identified deams's initial operational test and evaluation report .

did not fully meet system performance targets as of october 2015 , deams program officials reported that it did not meet all of its nine key performance parameters .

specifically , deams did not meet five key performance parameters: balance with treasury , accurate balance of available funds , timely report , period - end processing , and net - ready .

for example , an initial operational test and evaluation report , listed above , identified system performance issues , which included unstable change management issues , transaction backlogs , and ineffective reporting tools .

subsequently , the air force operational test and evaluation center provided 29 recommendations for the air force to implement to support the successful fielding of deams increment 1 , 17 of which were documented as being completed , while corrective action for the remaining 12 are still underway .

the program is expected to demonstrate improvement before it will be authorized to be deployed to all users .

in addition to the contact name above , the following staff also made key contributions to this report: eric winter , assistant director ; ronalynn ( lynn ) espedido ; corey evans ; rebecca eyler ; franklin jackson ; kate nielsen ; john ortiz ; kathleen sharkey ; and jeanne sung .

