the consequences of a poorly planned census are high given the billions of dollars spent on the enumeration and the importance of collecting quality data .

therefore , a rigorous testing and evaluation program is a critical component of the census planning process because it helps the u.s. census bureau ( bureau ) assess activities and related information technology ( it ) systems that show promise for a more cost - effective head count .

in preparing for the 2010 census , the bureau conducted a field test in 2004 and plans additional tests for 2005 and 2006 .

it is important that these early assessments lead to a design that is sufficiently mature so that the dress rehearsal for the 2010 census , now planned for 2008 , will demonstrate the feasibility of the various operations and technologies planned for the decennial under conditions that are as close as possible to the actual census .

the bureau designed the 2004 census test to examine the feasibility of using ( 1 ) handheld computers ( hhc ) for field data collection ; ( 2 ) new methods for improving coverage ; ( 3 ) redesigned race and ethnicity ( hispanic origin ) questions ; and ( 4 ) improved methods for defining and identifying nursing homes , prisons , college dormitories , and similar facilities known collectively as “group quarters.” the bureau established these test objectives as part of a broader effort to modernize and re - engineer the 2010 census .

major goals of this initiative are to improve the accuracy , reduce the risk , and contain the cost of the 2010 census , estimates of which now exceed $11 billion .

a rigorous planning and testing program is critical to this effort .

as agreed with your offices , our objectives for this review were to assess the soundness of the design of the 2004 census test and the extent to which the bureau implemented the test consistent with its plans .

we also agreed to review the quality of its it security practices , and identify initial lessons learned from the test and the implications they have for the bureau's future plans .

to address these objectives , we reviewed applicable planning , it , and other documents , and interviewed knowledgeable bureau officials responsible for key operations and computer security .

we also made several visits to the two test sites — an urban location in the northwestern portion of queens borough , new york , and a rural location in south central georgia .

we conducted our work from november 2003 through november 2004 in accordance with generally accepted government auditing standards .

congress , gao , the department of commerce inspector general , and even the bureau itself have all noted that the 2000 census was marked by poor planning , which unnecessarily added to the cost , risk , and controversy of the national head count .

in january 2003 , we named the 2010 census a major performance and accountability challenge because of our growing concern over the numerous obstacles to a cost - effective enumeration as well as its escalating price tag .

more recently , we reported that while the bureau's preparations for the 2010 census appeared to be further along than at a similar point during the planning cycle for the 2000 census , considerable risks and uncertainties remained .

thus , it is imperative that the bureau adequately test the various components of its design for the 2010 census .

a rigorous testing program provides at least four major benefits .

first , testing allows the bureau to refine procedures aimed at addressing problems encountered in past censuses .

during the 2000 census , for example , group quarters were sometimes counted more than once or counted in the wrong location ; the wording of the race and ethnicity question confused some respondents , which in some cases resulted in lower quality data ; and following up with nonrespondents proved to be costly and labor - intensive .

a second benefit is that sound testing can assess the feasibility of new procedures and technologies , such as hhcs ( see fig .

1 ) , that have never before been used in a decennial census .

third , a rigorous testing program helps instill a comfort level among members of congress and other stakeholders that the bureau ( 1 ) has chosen the optimal design given various trade - offs and constraints and ( 2 ) has identified and addressed potential risks and will be able to successfully execute its plan .

such confidence building , developed through regular updates and open lines of communication , is essential for continuing congressional support and funding .

and finally , proper testing early in the decade will help the bureau to conduct a dress rehearsal in 2008 that fully assesses all aspects of the census design under realistic conditions .

because of various late requirement changes , certain procedures that were added after the 1998 dress rehearsal for the 2000 census were not properly tested .

as agreed with your offices , our objectives for this report were to ( 1 ) assess the soundness of the bureau's design for the 2004 census test and whether the bureau implemented the test consistent with its plans , ( 2 ) review the quality of the bureau's it security practices , and ( 3 ) identify initial lessons learned from conducting the test and their implications for the 2010 census .

to assess the soundness of the design we reviewed pertinent documents that described the bureau's test and evaluation plans .

we systematically rated the bureau's approach using a checklist of design elements that , based on our review of program evaluation literature , are relevant to a sound study plan .

for example , we reviewed the bureau's approach to determine , among other things , ( 1 ) how clearly the bureau presented research objectives , ( 2 ) whether research questions matched the research objectives , and ( 3 ) the appropriateness of the data collection strategy for reaching the intended sample population .

as part of our assessment of the bureau's test design , we also reviewed evaluations of the prior decennial census to determine the degree to which the new operations being tested addressed problematic aspects of the 2000 census .

however , we did not assess the bureau's criteria in selecting its objectives for the 2004 census test .

to determine if the bureau implemented the test consistent with its plans , we made multiple site visits to local census offices in thomasville , georgia ; and queens borough , new york .

during these visits , we interviewed local census office mangers and staff , observed various data collection activities , and attended weeklong enumerator training .

we observed a total of 20 enumerators as they completed their daily nonresponse follow - up assignments — half of these were in southern georgia , in the counties of thomas , colquitt , and tift , and half were in queens ( see fig .

2 for maps of the test site areas ) .

the results of these observations are not necessarily representative of the larger universe of enumerators .

to evaluate the quality of the bureau's it security practices , we assessed risk management documentation associated with it systems and major applications for the 2004 census test .

we based our determination on applicable legal requirements , bureau policy , and leading practices described in our executive guide for information security management .

we also interviewed key bureau officials associated with computer security .

to identify lessons learned from the 2004 census test , we met with officials from the bureau's decennial management division regarding overall test plans and with officials from its technologies management office about using hhcs .

bureau officials and census workers from both test locations also provided suggestions on improving census operations .

we requested comments on a draft of this report from the secretary of commerce .

on december 20 , 2004 , the under secretary for economic affairs , department of commerce , forwarded written comments from the bureau ( see app .

i ) .

we address these comments in the “agency comments and our evaluation” section at the end of this report .

the bureau designed a sound census test and generally implemented it as planned .

however , in looking ahead , the bureau's planning and investment decisions could benefit from analyzing ( 1 ) the degree to which hhcs contributed to the bureau's cost containment goal and ( 2 ) the results of the targeted second mailing , an operation designed to increase participation by sending a follow - up questionnaire to nonresponding households .

future tests could also be more informative if the bureau developed quantifiable productivity and other performance requirements for the hhcs and then used the 2006 test to determine whether the devices are capable of meeting those requirements .

collectively , these refinements could provide bureau officials with better information to guide its it and other design decisions , as well as refine future census tests .

the design of the 2004 census test contained many components of a sound study ( see table 1 ) .

for example , the bureau identified test objectives , designed related research questions , and described a data collection strategy appropriate for a field test .

the bureau also developed evaluation plans for each of the test's 11 research questions , and explained how stakeholders were involved with the design , as well as how lessons learned from past studies were incorporated .

although the bureau plans to evaluate various aspects of the 2004 test , it does not currently plan to assess the impact that the hhcs and targeted second mailing had on cost savings and productivity .

according to the bureau , the census test was focused more on determining the feasibility of using the hhcs and less on the devices' ability to save money .

likewise , the bureau said it is not assessing the impact of the targeted second mailing because the operation is not one of its four test objectives for improving ( 1 ) field data collection using the hhc , ( 2 ) the coverage of undercounted groups , ( 3 ) questions about race and ethnicity , and ( 4 ) methods for defining special places and group quarters .

these decisions might be shortsighted , however , in that the bureau included the hhcs and targeted second mailing in the 2010 census design , in part , to reduce staff , improve productivity , and control costs .

for example , bureau studies have shown that sending out replacement questionnaires could yield a gain in overall response of 7 to 10 percent from households that do not respond to the initial census mailing , and thus generate significant cost savings by eliminating the need for census workers to obtain those responses via personal visits .

thus , information on the degree to which the hhcs and second mailing contribute to these key goals could help inform future budget estimates , investment and design decisions , as well as help refine future census tests .

moreover , the feasibility of a targeted second mailing is an open question , as the bureau has never before included this operation as part of a decennial census .

although a second mailing was part of the original design for the 2000 census , the bureau had abandoned it because it was found to be logistically unworkable .

a bureau official said that the second mailing was included in the 2004 test only to facilitate the enumeration process , and it would be better tested in a larger scale operation such as the 2008 dress rehearsal .

however , we believe that it would be more prudent to assess the second mailing earlier in the census cycle , such as , during the 2006 test so that its basic feasibility could be assessed , any refinements could be evaluated in subsequent tests , and the impact on savings could be estimated more accurately .

while the design of the 2004 test was generally sound , refinements could strengthen the next field test in 2006 .

opportunities for improvement exist in at least two areas: ensuring that ( 1 ) the hhcs can meet the demanding requirements of field data collection and ( 2 ) management of the local census offices mirrors an actual enumeration as much as possible .

with respect to the hhcs , because they replace the paper version of the nonresponse follow - up questionnaire the devices must function effectively .

further , this test was the first time the bureau used the hhcs under census - like conditions so their functionality in an operational environment was unknown .

bureau officials have acknowledged that for the 2004 test they had no predefined indicators of success or failure other than if there was a complete breakdown the test would be halted .

this is a very low standard .

now that the bureau has demonstrated the basic functionality of the computers , it should next focus on determining the specific performance requirements for the hhcs and assess whether the devices are capable of meeting them .

for example , the bureau needs productivity benchmarks for the number of interviews per hour and per day that is expected per census worker .

durability measures , such as how many devices were repaired or replaced , should be considered as well .

assessing whether the hhcs can meet the requirements of nonresponse follow - up will help inform future design and investment decisions for whether or not to include the devices in the 2010 design .

ensuring that key positions in the local census offices are filled from the same labor pool as they would be in an actual decennial census could also enhance future census tests .

such was not the case during the 2004 test when , according to the bureau , because of difficulties finding qualified applicants , it used an experienced career census employee to manage the overall day - to - day operations of the local census office at the queens test site .

another career employee occupied the office's regional technician slot , whose responsibilities included providing technical and administrative guidance to the local census office manager .

in the actual census , the bureau would fill these and other positions with temporary employees recruited from local labor markets .

however , because the bureau staffed these positions with individuals already familiar with census operations and who had ties to personnel at the bureau's headquarters , the queens test may not have been realistic and the test results could be somewhat skewed .

the bureau operated a number of it systems in order to transmit , manage , and process data for the test .

the equipment was located at various places including the bureau's headquarters in suitland , maryland ; its national processing center in jeffersonville , indiana ; a computer facility in bowie , maryland ; as well as the new york and georgia test sites .

under title 13 of the u.s. code , the bureau must protect from disclosure the data it collects about individuals and establishments .

thus , the bureau's it network must support both the test's telecommunications and data processing requirements , as well as safeguard the confidentiality and integrity of respondents' information .

the federal information security management act of 2002 ( fisma ) requires each agency to develop , document , and implement an agency - wide information security program for the it systems that supports its operations .

although the bureau took a number of steps to implement it security over the systems used for the test , based on available information , the bureau did not meet several of fisma's key requirements .

as a result , the bureau could not ensure that the systems supporting the test were properly protected against intrusion or unauthorized disclosure of sensitive information .

for example: it inventory was not complete .

fisma requires an inventory of major information systems and interfaces .

the bureau did not have a complete inventory that showed all applications and general support it systems associated with the test .

without such information , the bureau could not ensure that security was effectively implemented for all of its systems used in the test , including proper risk assessments , adequate security plans , and effectively designed security controls .

there was not sufficient evidence that the bureau assessed all of the devices used in the test for vulnerabilities , or that it corrected previously identified problems .

fisma requires that agencies test and evaluate the effectiveness of information security policies , procedures , and practices for each system at least annually and that agencies have a process for remediating any identified security weaknesses .

since the bureau could not provide us with a complete inventory of all network components used in the test , we could not determine if the bureau's tests and evaluations were complete .

moreover , there was not always evidence about whether the bureau had corrected past problems or documented reasons for not correcting them .

as a result , the bureau did not have adequate assurance that the security of systems used in the 2004 census test was adequately tested and evaluated or that identified weaknesses were corrected on a timely basis .

assessments were not consistent .

fisma requires agencies to assess the risks that could result from the unauthorized access , use , disclosure , disruption , modification , or destruction of information or information systems .

although the bureau performed risk assessments for some of the it components used in the 2004 census test , the documentation was not consistent .

for example , documentation of information sensitivity risks ( high , medium , and low ) for confidentiality , integrity , and availability of information were not consistent and did not always follow bureau policy .

in addition , documents showed different numbers of file servers , firewalls , and even different names of devices .

without complete and consistent risk assessment documentation , the bureau had limited assurance that it properly understood the security risks associated with the test .

the bureau did not always follow its own risk policies .

fisma requires the implementation of policies and procedures to prevent and / or mitigate security risks .

although bureau policies allowed for the waiver of security policies , if appropriate , we noted that such policies were not always followed .

for example , a waiver for the test of certain password policies was not properly documented and other system documents were not properly updated to reflect the waiver .

as a result , the risk assessment for the 2004 census test did not properly identify the related risks and did not identify any compensating controls to reduce the risk to an acceptable level .

as the bureau plans future tests and the census itself , it will be important for it to strengthen its it security risk management practices , ensuring they fully adhere to fisma requirements and its own it security policies .

the 2004 test suggests that while certain census initiatives have potential , formidable challenges remain .

for example , the hhcs show promise in that enumerators were successful in using them to collect data from nonrespondents and remove late mail returns .

still , they were not street ready as they experienced transmission and memory overload problems .

likewise , automated maps were difficult to use , certain questionnaire items confused respondents , and enumerators did not always follow interview protocols .

these problems shed light on issues in need of the bureau's attention as it develops solutions and incorporates refinements for additional testing in the years ahead .

the bureau purchased 1,212 hhcs for the test at a total cost of about $1.5 million .

the devices were sent directly to the two test sites packaged in kits that included a battery , ac adaptor , and modem card for transmitting data via the telephone .

the hhcs were also equipped with a global positioning system ( gps ) , a satellite - based navigational system to help enumerators locate street addresses .

the bureau anticipates the hhcs will allow it to eliminate the millions of paper questionnaires and maps that enumerators need when following up with nonrespondents , thereby improving their efficiency and reducing overall costs .

because the bureau had never used hhcs in the decennial census , an important goal of the test was to see whether enumerators could use them for interviewing nonrespondents ( see fig .

3 ) .

most workers we observed had little trouble using the device to complete the interviews .

in fact , most said they were pleased with the hhc's overall functionality , durability , screen clarity , and the ability to toggle between the questionnaire and taking gps coordinates .

another important function of the hhc was removing late mail returns from each enumerator's assignment area ( s ) .

between the georgia and queens test sites , over 7,000 late mail returns were removed , reducing the total nonresponse follow - up workload by nearly 6 percent .

the ability to remove late mail returns from the bureau's nonresponse follow - up workload could help save money in that it could eliminate the need for enumerators to make expensive follow - up visits to households that return their questionnaires after the mail - back deadline .

had the bureau possessed this capability during the 2000 census , it could have eliminated the need to visit nearly 773,000 late - responding households and saved an estimated $22 million ( based on our estimate that a 1-percentage point increase in workload could add at least $34 million in direct salary , benefits , and travel costs to the price tag of nonresponse follow - up ) .

because of the bureau's experience in 2000 , in our 2002 report on best practices for more cost - effective nonresponse follow - up , we recommended , and the bureau agreed , that it should develop options that could purge late mail returns from its nonresponse follow - up workload .

each day , enumerators were to transmit completed nonresponse follow - up cases to headquarters and receive assignments , software uploads , or both via a telephone modem ( see fig .

4 for a flowchart describing the file transmission process ) .

however , the majority of workers we interviewed had problems doing so , in large part because of technical reasons or because the bureau's training did not adequately prepare them for the complexity of the transmission procedure , which was a multistep process involving the connection of a battery pack , cables , and other components .

as reliable transmissions are crucial to the success of nonresponse follow - up , it will be important for the bureau to resolve these issues so that the hhcs can be reevaluated in 2006 .

difficulties began during training when the first transmission was supposed to occur and continued through the remainder of the test .

during that first transmission , the bureau needed to upload a number of software upgrades along with each census worker's first assignment .

many of these transmissions failed because of the volume of data involved .

thus , without cases , the trainees could not complete an important section of on - the - job training .

the bureau acknowledged that these initial problems could have been avoided if the final version of software had been installed on the devices prior to their distribution at training .

transmission problems persisted throughout nonresponse follow - up .

according to the bureau , during the first 2 weeks of this operation , successful data transmission occurred 80 percent of the time once a connection was made .

however , a number of enumerators never even established a connection because of bad phone lines , incorrect passwords , and improper setup of their modems .

other transmission problems were due to the local telecommunication infrastructure at both test sites .

for example , in georgia , older phone lines could not always handle transmissions , while in queens , apartment intercoms that used phone lines sometimes interrupted connections .

further , while the transmission rate ultimately increased to 95 percent - – roughly the maximum allowed by the technology — that level is still short of the performance level needed for 2010 .

during the 2000 census , a 95 percent success rate would have resulted in the failure to transmit around 30,000 completed questionnaires each day .

during the test , the bureau also had to contend with census workers who were “living off the grid” ; that is , they only used cellular phones and lacked landlines to transmit and receive data from their homes .

while individuals could make alternative arrangements , such as using a neighbor's telephone , an increasing number of people nationwide in the coming years might give up their landline service to rely on cellular phones , which could be problematic for the bureau .

bureau officials have noted that all these transmission problems need to be addressed before 2010 .

hhcs experienced memory overloads if too many assignment areas were loaded onto them .

an assignment area typically contains 40 housing units or cases that are assigned to an enumerator for nonresponse follow - up .

the design was to have an entire assignment area transmitted to the hhc even when as few as one case needed follow - up .

however , some enumerators' hhcs became overloaded with too much data , as cases had to be reassigned due to staff turnover , a larger - than - expected number of refusals , and reassignments resulting from language problems .

as such , when hhcs became overloaded they would crash and enumerators had to reconfigure them at the local census office , which made them less productive .

to the bureau's credit , during the test , it was able to work out a solution to avoid overloads by assigning individual cases instead of the entire assignment area to a census worker's hhc .

another problem that surfaced during the test was that the hhc's mapping feature was difficult to use .

to contain costs and increase efficiency , the bureau expects to replace paper maps with the electronic maps loaded on the hhcs for 2010 .

however , during the test , enumerators reported that they did not always use the mapping function because it ran slowly and did not provide sufficient information .

instead , they relied on local maps or city directories , and one worker explained that she found it easier to use an internet mapping service on her home computer to prepare for her route .

without the bureau's maps , enumerators might not properly determine whether a housing unit was located in the bureau's geographic database .

this verification is important for ensuring that housing units and the people who reside in them are in the correct census block , as local and state jurisdictions use census population figures for congressional redistricting and allocating federal funds .

enumerators were also unable to use the hhcs' “go back” function to edit questionnaires beyond a certain point in the interview .

in some cases , this led to the collection of incorrect data .

for example , we observed one worker complete half an interview , and then discover that the respondent was providing information on a different residence .

after the census worker entered the number of residents and their names , the “go back” function was no longer available and as a result that data could not be deleted or edited .

instead , the worker added information in the “notes section” to explain that the interview had taken place at the wrong household .

however , bureau officials told us that they had not planned to review or evaluate these notes and were not aware that such address mix - ups had been documented in the notes section .

to the extent address mix - ups and other inconsistencies occur and are not considered during data processing , accuracy could be compromised .

in earlier censuses when the bureau used paper questionnaires , if workers made mistakes , they could simply erase them or record the information on new forms .

as mistakes are inevitable , it will be important for the bureau to ensure that the hhcs allow enumerators to edit information , while still maintaining the integrity of the data .

we found that questions designed to improve coverage and better determine race and ethnicity were awkward for enumerators to ask and confusing for respondents to answer .

consequently , enumerators sometimes did not read the questions exactly as worded , which could adversely affect the reliability of the data collected for these items , as well as the bureau's ability to evaluate the impact of the revised questions .

our observations also highlight the importance of ensuring that workers are trained to follow interview protocols ; this issue will be discussed later in this report .

while the bureau attempts to count everyone during a census , inevitably some people are missed and others are counted more than once .

to help ensure that the bureau properly counts people where they live , the bureau revised and assessed its residency rules for the 2004 census test .

for example , under the residence rules , college students should be counted at their campus addresses if they live and stay there most of the time .

the bureau also added two new coverage questions aimed at identifying household residents who might have been missed or counted in error ( see fig .

5 for coverage questions ) .

enumerators were to show respondents flashcards with the residence rules to obtain the number of people living or staying in the housing unit and to read the two coverage questions .

however , during our field visits we noted that they did not consistently use the flashcards , preferring to summarize them instead .

likewise , enumerators did not always ask the new coverage questions as written , sometimes abbreviating or skipping them altogether .

a frequent comment from the workers we spoke with was that the two new coverage questions were awkward because the questions seemed redundant .

indeed , one census worker said that he asked the overcount and undercount questions more times than not , but if people were in a hurry , he did not ask the questions .

during one of these hurried interviews , we observed that the census worker did not ask the questions and simply marked “no” for the response .

collecting reliable race and ethnicity data is an extremely difficult task .

both characteristics are subjective , which makes accurate measurement problematic .

in 2003 , the bureau tested seven different options for formatting the race and ethnic questions , and selected what it thought was the optimal approach to field test in 2004 .

the bureau planned to examine respondent reaction to the new race and hispanic origin questions by comparing responses collected using the paper questionnaire to responses recorded on the hhcs during nonresponse follow - up .

one change the bureau planned to analyze was the removal of the “some other race” write - in option from the questionnaire .

in 2000 , the bureau found that when given this option , respondents would check off “some other race,” but did not always write in what their race was .

thus , in the 2004 test , the bureau wanted to assess respondents' reaction to the removal of the “some other race” write - in option .

specifically , the bureau wanted to see whether respondents would skip the item or select from one of the other options given .

however , we found that the bureau formatted the race question on the paper questionnaire differently from the question on the hhc .

as shown in figure 6 , on the paper version , there is not a category for another race other than those categories listed , thus forcing respondents to select a category or skip the question entirely .

this contrasts with the hhcs where , if respondents do not fit into one of the five race categories , the questionnaire format allows them to provide an “other” response and enumerators can record their answers .

in fact , the hhc requires enumerators to record a response to the race question and will not allow the interview to continue until a response is entered .

as a result , the data recorded by the two questionnaire formats are not comparable as they could produce different data depending on the data collection mode .

according to the bureau , it formatted the paper version of the race question differently from the hhc version because it considered the “other” response option on the hhc a respondent comment and not a write - in response .

nevertheless , if the bureau's purpose is to measure respondent reaction to eliminating the write - in option , it is uncertain what conclusions the bureau will be able to draw given that this option , even though in the form of a comment , is still available to the respondent during the nonresponse follow - up interview .

as was the case with the coverage measurement question , enumerators at both test locations did not always follow proper interview procedures because they felt the questions were awkward to ask and confused respondents .

for example , some workers did not use the flashcards designed to guide respondents in selecting categories for their race and ethnicity and to ensure data consistency .

one census worker said that rather than use the flashcards or ask the questions , he might “eyeball” the race and ethnicity .

another worker said that most people laughed at the spanish , hispanic , or latino origin question and she had complaints about the wording of this question .

a third census worker noted that he was “loose with the questions” because he could pose them better .

like lapses to the coverage improvement procedures for the 2004 census test , deviating from the interview procedures for the new race and ethnicity questions may affect the reliability of the data and the validity of the bureau's conclusions concerning respondent reaction to these questions .

since the 2004 census test , the 2005 consolidated appropriations act required that the bureau include “some other race” as a category when collecting census data on race identification .

consequently , the bureau said it will include this category on all future census tests and the 2010 census itself .

thus , while research into eliminating the “some other race” category is now moot , it will still be important for the bureau to have similar formats for the hhcs and paper questionnaires so that similar data can be captured across modes .

likewise , it will be important for the wording of those questions to be clear and for enumerators to follow proper procedures during interviews .

as noted previously , under its residence rules , the bureau enumerates people where they live and stay most of the time .

to facilitate the count , the bureau divides residential dwellings into two types: housing units , such as single - family homes and apartments , and group quarters , which include dormitories , prisons , and nursing homes .

the bureau tested new group quarters procedures in 2004 that were designed to address the difficulties the bureau had in trying to identify and count this population group during the 2000 census .

for example , communities reported instances where prison inmates were counted in the wrong county and residents of college dormitories were counted twice .

one refinement the bureau made was integrating its housing unit and group quarter address lists in an effort to avoid counting them once as group quarters and again as housing units , a common source of error during the 2000 census .

census workers were then sent out to verify whether the dwellings were in fact group quarters and , if so , to classify the type of group quarter using a revised “other living quarters facility” questionnaire .

a single address list could , in concept , help reduce the duplicate counting that previously occurred when the lists were separate .

likewise , we observed that census workers had no problems using the revised facility questionnaire and accompanying flashcard that allowed the respondent to select the appropriate type of living facility .

this new procedure addresses some of the definitional problems by shifting the responsibility for defining the group quarter type from the bureau to the respondent , who is in a better position to know about the dwelling .

another change tested in 2004 was the classification of group homes , which in 2000 was a part of the group quarter inventory .

group homes are sometimes difficult for census workers to spot because they often look the same as conventional housing units ( see fig .

7 ) .

as a result , they were sometimes counted twice during the 2000 census — once as a group quarter , and once as a housing unit .

for the 2004 test , the bureau decided to treat group homes as housing units and include them in the housing unit list .

early indications from the bureau suggest that including group homes as housing units , whereby they receive a short - form questionnaire in the mail , may not work .

according to the bureau , the format of the short form is not well suited to group home residents .

for example , the questionnaire asks for the “name of one of the people living or staying here who owns or rents this place.” since the state or an agency typically owns group homes , these instructions do not apply .

the bureau stated that it plans to reassess how it will identify and count people living in group homes .

we identified other problems with the bureau's group quarters validation operation during the 2004 census test .

for example , we were told that census workers were provided maps of the areas they were assigned but needed maps for adjoining areas so that they could more accurately locate the physical location of the group quarters .

in georgia , where workers used address data from the 2000 census , the crew leader explained that approximately one - third of all the addresses provided were incorrectly spotted on maps and had to be redone .

they also lacked up - to - date instructions — for example , they did not know that they were to correct addresses rather than just delete them if the addresses were wrong .

further , census workers said that scenarios in the manual and classroom training were based on perfect situations ; thus , they did not provide adequate training for atypical settings or when problems arose .

the success of the census is directly linked to the bureau's ability to train enumerators to do their jobs effectively .

this is a tremendous task given the hundreds of thousands of enumerators the bureau needs to hire and train in just a few weeks .

further , enumerators are temporary employees , often with little or no prior census experience , and are expected , after just a few days of training , to do their jobs with minimal supervision , under sometimes difficult and dangerous conditions .

moreover , the individuals who train enumerators — crew leaders — are often recent hires themselves , with little , if any , experience as instructors .

overall , few , if any , organizations face the training challenges that confront the bureau with each decennial population count .

to train the 1,100 enumerators who conducted nonresponse follow - up for the 2004 test , the bureau employed essentially the same approach it has used since the 1970 census: crew leaders read material word - for - word from a training manual to a class of 15 to 20 students .

the notable exception was that in transitioning from a paper questionnaire to the hhcs , the bureau lengthened the training time from 3 days to 5 days .

however , given the demographic and technological changes that have taken place since 1970 , the bureau might want to explore alternatives to this rigid approach .

as noted earlier , during nonresponse follow - up , enumerators experienced a variety of problems that could be mitigated through improved training .

the problems included difficulties setting up equipment to transmit and download data ; failure to read the coverage and race / ethnicity questions exactly as worded ; and not properly using the flashcards , which were designed to help respondents answer specific questions .

most of the shortcomings related to training that we observed during the test were not new .

in fact , the bureau had identified these and a number of other training weaknesses in its evaluation of the 2000 census , but it is clear they have not been fully resolved .

thus , as the bureau plans for the 2010 census , it will be important for it to resolve long - standing training problems as well as address new training issues , such as how best to teach enumerators to use the hhcs and their associated automated processes .

our observations of the test point to specific options the bureau might want to explore .

they include ( 1 ) placing greater emphasis on the importance of following prescribed interview procedures and reading questions exactly as worded ; ( 2 ) supplementing verbatim , uniform training with modules geared toward addressing the particular enumeration challenges that census workers are likely to encounter at specific locales ; and ( 3 ) training on how to deal with atypical situations or respondent reluctance .

to help evaluate its future training needs , the bureau hired a contractor to review the training for the 2004 test and recommend actions for improving it .

from gao's work on assessing agencies' training and development efforts , we have developed a framework that can also help in this regard .

though too detailed to discuss at length in this report , highlights of the framework , and how they could be applied to census training , include: 1. performing proper front - end analysis to help ensure that the bureau's enumerator training is aligned with the skill and competencies needed to meet its field data collection requirements and work processes and that the bureau leverages best practices and lessons learned from training enumerators and from past experience ; 2. identifying specific training initiatives that in conjunction with other strategies , improve enumerators' performance and help the bureau meet its goal of collecting high - quality data from nonrespondents ; 3. ensuring effective and efficient delivery of training that reinforces new and needed competencies , skills , and behaviors without being wedded to past , and perhaps outmoded , methods ; and 4. evaluating the training to ensure it is addressing known skill and competency weaknesses through such measures as assessing participant reactions and changes in enumerators' skill levels and behaviors .

several key features of the 2004 test were not test ready ; that is , they were not fully functional or mature when they were employed at the test sites .

this is a serious shortcoming because it hampered the bureau from fully evaluating and refining the various census - taking procedures that will be used in subsequent tests and the actual census in 2010 .

further , to the extent these features were integrated with other operations , it impeded the bureau from fully assessing those associated activities as well .

our work , and that of the department of commerce inspector general , identified the following areas where the bureau needed to be more prepared going into the test: the hhcs crashed , in part , because earlier testing did not identify software defects that caused the download of more data to the hhcs than their memory cards could hold .

transmission failures occurred during enumerator training , in part , because the hhcs were shipped without the latest version of needed software .

although the bureau ultimately provided the latest software after several weeks , the upgraded version was unavailable for training field operations supervisors and crew leaders and for the initial enumerator training .

according to the department of commerce inspector general , the bureau finalized the requirements for the new group quarter definitions too late for inclusion in group quarters training manuals .

consequently , the training lacked certain key instructions , such as how to categorize group homes .

the bureau experienced other glitches during the test that with better preliminary testing or on - site dry runs , might have been detected and possibly addressed before the test started .

these included the slow start - up of the hhc's mapping function , and the tendency for apartment house intercoms to interrupt transmissions .

an important objective of any type of test is to identify what is working and where improvements are needed .

thus , it should not be surprising , and , in fact , should be expected and commended , that shortcomings were found with some of the various activities and systems assessed during the 2004 test .

we believe that the deficiency is not the existence of problems ; rather it is the fact that several components were incomplete or still under development going into the test , which made it difficult for the bureau to gauge their full potential .

the bureau had a similar experience in the dress rehearsal for the 2000 census , when , because a number of new features were not test ready , the bureau said it could not fully test them with any degree of assurance as to how they would affect the head count .

because of the tight time frames and deadlines of the census , the bureau needs to make the most of its limited testing opportunities .

thus , as the bureau plans for the next field test in 2006 and the 2008 dress rehearsal , it will be important for the bureau to ensure the various census operations are fully functional at the time of the test so they can be properly evaluated .

the bureau is well aware that a successful enumeration hinges on early research , development , testing , and evaluation of all aspects of the census design .

this is particularly true for the 2010 census for which , under its current plan , the bureau will be relying on hhcs and other methods and technologies that ( 1 ) have never been used in earlier censuses and ( 2 ) are mission critical .

consequently , the 2004 test was an important milestone in the 2010 life cycle because it demonstrated the fundamental feasibility of the bureau's basic design and allows the bureau to advance to the next and more mature phase of planning and development .

nevertheless , while the test revealed no fatal flaws in the bureau's approach , the results highlighted serious technical , training , methodological , and procedural difficulties that the bureau will need to resolve .

since one of the purposes of testing is to determine the operational feasibility of the census design , it is not surprising that problems surfaced .

however , looking toward the future , it will be critical for the bureau to diagnose the source of these challenges , devise cost - effective solutions , and integrate refinements and fixes in time to be assessed during the next field test scheduled for 2006 .

it will also be important for congress to monitor the bureau's progress as it works to resolve these issues .

to facilitate effective census planning and development , and to help the bureau achieve its key goals for the census — reduce risks , improve accuracy , and contain costs , we recommend that the secretary of commerce direct the bureau to take the following eight actions: analyze the impact that hhcs and the targeted second mailing had on cost savings and other bureau objectives .

ensure the bureau's it security practices are in full compliance with applicable requirements , such as the fisma , as well as its own internal policies .

enhance the reliability and functionality of hhcs by , among other actions , ( 1 ) improving the dependability of transmissions , ( 2 ) exploring the ability to speed up the mapping feature , ( 3 ) eliminating the causes of crashes , and ( 4 ) making it easier for enumerators to edit questionnaires .

define specific , measurable performance requirements for the hhcs and other census - taking activities that address such important measures as productivity , cost savings , reliability , durability , and test their ability to meet those requirements in 2006. review and test the wording and formatting of the coverage and race / ethnicity questions to make them less confusing to respondents and thus help ensure the collection of better quality data , and ensure they are formatted the same way on both the hhc and paper versions of the census form .

develop a more strategic approach to training by ensuring the curriculum and instructional techniques ( 1 ) are aligned with the skills and competencies needed to meet the bureau's data collection requirements and methodology and ( 2 ) address challenges identified in the 2004 test and previous censuses .

revisit group quarter procedures to ensure they allow the bureau to best locate and count this population group .

ensure that all systems and other census - taking functions are as mature as possible and test ready prior to their deployment for the 2006 test , in part by conducting small - scale , interim tests under the various conditions and environments the bureau is likely to encounter during the test and actual enumeration .

further , to ensure the transparency of the census - planning process and facilitate congressional monitoring , we also recommend that the secretary of commerce direct the bureau to regularly update congress on the progress it is making in addressing these and any other challenges , as well as the extent to which the bureau is on track for meeting the overall goals of the 2010 census .

the under secretary for economic affairs at the department of commerce forwarded us written comments from the census bureau on a draft of this report on december 20 , 2004 , which are reprinted in appendix i .

the bureau noted that the 2004 test was its first opportunity to assess a number of the new methods and technologies under development for 2010 , and emphasized the importance of a sustained , multiyear planning , testing , and development program to its census modernization effort .

the bureau generally agreed with seven of our nine recommendations , and described the steps it was taking to address our concerns .

the bureau also provided additional context and clarifying language and we have added this information to the report where appropriate .

specifically , the bureau generally agreed with our recommendations relating to improving it security practices , the reliability of the hhcs , training , testing , and enumeration procedures — and reported it was already taking a number of steps to address our concerns .

we commend the bureau for recognizing the risks and challenges that lie ahead and taking action to address them .

we will continue to monitor the bureau's progress in resolving these issues and update congress on a regular basis .

at the same time , the bureau took exception to our recommendations to ( 1 ) analyze the impact that hhcs and targeted second mailings had on cost savings and other bureau objectives , and ( 2 ) define specific , measurable performance requirements for the hhcs and other census - taking activities and test their ability to meet those requirements in 2006 .

with respect to the first recommendation , the bureau noted that it did not establish cost - savings and other impacts as test objectives , in part , because the bureau believes that the national sample mail test that it conducted in 2003 provided a better method for determining the boost in response rates that could accrue from a second mailing .

the bureau maintains that analyzing the impact of the second mailing would provide it with no more information beyond what it has already established from the 2003 test and would be of little value .

we believe this recommendation still applies because it will be important for the bureau to assess the impact of the targeted second mailing on other bureau objectives .

as we noted in the report , the bureau included the hhcs and targeted second mailing in the 2010 census design , in part , to reduce staff , improve productivity , and control costs .

further , as we also note in the report , the feasibility of a targeted second mailing is an open question .

thus , information on the degree to which the hhcs and second mailing contribute to these key goals could help inform future budget estimates , investment and design decisions , as well as help refine future census tests .

in short , the purpose of the analysis we recommend would not be to see whether these features of the 2010 census will produce cost - savings , but the extent of those savings and the impact on other bureau objectives .

with respect to the second recommendation , the bureau noted that it had “baseline assumptions” about productivity , cost - savings , and other measures for the 2004 census test and that a key objective of the test was to gather information to help refine these assumptions .

according to the bureau , this will also be a key objective of the 2006 census test , although its performance goal will not be whether it meets specific measures .

instead , the bureau intends to focus on successfully collecting information to further refine those assumptions .

as a result , the bureau believes the 2006 test will not be a failure if hhc productivity is not achieved , but that it will be a failure if productivity data are not collected .

the bureau's position is inconsistent with our recommendation which we believe still applies .

as noted in the report , we call on the bureau to define measurable performance requirements for the hhcs as well as take the next step and assess whether the hhcs can meet those requirements as part of the 2006 test .

this information is essential because it will help the bureau gauge whether hhcs can meet its field data collection needs in 2010 .

should the hhcs fail to meet these pre - specified performance requirements during the 2006 test , the bureau would need to rethink how it employs these devices in 2010 .

as agreed with your offices , unless you release its contents earlier , we plan no further distribution of this report until 30 days from its date .

at that time , we will send copies of this report to the secretary of commerce and the director of the u.s. census bureau .

copies will be made available to others on request .

this report will also be available at no charge on gao's home page at http: / / gao.gov .

please contact me at ( 202 ) 512-6806 or daltonp@gao.gov or robert goldenkoff , assistant director , at ( 202 ) 512- 2757 or goldenkoffr@gao.gov if you have any questions .

key contributors to this report were tom beall , david bobruff , betty clark , robert dacey , richard donaldson , elena lipson , ronald la due lake , robert parker , lisa pearson , and william wadsworth .

the government accountability office , the audit , evaluation and investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents at no cost is through gao's web site ( www.gao.gov ) .

each weekday , gao posts newly released reports , testimony , and correspondence on its web site .

to have gao e - mail you a list of newly posted products every afternoon , go to www.gao.gov and select “subscribe to updates. .

