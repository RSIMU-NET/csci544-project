i appreciate the opportunity today to provide an update on the department of veterans affairs' ( va ) plans for implementing a new disability appeals process while still attending to appeals under the current , or legacy , process .

va provides cash benefits to veterans for disabling conditions incurred in or aggravated by military service , paying about $72 billion to about 4.5 million veterans in fiscal year 2017 .

if veterans are dissatisfied with va's initial decision they can appeal — first to the veterans benefits administration ( vba ) and then , if not satisfied there , to the board of veterans' appeals ( board ) , a separate agency within va. for appeals resolved in fiscal year 2017 , veterans waited an average of approximately 3 years from the date they initiated their appeal to resolution by either vba or the board — and an average of 7 years for appeals resolved by the board .

due in part to the challenges va faces managing large workloads and deciding disability claims and appeals in a timely manner , in 2003 we designated va disability compensation , along with other federal disability programs , as one of the government's highest risk areas .

the veterans appeals improvement and modernization act of 2017 ( act ) makes changes to va's disability appeals process by replacing it with one that gives veterans various options either for further review by vba or to bypass vba and appeal directly to the board .

these changes may generally take effect no earlier than february 2019 , which is about 18 months from the date of enactment .

the act also built in flexibility for va regarding this time frame by stating that most of these changes will not take effect until 30 days after the secretary of veterans affairs certifies that the agency is prepared to carry out timely processing of appeals under the new and legacy appeals process , in addition to giving va the option of phasing in implementation of the new process at that time .

the act further required va to submit a comprehensive plan for implementing the new appeals process to the appropriate committees of congress and gao .

 ( va submitted its plan to gao on november 22 , 2017. ) .

the act delineates 22 legally required elements — some with subparts — for this plan .

in addition , the act requires va to provide progress reports to the appropriate committees of congress and gao at least every 90 days until the act's changes to the appeals process generally go into effect and then at least every 180 days after this date for 7 years .

va submitted progress reports in february , may , august , and november 2018 .

the act also includes a provision for gao to assess whether va's appeals plan comports with sound planning practices and identify any gaps in the plan .

in response , we have issued a series of reports and testimonies assessing va's plans .

in our march 2018 report , we concluded that while va's november 2017 plan reflected aspects of sound planning , improvements in planning were still needed to ensure successful appeals reform .

we recommended va's plan ( 1 ) address all legally required elements in the act ; ( 2 ) articulate how va will monitor and assess the performance of appeals processes ; ( 3 ) augment its project plan for implementation ; and ( 4 ) address risk more fully .

va agreed with our recommendations .

in a july 2018 testimony we concluded that va had updated its plan and taken some steps to address aspects of these four recommendations , but further steps were needed .

my statement today addresses va's recent progress in implementing the four recommendations in our march 2018 report , what aspects of those recommendations va has yet to address , and the risks these gaps pose for successful implementation of appeals reform .

for this statement , we reviewed va's most recent progress reports on its appeals reform plan , dated august and november 2018 , and information we received from va officials about steps taken to implement our march 2018 recommendations .

we assessed va's schedules and supporting documentation against applicable best practices in gao's schedule assessment guide .

we also interviewed va officials and reviewed information related to va's progress in addressing related recommendations from work that we conducted prior to enactment of the act .

the work upon which this statement is based was conducted in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

va's process for deciding veterans' eligibility for disability compensation begins when a veteran submits a claim to va. staff in one of vba's 57 regional offices assist the veteran by gathering additional evidence , such as military and medical records , that is needed to evaluate the claim .

based on this evidence , vba decides whether the veteran is entitled to compensation and , if so , how much .

a veteran dissatisfied with the initial claim decision can generally appeal within 1 year from the date of the notification letter sent by vba .

under the current appeals process ( now referred to by va as the legacy process ) , an appeal begins with the veteran filing a notice of disagreement .

vba then re - examines the case and generally issues a statement of the case that represents its decision .

a veteran dissatisfied with vba's decision can file an appeal with the board .

in filing that appeal , the veteran can indicate whether a board hearing is desired .

before the board reviews the appeal , vba prepares the file and certifies it as ready for board review .

if the veteran requests a hearing to present new evidence or arguments , the board will hold a hearing by videoconference or at a local vba regional office .

the board reviews the evidence and either issues a decision to grant or deny the veteran's appeal or refers the appeal back to vba for further work .

according to va's appeals plan , va intends to implement the act by february 2019 , by replacing the current appeals process with a process offering veterans who are dissatisfied with vba's decision on their claim five options .

two of those options afford the veteran an opportunity for an additional review of vba's decision within vba , and the other three options afford them the opportunity to bypass additional vba review and appeal directly to the board .

under the new appeals process , the two vba options will be: 1 .

request higher - level review: the veteran asks vba to review its initial decision based on the same evidence but with a higher - level official reviewing and issuing a new decision .

2 .

file supplemental claim: the veteran provides additional evidence and files a supplemental claim with vba for a new decision on the claim .

the veteran can also request a vba hearing .

the three board options will be: 3 .

request board review of existing record: the veteran appeals to the board and asks it to review only the existing record without a hearing .

4 .

request board review of additional evidence , without a hearing .

5 .

request board review of additional evidence , with a hearing .

in november 2017 , va initiated a test of the new vba higher - level review and supplemental claim options .

according to va's appeals plan , a purpose of this test — the rapid appeals modernization program ( ramp ) — is to reduce legacy appeals by providing veterans with a chance for early resolution of their claims within vba's new process .

participation in ramp is voluntary , but veterans must withdraw their pending legacy appeal to participate , according to va's appeals plan .

in our march 2018 report , we found that va's november 2017 plan for implementing a new disability appeals process while attending to appeals under way in the current ( legacy ) process , addressed 17 of 22 elements required by the act .

for the 5 remaining elements , we found that it partially addressed 4 elements related to implementation monitoring , productivity projecting , and workforce planning , and did not address 1 element related to identifying total resources .

this element called for delineating the resources needed by vba and the board to implement the new appeals process and address legacy appeals .

we recommended in march 2018 that va address all 22 required elements in the act in va's appeals plan to congress — including delineating resources required for all vba and board appeals options — using sensitivity analyses and results from its test , ramp , where appropriate and needed .

since our march 2018 report , va has taken some action on each of the five elements that we found were not fully addressed at that time .

for example , va added details related to projecting staff productivity , identifying total resources , as well as determining personnel requirements and productivity projections for processing appeals .

for identifying total resources , va added fte information for other offices that help implement the appeals process and prepared a model to project resource needs .

although va now addresses the 1 element related to projecting productivity , it only partially addresses 4 elements related to monitoring implementation , workforce planning , and delineating the total resources .

for example , as of november 2018 , va's plan does not contain metrics for monitoring implementation .

moreover , for total resources , the updated plan does not delineate the total resources required by vba and the board , such as the resources necessary for information technology and training .

we acknowledge that in some cases delineating total resources could prove challenging , such as delineating information technology resources for the legacy and new appeals processes .

we also acknowledge that implementing corrective actions to fully address these 4 elements may be challenging within the next several weeks , but we continue to believe va has an opportunity to further address these 4 elements as part of certifying the agency's readiness prior to the full implementation of the new process .

in our march 2018 report , we found gaps in va's planning for how it will monitor and assess performance of the new appeals process when it is implemented .

specifically , we reported that the plan did not ( 1 ) establish timeliness goals for two of the three board options ( i.e. , board review of additional evidence without a hearing and board review of additional evidence with a hearing ) ; ( 2 ) articulate aspects of performance important for managing appeals , such as accuracy of decisions , veteran satisfaction with the process , or cost ; ( 3 ) explain how the performance of the new appeals process would be compared to that of the legacy process ; or ( 4 ) explain how the agency would monitor relative workloads of , and resources devoted to , the new and legacy appeals processes .

to address these gaps , we recommended that va clearly articulate in its appeals plan how va will monitor and assess the new appeals process compared to the legacy process , including specifying a balanced set of goals and measures — such as timeliness goals for all vba appeals options and board dockets , and measures of accuracy , veteran satisfaction , and cost — and related baseline data .

articulating a balanced set of goals that cover key aspects of managing appeals is important to avoid promoting skewed behaviors ( eg , favoring timeliness over accuracy ) and to fully understanding performance .

in its progress reports , va addressed some but not all aspects of this recommendation ( see table 1 ) .

va has made progress in monitoring performance and addressing workload changes in its new and legacy appeals processes , but still lacks a complete set of balanced goals and measures .

as we noted in our july 2018 testimony , va has developed sensitivity models and other analyses to monitor and forecast future vba and board workloads , production , and staffing requirements to help va manage the legacy and new appeals processes .

however , vba and the board have yet to specify a complete set of balanced goals for monitoring the performance of the new appeals processes .

according to the november 2018 progress report , the board plans to develop timeliness goals after va fully implements the new appeals process .

until va fully develops a set of balanced goals and measures , the agency risks not fully understanding how well the reforms are performing .

regarding comparing the performance of the new and legacy appeals processes , va has previously reported that the agency plans to implement the reporting requirements in section 5 of the act .

this section requires va to report performance measures related to , among other things , timeliness , productivity , and outcomes , without specifying whether or how va should compare performance of the new versus legacy processes .

in november 2018 , vba and board officials told us they intend to use timeliness and productivity metrics from section 5 to compare the two processes .

however , in its updated plans to date , va has been reporting average timeliness of decisions made to date under ramp — va's test of the two vba options — without reporting the average time cases are pending .

moreover , va has not been reporting timeliness data on both decisions and pending cases according to the month that they entered into ramp , which present a more balanced indication of performance and trends .

in november 2018 vba and board officials told us they would consider reporting timeliness using a monthly cohort that reflects when appeals were filed .

vba and board officials also said they have taken steps to collect , through surveys , comparable information on veterans' satisfaction with the new and legacy appeals processes .

according to vba and board officials , they have pre - tested the surveys — which is considered a best practice by survey methodologists — and are coordinating the survey efforts with one another .

vba and board officials also told us that the agency will report on accuracy and outcomes ( grants and denials of claims ) in the new process .

however , they also stated that these measures would not provide a fair comparison with the legacy process because the act eliminated several of the requirements formerly required in the legacy appeals administrative processes .

although va officials said they would develop a plan for comparing the performance of the two appeals processes after the new process is fully implemented , they did not indicate how soon they would do so .

developing such a plan would better position the agency to fully understand whether the new process is an improvement .

our march 2018 report identified elements of a high - quality and reliable implementation schedule that were missing from va's master schedule for appeals reform .

specifically , we reported that va's high - level master schedule — which the agency included with its november 2017 plan — did not ( 1 ) include all key activities ; ( 2 ) show which activities must finish prior to the start of other activities , or the amount of time an activity could be delayed before the delay affects va's estimated implementation date ; ( 3 ) reflect interim goals and milestones for monitoring implementation ; or ( 4 ) assign resources for activities .

we recommended that va augment the master schedule for its appeals plan to reflect all activities — such as modifications to information technology systems — as well as assigned responsibilities , interdependencies , start and end dates for key activities for each workgroup , and resources .

these steps establish accountability and reduce overall risk of implementation failures .

in response to our recommendation , the board , vba and other va administrations made progress over time with developing and integrating underlying plans into the integrated master schedule ( ims ) in spring and summer 2018 .

according to va officials , va set a baseline schedule for implementing appeals reform in response to the potential february 2019 implementation date established in the act .

since november 2017 , va's plan and progress reports have stated that va uses an agency - wide governance structure to coordinate implementation , and regularly uses the schedule as a management tool for monitoring progress on appeals reform .

for example , the board's project manager meets regularly with those responsible for major activities to check progress , including weekly meetings with leadership , and identifies and corrects issues related to schedule execution .

in october 2018 , va provided us with lower - level schedules and information that allowed us to conduct a more detailed assessment of va's ims against applicable best practices criteria .

the six criteria we assessed lower - level schedules against were: capturing all activities: schedule should reflect all activities necessary to perform work to accomplish a project's objective .

sequencing activities: activities should be logically sequenced in the order they are to be carried out so that critical program dates can be met .

assigning resources: schedule should reflect all resources necessary to complete work , verify whether resources will be available , and identify any constraints .

verifying horizontal and vertical traceability: schedule should be rational and logically sequenced , account for interdependencies among activities , and provide a way to evaluate the current status ( horizontal traceability ) .

also , the various levels of a schedule — summary , intermediate , and detailed — should be consistent with one another and enable different teams to work to the same schedule expectations ( vertical traceability ) .

updating the schedule using actual progress and logic: maintain and continually update the schedule to reflect a realistic forecast of start and end dates of activities .

maintaining a baseline schedule: use original configuration of the program plan as a point of comparison for the current plan to manage scope , timeframes , and required resources .

we found that , while va has made progress with providing more detail , its master and underlying schedules only minimally met sound practices for project management .

specifically , as with our march 2018 assessment , we found that the schedule does not contain enough detail to manage the work or provide a realistic representation of the resources and time needed for this project .

for example , the schedule did not contain a work breakdown structure that defines the work , activities , and resources necessary to accomplish implementation .

moreover , half of all the remaining activities are missing logic that shows which activities must finish prior to the start of other activities .

in addition , the schedule contains an invalid critical path , meaning that the schedule does not present the amount of time that key activities could be delayed before such delays affect va's estimated implementation date .

without a valid critical path , management cannot focus on activities that will detrimentally affect the key program milestones and deliveries if they slip .

to address our march 2018 recommendation , va would need to ensure that all activities are accounted for , that scheduled activities appear in the correct order , that resources are properly allocated , that all activities appear on the critical path , and that a schedule risk analysis accounts for all risks .

we provide a more detailed explanation of our assessment results in appendix i .

in addition , establishing an overly optimistic schedule can reduce capacity for carrying out a project and potentially create pressure to sacrifice the quality of work activities to meet deadlines .

moreover , many of va's activities are slated to be concurrently completed just before implementation , posing a significant risk to implementing reform in february .

for example , according to va's schedule , the agency needs to complete 117 activities after january 1 , 2019 .

further , other va efforts to redesign or update key aspects of va's disability compensation process — including the veterans benefits management system ( vbms ) — were not driven by robust , comprehensive planning and did not achieve their schedule goals .

while va intends to start full implementation in february , we do not know the extent to which the lack of a robust schedule poses risks to successful and smooth implementation .

even if taking corrective actions to address our findings may not be feasible before february , incorporating such lessons learned into future project planning could help va improve its project scheduling capabilities .

in our march 2018 report , we found that va's appeals plan could more fully assess key risks related to implementing the new appeals process .

in particular , we found that va's plan did not include testing of new board options or clearly define how it would assess the ramp test of the vba - only options before implementing them more broadly .

further , we reported that va's plan had not comprehensively reflected key risks because the agency had not established a complete and balanced set of goals and measures , which are a necessary pre - condition to effectively assessing risk .

we recommended that va ensure that the appeals plan more fully addresses risk associated with appeals reform by , for example , assessing risks against a balanced set of goals and measures , articulating success criteria and an assessment plan for ramp , and testing or conducting sensitivity analyses of all five appeals options before fully implementing the new appeals process .

in its progress reports , va took many steps to address our recommendation , although key steps are remaining for va to better assess risks associated with implementing appeals reform and managing appeals workloads in the legacy process ( see table 2 ) .

sound redesign and change management practices both suggest that tests be rigorously monitored and evaluated and that further roll - out occur only after an agency takes any needed corrective action and determines that the new process is achieving previously identified success criteria .

until va takes these remaining steps , it may not have comprehensively addressed key risks to better position the agency for successful implementation of appeals reform .

in conclusion , va is undertaking an ambitious effort to reform its disability appeals process — while onboarding hundreds of new staff and implementing new technology — that will affect the lives of hundreds of thousands of veterans with disabilities for years to come .

consistent with our prior recommendations , va has made concrete progress to improve its planning for disability appeals reform while it attends to legacy appeals .

efforts such as resuming sensitivity analysis to monitor workloads and testing vba and board appeals options will provide useful information to guide va through the uncertainty often associated with process change .

however , va has reported it plans to fully implement the new disability appeals process in february 2019 even though it has yet to fully address our recommendations .

while fully implementing our recommendations prior to february 2019 may not be feasible , doing so would better position va to ensure successful implementation .

nevertheless , va should still work to increase clarity around its plans prior to fully implementing reform .

moreover , many of the principles of sound planning practices that informed our recommendations remain relevant during process change .

by continuing to improve its approach to performance measurement , scheduling , and risk management , even after implementation , va could better ensure that the new process meets veterans' needs .

chairman roe , ranking member walz , and members of the committee , this concludes my prepared statement .

i would be pleased to respond to any questions you may have at this time .

for further information about this testimony , please contact elizabeth h. curda at ( 202 ) 512-7215 or curdae@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this testimony .

other key contributors to this testimony include james whitcomb ( assistant director ) , juaná collymore , michele grgich , sara pelton , and rachel pittenger .

in addition , key support was provided by susan aschoff , mark bird , alex galuten , jason lee , sheila r. mccoy , almeta spencer , and walter vance .

for this testimony , we assessed the steps that the department of veterans affairs ( va ) has taken to address our march 2018 recommendations and what aspects remain unaddressed , including the extent to which va is using sound practices for scheduling key projects .

in summary , we identified several areas where va's most recent schedule falls short of sound practices .

further incorporating sound practices into future project planning could help va improve its project scheduling capabilities .

we reviewed va's integrated master schedule ( ims ) for the appeals reform effort and underlying sub - schedules to assess them against 6 of the 10 best practices , which we determined most relevant to our march 2018 recommendation that va augment its master schedule for va's appeals plan to reflect all activities — such as modifications to information technology systems — as well as assigned responsibilities , interdependencies , start and end dates for key activities for each workgroup , and resources , to establish accountability and reduce the overall risk of implementation failures .

specifically , we analyzed the following related scheduling best practices: ( 1 ) capturing all activities , ( 2 ) sequencing all activities , ( 3 ) assigning resources to all activities , ( 4 ) verifying that the schedule can be traced vertically and horizontally , ( 5 ) updating the schedule using actual progress and logic and ( 6 ) maintaining a baseline schedule .

we assessed va's lower - level schedules against these 6 best practices by: checking for specific problems that could hinder the schedule's ability to respond to changes .

for example , we: o examined if there are any open - ended activities ( i.e. , activities with no predecessor and / or successors ) , o searched for activities with poor logic: for example , start to start successor only or finish to finish predecessor only which represent dangling logic , or logic on summary tasks rather than attached to detailed tasks ( summary tasks are for organizing the schedule and should not drive the logic ) .

o looked for activities with constraints which keep the schedule rigid ( eg , start no earlier than , finish no later than , etc .

 ) , o determined if activities were resource loaded — which helps to cost out the schedule — and examine whether resources are over - allocated or not available when needed , o examined the schedule's critical path to determine whether or not it was reliable and logical , o examined schedule float and determined if it was reasonable , and o examined whether the schedule was baselined , its status cycle , and what deviations there were from the original plan .

we also determined if there were any actual start or finish dates recorded in the future and whether there was any broken logic between planned tasks .

we also interviewed va officials responsible for managing the schedule .

we scored each scheduling leading practice on a five - point scale: “not met” , “minimally met” , “partially met” , “substantially met” and “fully met.” we determined the characteristic assessment rating by assigning each best practice rating a number and taking the average .

our resulting conclusions based on this assessment are as follows: va's project schedule minimally meets the best practice of capturing all activities .

the schedule does not have well - defined start and finish milestones and there is not a project work breakdown structure ( wbs ) or corresponding wbs dictionary to define the work for each wbs element .

we were not able to independently verify contractor work or major handoffs and deliverables in the schedule .

in addition , there were activities with duplicate names , which could make communication difficult between va teams , particularly between team members who are responsible for updating and integrating multiple schedules .

va's project schedule minimally meets the best practice of sequencing activities .

there are issues with missing dependencies , dangling activities , summary links , constraints and lags that affect the schedule meeting this best practice .

specifically , of the remaining activities , 55 percent have missing logic , over 12 percent are dangling , 42 percent have date constraints and 4 percent have leads assigned .

when activities are not correctly linked , the program cannot use the integrated master schedule ( ims ) to identify disconnects or hidden opportunities and cannot otherwise promote efficiency and accuracy or control the program by comparing actual to planned progress .

when this happens , the schedule will not allow a sufficient understanding of the program as a whole , and users of the schedule may lack confidence in the dates and the critical path .

va's project schedule minimally meets the best practice of assigning resources .

while the schedule contains ‘task owner' assignments , the task owner information has no effect on the durations or forecasted start and finish dates of detailed activities .

information on resource needs and availability in each work period assists the program office in forecasting the likelihood that activities will be completed as scheduled .

if the current schedule does not allow insight into the current or projected allocation of resources , then the risk of the program's slipping is significantly increased .

va's project schedule minimally meets the best practice of verifying the schedule is traceable horizontally and vertically .

there was no evidence in the schedule of hand - offs within the schedule — that is givers and receivers are easily identifiable in the schedule .

we were unable to determine the relationship between lower - lever activities in the project schedule and higher - level activities and milestones in the management briefs provided to us .

specifically , we could not map the activities in the briefs to activities in the schedule .

this inconsistency also prevented the verification of dates between the project schedule and higher - level management documents , even with documents that were provided from the same month as the october schedule .

products and outcomes were not easily traced through the sequencing of effort in the project schedule .

in both cases the schedule did not respond appropriately to “shocks” ; that is , greatly increasing the durations of some activities to increase the overall time required to complete the project did not affect the dates of key milestones .

the duration increase of each activity did not affect the overall time line because the activity in question had a constraint that would not allow the project to appropriately extend .

va's project schedule minimally meets the best practice of updating the schedule using progress and logic .

date anomalies , such as planned dates in the past or actual dates in the future , were found .

the schedule was not current as of the date delivered to gao .

while officials report that they update the schedule regularly , a schedule narrative document does not accompany the schedule update that would detail changes to the current schedule and describe information such as the status of key milestone dates , changes in network logic , and a description of the current critical path ( s ) .

va's project schedule minimally meets the best practice of maintaining a baseline schedule .

officials said that the baseline schedule is the basis for performance measurement .

but while baseline start and baseline finish dates were provided in the initial schedule , its activities were too high level , obfuscating the calculation of detail variances in subsequent schedules .

there is also no evidence of a schedule basis document , which would include a general overview of the purpose of the schedule , other key basis information such as an overview of assumptions , rationale for durations specific to the cmr schedule , and required software settings .

there is also no evidence of performance measuring .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

