for more than 100 years , the federal government has invested in conducting education research and collecting education data .

more recently , the education sciences reform act of 2002 ( esra ) established the institute of education sciences ( ies ) and outlines a broad mission for ies to expand fundamental knowledge and understanding of education and to provide this information to a wide variety of stakeholders , including parents , educators , researchers , policymakers , and the general public .

as the primary research and evaluation arm of the department of education ( education ) , ies is charged with providing information about educational policies , programs , and practices that improve academic achievement and access to educational opportunities for all students .

in fiscal year 2013 , ies had a budget of just under $600 million , which it used to support a range of research , data collection , and evaluation activities .

specifically , ies funds projects on topics that range from studying the effectiveness of basic reading or math initiatives , to evaluating the impact of federal grant programs , to collecting student performance data through the national assessment of educational progress , a nationally representative measure of student academic performance .

you requested that we review ies's performance in addressing its critical and wide - ranging mission .

this report examines: ( 1 ) the extent to which ies has demonstrated its ability to support high - quality research and fulfill its mission ; ( 2 ) the extent to which selected education research and technical assistance groups disseminate relevant products to the education field ; and ( 3 ) how ies coordinates its activities with other relevant federal research entities and within education .

to assess the extent to which ies has demonstrated its ability to support high - quality research and fulfill its mission , we reviewed relevant federal laws and regulations as well as agency documents describing ies's performance measures and key agency processes .

we conducted interviews with officials from ies and a range of stakeholder groups representing researchers , policymakers , and practitioners , as well as several members of ies's advisory board — the national board for education sciences ( nbes ) .

we obtained ies data for the most recent four years , fiscal years 2010 through 2013 , on the average number of work days from the submission of the initial manuscript to final approval for reports sponsored by ies that underwent external peer review before being publicly released .

we assessed the reliability of ies data by obtaining written responses from agency officials knowledgeable about the data .

we determined that the data were sufficiently reliable for the purposes of this report .

to address the extent to which selected education research and technical assistance groups disseminate relevant products , we obtained documents from each of these groups: the regional educational laboratories ( rel ) , the comprehensive technical assistance centers ( comprehensive centers ) , and the research and development centers ( r & d center ) .

we analyzed these documents to identify what is known about the relevance , dissemination , and utilization of the research and products these groups produce .

we also met with the directors of the rels and comprehensive centers as well as program staff at ies and education's office of elementary and secondary education ( oese ) who administer these programs .

we administered a short survey via e - mail to the directors of ies's r & d centers to obtain additional information .

we fielded this survey from april 2013 to may 2013 and achieved a 94 percent ( 17 of 18 ) response rate .

we selected these three research and technical assistance groups because they are all authorized by the same law that established ies .

to describe how ies coordinates with other relevant federal research agencies and within the department , we reviewed relevant federal laws and regulations and other relevant documents .

we also interviewed education officials responsible for planning evaluations of departmental programs .

finally , we met with officials from two federal research organizations that conduct education - related research — the eunice kennedy shriver national institute of child health and human development ( nichd ) and the national science foundation ( nsf ) — to learn about their processes and coordination with ies .

for all three objectives , we compared agency documents and procedures to gao's criteria on internal controls , performance measurement and reporting , and collaboration , as well as a framework gao developed to identify key elements of sound federal research and evaluation programs .

appendix i describes our scope and methodology in more detail .

we conducted this performance audit from july 2012 to december 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

prior to the establishment of ies , federal education research was carried out by the office of educational research and improvement , a federal agency that — according to observers both within and outside the federal government — was challenged by frequent leadership changes , competing interests , and lack of a focused mission .

the 2002 reauthorization of the elementary and secondary education act ( esea ) made numerous references to the use of research - based evidence in educational decision making and federal education programs .

with its enactment later in 2002 , esra established ies , an arm of the department of education dedicated to the improvement of federal education research , statistics , evaluation , information , and dissemination .

ies's mission is “to provide national leadership in expanding fundamental knowledge and understanding of education from early childhood through postsecondary study , in order to provide parents , educators , students , researchers , policymakers , and the general public with reliable information about ( 1 ) the condition and progress of education in the united states , including early childhood education ; ( 2 ) educational practices that support learning and improve academic achievement and access to educational opportunities for all students ; and ( 3 ) the effectiveness of federal and other education programs.” furthermore , in carrying out its mission , esra calls upon ies to “compile statistics , develop products , and conduct research , evaluations , and wide dissemination activities in areas of demonstrated national need.” esra authorizes ies to conduct and support many different types of research and evaluations in support of its mission .

specifically , esra contains several key provisions related to the management , core functions , and processes of ies: all research conducted by ies is to use scientifically based research standards that include , where appropriate , making claims of causal relationships only in random assignment experiments ; education evaluations conducted by ies are to employ experimental designs using random assignment , when feasible ; all research , statistics , and evaluation reports conducted by or supported through ies must be subjected to rigorous peer review before being published or otherwise made available to the public ; the director of ies will be appointed by the president , by and with consent of the senate , for a 6-year term and will propose the institute's overall research priorities and report biennially to congress and others on the institute's activities , among other duties ; and a board of directors is established — the nbes — whose duties include , among other things , ( 1 ) advising and consulting with the director of ies regarding its policies and approving the director's overall research priorities , ( 2 ) reviewing and approving procedures for peer review , and ( 3 ) reviewing the work of ies to ensure the consistency of scientifically valid research .

all of ies's work is carried out by four centers , created by law , that exist within ies .

see table 1 for more information on these centers and their key activities .

education administers programs that support education research and technical assistance through grants and contracts involving several research groups — the rels and r & d centers within ies and the comprehensive centers within oese ( see fig .

1 ) .

the national center for education evaluation and regional assistance ( nceera ) administers the rel program , a network of 10 regional entities that conduct applied research ; develop and disseminate research and products ; and conduct technical assistance and other activities to support the needs of state and local educational agencies in their region .

the rels were first established in 1965 under title iv of esea with the broad goal of supporting general educational improvement efforts in their regions .

since ies was created and began to administer the rel program , the rels have completed two contract cycles , and the current rels were launched in 2012 under new 5-year contracts .

esra also includes specific requirements for the administration of national r & d centers , which are designed to address areas of national need and each of which addresses at least one of the broad research topics outlined in the law .

r & d centers are also responsible for the production of rigorous evidence and dissemination of products that provide practical solutions to important education problems in the united states .

each r & d center is funded for no more than 5 years , and in fiscal year 2013 , the national center for education research ( ncer ) and the national center for special education research ( ncser ) administered 18 r & d centers in total .

the comprehensive centers program is a network of 22 technical assistance grantees that help to increase the capacity of state educational agencies ( sea ) to assist districts and schools in meeting student achievement goals .

established in 1994 , comprehensive centers provide services primarily to seas to enable them to assist school districts and schools , especially low - performing schools .

comprehensive centers provide training and technical assistance in ( 1 ) the implementation and administration of programs authorized under esea and ( 2 ) the use of research - based information and strategies .

selected comprehensive centers focus on specific content areas and produce research - based information and products for use by seas .

in addition to ies , other entities conduct education - related research and evaluations , and esra includes general requirements for the director of ies to coordinate its research and evaluation work with these entities , both within education and across the rest of the federal government .

within education , the office of planning , evaluation , and policy development ( opepd ) conducts analyses and program evaluations on behalf of the department .

other federal agencies , such as nsf and nichd — part of the national institutes of health — also support education - related research and the directors of these agencies serve as nonvoting ex officio members on the nbes .

in addition , many nongovernmental organizations conduct education - related research and evaluations that inform general knowledge and understanding of educational policies , programs , and practice .

for example , foundations such as the bill & melinda gates foundation , the william t. grant foundation , and the spencer foundation , conduct education research or program evaluation in specific topic areas to advance the public interest or their organization's mission or goals .

additionally , other research organizations such as mathematica policy research , mdrc , and sri international , contract with the federal government or other clients to conduct agreed upon research or program evaluations on their behalf .

ies has substantially improved the education research field .

in 2007 , the office of management and budget ( omb ) assessed ies's research and concluded that ies had transformed the quality and rigor of research within education and increased the demand for scientifically based evidence of effectiveness in the education field as a whole .

likewise , many stakeholders told us that ies's research standards had improved education's research and had a positive influence on education research generally .

more specifically , several stakeholders told us that ies products , such as its publications of education statistics reports , were useful for their work .

in addition , one regional comprehensive center director told us that a practice guide ies released on dropout prevention has become the framework that one of the states in its region is using for dropout prevention efforts statewide .

while ies's research grants and evaluations have resulted in many randomized controlled studies since the agency was established over 10 years ago , its research standards also include guidelines for the implementation of other rigorous research methodologies , and it has recently funded studies using those methodologies , such as regression discontinuity or single - case designs .

further , ies officials also described a variety of other types of research that the agency supports , such as data analyses and correlational analyses .

ies's support of these multiple types of methodologies allows it to better meet its various stakeholders' needs .

at the same time , ies's research can be of limited usefulness to policymakers and practitioners if it is not released in a timely manner .

some stakeholders told us that research and evaluations supported by ies may not be completed soon enough to inform the decision making of policymakers and practitioners on important questions , which is a key component of its mission .

for example , officials from one constituency - based organization for policymakers told us that ies's evaluation of education's race to the top and school improvement grant programs are of great interest to states , but by the time the results of these evaluations are released in 2014 , states will not have much time to implement lessons learned from these studies before their program's funding expires .

ies's peer review process may also exacerbate timeliness concerns .

in order to ensure the high quality of ies's work , esra requires ies - supported research reports to be peer reviewed before being published .

in recent years however , the time it takes to complete this review process substantially increased , from an average of 117 days in fiscal year 2011 to 175 days in fiscal year 2012 and 150 days in fiscal year 2013 .

as long ago as 2008 , peer review timeliness was a concern of the nbes and at that time , they recommended that ies establish procedures to ensure the timely receipt of reports and revisions from its contractors .

when asked for explanations for the recent increase in its peer review timeframes , senior ies officials told us that the timeliness of ies contractor responses to peer review comments may still be a factor .

they also cited factors such as the complexity of the reports reviewed recently and the time it took ies's centers to work with its contractors on suitable responses to peer review comments .

however , ies officials told us that while the peer review office within ies monitors the time its own staff take to review reports , the peer review office does not monitor the time ies's centers or contractors take to respond to peer review comments .

such monitoring would allow ies to take steps to mitigate delays , such as by holding contractors more accountable .

in accordance with federal internal control standards , program managers should have access to and use operational data to determine whether they are meeting their agencies' goals for accountability for effective and efficient use of resources .

to ensure that its research addresses the needs of a range of stakeholders and to address concerns about how relevant its research is to these stakeholders , ies is soliciting feedback from practitioners and redesigning some existing programs .

current ies officials said that ies has in the past 10 years established the quality of ies - supported research , and that they are continuing to prioritize engagement with policymakers and practitioners .

several stakeholders with whom we spoke also noted that ies has recently devoted more attention to policymaker and practitioner outreach to improve its relevance to the education field .

for example , ies recently convened a group of 17 state and local education officials and other stakeholders to discuss the strengths and weaknesses of products from the rels and the what works clearinghouse , a program that ies administers to evaluate the merits of and disseminate education research evidence .

according to ies , at this meeting participants discussed their opinions on the usability , relevance , and accessibility of current rel and what works clearinghouse products .

in addition , in june 2013 , ies officials told us they were planning to convene a small group of researchers to assess potential gaps in the research conducted by two of its centers: ncer and ncser .

ies also recently held a discussion with its board of directors about how to make its research more relevant to policymakers and practitioners .

ies has also reorganized several of its existing grant programs and initiated a new grant program to support research that will better target and understand the needs of the education field .

for example , one of these grant programs focuses on collaborations between researchers and state or local educational agencies , and in july 2013 , six new grants were awarded for collaborative research projects , such as the design of a randomized postsecondary developmental education experiment in texas and the development of a system to monitor students' social and emotional learning in a school district in nevada .

additionally , ies is planning to fund a new r & d center in fiscal year 2014 on knowledge utilization to study how education researchers can make their work more relevant and useful to practitioners and how practitioners can make productive decisions based on research evidence .

despite ies's recent efforts to increase the relevance of its research , ies does not have a structured process for incorporating feedback from policymakers and practitioners into its research agenda .

esra requires that ies's board of directors be composed of education researchers and other stakeholders , including educators , policymakers , parents , or school administrators .

however , according to ies officials , ies has had difficulty with the nomination and approval process for appointing board members and , according to one long time board member , the nbes has , at times , had difficulty retaining members from these stakeholder groups .

further , according to ies officials , ies does not have in place any other ongoing , structured outreach to stakeholders for input to its research agenda , though the senior leaders of ies's centers and the director of ies periodically engage with policymakers and practitioners on specific topics or projects .

inconsistent outreach by ies may have contributed to gaps in its research .

for example , stakeholders said that there is a shortage of research using mixed methodologies that could allow for shorter turnaround times among ies - funded research projects .

while obtaining and integrating policymaker and practitioner feedback into a research agenda can be difficult , other research agencies have external groups of practitioner - stakeholders to which they routinely turn for perspectives on the overall direction of their research .

officials at nichd , for example , told us that , in addition to its statutorily required national advisory council , they consult with an independent group called the friends of nichd , comprised of organizations that use nichd research , to gather input on nichd research plans and agendas .

similarly , in addition to the national science board , nsf officials told us that several advisory committees , composed of outside experts and stakeholders , regularly provide recommendations regarding the direction of the agency's research .

though there is no single or ideal way for government agencies to conduct research , we previously developed a framework to identify key elements that promote a sound federal research program , using guidelines from several leading national research organizations .

within that framework , we found that agencies should establish a structured process for developing their research and evaluation priorities that considers key stakeholders' input .

for example , we reported on one federal research agency that has procedures that call for routinely consulting with stakeholders — including policymakers and key institutions that influence public policy — about past research accomplishments and program effectiveness and impact .

ies cannot demonstrate the impact of its efforts to improve the quality and relevance of its research in some areas because its performance measures have not been updated to reflect its current programs .

since 1993 , all federal executive branch agencies have been required to set strategic goals , measure performance , and report on the degree to which goals were met in an effort to ensure government accountability and enhance public awareness about agencies' accomplishments .

in addition , according to federal internal control standards and leading practices on performance management , agencies should establish performance measures for their activities and continually compare actual performance data against these goals .

however , ies officials told us that ies's current performance measures , which were developed after the agency was created by law in 2002 , no longer capture the scope of ies's current research and priorities .

furthermore , in some cases , senior ies officials told us these performance measures are no longer relevant to managing the agency's operations .

for example , one performance measure relies on the results of a survey of potential users of the what works clearinghouse , but ies officials told us they have never conducted this survey because they do not feel it would yield enough useful information to be worth the investment .

to measure the effectiveness of its research grants , ies currently reports on a measure that counts the number of ies - supported interventions that have been determined to be effective in improving student outcomes in particular areas .

while a new performance measure was reported in education's fiscal year 2014 budget request for ies that better reflects the results of its recent research , this measure still does not include certain areas , such as research on the organization and management of schools and education systems .

in addition , ies does not publicly report on the overall performance of the rel program , which constitutes one of the agency's largest investments .

as we have reported , without performance measures that include targets or goals to demonstrate results , agencies may be at risk for failing to achieve their goals .

ies officials told us they have begun work on revising their agency's overall performance measures .

officials told us that they plan to include revised performance measures in education's fiscal year 2015 budget request for ies , and that they have begun discussions with omb to establish these new measures .

as of august 2013 , ies officials told us they intend for the new performance measures they are developing to include all programs , including the rel program and its new grant programs for researcher - practitioner partnerships .

rels , comprehensive centers , and r & d centers have taken various steps to provide relevant research to the education field .

according to statute , rels and comprehensive centers are required disseminate information that can be used by practitioners and policymakers to improve academic achievement .

similarly , ies requires r & d centers to disseminate research to policymakers and practitioners to improve teaching and learning , and ultimately , student achievement .

to identify topics of relevance to the education field , all three groups have engaged policymakers and practitioners in planning research activities .

for example , beginning in 2012 , rels were required to conduct their work through new or existing partnerships of practitioners , policymakers , and others — called research alliances , which would work together to use data and research to address specific problems in education .

according to ies officials , rel projects must be based on needs identified and agreed to by the research alliances .

for example , the rel that includes the silicon valley area works with a research alliance consisting of local school districts , county education officials , foundation leaders , university faculty , and intervention specialists to boost math achievement .

this alliance began because local districts were concerned about low student achievement in math , given its importance for the local technology industry .

with input from the alliance , this rel is developing and analyzing strategies to ensure adequate preparation in math with the goal of preparing students for postsecondary education .

despite efforts by these groups to ensure the relevance of their work , stakeholders , including teachers and policymakers , have raised concerns about the applicability of some of the research and products these groups have produced , as well as their timeliness .

for example , a stakeholder group we spoke with that represents local school districts , as well as two superintendents we spoke with , said they did not find rel research as relevant or as timely as other sources they turn to for research information .

additionally , teachers we spoke with told us it would be helpful if ies - supported groups produced more products that synthesize research findings so that they are more applicable to classroom practice , similar to research - based products they use from professional associations or other intermediary organizations .

stakeholders also noted that the timeliness of rel products has been a concern .

rel research findings cannot be released to the public until they have cleared the ies peer review process , which as we noted earlier , has taken longer in recent years .

some stakeholders expressed concern that in the absence of timely and applicable information from ies , other entities may provide practitioners and policymakers with research - based information that may not be conducted with comparable research standards or be as objective and unbiased as ies - supported research .

in addition , the research topics and the products produced by the r & d centers primarily reflect the priorities of researchers , according to many of the researchers we spoke with , even though the centers have multiple audiences , including policymakers and practitioners .

stakeholder groups representing policymakers and practitioners also said that the r & d centers could do more to adapt their research findings to formats readily accessible by these audiences , such as by producing nontechnical reports and shorter research summaries .

all three groups use a range of methods to disseminate their research evidence and products , such as publications and conferences .

for example , in our survey of r & d center directors , we found the common methods they used to disseminate their research were academic journal publications and presentations at conferences , as well as hosting conferences .

comprehensive centers disseminate products and tools they produce by sharing them on their websites , via e - mail distribution lists , and with other comprehensive centers .

in addition , comprehensive centers create summaries of current research for their state clients , and also develop other technical assistance resources specifically for teachers , such as professional development courses .

although largely successful in reaching academicians , rel and r & d center dissemination efforts do not always ensure that research reaches policymakers and practitioners .

regarding rels , researchers and several groups representing policymakers and practitioners told us that rel work was not reaching these audiences , and teachers we spoke with were generally unfamiliar with rels .

a few stakeholders also noted that rel productivity varied widely , and that some have produced very few reports .

stakeholders gave several reasons why rels may struggle to reach practitioner audiences .

for example , some intermediaries — such as industry associations — we spoke with said their organizations help to disseminate research information to policymakers and practitioners .

however , some noted that further efforts are needed to leverage intermediary groups to better market rel and r & d center work to reach ies's target audiences .

ies requires rels to report some information about relevance and dissemination that could be used to evaluate their efforts , but this information has not been collected in a consistent manner and is therefore difficult for ies to use to improve program management .

for example , rels use a stakeholder feedback survey instrument to capture feedback from participants on relevance and other aspects of the products , activities , and events they sponsor .

this survey includes a set of questions for five different types of rel activities and products , such as technical assistance workshops or research alliance participation .

however , as administered in 2012 — the first year of the current rels' operation — rels were not consistent in how they administered the survey .

for example , some rels administered the complete survey for their activity or product type , while other rels selected specific questions for respondents .

the results , therefore , are not comparable across all rel activities and products .

further , the data collected only applies to participants who elected to provide feedback , rather than all participants in rel programs or potential users .

in addition to the feedback survey , rels are required to annually report to ies on a series of indicators that they developed collaboratively with ies in march 2012 .

however , this preliminary information is not sufficient for ies's management of rels' efforts to produce and disseminate relevant research .

the rel program has six expected outcomes , one of which is to build a body of knowledge in topics that address regional needs .

to assess performance on this and the other five program outcomes , rels must report to ies on 24 indicators .

for example , regarding dissemination , rels must report the number of events held and the total number of attendees at these events .

however , ies has not established performance targets or goals for these indicators to help rels prioritize their activities or assess their performance in these areas .

were ies to establish targets or goals , the rels would likely have more incentive to perform at the agency's desired level , and ies would be better positioned to determine if the rels are meeting its expectations and hold them more accountable for performance .

our previous work has indicated agencies successful in measuring performance had performance measures including targets or goals that ( 1 ) demonstrate results ; ( 2 ) are limited to the vital few ; ( 3 ) cover multiple priorities ; and ( 4 ) provide useful information for decision making .

regarding the r & d centers , ies has few requirements for tracking relevance and dissemination , although some centers collect additional information that could be helpful for assessing performance in these areas .

in combination with other factors , ies assesses the relevance of the r & d centers' proposed research projects when making funding decisions , but according to officials , this is the only formal assessment of the relevance of r & d center work .

however , although they are not required to do so , most r & d center directors responding to our survey said they use quantitative data to gauge their relevance to policymakers and practitioners .

for example , one r & d center has conducted national surveys of certain constituent groups , such as state policymakers , asking about the relevance of the center's findings and products for their use .

in their annual performance reports , ies asks r & d centers to report whether they disseminated research information , but does not specify what information they should provide , such as how they disseminated information or the effectiveness of these efforts .

though it can be difficult to demonstrate how research programs with broad dissemination goals — such as the r & d centers — are able to inform others and contribute to outcome - oriented goals , there are several evaluation strategies to assess how programs with great breadth and flexibility contribute to agency goals .

ies has not , however , set utilization or dissemination goals and objectives , or investigated whether dissemination strategies were effective in improving the efforts of these groups .

according to officials , ies has no plans to conduct formal evaluations for the current group of rels and r & d centers to comprehensively assess their relevance and dissemination activities .

ies is still in the process of conducting a mandated evaluation of the prior group of rels , whose contracts ended in 2011 .

according to study design documents and ies officials , this ies - contracted evaluation will include information from a customer satisfaction survey of a representative sample of state and district staff regarding their views of the relevance and usefulness of rel work .

the evaluation will also include an assessment by expert panelists of the quality and relevance of selected rel reports .

however , it will not include an assessment of rel dissemination activities .

this evaluation began in 2009 and an interim report was initially expected to be released in spring 2012 ; however , the interim report was delayed and was released in late september 2013 .

ies has no further plans to evaluate the rel program beyond the forthcoming evaluation of the prior group .

officials told us although there was a clear requirement in esra for this evaluation , they did not believe that the law required any subsequent evaluation of the rel program .

in addition , as of august 2013 , ies had no plans to conduct a formal evaluation of the r & d center program , and there is no requirement to do so .

according to ies officials , they do not plan to evaluate them because they do not view them as a program , but rather as one vehicle for supporting ies's programs of research , as with its education research grants .

however , research on dissemination practices funded by other federal research programs has shown that dissemination efforts should be assessed periodically to improve further efforts to communicate key messages to target audiences .

as required by esra , ies has previously evaluated the comprehensive centers .

a 2011 ies - contracted evaluation reviewed how well the comprehensive centers addressed state client needs , as well as the quality , relevance , and usefulness of their assistance , among other things .

the evaluation included a survey of sea administrators and a review of comprehensive center products .

evaluation data were collected annually in 3 of the 5 program years in which the centers operated .

comprehensive centers were rated higher in each successive year , and in 2008-2009 , 56 percent of state managers surveyed said that technical assistance from the comprehensive centers served the state's purposes completely .

for the state managers who said the comprehensive centers did not completely meet their needs , the main reason cited was that center staff had limited time to work with their state .

the evaluation also included a quality assessment of a sample of comprehensive center projects by expert panelists and project participants .

each sampled project was independently rated by a panel of experts using a 5-point scale .

sea staff who participated in these projects ( eg , by serving on a work group associated with the project ) were asked via survey to rate the relevance and usefulness of each project .

for the sample of projects assessed , the evaluation showed that the comprehensive centers' technical assistance was rated higher on each measure in each successive year , from 2006-2007 to 2008-2009 .

on average , across each of the 3 years , expert panels rated sampled project materials from “moderate” to “high” for quality , and project participants rated the sampled projects “high” for relevance and usefulness .

while these results indicate a positive trend in comprehensive centers' performance , this evaluation covered the prior group of comprehensive centers and ies recently released a new request for proposals to evaluate the current group .

ies coordinates with relevant federal research agencies on projects to increase federal agencies' use of research evidence in guiding funding decisions .

according to federal agency officials and stakeholders we spoke with , nichd and nsf are the two other primary federal agencies conducting education - related research .

ies officials meet regularly with their nichd and nsf counterparts to jointly sponsor projects and ensure their efforts are complementary .

for example , ies co - led a joint education - nsf working group to develop common evidence guidelines for education research and development .

these guidelines are intended to help guide nsf's and ies's respective decisions about investments in education research and clarify for potential grantees and peer reviewers the evidence expected from each type of study .

several researchers we interviewed , as well as officials from three federal agencies we spoke with , said these guidelines will benefit the education field .

in addition , developing common evidence guidelines is consistent with our key practices for enhancing and sustaining collaboration among federal agencies .

in particular , we have found that establishing compatible policies , procedures , and other means helps to align the partner agencies' activities , core processes , and resources to accomplish a common outcome .

consistent with this key practice , ies also coordinates with other federal agencies to leverage its expertise in planning federal research agendas on topics such as early learning and prisoner re - entry , as well as increasing the quality of and access to data on education outcomes .

for example , ies sits on the early learning policy board , a joint education - health and human services body , to discuss ways to coordinate federally - funded research on early childhood topics .

ies's statistics center leads the interagency working group on expanded measures of enrollment and attainment , to improve national data on education , training , and credentials for work .

this effort includes omb , the council of economic advisors , and other federal statistical agencies , such as the census bureau and the bureau of labor statistics at the department of labor .

in addition , ies has worked with the defense advanced research projects agency at the department of defense on its small business innovation research program , and in december 2012 they released a joint solicitation to support the development and evaluation of education technology games .

ies's efforts to help develop a systematic , transparent evidence review process and incorporate a tiered evidence framework for awarding grants for education's investing in innovation fund grant program have had an impact on the use of evidence in federal decision making both within and outside the department .

according to an official from the administration of children and families at the department of health and human services , for some of its grant programs , the administration of children and families has recently used tiered evidence standards similar to those ies helped to develop for education's investing in innovation fund , reserving the majority of funds for applicants proposing to implement service models with evidence of effectiveness .

ies is also working with education program offices to incorporate tiered evidence standards in awarding their grants , such as the strengthening institutions program in the office of postsecondary education .

in addition to working with other federal agencies , ies coordinates within education to facilitate coordination between the rel and comprehensive center programs .

there is a statutory requirement for each comprehensive center to coordinate its activities and collaborate with the rels in its region , as well as with certain other entities .

prior to the current award cycle , ies and oese restructured rel and comprehensive center regions so that the number of comprehensive centers in each rel region was reduced and aligned more closely with rel regions .

according to oese officials , its comprehensive center grantees and states found the lack of alignment between the regions in the prior contract cycle to be a barrier to coordination .

further , ies has provided guidance regarding its expectations for coordination between rels and comprehensive centers .

for example , ies has encouraged rels to partner with comprehensive centers and has allowed rels to budget resources for these collaborative efforts .

according to directors of these groups , coordination has improved with the current group of rels and comprehensive centers , which began in 2012 .

one rel director we spoke with told us that the realignment of the regions has facilitated improved coordination between the groups .

in addition , some directors said that the rels' new structure of research alliances has improved coordination .

for example , some comprehensive centers are members of rel research alliances and have partnered with rels to host “bridge” events , where state officials and researchers meet to discuss research findings relevant to issues of policy and practice .

rel directors told us that although there can still be confusion among some sea officials about the appropriate role and tasks performed by each , rels and comprehensive centers have used several strategies to reduce confusion and improve coordination .

for example , some groups have conducted joint visits with sea officials so that they can describe their respective roles and responsibilities and specifically discuss how each group can address the state's various technical assistance needs .

in addition , some directors described their groups' efforts to sequence their work for their state clients to better meet their needs .

in some instances , for example , the comprehensive center may conduct initial planning work for a project , and the rel would later assist with any aspects of the project requiring data analysis or original research .

for example , one comprehensive center director told us that , through joint efforts with the rel in their region , it developed a framework for measuring teacher effectiveness .

for this project , the rel formed a research alliance with other comprehensive centers and seas interested in piloting the framework .

the comprehensive center is helping states implement the pilot , and the rel will conduct an evaluation of the pilot projects .

this center director told us that it will help the states adjust their strategies based on the findings from the pilot evaluation .

ies coordinates within the department to plan evaluations of education programs .

beginning in 2010 , opepd and ies have jointly led an annual department - wide evaluation planning process to identify evaluation projects to conduct .

although there is no official guidance delineating these roles , education officials said that the annual evaluation planning process is used to help determine which of these offices will conduct each project .

according to education , this planning process has helped to improve the quality of evaluations conducted , and made evaluations more responsive to the information needs of the department .

for example , ies is conducting an implementation and quasi - experimental evaluation of the race to the top program .

an ies official reported that input from the education program office running this program helped ensure that the evaluation's data collection instruments focused on implementation issues most important to education and state education leaders .

ies is also working with this program office to select topics of greatest immediate interest to education for short implementation briefs to be released as the study progresses .

however , efforts to coordinate internally and prioritize evaluation research projects through this annual process are challenged in part by statutory requirements related to esea evaluation funding .

for these programs , evaluation funds are typically set aside as a percentage of program funding or as national activities funds .

with some exceptions , esea authorizes education to reserve up to 0.5 percent of the amount appropriated to carry out each program or project authorized under that act to conduct evaluations .

however , according to education officials , due to the statutory limitation , the funds available for evaluation are often insufficient to conduct high - quality evaluations of these programs .

this is particularly true with respect to certain discretionary grant programs where overall program funding is itself relatively small .

as a result , opepd and ies officials said that some evaluations , including high - priority evaluations , may not occur .

for example , ies officials told us that the department is interested in conducting evaluations of the mathematics and science partnerships and promise neighborhoods programs , but there is not sufficient evaluation funding under these programs due to the statutory limitation and education does not have the authority to combine evaluation funds across the department and use them to conduct evaluations of these or other high - priority programs .

we have previously reported that many education programs , especially smaller programs , have not been evaluated , which can limit the ability of congress to make informed decisions about which programs to continue , expand , modify , consolidate , or eliminate .

for example , in 2009 , we reported that 11 of education's programs focused on teacher quality had been operating for over seven years and had never been evaluated .

in addition , according to opepd officials , ideas for some high - priority evaluations that department officials have discussed during the annual evaluation planning process are never developed into plans because officials know that evaluation funds are not available to carry them out .

however , according to officials from opepd , beginning in fiscal year 2015 , education will maintain a list of all of the department's evaluation needs should funds for conducting these evaluations become available .

according to a senior ies official , if the department were able to combine funds authorized to be used for evaluation it would have more flexibility to conduct appropriate evaluations of any of esea programs that need evaluation .

the president's fiscal year 2014 budget request for education contained a proposal to increase education's flexibility to conduct program evaluations by allowing the department to use funds from certain programs across the department for this purpose .

an omb staff member told us that the ability to combine evaluation funds and prioritize evaluations is critical in the current environment of constrained funding to help make the best use of limited resources and to fill gaps in topic areas where research is needed .

in fiscal year 2012 , congress granted the department of labor temporary authority to combine program - specific funds to support evaluations of its programs .

the department of labor is required to provide a plan to the congressional appropriations committees before combining its funds in this manner .

this plan is to describe the specific evaluations it plans to carry out using these funds and helps ensure transparency and accountability in the department's use of this authority .

in the department of labor's fiscal year 2014 budget justification , it reported that the availability of flexible evaluation funds has allowed the department to evaluate large and high - priority programs , as well as programs that may not have been fully evaluated previously .

since its creation more than a decade ago , ies has made significant contributions to strengthening the rigor of the education research field and has promoted the use of scientifically based research in our nation's education system .

however , ies's ability to release timely information to policymakers and practitioners is exacerbated by its peer review process which , while promoting rigorous research , can take a substantial amount of time .

without better use of available data to manage this peer review process , it will remain difficult for ies officials to identify potential causes of these delays and develop strategies to mitigate these challenges .

more recently , the agency has begun working to make its products more relevant to a range of stakeholders , particularly policymakers and practitioners .

producing education research that is relevant for these stakeholders is critical to inform decision making in congress and in the field about key federal investments in education .

despite progress ies has made in ensuring that its research is relevant to its stakeholders , ies does not have a formal , structured process for engaging with these groups in the research - agenda setting process .

until such a process is in place , ies limits its ability to ensure the relevance , and ultimately , usefulness of its research to the field .

ies's relevance to the field is unclear in other ways as well .

for example , without publicly reported performance measures for key investments — such as the rels or new high - priority grant programs — stakeholders , such as congress and the public , are unable to determine the effectiveness of ies's activities or evaluate the merits of the federal investment in these programs .

ies's research and technical assistance grantees and contractors take a number of steps to help ensure they produce and disseminate relevant research and products .

however , without assessing the effectiveness of these groups' dissemination efforts , ies may miss opportunities to think more strategically about dissemination and ensure that these groups are using the most effective strategies to reach their target audiences .

providing education with the authority to combine funds authorized for evaluations under esea would allow the department more flexibility to prioritize and conduct effective evaluations of programs that represent key federal investments and critical subject areas .

without this authority , it will be difficult for education to make the best use of its limited resources and to support congress in making informed decisions about which programs to continue , expand , modify , consolidate , or eliminate .

to ensure that education can conduct critical program evaluation work , congress should consider granting education authority to combine funds authorized for evaluations of esea programs and target them to high - priority evaluations , with appropriate measures to ensure transparency and accountability for how the funds will be used .

to improve the management and accountability of ies's research and evaluation efforts , we are making the following recommendations: 1 .

ies should use available data to routinely monitor all stages of its peer review process and identify opportunities to improve the timeliness of its reviews .

2 .

ies should develop a structured process to systematically gather input from policymakers and practitioners and use this input when developing its research agenda .

3 .

ies should develop performance measures , including targets and goals that clearly demonstrate results , and that reflect its current programs and all key agency activities , such as the performance of the rels and its new grant programs supporting researcher - practitioner partnerships .

4 .

in order to identify leading practices and target areas for improvement , ies should assess the effectiveness of rel and r & d center dissemination strategies by , for example , collecting consistent data and lessons learned from these groups to inform future dissemination efforts .

we provided a draft of this report to the department of education for review and comment .

their comments are reproduced in appendix ii .

education officials also provided technical comments , which we incorporated into the final report , as appropriate .

education generally agreed with our recommendations and has several plans in place to address them .

specifically , ies noted that it continues to be committed to producing high - quality reports while shortening the time period for its peer review process , and stated that it will develop a system to more comprehensively monitor all of the stages in its report production process so that it is able to identify and address potential problems more easily and quickly .

to further emphasize its commitment to obtaining practitioner and policymaker input , ies also said it will begin an annual solicitation for practitioner and policymaker input on its research agenda by posting an invitation on its web site and widely advertising the opportunity through its ies newsflash email notices .

additionally , as we noted in our report , ies commented that it is currently working to align its performance measures with its current investments , including developing measures for the rel program .

finally , ies agreed that it is important to measure and assess the effectiveness of ies - supported dissemination activities .

ies noted it will build knowledge in this area through its fiscal year 2014 request for applications for a new r & d center focused on knowledge utilization to develop measures for how practitioners use research evidence to guide what they do in schools and classrooms .

we support ies's continued focus and emphasis on advancing knowledge and providing usable information to improve education policy and practice .

we are sending copies of this report to relevant congressional committees and the secretary of education .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or our staffs have any questions about this report , please contact me at ( 202 ) 512-7215 or scottg@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

our review examined ( 1 ) the extent to which the institute for education sciences ( ies ) demonstrates its ability to support high - quality research and fulfill its mission ; ( 2 ) the extent to which selected department of education ( education ) research and technical assistance groups disseminate relevant products to the field ; and ( 3 ) how ies coordinates its activities with other relevant federal research entities and within education .

for all three objectives , we reviewed the education sciences reform act of 2002 ( esra ) to identify statutory requirements related to ies management , processes , and procedures .

we obtained and analyzed agency documents detailing ies's processes and performance .

we also conducted interviews with senior ies officials , including commissioners at all four ies centers as well as selected program staff , several current and former members of ies's advisory board , the national board for education sciences , and selected former senior ies officials to gather additional perspectives on ies's management and performance over time .

specifically , to assess the extent to which ies has demonstrated its ability to support high - quality research and fulfill its mission , we reviewed documents and data from ies's standards and review office , which administers the peer review of ies reports and grant applications .

in part to evaluate ies's timeliness in producing relevant research , we obtained ies data for the most recent 4 years , fiscal years 2010 through 2013 , on the average number of work days from the submission of the initial manuscript to final approval for reports sponsored by ies that underwent external peer review before being publicly released .

we assessed the reliability of ies data by obtaining written responses from agency officials knowledgeable about the data .

we determined that the data were sufficiently reliable for the purposes of this report .

to further assess ies's overall processes and operations , we applied a framework gao previously developed to evaluate the soundness of federal research and evaluation programs .

this framework includes five key phases of the research and evaluation process: ( 1 ) agenda setting , ( 2 ) selecting research , ( 3 ) designing research , ( 4 ) conducting research , and ( 5 ) disseminating research results .

while we assessed ies's activities in each of the five phases , we primarily focused on the first and fifth phases of this process: agenda setting and disseminating research , as these two phases most directly addressed our research questions .

additionally , we reviewed publicly reported performance information contained in education's most recent budget request to congress for ies to compare performance trends over time and to assess the completeness of ies's performance measures against gao's leading practices for performance management and federal standards for internal control in the areas of information and communications , and performance reporting and monitoring .

we also interviewed officials at the research and evaluation arm of the administration for children and families at the department of health and human services , and at the office of management and budget that were recommended to us as knowledgeable of federal research and performance reporting to obtain their insight on ies's research , evaluation , and performance measurement .

lastly , we observed two public meetings of the national board for education sciences that took place during the course of our review on october 5 , 2012 , and february 22 , 2013 .

we also conducted more than 40 interviews with several categories of ies stakeholders: ( 1 ) education researchers ; ( 2 ) organizations representing education policymakers and state - level administrators ; and ( 3 ) practitioner organizations , individual teachers , and district leaders .

to identify appropriate stakeholders in all three categories , we obtained recommendations on individuals or groups to contact from key organizations , contacted those individuals or groups , and obtained further recommendations from them on additional individuals or groups to contact .

we continued to ask for names from the subsequent individuals and organizations until we began getting duplicate referrals .

we also identified individuals and groups to contact through our prior work on education research and internal expertise .

for the education researchers , we sought to identify education researchers that were considered to be knowledgeable about education research and the activities of ies and its predecessor organizations .

for the education policymaker organizations , we ensured representation from groups that represent the wide range of topical areas in which ies funds research and evaluation , such as early childhood education , special education , and postsecondary education .

we interviewed teachers and superintendents because they are part of ies's target population of end - users , though they had various levels of familiarity with ies prior to our interview .

we conducted two discussion groups with experienced education researchers at the 2013 annual conference of the american educational research association ( aera ) .

we assembled senior researchers for these discussion groups with the help of aera staff .

invitations to participate in these discussion groups were provided to selected members of the aera fellows program that were culled by aera staff to ensure representation from a balanced range of subject matter areas .

during these discussion groups , we met with a total of 16 senior education researchers and discussed issues such as ies's research standards , peer review process , and the relevance and dissemination practices of ies overall , as well as those of selected ies research and technical assistance groups , specifically — the regional educational laboratories ( rel ) and research & development centers ( r & d center ) .

to address our second research objective , we focused on three types of research and technical assistance groups .

two are administered by ies — the rels and r & d centers — and one is administered by the office of elementary and secondary education ( oese ) — the comprehensive technical assistance centers ( comprehensive centers ) .

we selected these three research and technical assistance groups because they are all authorized by the same law that established ies .

moreover , there is a statutory requirement for the comprehensive centers to coordinate their activities and to collaborate with the rels , among others .

specifically , to assess the extent to which these groups disseminate relevant products to the education field , we obtained and analyzed the most recent annual performance report submitted to ies and oese , respectively , from each of these groups .

we analyzed these documents to identify what quantitative and qualitative information ies collects about the relevance , dissemination , and utilization of the research and products these groups produce .

we also interviewed directors of the rels and the comprehensive centers as well as program staff at ies and oese that administer the programs to discuss relevance , dissemination , and coordination activities .

to further understand the comprehensive centers activities related to the production and dissemination of relevant research and products to the field , we reviewed a contractor - led evaluation of the comprehensive center program .

in particular , we reviewed findings related to relevance of the products these groups produced and the levels of satisfaction reported by their customers .

we also reviewed early study design documents and an interim report for an ongoing evaluation of the rel program .

we reviewed the scope and methodology for each of these evaluations and determined they were sufficiently reliable for our reporting purposes .

we also obtained information from ies staff responsible for overseeing these evaluations to obtain additional information .

we administered a short survey to the directors of ies's 18 r & d centers that were operational in the most recent full program year ( 2011- 2012 ) at the time of our review .

we asked questions about activities the r & d centers undertake to disseminate relevant research to the field and what qualitative and quantitative data they collect on their activities .

we conducted pretests to check that ( 1 ) the questions were clear and unambiguous , ( 2 ) terminology was used correctly , ( 3 ) the survey did not place an undue burden on respondents , ( 4 ) the information could be feasibly obtained , and ( 5 ) the survey was comprehensive and unbiased .

we conducted two pretests with r & d centers , one grantee of the national center for education research and one grantee of the national center for special education research .

we made changes to the survey after both pretests based on the feedback we received .

we sent the survey by e - mail in an attached microsoft word form on april 2 , 2013 .

surveys were completed by 17 of 18 r & d center directors , for a 94 percent response rate , by may 10 , 2013 .

because this was not a sample survey , it has no sampling errors .

however , the practical difficulties of conducting any survey may introduce errors , commonly referred to as nonsampling errors .

for example , difficulties in interpreting a particular question , sources of information available to respondents , or entering data into a database or analyzing them can introduce unwanted variability into the survey results .

we took steps in developing the questionnaire , collecting the data , and analyzing them to minimize such nonsampling error .

to describe how ies coordinates with other relevant federal research agencies and within the department , we reviewed pertinent documents and statutory requirements for coordination contained in esra and compared agency efforts to criteria we previously developed for practices agencies can use to help enhance and sustain interagency collaboration .

to identify relevant federal research agencies , we obtained recommendations on federal research agencies that conduct education - related research during interviews with knowledgeable stakeholders .

we continued to ask for recommendations during stakeholder interviews until we established that the vast majority of recommendations identified two other federal research agencies: the education directorate at the national science foundation ( nsf ) and the eunice kennedy shriver national institute of child health and human development ( nichd ) , an arm of the national institutes of health .

while stakeholders and officials at ies , nsf , and nichd noted some key differences between these agencies , such as the types of research funded , there was general consensus that there were clear commonalities in the research missions and some of the topics of research funded by all three agencies .

as such , we interviewed officials at nsf and nichd about their processes and procedures as well as their coordination with ies , asking for examples of coordination activities and any potential areas for improvement .

additionally , during our interviews with ies officials , we asked about their coordination activities with nsf and nichd .

to describe ies's coordination within the department , we reviewed agency documents such as those describing the annual evaluation planning process at education and conducted interviews with ies officials and officials responsible for evaluation and research within the department's office of program evaluation and policy development .

finally , we also asked about coordination activities between the rel and comprehensive centers programs during our interviews with program staff administering these programs , as well as during our interviews with directors of these groups , to assess the extent and nature of the coordination between these technical assistance programs .

we conducted this performance audit from july 2012 to december 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , scott spicer , assistant director ; lucas alvarez , nora boretti , and dana hopings made significant contributions to this report .

also contributing to this report were sandra baxter , deborah bland , david chrisinger , elizabeth curda , helen desaulniers , sheila mccoy , jean mcsween , mimi nguyen , james rebbe , and sarah veale .

education research: preliminary observations on the institute of education sciences' research and evaluation efforts .

gao - 13-852t .

washington , d.c.: september 10 , 2013 .

managing for results: executive branch should more fully implement the gpra modernization act to address pressing government challenges .

gao - 13-518 .

washington , d.c.: june 26 , 2013 .

employment and training administration: more actions needed to improve transparency and accountability of its research program .

gao - 11-285 .

washington , d.c.: march 15 , 2011 .

program evaluation: experienced agencies follow a similar model for prioritizing research .

gao - 11-176 .

washington , d.c.: january 14 , 2011 .

department of education: improved dissemination and timely product release would enhance the usefulness of the what works clearinghouse .

gao - 10-644 .

washington , d.c.: july 23 , 2010 .

employment and training administration: increased authority and accountability could improve research program .

gao - 10-243 .

washington , d.c.: january 29 , 2010 .

program evaluation: strategies for assessing how information dissemination contributes to agency goals .

gao - 02-923 .

washington , d.c.: september 30 , 2002 .

education research: education should improve assessments of r&d centers , regional labs , and comprehensive centers .

gao - 02-190 .

washington , d.c.: january 24 , 2002 .

