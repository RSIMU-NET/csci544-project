the federal emergency management agency ( fema ) , a component of the department of homeland security ( dhs ) , leads the federal effort to mitigate , respond to , and recover from disasters .

fema is responsible for saving lives and protecting property and public health and safety in a natural disaster , act of terrorism , or other manmade disaster .

to meet its mission , it relies heavily on the use of information technology ( it ) .

according to fema's it investment portfolio for fiscal year 2015 , the agency reportedly spent at least $366.4 million for its it investments .

hurricane katrina in 2005 was the largest , most destructive natural disaster in our nation's history .

fema estimated that hurricane katrina caused an estimated $108 billion in damages .

following the federal response to hurricane katrina in 2005 , the post - katrina emergency management reform act of 2006 ( post - katrina act ) was enacted into law .

the act enhanced fema responsibilities to lead the country's emergency management system .

in doing so , the act required fema to address the shortcomings identified in the preparation for and response to hurricane katrina .

for example , it required the fema administrator , in coordination with the dhs chief information officer ( cio ) , to take appropriate measures to update and improve the agency's it systems .

in view of the act's requirements , you asked us to review fema's efforts to improve its it systems .

our objectives were to ( 1 ) identify challenges associated with ensuring fema's it systems adequately support the agency's ability to respond to major disasters and ( 2 ) assess the extent to which fema has implemented key it management controls for selected emergency management systems .

to address the first objective , we obtained and analyzed fema documentation ( eg , it strategic plans , fema's hurricane sandy after - action report ) , prior gao reports , and dhs inspector general ( ig ) reports to identify challenges associated with ensuring fema's it systems adequately support its ability to respond to major disasters .

we also interviewed officials from the national advisory council ; the disaster emergency communications division ; regions 2 , 4 , and 6 ; mobile emergency response center ( thomasville , georgia ) ; and the national processing center ( maryland ) .

further , we interviewed agency officials from fema's office of the chief information officer ( ocio ) , its office of policy & program analysis , and dhs's ig office to obtain their perspectives on the challenges associated with fema's it systems .

we then aggregated the key challenges .

to determine the extent to which fema had adequate controls in place to address these challenges , we compared the agency's efforts to best practices we have identified in the areas of it investment management , human capital management , and strategic planning .

for the second objective , we selected three fema it programs to determine the extent to which fema has implemented key management controls .

the three programs we reviewed were the disaster assistance improvement program ( daip ) , emergency management mission integrated environment ( emmie ) , and the integrated public alert and warning system ( ipaws ) .

in selecting these programs , we relied on the office of management and budget's exhibit 53 and applied selection criteria , including whether the program was associated with objectives of the systems identified in the post - katrina act or had spending in fiscal year 2015 .

we then identified key practices in the areas of risk management , requirements management , project planning , and systems integration and testing from the software engineering institute's capability maturity model® integration for acquisition ( cmmi - acq ) and the project management institute's guide to the project management body of knowledge ( pmbok® guide ) .

to determine the extent to which each of the three programs had implemented key practices in these areas , we assessed each of the three program's relevant documentation against these criteria .

we conducted this performance audit from january 2015 to april 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

additional details of our scope and methodology are contained in appendix i .

fema's mission is to support citizens and first responders to build , sustain , and improve the nation's capability to prepare for , protect against , respond to , recover from , and mitigate all hazards .

it supplies immediate needs , such as ice , water , food , and temporary housing in response to disasters .

fema also provides financial assistance to individuals who have sustained damage to their personal property , and to state and local governments for damage to public property .

to support its mission , fema has more than 4,900 full - time employees located at its headquarters in washington , d.c. , its 10 regional offices , two area offices , five recovery offices , and various sites across the country .

additionally , fema has more than 3,700 standby disaster assistance employees who are available for disaster deployment and nearly 5,000 reserve employees .

the agency also partners with other organizations that are part of the nation's emergency management system , including 27 federal agencies , state and local emergency management agencies , and the american red cross .

for fiscal year 2015 , fema's budget request was approximately $14.7 billion , representing 24 percent of the dhs budget request of approximately $60.9 billion .

the agency is headed by an administrator and its primary mission areas include federal insurance and mitigation , mission support , protection and national preparedness , regional operations , response and recovery , and united states fire administration , as depicted in figure 1 .

each component has multiple directorates and program offices that carry out disaster response missions and functions , as well as administrative support .

within the mission support component , the ocio is responsible for developing , enhancing and maintaining the it systems , and increasing efficiencies and cooperation across the entire fema organization .

ocio partners with fema programs and regional offices to provide for systems development , testing , implementation , and operations and maintenance efforts .

for example , the regions have it employees that report directly to the regional leadership ; these staff are to coordinate with the ocio on it responsibilities such as managing and supporting it hardware and software ( eg , needs assessment and hardware issuance , inventory , and decommissioning ) .

according to ocio officials , in fiscal year 2015 , fema's ocio was made up of nearly 300 federal employees and 228 contractors and had a budget of approximately $122.1 million , which is about one - third of the fema it budget for fiscal year 2015 .

as shown in figure 1 , the office is organized into nine offices and divisions to plan and manage fema's it environment .

it systems play a critical role in supporting fema's response and recovery efforts .

for example: the disaster assistance improvement program ( daip ) has a major interagency support system that provides disaster survivors an online registration vehicle ( including mobile capabilities ) to apply for disaster assistance from 17 federal partners , including individual assistance from fema , and is managed by fema's office of response and recovery .

daip is currently in the mixed life cycle phase of the systems engineering life cycle ; spending on the program was about $13.7 million in fiscal year 2015 , of which $12.9 million is from fema and the rest is from other federal partners .

the emergency management mission integrated environment ( emmie ) program has an internet - based enterprise - wide electronic system for fema to manage grants throughout the entire grant life cycle using a standardized web - based interface , and is also managed by fema's office of response and recovery .

emmie is currently in the operations and maintenance phase of the systems engineering life cycle ; the agency spent about $2.7 million on the program in fiscal year 2015 .

fema does not plan to make any enhancements to emmie's functionality because the system is scheduled to be decommissioned once the agency's grants management modernization system , the enterprise - wide capability for the management of all fema disaster and non - disaster grants , is implemented .

according to the emmie officials , the implementation date is estimated to occur in 2020 .

the integrated public alert and warning system ( ipaws ) program has a major system that provides a broad range of messaging capabilities through multiple pathways to ensure that the delivery of alerts and warnings reaches more people , and is managed by fema's national continuity programs directorate .

ipaws is currently in the mixed life cycle phase of the systems engineering life cycle ; fema spent about $10.9 million on the program in fiscal year 2015 .

in addition , fema's enterprise applications development , integration , and sustainment ( eadis ) program is intended to develop and integrate new and modernized applications while supporting the sustainment of existing technology , services , and applications .

there are almost 80 fema systems , including grant tracking , family locator , and assessment reporting , that are supported by eadis .

the current fema eadis contract consists of many individual platform - specific work orders that encompass a spectrum of new and old technologies , architectures , platforms , and tools that include a variety of personal computer - based , client - server , web - based , and service - oriented components .

ocio is also responsible for managing and maintaining the networks , databases , desktops , and telephone systems to support the operations of permanent facilities at fema .

the cio is also responsible for providing the it infrastructure to support hundreds of emergency personnel at temporary disaster field offices and recovery centers , often in remote locations .

this involves running cable , establishing networks , supplying wireless connectivity , and installing equipment for information processing and data and voice communications .

in addition , a national it helpdesk is to assist internal users in various ways such as providing and maintaining system accounts , ensuring remote access , troubleshooting systems problems , and making referrals to engineers for systems fixes .

prior gao and ig assessments have identified concerns with several aspects of fema's it management .

for example: in 2012 , we reported that a subsidiary project for fema's daip , which included usability enhancements to the agency's disasterassistance.gov website , was delayed .

specifically , technical issues in establishing a testing and development environment that matched the production environment delayed project testing and caused cost and schedule shortfalls .

for example , costs for a project under fema's daip investment rose approximately 27 percent ( $210,000 ) due , in part , to the delayed deployment of another investment .

we recommended that the department define and document corrective actions for these shortfalls .

the department agreed with our recommendation and subsequently developed a remediation plan for most of the shortfalls to limit the negative impact .

in 2011 , we reported that fema faced significant management challenges in areas that affect the national flood insurance program , which is to provide direct disaster relief after floods .

these challenges included ineffective collaboration among offices within the agency that were responsible for administering the program , including the mission support office , which provided mission - critical functions , such as it , acquisition , and financial management .

to address these challenges , we recommended , among other things , that fema develop protocols to encourage and monitor collaboration between the office that administers the national flood insurance program and relevant support offices , including those for it , acquisition management , and financial management .

in response , the mission support bureau had an ongoing effort to meet with fema program offices to better understand the needs of their customers and encourage collaboration between the bureau and these offices .

in 2015 , we identified this program on our high - risk list due to the structural weaknesses on how the program is funded and weaknesses in management and program operations .

in response , officials stated that the agency is in the acquisition stage for a new it system for the entire national flood insurance program .

we also reported , in 2009 , that fema faced coordination issues and technical challenges in developing and implementing the ipaws program .

specifically , the agency faced challenges related to systems integration and development .

subsequently , we recommended that the secretary of homeland security direct the administrator of fema , as ipaws is developed and deployed , to establish and implement a plan to verify , among other things , the dependability and effectiveness of systems used to disseminate alerts .

dhs agreed with our recommendation and stated that fema was developing an ipaws strategic plan and had modified existing plans to address these issues .

similarly , in november 2015 , the dhs ig reported that fema has taken steps to improve its it management since the ig's 2011 audit , but more remained to be done .

specifically , in 2011 , the ig had reported that fema it systems did not fully support mission needs , were not integrated , and did not fully provide needed capabilities because , in part , they had been developed , patched , and interconnected in an ad hoc manner .

as a result of system limitations , the ig noted that end users were engaged in inefficient , time - consuming business practices , creating their own tools such as spreadsheets and databases .

accordingly , the ig recommended that the cio implement a plan of action and milestones to address the integration and reporting limitations of existing systems .

the ig also recommended that the cio implement and enforce a standardized , agency - wide process that sufficiently defines and prioritizes the acquisition , development , operation , and maintenance requirements for all systems .

in november 2015 , the ig reported that this recommendation remained open .

fema faces a number of challenges in ensuring that its it systems adequately support the agency's ability to respond to major disasters .

specifically , the agency has not ( 1 ) established a sufficient framework for providing governance and oversight of its it investments , ( 2 ) developed adequate plans for modernizing its it environment to reduce its reliance on duplicative and outdated systems , and ( 3 ) taken sufficient steps to address gaps in its it workforce .

these weaknesses can be attributed , in part , to the agency not viewing these challenges as a management priority .

until it adequately addresses these challenges , fema will be hindered in ensuring that its it systems provide the needed support for its disaster response mission .

gao's it investment management framework is composed of five progressive stages of maturity that mark an agency's level of sophistication with regard to its it investment management capabilities .

such capabilities are essential to the governance of an organization's it investments .

at the stage 2 level of maturity , an organization lays the foundation for sound it investment processes that help it attain successful , predictable , and repeatable investment control processes at the project level .

these processes focus on the agency's ability to select , oversee , and review it projects .

according to the framework , a foundational component of effective it investment management includes the following practices: establishing an it investment review board composed of senior executives from both it and business units that is responsible for defining and implementing the department's it investment governance process .

in instituting an investment board , the organization's it investment process guidance should lay out the roles of key boards , working groups , and individuals involved in the organization's it investment processes .

establishing and implementing policies and procedures for selecting and reselecting it investments that meet the agency's needs , and integrate these with funding and selection decisions .

this includes selecting projects by identifying and analyzing projects' risks and returns before committing any significant funds to them and selecting those that will best support the agency's mission needs .

establishing and implementing policies and procedures for overseeing it projects by reviewing the progress of projects against expectations and taking corrective action when these expectations are not being met .

while fema has begun to address these practices , its investment governance framework does not adequately incorporate key elements: establishing it governance boards .

in september 2014 , the agency reestablished its it governance board , co - chaired by the deputy administrator and the chief information officer and made up of senior leadership from each of fema's major headquarters program and support organizations and three regional administrators .

subsequently , in may 2015 , a charter was finalized that documented the mission of the board to assist and support leaders in their responsibility to ensure that it roles , responsibilities , and resources are properly aligned with business needs ; it performance is managed ; risks are mitigated ; and benefits are realized .

since the board was reestablished , it has met on a monthly basis to review progress and issues related to fema's it .

for example , in july 2015 , the board discussed the need for a detailed breakdown of the tasks associated with the $68.7 million eadis task order .

in particular , the board members stated that some of the lack of detail could be attributed to funds being obligated to the contractor without having firm requirements .

during the september 2015 governance meeting , the ocio discussed , among other things , their approach for strengthening service - level management capabilities by conducting a series of workshops with fema leadership and programs .

while the it governance board is operating and has defined the membership of the board , it has not defined the roles and responsibilities of key board members , working groups , and individuals involved in the organization's it investment processes .

for example , the agency's charter describes the role of the board's co - chairs and chief of staff , but it does not specify the responsibilities of the other board members , including the regional administrators .

in addition , fema has not defined how its regional working group is to collaborate with the it governance board .

in april 2014 , fema established the regional it chief committee , a subordinate committee to the ocio , co - chaired by the it disaster operations branch chief and an elected individual from the group .

according to the draft charter , this committee is intended to be responsible for providing information to the governance board to ensure that it initiatives will be viewed from a field perspective during all phases .

specifically , this committee is to serve as the it authority on regional resources and capabilities , including developing and maintaining inventories of regional it assets and working with senior it management to apply standardization across fema regions .

however , the charter does not define the procedures for how the committee is to collaborate with the it governance board to carry out its duties .

in particular , according to fema ocio officials , regional administrators regularly update each other on significant topics and issues , but this is done on an ad hoc basis .

further , the committee's charter is not yet finalized and , according to fema officials , the current membership is being reviewed to include the cio and deputy cio .

nonetheless , the officials did not identify time frames for finalizing the charter .

without clearly defined roles and responsibilities , fema has less assurance that investments will be reviewed by those with the appropriate authority and aligned with agency goals .

establishing policies and procedures for selecting investments that meet business needs .

while fema's governance board has policies for selecting and reselecting investments , it lacks procedures for doing so .

specifically , while the charter calls for the review and selection of it investments at least annually by recommending and presenting a list of it investments proposed for funding to the board and executive - level business decision making authority , it does not specify the procedures to do so .

in addition , fema's charter states that the board is to make a recommendation to terminate or retire an investment from the portfolio to the it investment business owner , but it does not specify the process for doing so .

in the absence of such procedures , fema's decision makers lack a common understanding of the process and the cost , benefit , schedule , and risk criteria that will be used to select or reselect it projects .

establishing policies and procedures for providing investment oversight .

the agency's board does not have policy and procedures to ensure that corrective actions and related efforts are executed by the project management team and tracked by the board until the desired outcomes occur .

in particular , fema requires the investment board to review and approve project planning materials , such as business cases , project plans , and acquisition strategies for addressing system issues .

however , there are no procedural rules for the investment board's operation and for decision making during project oversight or that require corrective actions when the project deviates significantly from its plan .

consequently , fema cannot ensure that investments are meeting cost and schedule expectations and that appropriate actions are taken if these expectations are not being met .

strategic planning is essential for an organization to define what it seeks to accomplish , identify strategies to efficiently achieve the desired results , and effectively guide modernization efforts .

key elements of it strategic planning include an it strategic plan with well - defined goals , strategies , measures , and timelines to guide these efforts .

our prior work has found that , according to best practices , an it strategic plan should define the agency's vision and provide a road map to help align information resources with business strategies and investment decisions .

additionally , our work has found that effective modernization planning includes defining the scope of the effort , an implementation strategy , and a schedule , as well as establishing results - oriented goals and measures .

fema has strategic planning efforts under way to guide its it modernization , but the plans are not current and not yet complete .

for example , the it strategic plan describes the cio's mission , goals , and objectives for fiscal year 2013 through 2016 , but has not been updated since 2013 , even though the plan calls for an annual update .

furthermore , it contains elements that are no longer current with fema's ongoing modernization efforts , as the following examples illustrate: the plan is intended to align with the fema strategic plan , which has been updated since to reflect fiscal years 2014 to 2018 and the dhs it strategic plan , which has been updated to reflect fiscal years 2015 to 2018 .

the plan identifies specific cio priorities only for fiscal year 2012 to 2013 .

ocio officials acknowledged that the plan is no longer current .

the cio explained that updating the current strategic plan had not been a priority and the agency would rather spend resources on developing a new plan that incorporates a global fema it strategic vision .

however , a date by which a revised plan will be completed has not been established .

establishing a current strategic plan would allow the agency to align its information resources with its business strategies and investment decisions .

according to fema ocio officials , modernizing fema's current it environment is important because it suffers from duplication and inefficiencies .

for example , according to ocio officials , its systems are not intuitive and they require managers and leaders to spend significant time logging into various systems to complete and approve numerous forms .

regional staffs are frustrated by having to remember 18 to 20 passwords in order to access systems that minimally support their work needs .

according to ocio officials , the agency's systems need to be replaced with more secure and easy to use tools that better support activities focused on disaster survivors .

the current systems used to deliver and manage these services have been neglected or cobbled together based on immediate needs ( “disaster - driven” ) , often without an orchestrated fema - wide approach for acquiring and managing it .

additionally , over half of the physical servers in fema are more than 5 years old , increasing the risk of hardware failures that could affect the delivery of mission - essential systems .

while the cio has begun to develop a draft it modernization plan that includes the scope of its efforts , an implementation strategy , and schedule of its overall modernization effort , it is not yet complete .

specifically , fema ocio officials stated that the it modernization plan is to provide the steps and investments needed to realize a transformed it .

according to the ocio , the plan is to be completed by april 2016 .

without complete and up - to - date planning documents , including the strategic and modernization plans , fema will be unable to move toward its ultimate goal of modernizing and eliminating duplicative it investments .

key to an agency's success in managing its it systems is sustaining a workforce with the necessary knowledge , skills , and abilities to execute a range of management functions that support the agency's mission and goals .

achieving such a workforce depends on having effective human capital management , which includes assessing current and future agency skill needs by , for example , analyzing the gaps between current skills and future needs , and developing strategies for filling the gaps .

taking such steps is consistent with activities outlined in human capital management models that we and the office of personnel management have developed .

in july 2015 , we reported that fema had not yet resolved and fully addressed various long - standing workforce management challenges in completing and integrating its strategic workforce planning efforts , which we have identified since 2007 .

in particular , we reported that fema had taken or was planning various actions to address a number of these issues .

however , since many of these efforts were ongoing and not yet completed , it was not clear whether they would be effective or to what extent they would address our prior recommendations and challenges that we and others had identified .

similarly , while the agency has taken initial steps to assess the needs of its it workforce , it has not yet established time frames for completing workforce planning efforts and it lacks an understanding of its regional it workforce .

in april 2014 , ocio conducted two activities to obtain a greater understanding of fema ocio personnel resources .

for example , it completed a workforce capability survey designed to determine the skill levels of ocio employees and a workload analysis in which almost 800 it activities performed by the ocio were assessed to determine staffing requirements .

the report resulting from these activities noted that the fema ocio organization had a shortage of 150 full - time equivalent positions .

accordingly , the report recommended three actions for the ocio to close the workforce gaps , including to develop and implement ( 1 ) a human capital management strategy , ( 2 ) an ocio workforce staffing plan based on in - depth staffing analysis and managed at the corporate governance level , and ( 3 ) an ocio workforce training plan based on in - depth analysis and managed in conjunction with the staffing plan at the corporate governance level .

however , these actions have not yet been completed .

according to the cio , the agency will revisit this report , including the recommendations , upon completion of its it modernization plan .

additionally , the analysis fema conducted only covered the ocio , and according to agency officials , there are significant numbers of it staff and related expenditures outside this office , including in the regions .

the regions included in our review all stated that it workforce planning was a significant challenge .

for example , it officials from one regional office stated that the biggest challenge it faces is a lack of it staff ( eg , highly skilled computer engineers and network engineers ) .

further , officials from another regional office stated they do not have enough full - time equivalents to handle disasters .

thus , they need to utilize external resources , which take additional time to deploy .

while the ocio officials stated that the office is in the process of baselining the number of it staff who perform it functions in the regions and offices , it has not yet done so .

according to the report and cio , analyses of regions and offices will be integrated into future workforce planning efforts .

fully assessing and implementing actions to close these competency gaps will be critical to ensuring fema has the skills it needs to adequately support its ability to respond to major disasters .

without a better understanding of its current it workforce , including staff in all regions and offices , fema will be unable to solve its workforce planning needs , as the cio has acknowledged .

in addition , until it fully develops plans for modernizing its it infrastructure ( as previously discussed ) , the agency will not be positioned to assess the skills its workforce will need in the future .

the weaknesses in governance , it strategic planning , and workforce efforts are due , in part , to the agency not viewing these challenges as a management priority .

for example , as discussed earlier , the agency has not yet established time frames for completing key it strategic planning activities , such as updating the strategic plan or workforce gap reviews .

until fema addresses these weaknesses , it will be difficult for the agency to ensure that its it systems adequately support the ability to respond to major disasters .

the three selected major emergency management programs that we reviewed had not consistently implemented it management controls in four key areas: ( 1 ) risk management , ( 2 ) requirements development , ( 3 ) project planning , and ( 4 ) systems integration and testing .

for example , they had not always developed complete risk mitigation plans , ensured that stakeholders were adequately involved in identifying requirements , developed acquisition strategies or maintained cost and schedule estimates , and developed plans for integrating and testing system components .

these weaknesses stemmed in part from gaps in fema policy for implementing these controls .

as a result , the agency has less assurance that its emergency management systems , once delivered , will be within cost and schedule parameters and that current systems meet the capabilities needed by its users .

according to the software engineering institute's capability maturity model integration for acquisition ( cmmi - acq ) and the pmbok® guide , an effective risk management process identifies potential problems before they occur , so that risk - handling activities may be planned and invoked as needed across the life of the project in order to mitigate adverse impacts on achieving objectives .

specifically , key risk management practices include identifying risks , threats , and vulnerabilities that could negatively affect work efforts ; evaluating and categorizing each identified risk using defined risk categories and parameters , such as likelihood and consequence , and determining each risk's relative priority ; developing risk mitigation plans and milestones for key mitigation deliverables for selected risks to proactively reduce the potential impact of risk occurrence , which includes a period of performance , identification of resources needed , and responsible parties ; and monitoring the status of each risk periodically and implementing the risk mitigation plan as appropriate .

while all three programs had fully identified and evaluated key risks , none of the programs had developed adequate mitigation plans , and only one was fully monitoring risks' status .

table 1 provides a summary of the status of the three programs' implementation of key risk management activities .

additional details on each program's implementation of these practices are provided below the table .

daip: the daip program office identified risks , threats , and vulnerabilities that could negatively affect work efforts in its risk register , including risks related to schedule and technology .

for example , a key risk identified by the program's august 2015 risk register was not having participation from key security stakeholders in the integrated project teams , which could result in not receiving early feedback to inform the proposed solution .

this could lead to either a delay in the design and progression of the solutions or the team having to spend time and money heading down a path that will not be approved .

the program office also evaluated and categorized each identified risk using defined risk categories and parameters , such as likelihood and consequence , and determined each risk's relative priority .

for example , the program office reported that the key risk mentioned above had a high probability of occurrence , high scope impact , high schedule impact , and medium cost impact , thus resulting in a high priority level .

nonetheless , the program office did not develop adequate risk mitigation plans for selected risks to proactively reduce the potential impact of their occurrence .

for example , the risk mitigation plan described the overall mitigation action and who was responsible , but did not include details on what , when , and how it will be done to avoid the risk or minimize consequences if the risk becomes a liability .

as an example , for the key risk previously mentioned , the mitigation plan was for the program management office to work to engage and obtain committed participation from key security stakeholders , but additional details were not provided .

further , while the program office monitored each risk on a monthly basis , it could not demonstrate that it had implemented the risk mitigation plans as appropriate .

emmie: the emmie program identified and analyzed risks by assigning a severity rating to risks and reviewing and evaluating these risks during monthly program risk management meetings .

as of february 2016 , the program office had identified four risks: ( 1 ) the test environment would not have needed capabilities , ( 2 ) the system does not have a backup or alternate processing site , ( 3 ) the system does not have a backup or alternative processing site , and ( 4 ) there are insufficient funds for cost estimates and the alternate processing site .

for these risks , the program office also evaluated and categorized its risks based on probability and impact .

for example , the program reported that the risks above had a “medium” exposure rating ( meaning it may cause some increase in cost , disruption of schedule , or degradation of performance ) .

however , the program office's mitigation plans for identified risks were not adequate .

for example , the risk mitigation action for the system backup states that there is planned migration of attachments into an enterprise - wide solution , but no details were provided on who will do this , what specifically will be done , or when and how it will be done to avoid the risk or minimize consequences if the risk becomes a liability .

further , while the program office monitored the risks on a monthly basis , it could not demonstrate that it implemented the risk mitigation plans as appropriate .

ipaws: the ipaws program had identified risks , threats , and vulnerabilities that could negatively affect work efforts .

for example , in august 2015 , key risks identified by the program office included the lack of trained acquisition staff in the program office and the possibility of losing a private broadcast station due to industry action or natural disaster , which could cause a major reduction in the ability to broadcast emergency alerts to the u.s. population .

the program office also evaluated and categorized its risks based on probability and impact .

for example , the office reported that the training risk mentioned above has a high risk impact ( meaning it can cause a high degree of cost overruns , schedule delays , and performance failures ) .

while ipaws developed mitigation plans to reduce the potential impact of risk occurrence , they are not adequate .

for example , risk mitigation plans for the two aforementioned key risks included escalating the issue to senior leadership to obtain staff for these positions and continuing outreach activities to industry partners , coordination with government entities , and emphasizing the benefits of services to the public .

the plans did not include a period of performance or identification of resources needed , such as costs associated with implementing the plan .

however , the program office monitors and reports on the status of each risk on a monthly basis , including the steps to be taken to mitigate the risks .

there is no fema guidance to assist programs in establishing a robust risk management process and ocio officials acknowledged the weaknesses in this area .

a robust process identifies potential problems before they occur , such as requiring programs to develop a risk management plan .

until the program offices and ocio establish a risk management process that includes adequate risk mitigation plans and monitoring , they will lack assurance that they are properly managing all program risks .

according to cmmi - acq and the pmbok® guide , appropriate requirements development involves eliciting and developing customer and stakeholder requirements , and analyzing them to ensure that they will meet users' needs and expectations .

it also consists of validating requirements as the system is being developed to ensure that the final system to be deployed will perform as intended in an operational environment .

specifically , key risk requirements development practices include eliciting stakeholder needs , expectations , and constraints , and transforming them into prioritized customer requirements ; developing and reviewing operational concepts and scenarios to refine and discover requirements ; analyzing requirements to ensure that they are complete , feasible , analyzing requirements to balance stakeholder needs and constraints ; and testing and validating the system as it is being developed .

the selected programs in our review varied in the degree to which they implemented the requirement practices .

table 2 provides a summary of the status of the three programs' implementation of key requirements development activities .

additional details on each program's implementation of these practices are provided below the table .

daip: the program's contractor elicited stakeholder needs , expectations , and constraints , and transformed them into prioritized customer requirements .

according to program officials , the program's product management and project management teams collaborate with contracted developers to perform requirements development and management activities using a backlog cataloging system .

for example , changes to requirements are vetted through two different change control boards and backlog reviews , one conducted weekly at the program level among program staff , and the other conducted bi - weekly in conjunction with all developer resources .

however , the program did not provide evidence that it completed operational concepts and scenarios to refine and discover requirements .

also , while the program analyzes requirements to ensure that they are complete , feasible , and verifiable by tracking them in a requirements traceability matrix , it has not provided evidence that it has analyzed requirements to balance stakeholder needs and constraints .

for example , while , according to officials , daip has several mechanisms in place for analyzing requirements to balance stakeholder needs and constraints , including collecting and analyzing feedback from the users as to the functionality in the form of surveys , the program could not demonstrate that this was occurring .

lastly , the program office tests and validates the system .

for example , it performs and validates daip user acceptance testing to verify that requirements are met for systems .

emmie: the program office lacked documentation to show that it had implemented the key requirement development activities .

specifically , according to program officials , much of the documentation , including plans and policies for developing and managing requirements , was not completed because the program was originally a task order to an existing contract .

for example , while program officials stated that initial requirements were validated by user representatives designated by fema's public assistance program , there was no evidence of this .

additionally , the program office did not trace each requirement in a requirements traceability matrix to the program's desired capabilities and technical baselines .

program officials stated that , in the absence of a requirements management process , the emmie program manager works with program leadership to prioritize additional functionality or other changes to the system , translate those requests into change requests , and manage the delivery of requested changes by the emmie contractor .

for example , according to emmie officials , ongoing requirements are validated on a bi - weekly basis by user groups compromised of representatives from fema headquarters and each of the 10 regions .

furthermore , fema officials stated that the agency performs annual reviews with the contractor to evaluate the operational results , but documentation is not available because it is not a written review .

however , the program office did provide evidence that testing and validating of the system was completed as it was being developed .

nonetheless , without documenting critical requirements development activities , the agency lacks assurance that emmie meets the needs of the users .

ipaws: the program office had obtained stakeholder needs and expectations and translated them into draft customer requirements .

specifically , the office compiled and drafted the emergency alert requirements from various executive orders , presidential memoranda , and laws .

additionally , the office developed operational concepts and scenarios to refine and discover requirements and analyze requirements to ensure they are complete , feasible , and verifiable .

for example , it developed a concept of operations that included , among other things , a description of the environment in which the program will operate , including boundaries and constrains , and scenarios , such as the mission support and functional capabilities scenarios .

the program office also analyzed requirements to make sure they were complete , feasible , and verifiable .

for example , requirements and changes to requirements were discussed in monthly change control board meetings .

the ipaws program office did not fully analyze requirements to balance stakeholder needs and constraints .

for example , it did not complete key steps to analyze ipaws requirements , including performing a risk assessment on requirements and design constraints and did not perform a cost - benefit analysis to assess the impact of the requirements on the overall acquisition strategy .

however , the program office had tested and evaluated the system as it was being developed .

specifically , the office had a test plan and test cases to determine whether the system was working as intended .

according to all three program offices and ocio officials , fema policy guidance for requirements management does not exist .

given the lack of guidance , program offices' cannot implement effective requirements management practices .

without it , fema lacks assurance that these programs are delivering systems that will provide functionality that meets users' needs .

according to cmmi - acq , the pmbok® guide , and our prior work , an effective project planning process establishes project objectives and outlines the course of action required to attain those objectives .

it also provides a means to track , review , and report progress and performance of the project by defining project activities and developing cost and schedule estimates , among other things .

key activities in planning the program include establishing and maintaining the program's acquisition strategy ; developing and maintaining the overall project plan , and obtaining commitment from relevant stakeholders ; developing and maintaining the program's cost estimate ; establishing and maintaining the program's schedule estimate ; and identifying the necessary knowledge and skills needed to carry out the program .

the three selected programs varied in the extent to which they implemented these practices .

for example , while ipaws had developed and maintained acquisition strategies , daip and emmie had not .

additionally , all three programs had established a project plan , but did not regularly maintain it , and all three programs had developed cost estimates and schedules , but did not regularly maintain them .

table 3 provides a summary of the status of the three programs' implementation of key project planning activities .

additional details on each program's implementation of these practices are provided below the table .

daip: the daip program office established an acquisition strategy that identified the capabilities the program was intended to deliver , the acquisition approach , and objectives .

additionally , the daip program office developed an overall project plan that included the scope of the program , identified key program milestones , and obtained commitment from stakeholders .

however , the program office had not updated the documents since september 2013 to reflect changing requirements .

further , it also developed a life - cycle cost estimate of about $240 million and a schedule estimate .

however , neither the schedule nor the life - cycle cost estimate had been updated since september 2013 .

according to officials , the program is currently creating a cost estimating baseline document that will better inform its life - cycle cost estimate , which is expected to be completed in early spring 2016 .

finally , the program plan did not identify the knowledge and skills needed to perform the project .

emmie: according to program officials , an acquisition strategy for emmie was not developed .

while the program office provided a draft project plan from april 2006 that included a cost estimate and schedule , it was not maintained on a regular basis .

additionally , while the life - cycle cost estimate for the program was about $28.3 million , the program office was unable to provide documentation on the total amount spent to date for emmie .

officials stated that it is difficult to identify the total amount spent to date for emmie because the total amount is spread across all public assistance grant systems rather than for emmie as a single system .

nonetheless , these officials stated that a reasonable estimate of total amount spent to date is about $22.7 million .

further , the program office could not demonstrate that it had obtained commitment from relevant stakeholders or identified the knowledge and skills needed to carry out the program .

according to program officials , the recovery technology program division does not have a records repository for the emmie program and most key program documentation was not available , including the original program baseline , work orders related to emmie , or performance work statements .

ipaws: the ipaws program had established and maintained an acquisition strategy .

specifically , the strategy identified the capabilities that the program is intended to deliver , the acquisition approach , and the objectives of the acquisition .

the ipaws program also developed an overall project plan that included the scope of the program and identified key program milestones along with obtaining commitment from relevant stakeholders .

however , the program office has not maintained the project plan since june 2010 .

the program office did develop and maintain the program's cost estimate , and as of march 2015 , the life - cycle cost estimate for ipaws was $456 million .

similar to the program's cost estimate , ipaws officials had developed and updated the program's schedule .

according to program officials , they are reviewing the schedule , cost , and performance data on a monthly basis .

however , the program plan did not identify the knowledge and skills needed to perform the project .

there is no fema guidance to assist program offices with project planning and ocio officials acknowledged fema's weakness in this area .

by not implementing the key activities of project planning , the program offices are not ensuring that the programs will be effectively implemented and managed going forward .

testing an it system is essential to validate that the system will satisfy the requirements for its intended use and user needs .

best practices developed by the institute of electrical and electronics engineers ( ieee ) recommend that systems testing should be conducted early and often in the life cycle of a systems development project to allow for the modification of products in a timely manner , thereby reducing the overall project and schedule impacts .

additionally , according to cmmi - acq , to ensure that all components of a system are appropriately integrated , a systems integration plan should be developed .

key activities for systems testing and integration include developing test plans and test cases that include a description of the overall approach for testing , identification of the test items that are the object of testing , the set of tasks necessary to prepare for and perform testing , the roles and responsibilities for individuals or groups responsible for testing , the risk issues that may adversely impact successful completion of the planned testing activities , and the criteria to be used to determine whether each test item has passed or failed testing ; and developing a systems integration plan to identify all systems to be integrated , define roles and responsibilities of all relevant participants , establish the sequence and schedule for every integration step , and describe how integration problems are to be documented and resolved .

the emmie and ipaws program offices had developed adequate test plans and test cases , but only ipaws had developed systems integration plans to ensure all systems to be integrated are identified and describe how integration problems are to be documented and resolved .

table 4 provides a status of the three programs' implementation of key system testing and integration activities .

additional details on each program's implementation of these practices are provided below the table .

daip: the daip program office did not develop adequate test plans or prepare for system integration .

specifically , the program's test plans identified test items but did not identify roles and responsibilities for individuals or groups responsible for testing , and specify the criteria to be used to determine whether each test item has passed or failed testing .

additionally , while program officials stated that about 15 systems interface with daip's assistance center , the program lacks a systems integration plan .

according to program officials , one was not developed and daip was approved to operate without one in 2007 .

officials stated that all system integration problems are tracked and analyzed by both fema and contractors to determine if they are new requirements or defects .

for example , each item is further analyzed for the severity of the issue and a determination is made on how to most efficiently resolve it .

emmie: the emmie program office tested and evaluated the system as it was being developed .

specifically , the program developed a testing plan that identifies the overall scope , method , and strategy to perform the testing .

the plan also identifies the roles and responsibilities of the individuals responsible for testing .

furthermore , program officials stated that during testing , any deficiencies identified are captured as additional requirements .

the change control board , chaired by the program manager , reviews any requested changes and deficiencies on a bi - weekly basis .

however , the program office did not develop a systems integration plan that includes all systems to be integrated , roles and responsibilities for all relevant participants , the sequence and schedule for every integration step , and how integration problems are to be documented and resolved .

officials stated that overall systems integration governance is described in the draft project management plan dated 2006 , but this plan did not discuss , among other things , the strategy for system integration , environment needed to support the integration of the product components , or procedures and criteria for integration of the components .

the program office also lacks documentation of the process associated with updating and maintaining the integration of the systems .

ipaws: the ipaws program office tested and evaluated the system as it was being developed .

specifically , the office developed a testing plan that identifies the roles and responsibilities of the individuals responsible for testing .

it also identifies the overall scope , method , and strategy to perform the testing .

further , ipaws test cases included the necessary elements for a test case , such a unique identifier , which is used to identify each test case ; specified object , inputs , and outputs ; and the steps to take for the execution of the test .

in addition , it identifies the necessary test environment and the criteria used to determine whether a test passed or failed .

furthermore , the office developed an integration plan that includes key criteria , such as ensuring that component interfaces , both internal and external , are compatible .

similar to the other selected it controls , ocio officials acknowledged that fema needs improvements in this area and there is no fema guidance on developing systems testing and integration plans .

without such plans , a risk exists that system testing will occur in an ad hoc and undisciplined fashion and the program offices will be limited in their ability to ensure that the resulting systems are integrated , functioning properly , and delivered on time and within budget to the users .

additionally , without a systems integration plan , the daip program is not ensuring that it is identifying all systems to be integrated and describing how integration problems are to be documented and resolved .

fema has acknowledged the urgency of modernizing its it environment given the duplication and inefficiencies in its systems that agency officials have identified .

however , the agency continues to face a number of challenges .

until the agency addresses shortcomings in its it governance and oversight , strategic planning , and workforce planning , its progress is likely to be limited .

further , inconsistent implementation of key it management controls calls into question how well fema is positioned to efficiently acquire major emergency management systems .

ensuring that these controls are fully implemented and reflected in agency policy will help fema deliver these systems on time and at an acceptable cost , and that they will provide needed capabilities to support the nation's emergency response efforts .

to ensure that fema's it systems can adequately support its ability to respond to major disasters , we are recommending that the secretary of dhs direct the fema administrator to take the following actions: ensure that the it governance board has fully defined and implemented its roles and responsibilities for key boards , working groups , and individuals , and procedures for selecting and overseeing it investments .

define the scope , implementation strategy , and schedule of the agency's overall modernization approach , with related goals and measures for effectively overseeing the effort .

at a minimum , the agency should update its it strategic plan and complete its modernization plan .

establish time frames for current and future it workforce planning during its modernization efforts and ensure all regions and offices are included in these initiatives .

to ensure that fema adequately manages the selected emergency management systems , we recommend that the fema administrator direct the daip , emmie , and ipaws program offices , in conjunction with the fema cio , to implement a robust risk management process that identifies potential problems a requirements management process to ensure requirements are well complete program plans that define overall budget and schedule , key deliverables and milestones , assumptions and constraints , description and assignment of roles and responsibilities , staffing and training plans , and an approach for maintaining these plans ; and a system integration plan that include all systems to be integrated with the system , roles and responsibilities for all relevant participants , the sequence and schedule for every integration step , and how integration problems are to be documented and resolved .

as part of the effort of improving it management at the three programs , we recommend that the fema administrator direct the cio to ensure that fema policy for managing it programs includes guidance for implementing the key management practices .

we received written comments on a draft of this report from dhs's director for the departmental gao - oig liaison office .

the comments are reprinted in appendix ii .

in the comments , dhs concurred with our recommendations .

in this regard , the director described ongoing and planned actions to address the recommendations , and provided milestones for completing these actions .

for example , the director stated that the department plans to establish charters that clearly define roles and responsibilities , as well as procedures , standards , and guidelines for selection and management of investments in the it investment portfolio by december 31 , 2016 .

officials also provided technical comments , which we have incorporated as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to the appropriate congressional committees ; the secretary of the department of homeland security , the administrator of the federal emergency management agency ; and other interested parties .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

should you or your staff have any questions on information discussed in this report , please contact me at ( 202 ) 512-4456 or chac@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

our objectives were to ( 1 ) identify challenges associated with ensuring the federal emergency management agency's ( fema ) information technology ( it ) systems adequately support the agency's ability to respond to major disasters and ( 2 ) assess the extent to which fema has implemented key it management controls for selected emergency management systems .

to address our first objective , we obtained and analyzed fema documentation ( eg , fema's hurricane sandy after - action report ) , prior gao reports , and department of homeland security ( dhs ) inspector general ( ig ) reports .

in addition , we interviewed officials from the national advisory council ; the disaster emergency communications division ; regions 2 , 4 , and 6 ; the mobile emergency response center ( thomasville , georgia ) ; and the national processing center ( maryland ) .

we selected regions 2 , 4 , and 6 because , according to fema officials , these three regions contained the states that had the 10 most costly natural disasters since 2005 .

we also interviewed agency officials from fema's office of the chief information officer ( ocio ) , its office of policy & program analysis , and dhs's ig office to determine challenges associated with its it systems .

subsequently , we identified three challenges that were identified by three or more sources and focused our review on fema's efforts to address them .

these challenges were fema's ( 1 ) it governance board , ( 2 ) it modernization efforts , and ( 3 ) fema's it workforce .

to determine the extent to which fema had adequate controls in place to address these challenges , we compared the agency's efforts to best practices we have identified in the areas of it investment management , human capital management , and strategic planning .

specifically , we compared the controls fema had in place to address the first challenge — the agency's it governance board — against critical processes associated with stage 2 of gao's information technology investment management framework .

in particular , stage 2 of the framework includes the following key processes for effective governance: providing investment oversight .

instituting the investment board , selecting investments that meet business needs , and for the second challenge — the agency's it modernization efforts — we compared fema's controls to the best practices found in , among other sources , office of management and budget guidance .

those practices include developing a strategic plan that includes defining the agency's vision and providing a road map to help align information resources with business strategies and investment decisions .

lastly , for the third challenge — it workforce — we compared fema's it management controls to gao's a model of strategic human capital management and the office of personnel management's human capital assessment and accountability framework .

those practices include analyzing the gaps between current skills and future needs and developing strategies for filling the gaps .

for the second objective , we used the following criteria to select three programs to review: the program was associated with objectives of the systems identified in the post - katrina emergency management reform act of 2006 .

at least one program must have been identified as a major it investment , as defined by the office of management and budget .

the program must have planned spending in fiscal year 2015 .

the program must be disaster - related .

the program must not be fully deployed or have been recently approved for termination .

the program must not have been included in a recent gao or ig review that examined the program's it management controls .

using the above criteria , we selected the following three programs: 1 .

disaster assistance improvement program ( daip ) : a major system used for conducting phone and web surveys with fema external customers and internal personnel .

2 .

emergency management mission integrated environment ( emmie ) : an internet - based enterprise - wide electronic system to manage grants throughout their entire life cycle using a standardized web - based interface .

3 .

integrated public alert & warning system ( ipaws ) : a major system that provides a broad range of messaging capabilities through multiple pathways to ensure the delivery of alerts and warnings reaches more people , more reliably , increasing resilience of local systems to ensure operational readiness .

we then determined the extent to which the three selected programs identified above were implementing key it acquisition practices in the areas of risk management , requirements management , project planning , and systems integration and testing .

we selected these key it management control areas because they are consistent with the requirements of the post - katrina act for fema to take steps to improve its it systems , including: ensuring that the information technology systems of the agency have the capacity to track disaster response personnel , mission assignments task orders , commodities , and supplies used in response to a natural disaster , act of terrorism , or other man - made disaster ; ensuring that the multiple it systems of the agency are , to the extent practicable , fully compatible and can share and access information , as appropriate , from each other ; ensuring technology enhancements reach the headquarters and regional offices of the agency in a timely fashion , to allow seamless integration ; and developing and maintaining a testing environment that ensures that all system components are properly and thoroughly tested before their release .

to determine the extent to which the three programs implemented it management controls , we reviewed documentation from the three selected programs and compared it to key management best practices , including the software engineering institute's capability maturity model® integration for acquisition ( cmmi - acq ) and the project management institute's guide to the project management body of knowledge ( pmbok® guide ) .

we assessed the program as having implemented a practice if the agency provided evidence that it fully addressed this practice , partially implemented if the agency provided evidence that it addressed some , but not all , portions of this practice , and not implemented if the agency did not provide any evidence that it addressed this practice .

in particular , the key risk management best practices were identifying risks , threats , and vulnerabilities that could negatively affect work efforts ; evaluating and categorizing each identified risk using defined risk categories and parameters , such as likelihood and consequence , and determining each risk's relative priority ; developing risk mitigation plans for selected risks to proactively reduce the potential impact of risk occurrence ; and monitoring the status of each risk periodically and implementing the risk mitigation plan as appropriate .

the key requirements development best practices were eliciting stakeholder needs , expectations , and constraints , and transforming them into prioritized customer requirements ; developing and reviewing operational concepts and scenarios to refine and discover requirements ; analyzing requirements to ensure that they are complete , feasible , analyzing requirements to balance stakeholder needs and constraints ; testing and validating the system as it is being developed .

the key project planning best practices were establishing and maintaining the program's acquisition strategy ; developing and maintaining the overall project plan , and obtaining commitment from relevant stakeholders ; developing and maintaining the program's cost estimate ; establishing and maintaining the program's schedule estimate ; and identifying the necessary knowledge and skills needed to carry out the program .

lastly , the key systems testing and integration best practices were developing test plans and test cases ; and developing a systems integration plan to identify all systems to be integrated .

to determine the status of each program's key risks and the actions that were taken to manage these risks , we analyzed program risk documentation , including monthly risk logs and reports , risk management plans , and risk mitigation plans .

for requirements development , we analyzed monthly program management review briefings , business cases , acquisition strategies , concepts of operations , and system requirements documentation .

for project planning , we analyzed project plans , business cases , acquisition strategies , master schedules , and life - cycle cost estimates .

for systems testing and integration , we analyzed testing strategies , test execution plans , change control board meeting minutes and test cases .

additionally , we interviewed program officials to obtain additional information on each program's management control practices .

to assess the reliability of the data that we used to support the findings in this report , we corroborated relevant program documentation and interviews with agency officials .

we determined that the data used in this report were sufficiently reliable .

we conducted this performance audit from january 2015 to april 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact name above , the following staff also made key contributions to this report: eric winter ( assistant director ) , chris businsky , lee mccracken , tyler mountjoy , kate nielsen , teresa smith , and niti tandon .

