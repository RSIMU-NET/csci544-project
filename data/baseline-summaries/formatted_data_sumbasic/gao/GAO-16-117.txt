commercial aircraft have long been a target of terrorists , who have carried out attacks with explosives smuggled onboard in checked baggage as well as with passenger - carried contraband and explosives .

the 1988 bombing of a u.s. aircraft over lockerbie , scotland as well as the terrorist attacks of september 11 , 2001 , illustrate the real threat posed by terrorists against aircraft .

the threat continues today as new and innovative methods — such as the use of artfully concealed homemade explosives — proliferate .

the transportation security administration ( tsa ) , a component of the department of homeland security ( dhs ) , is responsible for overseeing security operations at the nation's roughly 440 commercial airports as part of its mission to protect the nation's civil aviation system .

tsa screens individuals , their carry - on luggage , and their checked baggage to deter , detect , and prevent carriage of any prohibited items , such as explosives and contraband , on board commercial aircraft .

to carry out these activities , the agency relies to a large extent on security - related screening technologies , such as explosives detection systems and advanced imaging technology devices .

in our past work , we have found that tsa encountered challenges in effectively acquiring and deploying passenger and baggage screening technologies and had not consistently implemented dhs policy and best practices for procurement .

in light of these concerns , among other issues , congress enacted the transportation security acquisition reform act in december 2014 , which contained a provision for gao to assess tsa's test and evaluation activities for security - related technologies .

this report assesses the extent to which ( 1 ) tsa's test and evaluation process helps tsa meet mission needs through the acquisition of passenger and baggage screening technologies , and ( 2 ) tsa's planned actions to improve the test and evaluation process address factors contributing to inefficiencies , if any , in acquiring those technologies .

to assess the extent to which tsa's test and evaluation process helps the agency meet mission needs , we reviewed dhs and tsa acquisition documentation for passenger and baggage screening technologies tested since june 2010 .

this documentation included , for example , letters of assessment and acquisition decision memorandums .

we conducted interviews with tsa officials to determine the influence of test and evaluation results on tsa's decisions about which technologies to procure and any reasons for repeated testing , which affected tsa's ability to procure the technologies .

we also met with relevant dhs officials regarding their roles and responsibilities related to tsa's test and evaluation process , which included site visits to the two primary testing facilities for tsa's security - related technologies — the tsa systems integration facility in arlington , virginia and the dhs transportation security laboratory in atlantic city , new jersey .

to identify the extent to which tsa's planned actions to improve the test and evaluation process address factors contributing to acquisition inefficiencies , if any , we reviewed tsa's documentation related to the planned actions .

we also met with representatives from two industry groups with experience related to tsa's test and evaluation process to obtain industry views on any challenges in the test and evaluation process for screening technologies , areas for potential improvement , and tsa's planned actions to improve the process .

finally , we conducted interviews with tsa testing officials regarding tsa's existing and planned mechanisms for tracking technologies throughout the test and evaluation process , and any efforts to assess testing data by looking at , for example , timeframes for completing testing and costs incurred .

this report focuses on the test and evaluation process tsa uses as it acquires screening systems and the extent to which it procures the systems that it tests , and does not discuss the ability of systems to meet mission needs once deployed in the field .

additional details about our scope and methodology are discussed in appendix i .

we conducted this performance audit from april 2015 to december 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

tsa uses security - related technologies to help secure approximately 1.8 million passengers , 1.2 million checked bags , and 3 million carry - on bags on 25,000 flights at roughly 440 federally regulated airports every day .

as of august 2015 , tsa had deployed about 15,000 units of security - related technology to airports nationwide and anticipates spending a significant portion of its $3.6 billion security capability budget on technologies such as these over the next 5 years .

within tsa , the office of security capabilities ( osc ) provides security - related technology solutions through two major dhs acquisition programs , the passenger screening program ( psp ) and the electronic baggage screening program ( ebsp ) : psp technologies , such as advanced imaging technology and bottled liquid scanners , work in combination at airport checkpoints to screen passengers and their carry - on baggage for threats .

ebsp technologies , such as explosives trace detectors and explosives detection systems , help ensure that tsa screens 100 percent of checked baggage for explosives , as mandated by law .

for each technology type , vendors develop their own specific version or system .

see table 1 for an overview of key psp and ebsp technology types .

according to tsa , ebsp and psp are transitioning into a sustainment mode , meaning they are largely not procuring new types of screening technologies and are instead focused on the recapitalization of over 2,400 systems that are reaching their end - of - life over the next 5 years .

in our past work , we found key challenges related to tsa's efforts to acquire technologies .

for example , in march 2014 , we found that tsa's performance assessments of advanced imaging technology systems with automatic target recognition capability did not account for all factors affecting the effectiveness of the system , such as how well the system would perform with human operators , because the assessments were conducted in a laboratory .

we recommended that tsa establish protocols to facilitate capturing operational data on secondary passenger screening at the checkpoint to determine the extent to which rates of false alarms for various advanced imaging technology systems affect operational costs once the systems with automatic target recognition capability are networked .

tsa concurred with this recommendation .

in its comments to our report , tsa stated that it will monitor , update , and report the results of its efforts to capture operational data and evaluate its associated impacts to operational costs .

in january 2012 , we found that tsa did not fully follow dhs acquisition policies when acquiring a different version of advanced imaging technology , which resulted in dhs approving nationwide deployment without full knowledge of tsa's revised specifications .

among other things , we found that tsa had changed key performance parameters during the acquisition process without formally informing dhs acquisition officials , in violation of dhs policy .

we recommended that tsa develop a road map describing when vendors will meet milestones for further developing advanced imaging technology and that tsa brief congress as part of the budget process .

findings from this report resulted in a reduction in planned advanced imaging technology purchases amounting to approximately $1.4 billion .

dhs policies and processes for managing its major acquisition programs , such as tsa's psp and ebsp , are primarily set forth in dhs acquisition management directive 102-01 and dhs instruction manual 102-01-001 , acquisition management instruction / guidebook .

within tsa , the psp and ebsp program management offices are responsible for planning and executing the acquisition programs within the cost , schedule , and performance parameters in their program baselines , which are approved by dhs .

dhs's deputy secretary or under secretary for management serve as the decision authority for psp and ebsp acquisitions since both are large acquisition programs with life cycle cost estimates of over $1 billion .

the decision authority reviews programs at five predetermined acquisition decision events to assess whether the programs are ready to proceed through each of the four acquisition life cycle phases .

during the first phase of the acquisition life cycle — the “need” phase — tsa develops a mission need statement for a potential solution to an identified problem , such as threats concealed on passengers and in their carry - on items .

if the acquisition decision authority concludes that the need is of sufficiently high priority to dhs , the program moves to the second phase known as the “analyze / select” phase .

the tsa program management offices — such as psp and ebsp — communicate this need to potential vendors through requests for information , requests for proposals , and broad agency announcements in search of potential solutions , such as innovative screening technologies .

program managers select the best technology option and , if the acquisition decision authority approves the proposed technology , the program moves to phase three .

the third phase is the “obtain” phase in which tsa develops , tests , and evaluates systems — vendors' versions of the selected technology .

with acquisition decision authority approval , programs enter the final phase , the “produce / deploy / support” phase , and tsa can proceed with procurement and full deployment of tested and qualified systems .

the goal of test and evaluation is to make sure that a product works as intended before it is provided to end - users , such as tsa's transportation security officers .

according to dhs policy , the primary purpose of test and evaluation is to provide timely , accurate information to managers , decision makers , and other stakeholders to reduce programmatic , financial , schedule , and performance risk .

tsa's test and evaluation process is guided by dhs and tsa policies .

dhs issued its test and evaluation policy in may 2009 and tsa issued its policy in july 2010 .

within osc , the operations support division is responsible for testing and evaluating screening technologies in coordination with dhs's transportation security laboratory .

table 2 provides an overview of dhs and tsa test and evaluation roles and responsibilities .

as noted in the table , dhs and tsa both have responsibilities for evaluating vendors' qualification data packages and determining whether to accept systems for testing .

once accepted , the systems undergo tsa's three phase pre - acquisition test and evaluation process to verify requirements .

responsibility for verifying requirements is assigned to testers for each of the three phases: ( 1 ) qualification and certification testing at the transportation security laboratory , ( 2 ) qualification testing at the tsa systems integration facility , and ( 3 ) operational testing at selected airports .

1 .

qualification and certification testing at the transportation security laboratory .

qualification testing is the formal verification and validation of a system's performance and must provide confidence that the system will satisfy desired capabilities in an operational environment .

qualification testing is typically conducted at two facilities , each serving distinct functions .

testing conducted at the dhs science and technology directorate's transportation security laboratory in atlantic city , new jersey is focused on qualifying or certifying that a system meets the probability of detection of all categories of explosives while meeting tsa's designated false alarm requirement .

typically , systems complete detection testing at the transportation security laboratory prior to undergoing the balance of qualification testing at the tsa systems integration facility .

2 .

qualification testing at the tsa systems integration facility .

tsa conducts additional qualification testing at the tsa systems integration facility in arlington , virginia to verify system performance against defined functional requirements , such as system capacities , human factors , physical characteristics , user safety , and system reliability , availability , and maintainability .

this testing may include transportation security officers who exercise standard operating procedures to provide feedback regarding the system's functions in a controlled environment .

systems must pass qualification testing and undergo an operational test readiness review before they can proceed to operational testing .

3 .

operational testing at select airports .

tsa conducts operational testing at select airports to evaluate a system's operational effectiveness and suitability in a realistic environment and to ensure that the airport infrastructure is ready to accept the system .

according to tsa , it selects test sites , in coordination with various stakeholders , using criteria intended to facilitate an unbiased evaluation of the systems .

tsa injects mock threats during operational testing to assess performance of the entire system , including users and standard operating procedures .

operational tests focus on demonstrating that operational requirements have been met and that all critical operational issues have been resolved .

following operational testing , tsa prepares a system evaluation report , which provides effectiveness and suitability determinations along with potential system improvement recommendations .

the dhs director of operational test and evaluation reviews the report and prepares a letter of assessment .

the letter assesses the system evaluation report and the adequacy of tsa's operational test and concludes whether the system is operationally suitable and effective for procurement .

tsa's program management offices place approved systems on qualified products lists , which contain systems that have successfully completed the test and evaluation process and have been approved by dhs .

once a system is on a qualified products list , which tsa maintains for most technology types , such as explosives detection systems , the vendor can participate in tsa's procurement process .

however , placement on a qualified products list does not guarantee that the vendor will receive a procurement contract .

tsa conducts acceptance testing on systems at vendors' factories following production and again once installed at airports to ensure consistency of the manufacturing process , system configuration , and functionality .

post deployment , tsa's office of inspection and the dhs office of inspector general conduct covert tests in airports to identify vulnerabilities in screening processes .

tsa and dhs test and evaluate systems prior to procurement , but covert testing has demonstrated that systems , when integrated into tsa's screening process , do not always work as intended once deployed .

tsa's test and evaluation activities are funded by the program offices , and tsa's life cycle cost estimate for ebsp reflects average annual testing costs of approximately $24 million .

psp officials estimate that the program averages $10 million in annual testing costs .

tsa's security screening must adapt to meet evolving threats since potential terrorists use their knowledge of aviation security measures when planning aviation - related attacks .

new explosives threats can result in heightened detection standards , which may require software upgrades to existing systems or the development of new technologies .

dhs and tsa partner with industry to build , test , field , and sustain security - related technologies .

for example , dhs makes available testing facilities and expertise at the transportation security laboratory that vendors can use , by their choice , to further the development and evaluation of their systems .

tsa's august 2015 strategic five - year technology investment plan for aviation security noted the market for security - related technologies is quite limited , with only a handful of vendors , which creates a significant challenge for small businesses .

according to tsa , substantial funding and time are required by industry to develop , qualify , and produce tsa's screening technologies , which represents a significant barrier to entry .

for example , the procurement cost of an explosives detection system can exceed over half a million dollars .

tsa's approximately 15,000 systems were manufactured by 13 different vendors , with 5 vendors accounting for approximately 81 percent of deployed systems .

consistent with departmental guidance and acquisition best practices , tsa's test and evaluation process supports its acquisition decisions by providing information regarding the ability of passenger and baggage screening technologies to meet mission needs prior to full production decisions .

from june 2010 to july 2015 , only half of the 22 systems that tsa and dhs tested successfully passed qualification and operational testing and were therefore deemed effective and suitable for deployment .

tsa procured all but one of the successful systems .

technology failures during testing , as a result of vendors' immature technologies entering the test and evaluation process , often required significant fixes and have contributed to inefficiencies in tsa acquiring the technologies for use in airports .

tsa's test and evaluation process is a critical means of providing dhs and tsa officials with accurate information about security - related technologies to support acquisition decisions .

we found that the process is consistent with dhs and tsa policies and guidance and helps tsa ensure that the technologies it acquires fulfill mission needs upon deployment .

for example , tsa has integrated end users , such as transportation security officers into its test and evaluation process , consistent with tsa test and evaluation policy .

also , tsa officials said they have designated program office liaisons to improve communication between testing officials and the tsa program offices responsible for procuring the screening technologies to help ensure program offices have the testing information they need to inform the procurement decision , in line with dhs test and evaluation policy .

in addition , tsa establishes system evaluation teams for each system undergoing test and evaluation ; the teams consist of the tsa and dhs officials that plan for and evaluate system effectiveness and suitability throughout the testing process .

further , tsa's test and evaluation process is designed to identify and evaluate existing technologies that can be adjusted or repurposed to meet the agency's needs .

this goal is in line with department guidance underlining the importance of pursuing technologies , specifically commercial off - the - shelf technologies that do not require a substantial investment of time and money to ensure they are effective once procured .

we previously found that the validation of product knowledge early in the acquisition process and before key investments are made is consistent with best practices by commercial firms .

specifically , leading commercial firms strive to detect as many problems as possible during testing , which in turn leads to easier and less expensive improvements to products down the road .

leading commercial firms highlighted the importance of perceiving problems during testing , not as failures , but as knowledge which can be used to improve a product .

consistent with this practice , while vendors' proposed systems may fail tsa's test and evaluation process , these failures can help the government identify needed fixes to better ensure that they will be effective and suitable prior to being deployed to airports nationwide .

further , we previously found that cost overruns and underperformance of technologies are exacerbated when problems discovered during testing are not resolved .

for example , costly redesigns and retrofits could be required to achieve satisfactory performance of units already deployed in the field .

by conducting operational testing on systems prior to procurement , tsa is working to prevent such issues .

of the 22 psp and ebsp systems tsa and dhs tested from june 2010 to july 2015 , 11 successfully completed qualification and operational testing , and tsa procured all but one of the 11 .

an additional 8 systems were tested during this period and testing remains ongoing .

in addition , during this period one vendor withdrew its system from the testing process .

these 9 systems are not depicted in figure 1 below , which shows the number of systems that made it through each stage of tsa's test and evaluation process during this period .

additional detail on the outcomes of vendors' proposed systems follows .

four additional systems were proposed by vendors during the 5-year period , but did not proceed to qualification testing because tsa did not accept the vendors' qualification data packages .

tsa officials told us they return nearly all qualification data packages to vendors at least once for additional clarification and documentation .

four systems did not pass qualification testing and therefore did not proceed to operational testing .

for example , one proposed passenger screening system did not meet 15 of tsa's 165 functional requirements .

in this case , tsa identified 6 of the 15 failures as issues that could adversely affect an operational or mission essential capability and for which there is no known work - around solution .

deficiencies for this system included issues with safety , information security , maintenance , and system dimensions , among others .

seven systems either did not pass operational testing or had operational testing halted by tsa during this 5-year period .

operational testing is the first time a system is subject to realistic operational demands and used by a variety of transportation security officers ; thus , problems with some systems are generally not detected until this phase of testing .

eleven systems successfully passed qualification and operational testing ; tsa procured 10 of them .

of these , tsa placed 9 on its qualified products lists , meaning that they were eligible for procurement .

tsa procured the 10th system , second generation advanced imaging technology , independent of a qualified products list .

tsa officials stated that test and evaluation results are a primary input into their acquisition decisions , but that they also consider factors such as mission , threats , and cost .

a senior tsa acquisition official explained that tsa could decide to procure systems that have minor issues during testing and perform additional testing after the system is in airports to ensure that the problems are resolved .

similarly , tsa may choose not to procure systems that successfully complete tsa's test and evaluation process .

placement on tsa's qualified products list allows a vendor to participate in tsa's procurement process , but it does not guarantee a procurement contract .

however , tsa has awarded a contract to buy all 9 systems it tested and placed on its qualified products list since june 2010 .

industry officials noted that there is an expectation among vendors that if they meet the requirements as outlined by tsa they will at least be able to compete for procurement .

we found that , from june 2010 to july 2015 , one system — a portable explosives trace detector system — passed operational testing but was not procured .

this system was intended to enhance the ability of transportation security officers to randomly screen passengers' hands and their accessible property for traces of explosives residue .

in july 2012 , following approximately three and a half months of testing , tsa found that this vendor's system met requirements , including probability of detection of required detection threats , and it was found to be operationally effective and operationally suitable with limitations .

however , according to senior tsa officials , a new threat subsequently emerged .

the tsa acquisition review board , which reviewed the potential acquisition in advance of the dhs acquisition decision authority , decided to wait for a system that met tsa's new detection requirements ; thus , tsa issued an acquisition decision memorandum in november 2012 deferring the procurement .

a senior tsa acquisition official stated that this decision was a positive action because the system did not meet tsa's mission needs and therefore should not have been procured at that time .

tsa officials emphasized that immature technologies submitted by vendors are a key driver of testing failures and therefore delays in tsa's ability to buy screening systems for use in airports .

because immature technologies often experience multiple failures during testing and require multiple retests , testing costs more and takes longer than originally anticipated .

after a testing related failure , vendors will usually modify their systems to address deficiencies and then re - submit their systems for additional tsa testing .

according to tsa leadership , the security - related technologies industry is still maturing , since it primarily developed after the terrorist attacks of september 11 , 2001 .

as a result , tsa has worked extensively with industry to help develop systems that will meet tsa's mission needs .

however , tsa's extensive work with vendors has contributed to inefficiencies in purchasing screening systems .

tsa's assistant administrator for osc , who serves as the tsa chief technology officer , stated that tsa wants to reduce its role in system development in order to increase efficiency and certainty in the test and evaluation process so that it can be a true purchaser of these systems .

we found that 4 of the 11 systems that successfully passed tsa's testing process since june 2010 required at least two formal rounds of qualification or operational testing .

testing officials noted that most of the systems tested during this period required even further testing in addition to these formal rounds .

for example , tsa or dhs also conducted regression testing to verify that corrections made to the previously failed systems did not adversely affect system performance , according to these officials .

tsa testing officials stated that nearly all systems need some modification by the vendor during testing .

industry representatives involved in testing these systems also told us that systems are not always mature when they enter tsa's test and evaluation process , and that they can require significant modifications and retesting before they are ready to be bought and deployed to airports .

tsa officials provided us with examples of three explosives detection systems that required multiple retests , which resulted in acquisition delays of several years .

tsa also ended up spending over $3 million in additional costs incurred in retesting to ensure the systems were effective and suitable .

all three systems had met explosives detection certification and false alarm rate criteria during qualification testing at the transportation security laboratory , but failed to meet additional requirements during qualification testing at the tsa systems integration facility and operational testing at selected airports .

some details of these delays and additional costs are below .

one explosives detection system experienced failures during testing that led to a 19 month delay to the scheduled full rate production decision and an additional $1.2 million in testing costs .

during operational testing in summer 2011 , tsa found that the system was effective with minor limitations , but not suitable because it failed to meet tsa's reliability requirements and negatively impacted the ability of transportation security officers to detect threats at a certain level .

the vendor made significant modifications to the system in fall 2012 , and following additional testing dhs approved full rate production of the system in august 2014 .

the second system experienced a 39 month delay to its scheduled full rate production decision and incurred an additional $1.1 million in testing costs as a result of multiple rounds of retests .

the system began testing in fall 2010 and after follow - on operational testing in fall of 2012 , tsa found the system to be effective with limitations and not suitable due to , among other things , issues with resetting or restarting the system after bag jam failures and reliability .

the system underwent testing by a third party entity in late 2014 to verify that modifications made to the system improved its performance , and dhs approved full rate production in may 2015 .

at the time of our engagement , the third explosives detection system's scheduled full rate production decision had not yet occurred , but it had already been delayed by more than 60 months , resulting in an additional $1.2 million in testing costs .

tsa began qualification testing of the system in fall 2010 , and , after operational testing in fall 2011 , tsa found the system to be effective with major limitations and suitable with minor limitations .

tsa encountered issues with bag tracking and reliability , among other things .

as of summer 2015 , after five rounds of retests , the vendor was pursuing a third party test of the system .

according to tsa and dhs testing officials , the actual single rounds of qualification and operational testing timeframes are not long , but it takes vendors a significant amount of time to correct issues identified during testing , which results in a delay to the start of tsa's next round of testing .

we found in june 2014 that the difficulties in getting vendors systems to meet the higher level detection for advanced imaging technology required the vendors to go back and conduct remedial developmental testing activities .

we also found that the qualification testing environment is not conducive to remediating deficiencies that require extensive additional development work because feedback to vendors during this phase is limited .

tsa testing officials stated they believe that vendors submit immature technologies and use the testing process to inform further system development , but tsa expects systems to be fully mature at the start of testing .

tsa has acknowledged the need to better ensure technology maturity at the start of testing to improve the efficiency of its test and evaluation process .

in addition to other efforts , a key action tsa is taking to achieve this goal is developing a third party testing strategy — through which a third party tester will help ensure systems are mature prior to entering tsa's test and evaluation process .

tsa plans to implement this strategy in 2016 , but it is too soon to tell whether the strategy will address all of the factors that contribute to acquisition inefficiencies because tsa has yet to finalize its key aspects .

for example , tsa has not identified whether there are a sufficient number of eligible third party testers or established a mechanism to oversee the testing they will perform .

further , tsa did not conduct a comprehensive assessment of testing data prior to developing its third party testing strategy and may be missing opportunities to identify additional areas for improvement in its acquisition process .

as discussed earlier in the report , tsa's test and evaluation process is consistent with policies and guidance outlined by dhs and tsa as well as acquisition best practices ; however , tsa has acknowledged the need to improve the efficiency of the process by better ensuring technology maturity at the start of testing .

for example , the office of security capabilities' most recent strategic plan as well as tsa's overall 2015 strategic technology investment plan both note that the test and evaluation process needs to be more efficient and agile in responding to emerging threats .

tsa has recently initiated two specific reforms to improve the efficiency of the test and evaluation process: increased transparency with vendors and a third party testing strategy .

first , to increase transparency , tsa officials told us that they are sharing test plans with vendors to better prepare them for testing .

while industry officials agreed that tsa has become more transparent , they said that the number of test plans that tsa has shared thus far have been limited .

tsa also created a test and evaluation process guide to inform stakeholders — particularly industry — of the test and evaluation process , including the test and evaluation phases and roles / responsibilities of tsa , dhs , and industry .

tsa also holds periodic industry days to update vendors on planned procurements and changes to acquisition processes , among other things .

however , tsa test and evaluation guidance necessarily limits the extent to which testing officials can share information with industry throughout the testing process .

tsa's director of test and evaluation stressed that , to maintain the integrity of the test process , they do not intend to provide vendors with detailed information that could be used to “game” the tests .

for example , according to officials from the transportation security laboratory's independent test and evaluation division , the certification and qualification testing it conducts — unlike developmental testing assistance made available by the laboratory to vendors — provides , by design , limited feedback between testers and the vendors , to ensure the integrity of the test and evaluation process .

second , tsa is developing a third party testing strategy , which it has partially implemented for technologies that have already entered the test and evaluation process .

according to interim guidance effective in july 2014 , a vendor experiencing a significant failure during testing is required to fund and undergo third party testing and provide results to tsa , demonstrating that the system has met the previously failed requirements before the system is allowed to resume tsa's testing process .

officials told us that the guidance exempts detection - related testing .

at this time , according to tsa and dhs officials , third party testing cannot replace the explosives detection testing conducted by the transportation security laboratory because no other testing facility has the same capabilities .

since tsa issued the guidance , one vendor has utilized a third party tester to verify that an operational availability problem discovered during follow - on operational testing was resolved .

tsa is in the process of implementing the primary piece of the strategy , which establishes additional third party testing requirements that vendors must meet before entering the test and evaluation process .

figure 2 provides an overview of tsa's use of third party testing following a technology failure during testing .

tsa is developing a third party testing strategy and plans to implement it for select technologies in 2016 .

the strategy is a key tsa effort to improve technological maturity and ensure readiness for testing in order to reduce the number of technology failures during testing .

a key aspect of this strategy is to strengthen requirements for vendors' qualification data packages , which are required to enter the test and evaluation process .

currently , tsa requires some third party testing to confirm certain requirements in these data packages , such as compliance with emissions and radiation standards .

but for other requirements , at present vendors simply can attest that they have been met rather than provide independent verification .

the strategy could increase the number of requirements that must be independently verified in qualification data packages for some technologies , though tsa has not yet worked out the details .

tsa has taken some steps to develop the strategy .

for example , tsa is working in coordination with the national institute of standards and technology to develop requirements for third party testers in accordance with international standards for test laboratories .

tsa has communicated the developing strategy to key stakeholders , including dhs and industry , through meetings with dhs officials , industry days , and a request for information for potential third party testers .

also , tsa is taking steps to prioritize the requirements that should be independently verified in vendor qualification data packages .

however , tsa has yet to finalize key aspects of the strategy to ensure that it will be successful before it is formally required next year .

tsa officials are unsure of how many third party testers may be able to provide the necessary services to vendors .

without knowledge of the number of third party testers that can provide the necessary services , it is unclear whether there will be enough testers to satisfy the demand for this service .

further , if the number of testers is limited , it may increase the cost of testing services .

tsa has not yet established a process for approving or monitoring third party testers , or established how often they will need to be recertified .

tsa officials stated they may choose to have an independent party approve third party testers after initial implementation of the strategy .

at the time of our review , tsa was finalizing the third party test application process for potential third party testers to apply for tsa approval .

tsa officials estimated that this process would be completed by the end of march 2016 .

tsa has not established a mechanism to oversee third party testing .

dhs and tsa officials stated that the extent of dhs's role is dependent on how the third party testing strategy is implemented .

tsa officials believe that the strategy will reduce the number of rounds of retests and decrease the time it takes for tsa to test systems .

however , officials stated that there is no guarantee that third party testing will shorten acquisition timelines .

tsa officials are unsure whether the third party testing strategy will save overall acquisition costs , which they have highlighted as a potential benefit .

specifically , while third party testing will shift the costs of retesting from tsa to the vendors , industry officials told us it is probable that vendors will reflect these additional costs in their pricing .

tsa officials responsible for planning and implementing the third party testing strategy told us they had not assessed the potential impacts of vendors paying for testing on a system's costs , or the possibility that third party testing costs could be a barrier to entering the market for new vendors .

as we established in prior work , components of sound planning include , among other items , identifying: problems and causes ; resources , investments , and risks ; roles , responsibilities , and coordination ; and integration among and with other entities .

if tsa does not finalize key aspects of its strategy prior to implementing further third party testing requirements for vendors to enter testing , it may cause unintended consequences , such as increasing acquisition costs or creating a barrier to market entry for new vendors , such as small businesses .

at the time of our review , tsa had not conducted a comprehensive assessment of testing data , such as timeframes for completing testing and costs incurred , because it lacked a mechanism to track and consolidate testing data across all technologies , such as testing delays , costs , timeframes , and results .

thus , tsa does not have any documented assessment supporting the decision to implement the third party testing strategy and was unable to provide us with testing timeframes for each of the 22 systems tested in the past five years .

tsa tracks testing delays and costs in separate systems and provides actual testing timeframes in the test reports for each system .

however , after we raised this point during the course of our review , tsa officials developed a master testing tracker to more comprehensively track testing data .

the master tracker consolidates testing data to generate a holistic view of specific technologies by summarizing testing timeframes , delays , costs , and progression through testing , among other things .

tsa testing officials stated that the master tracker will help them to support tsa leadership by consolidating data for each system in one place , thereby eliminating the need to go through individual test reports to gather specific testing information , such as testing timeframes .

as noted above , in osc's strategic plan for 2013 to 2016 , it expressed a commitment to risk - informed , data driven analysis and decision - making .

while the master testing tracker tsa plans to develop is a positive first step towards more informed decision - making , officials have not established a plan for assessing the information collected from the tracker .

conducting a comprehensive assessment of these testing data is the critical next step .

tsa has previously failed to assess information it collected with respect to third party testing .

specifically , while developing its third party testing strategy tsa issued a request for information in december 2012 about potential third party testers and their capabilities .

however , senior tsa testing officials stated that tsa did not evaluate any of the responses that it received from potential third party testers because tsa intended the request to be a mechanism to provide vendors with information .

according to standards for internal control in the federal government , ongoing monitoring should occur in the course of normal operations .

in addition , we previously found that agencies can use performance information to identify problems in existing programs , to try to identify the causes of problems , and / or to develop corrective actions .

the benefit of collecting performance information is only fully realized when this information is actually used by agencies to make decisions oriented toward improving results .

tsa's actions to address the repeated technology failures during testing which have led to acquisition inefficiencies — in large part through its third party testing strategy — focus on improving technological maturity .

however , tsa and industry officials we spoke with identified additional issues that may be contributing to the inefficiencies , which third party testing may not address .

specifically , tsa and industry officials highlighted issues pertaining to the program management offices' development of acquisition schedules .

some industry officials we met with stated that the testing timeframes in acquisition schedules , specifically for explosives detection systems , are unrealistic .

for example , tsa has communicated to vendors that testing for explosives detection systems will take around three years ; however , according to an industry official this testing typically takes four or more years .

testing officials stated that they provide the program offices with time estimates for how long a single round of testing should take , but that it is ultimately the program office's decision regarding how many rounds of testing to include in the schedule .

tsa program officials acknowledged that they typically set optimistic schedules , assuming that technologies will not require multiple rounds of retesting .

however , testing officials have stated it is rare for technologies to pass testing the first time .

in addition , industry officials identified issues with how requirements have been defined and interpreted in the past .

for example , one industry official stated that there is a disconnect among tsa , dhs , and vendors about how operational and functional requirements will be interpreted and evaluated during the testing process .

this official stated that when tsa tested a system submitted by the company that the official represents , the vendor and tsa interpreted a requirement about operational availability differently , specifically with regards to whether the requirement applied to the system on its own or to the system as integrated into the airport infrastructure .

the vendor failed testing and was required to make system modifications prior to tsa's qualification of the system .

tsa testing officials also noted problems with how tsa has defined , interpreted , and shared requirements .

for example , tsa developed two sets of requirements for its 2010 competitive procurement for explosives detection systems — requirements that systems must meet and requirements that tsa would like the systems to meet , which are referred to as minimum “shall” and non - minimum “shall” requirements , respectively .

however , key stakeholders have disagreed on how these requirements should be interpreted as systems go through the testing process .

in the case of one explosives detection system , the program office disagreed with how testing officials evaluated the test results , noting that the non - minimum “shall” requirements should not have been considered in the evaluation of effectiveness or suitability .

testing officials stated that systems needed to meet some of the non - minimum “shall” requirements in order to fulfill minimum “shall” requirements and therefore those should have been identified as minimum “shalls.” in addition , tsa did not release the operational requirements document for explosives detection systems to vendors prior to testing , and , as a result , vendors did not know what to expect during operational testing , according to testing officials .

tsa is taking steps to address some of these issues .

for example , testing officials stated that tsa plans to have one set of requirements for the next competitive procurement of explosives detection systems .

in addition , tsa's mission analysis division is focused on improving the requirements development process through a more rigorous and structured method that involves the participation of relevant stakeholders , such as industry , dhs , and end - users , who tsa officials stated have not historically been involved in the process .

in march 2015 , industry officials identified additional actions that tsa could take to improve the transparency of requirements .

specifically , industry officials provided formal recommendations to tsa regarding how to further improve the transparency of requirements , such as developing a limited set of key requirements , which are critical to executing tsa's mission , sharing requirements documents in draft form prior to final release , and defining and prioritizing requirements for each phase of the testing process , among other things .

without conducting and documenting an assessment of testing data available to date , such as testing timeframes , costs incurred , and testing delays across all technologies and sharing it with key stakeholders , it is too soon to tell to what extent tsa's reforms will address factors contributing to any acquisition inefficiencies .

specifically , tsa may be missing opportunities to identify other factors , in addition to technology immaturity , that are outside the purview of testing officials .

an overall assessment of testing data could inform potential areas for improvement in the acquisition process , such as incomplete or unclear requirements and unrealistic acquisition schedules .

for over a decade , tsa has secured the nation's civil aviation system amid continued threats .

due to the significant challenge the agency faces in balancing security concerns with efficient passenger movement , it is important that tsa procure and deploy effective passenger and baggage screening technologies .

as the security - related technology industry evolves to meet new and changing threats , tsa's test and evaluation processes are also evolving .

since 2010 , though , only half the systems tested have successfully passed testing and been qualified by tsa for procurement .

technology failures during testing have resulted in multiple retests and inefficiencies in acquiring systems that could potentially help tsa execute its mission .

tsa has taken steps that could improve the maturity of technologies put forth by industry and reduce the burden on tsa's own testing resources .

tsa plans to implement its third party testing strategy for selected technologies in 2016 .

however , because tsa has not finalized certain aspects of this strategy — for example , determining how many potential third party testers are qualified to assist tsa , and how tsa will oversee this component of testing — there is a risk that the strategy will not be as effective as envisioned .

unintended consequences , such as increased acquisition costs , could result .

while the third party testing strategy is likely to help improve system readiness for entering the test process , tsa has not conducted a comprehensive assessment of testing data for baggage and passenger screening systems , and therefore cannot be certain whether the steps currently planned will address other factors contributing to acquisition inefficiencies .

given tsa's emphasis on improving its acquisition and test and evaluation processes , an assessment of tsa's testing data would help guide future tsa reforms .

we recommend that the administrator of tsa take the following two actions: 1 .

to help ensure that actions taken to improve the test and evaluation process address identified challenges , finalize all aspects of the third party testing strategy before implementing further third party testing requirements for vendors to enter testing .

2 .

to help ensure that the reforms tsa has underway are informed by existing information , conduct and document a comprehensive assessment of testing data available to date , such as timeframes for completing testing , costs incurred , and testing delays across all technology areas to identify key factors contributing to any acquisition inefficiencies and potential areas for reform .

we provided a draft of this product to dhs for comment .

in its written comments , reproduced in appendix ii , dhs concurred with both of our recommendations and provided plans of action and estimated completion dates for them .

in response to our first recommendation — that tsa finalize all aspects of the third party testing strategy before implementing further third party testing requirements for vendors to enter testing — dhs stated that tsa plans to begin implementing the third party test program in a phased approach by december 31 , 2016 .

tsa's planned actions include , among other things , finalizing the third party tester application process , communicating the new approach to industry , and developing third party testing requirements .

we believe that these are positive steps to finalize the strategy , but that tsa should also consider the potential consequences of implementing the strategy , such as the cost impacts to tsa and vendors , which will help to ensure the effectiveness of the strategy .

dhs also provided technical comments that we incorporated into the report as appropriate .

we are sending copies of this report to the secretary of homeland security , the tsa administrator , and the appropriate congressional committees .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-4841 or mackinm@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made significant contributions to this report are listed in appendix iii .

the transportation security acquisition reform act included a provision for gao to examine the transportation security administration's ( tsa ) test and evaluation process for security - related technologies .

specifically , this report examines the extent to which ( 1 ) tsa's test and evaluation process helps tsa meet mission needs through the acquisition of passenger and baggage screening technologies , and ( 2 ) tsa's planned actions to improve the test and evaluation process address factors contributing to inefficiencies , if any , in acquiring those technologies .

for our first objective , we reviewed department of homeland security ( dhs ) and tsa acquisition and testing policies and guidance to determine the role of test and evaluation in tsa's acquisition process and compared them to acquisition best practices .

we also analyzed testing documentation provided to us by tsa for the 22 passenger screening program ( psp ) and electronic baggage screening program ( ebsp ) systems tsa has tested since june 2010 , such as test and evaluation plans , test reports , system evaluation reports , data packages for entry into testing , and information regarding the number of retests during testing to determine the number of systems that passed each stage of tsa's testing process .

in addition , we reviewed tsa and dhs acquisition documentation for passenger and baggage screening technologies , including requirements documents , schedules , letters of assessment , acquisition decision memorandums , contract award notices , and information regarding the number of each technology deployed in airports nationwide to determine the number of systems that tsa has tested , qualified for use , and procured since june 2010 .

additionally , we conducted interviews with tsa officials from the office of security capabilities ( osc ) , the office of acquisitions , psp , ebsp , and the mission analysis division , regarding the acquisition and test and evaluation processes , the influence of test and evaluation results on tsa's decisions about which technologies to procure , and reasons for any acquisition inefficiencies .

we also met with dhs officials from the science and technology directorate and the office of program accountability and risk management regarding their roles and responsibilities related to tsa's test and evaluation process .

further , we attended a joint industry and tsa workshop hosted by the airport consultants council focused on tsa's acquisition process for security - related technologies , including its test and evaluation process .

finally , we visited the two primary testing facilities for tsa's security - related technologies — the tsa systems integration facility , in arlington , virginia and dhs's testing facility , the transportation security laboratory , in atlantic city , new jersey to interview testing officials and observe the testing process .

we chose these locations because they are tsa's two primary testing facilities for security - related technologies .

for our second objective , we reviewed tsa strategic planning documents , including tsa's august 2015 strategic five - year technology investment plan for aviation security , osc's strategic plan for 2013 to 2016 , and osc's may 2014 transportation security strategic capability investment plan , to identify tsa's stated challenges with the test and evaluation process and its plans for improvement .

we also reviewed tsa's interim third party testing guidance , implementation timeline , and briefings to industry as well as testing examples , which served as tsa's rationale for implementing the third party testing strategy .

in addition , we conducted interviews with tsa and dhs testing officials , including testers and evaluators , regarding challenges of tsa's acquisition and test and evaluation processes as well as ongoing and planned efforts to improve testing efficiency and transparency .

we also met with representatives from two industry groups with experience related to tsa's test and evaluation process to obtain industry views on any challenges in the test and evaluation process for screening technologies , areas for potential improvement , and their perspectives on tsa's third party testing strategy .

we met with representatives from the airport consultants council's security manufacturers coalition , who represented three of tsa's largest vendors receiving 8 of psp and ebsp's 10 largest contracts to date .

we also met with officials from the homeland security and defense business council , who represented companies providing test and evaluation support services to dhs and tsa .

additionally , we conducted an interview with the dhs director of operational test and evaluation regarding planned efforts to expand dhs oversight of component - level testing , including tsa , from operational to developmental testing and its involvement in the development of tsa's third party testing strategy .

further , we met with officials from the national institute of standards and technology involved in the development of tsa's third party testing strategy to determine tsa's coordination with stakeholders in the development and implementation of the strategy .

finally , we conducted interviews with tsa testing officials regarding tsa's existing and planned mechanisms for tracking systems throughout the test and evaluation process and any efforts to assess testing data , such as timeframes for completing testing and costs incurred and compared them to standards for internal control in the federal government .

we conducted this performance audit from april 2015 to december 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , katherine trimble ( assistant director ) , charlie shivers iii ( analyst - in - charge ) , peter anderson , molly callaghan , william carrigg , kristine hassinger , mark hoover , michael kaeser , jean mcsween , lindsay taylor , and ozzy trevino made significant contributions to this report .

