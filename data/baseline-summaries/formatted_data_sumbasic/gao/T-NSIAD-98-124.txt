i am pleased to be here today to discuss the department of defense's ( dod ) process for assessing and reporting on military readiness .

questions about the validity and thoroughness of this process have been raised for some time now .

various audit and oversight organizations , including our office , have periodically reported on limitations to official unit readiness reports .

congress has expressed concern regarding apparent inconsistencies between official readiness reports and the actual readiness of units in the field .

in addition , congress has expressed concern about dod's lack of progress in integrating additional readiness indicators into official readiness reports and recently passed legislation requiring dod to include the indicators in its reports to congress .

with this history in mind , i would like to frame my comments today around three questions: what corrective actions has dod taken to improve its readiness assessment system ? .

do military readiness reports provided quarterly to congress effectively support congressional oversight ? .

are further improvements to dod's reporting process needed ? .

before i answer these questions , some background on dod's readiness assessment system may be helpful .

dod assesses military readiness at three levels — ( 1 ) the individual unit level ; ( 2 ) the joint force level ; and ( 3 ) the aggregate , or strategic , level .

“unit readiness” refers to the ability of units , such as army divisions , navy ships , and air force wings , to provide capabilities required of the combatant commands and is derived from the ability of each unit to deliver the outputs for which it was designed .

“joint readiness” is the combatant commands' ability to integrate and synchronize units from one or more services to execute missions .

“strategic readiness” is a synthesis of unit and joint readiness and concerns the ability of the armed forces as a whole , including the services , the combatant commands , and the combat support agencies , to fight and meet the demands of the national security strategy .

any discussion of readiness measurement must start with sorts .

this automated system , which functions as the central listing for more than 9,000 military units , is the foundation of dod's unit readiness assessment process and is a primary source of information used for reviews at the joint and strategic levels .

the system's database indicates , at a selected point in time , the extent to which these units possess the required resources and training to undertake their wartime missions .

units regularly report this information using a rating system that comprises various indicators on the status of personnel , equipment , supplies , and training .

sorts is intended to enable the joint staff , the combatant commands , and the military services to , among other things , prepare lists of readily available units , assist in identifying or confirming major constraints on the employment of units , and confirm shortfalls and distribution problems with unit resources .

until the early 1990s , dod defined “readiness” narrowly in terms of the ability of units to accomplish the missions for which they were designed , and sorts was the only nonservice - specific system dod had to measure readiness .

even today , sorts remains an important component of readiness assessment in that data from the system is used extensively by the services to formulate a big - picture view of readiness .

however , limitations to sorts have been well documented for many years by various audit and oversight organizations .

for example , prior reviews by our office and others have found: sorts represents a snapshot in time and does not signal impending changes in readiness .

sorts relies on military judgment for certain ratings , including the commanders' overall rating of unit readiness .

in some cases , sorts ratings reflect a higher or lower rating than the reported analytical measures support .

however , dod officials view subjectivity in sorts reports as a strength because the commanders' judgments provide professional military assessments of unit readiness .

the officials also note that much of the information in the sorts reports is objective and quantitative .

the broad measurements that comprise sorts ratings for resource availability may mislead managers because they are imprecise and therefore may mask underlying problems .

for example , sorts allows units to report the same capability rating for personnel strength even though their personnel strength may differ by 10 percent .

sorts data is maintained in multiple databases located at combatant commands , major commands , and service headquarters and is not synchronized across the databases .

sorts data may be out - of - date or nonexistent for some units registered in the database because reporting requirements are not enforced .

army sorts procedures that require review of unit reports through the chain of command significantly delay the submission of sorts data to the joint staff .

dod is taking actions to address some of these limitations .

the chairman of the joint chiefs of staff was directed last year — in the defense planning guidance — to develop a plan for improving dod's readiness assessment system .

although it has yet to be approved , the joint staff plan calls for a phased improvement to the readiness assessment system , starting with upgrades to sorts .

during the first phase of the plan , the joint staff is addressing technical limitations of sorts .

one of the objectives , for instance , is to ensure that the data is synchronized dod - wide across multiple databases .

future phases of the joint staff plan would link sorts with other databases in a common computer environment to make readiness information more readily accessible to decisionmakers .

in addition , the joint staff plan calls for upgrades to sorts that will make the system easier to use .

separate from the joint staff plan , the services are developing or implementing software to automate the process of entering sorts data at the unit level .

these technical upgrades are aimed at improving the timeliness and accuracy of the sorts database and , therefore , are positive steps .

they , however , will not address some of the inherent limitations to the system .

for instance , the upgrades will not address the inability of the system to signal impending changes in readiness .

in addition , the upgrades will not address the lack of precision in reporting unit resources and training .

another step dod has taken to improve its readiness assessment capability is to institute a process known as the joint monthly readiness review .

the joint review was initiated toward the end of 1994 and has matured over the last year or so .

it represents dod's attempt to look beyond the traditional unit perspective provided by sorts — although sorts data continues to play an important role — and to introduce a joint component to readiness assessment .

we believe the joint review process has several notable features .

first , it brings together readiness assessments from a broad range of dod organizations and elevates readiness concerns to senior military officials , including the vice chairman of the joint chiefs of staff .

second , the joint review emphasizes current and near - term readiness and incorporates wartime scenarios based on actual war plans and existing resources .

third , it adds a joint perspective by incorporating readiness assessments from the combatant commands .

the services and combat support agencies also conduct readiness assessments for the joint review .

fourth , the joint review is conducted on a recurring cycle — four times a year — that has helped to institutionalize the process of readiness assessment within dod.finally , the joint review includes procedures for tracking and addressing reported deficiencies .

i would like to note , however , that the dod components participating in the review are accorded flexibility in how they conduct their assessments .

the 11 combatant commands , for instance , assess readiness in eight separate functional areas , such as mobility , infrastructure , and intelligence , surveillance , and reconnaissance , and to do this each command has been allowed to independently develop its own measures .

in addition , the process depends heavily on the judgment of military commanders to formulate their assessment .

officials involved with the joint review view this subjectivity as a strength , not a weakness , of the process .

they said readiness assessment is influenced by many factors , not all of which are readily measured by objective indicators .

one consequence , however , is that the joint review cannot be used to make direct comparisons among the commands in the eight functional areas .

we should also point out that the services , in conducting their portion of the joint review , depend extensively on sorts data .

as i mentioned earlier , sorts has certain inherent limitations .

dod is required under 10 u.s.c .

482 to prepare a quarterly readiness report to congress .

under this law , dod must specifically describe ( 1 ) each readiness problem and deficiency identified , ( 2 ) planned remedial actions , and ( 3 ) the key indicators and other relevant information related to each identified problem and deficiency .

in mandating the report , congress hoped to enhance its oversight of military readiness .

the first report was submitted to congress in may 1996 .

dod bases its quarterly reports on briefings to the senior readiness oversight council .

the council , comprising senior civilian and military leaders , meets monthly and is chaired by the deputy secretary of defense .

the briefings to the council are summaries from the joint monthly readiness review .

in addition , the deputy secretary of defense periodically tasks the joint staff and the services to brief the council on various readiness topics .

from these briefings , the joint staff drafts the quarterly report .

it is then reviewed within dod before it is submitted to congress .

we recently reviewed several quarterly reports to determine whether they ( 1 ) accurately reflect readiness information briefed to the council and ( 2 ) provide information needed for congressional oversight .

because minutes of the council's meetings are not maintained , we do not know what was actually discussed .

lacking such records , we traced information in the quarterly readiness reports to the briefing documents prepared for the council .

our analysis showed that the quarterly reports accurately reflected information from these briefings .

in fact , the quarterly reports often described the issues using the same wording contained in the briefings to the council .

the briefings , as well as the quarterly reports , presented a highly aggregated view of readiness , focusing on generalized strategic concerns .

they were not intended to and did not highlight problems at the individual combatant command or unit level .

dod officials offered this as an explanation for why visits to individual units may yield impressions of readiness that are not consistent with the quarterly reports .

our review also showed that the quarterly reports did not fulfill the legislative reporting requirements under 10 u.s.c .

482 because they lacked the specific detail on deficiencies and planned remedial actions needed for congressional oversight .

lacking such detail , the quarterly reports provided congress with only a vague picture of dod's readiness problems .

for example , one report stated that army personnel readiness was a problem , but it did not provide data on the numbers of personnel or units involved .

further , the report did not discuss how the deficiency affected the overall readiness of the units involved .

also , the quarterly reports we reviewed did not specifically describe planned remedial actions .

rather , they discussed remedial actions only in general terms , with few specific details , and provided little insight into how dod planned to correct the problems .

congress has taken steps recently to expand the quarterly reporting requirements in 10 u.s.c .

482 .

beginning in october 1998 , dod will be required to incorporate 19 additional readiness indicators in the quarterly reports .

to understand the rationale for these additional indicators , it may be helpful to review their history .

in 1994 , we told this subcommittee that sorts did not provide all the information that military officials believed was needed for a comprehensive assessment of readiness .

we reported on 26 indicators that were not in sorts but that military commanders said were important for a comprehensive assessment of readiness .

we recommended that the secretary of defense direct his office to determine which indicators were most relevant to building a comprehensive readiness system , develop criteria to evaluate the selected indicators , prescribe how often the indicators should be reported to supplement sorts data , and ensure that comparable data be maintained by the services to facilitate trend analysis .

dod contracted the logistics management institute ( lmi ) to study the indicators discussed in our report , and lmi found that 19 of them could be of high or medium value for monitoring critical aspects of readiness .

the lmi study , issued in 1994 , recommended that dod ( 1 ) identify and assess other potential indicators of readiness , ( 2 ) determine the availability of data to monitor indicators selected , and ( 3 ) estimate benchmarks to assess the indicators .

although our study and the lmi study concluded that a broader range of readiness indicators was needed , both left open how dod could best integrate additional measures into its readiness reporting .

the 19 indicators that congress is requiring dod to include in its quarterly reports are very similar to those assessed in the lmi study .

 ( see app .

1 for a list of the 19 indicators dod is to include in the quarterly reports. ) .

last month , dod provided congress with an implementation plan for meeting the expanded reporting requirements for the quarterly report .

we were asked to comment on this plan today .

of course , a thorough assessment of the additional readiness indicators will have to wait until dod begins to incorporate them into the quarterly reports in october 1998 .

however , on the basis of our review of the implementation plan , we have several observations to make .

overall , the implementation plan could be enhanced if it identified the specific information to be provided and the analysis to be included .

the plan appears to take a step backward from previous efforts to identify useful readiness indicators .

in particular , the lmi study and subsequent efforts by the office of the secretary of defense were more ambitious attempts to identify potentially useful readiness indicators for understanding , forecasting , and preventing readiness shortfalls .

the current implementation plan , in contrast , was developed under the explicit assumption that existing data sources would be used and that no new reporting requirements would be created for personnel in the field .

further , the plan states that dod will not provide data for 7 of the 19 indicators because either the data is already provided to congress through other documents or there is no reasonable or accepted measurement .

dod officials , however , acknowledged that their plans will continue to evolve and said they will continue to work with this subcommittee to ensure the quarterly report supports congressional oversight needs .

lastly , the plan does not present a clear picture of how the additional indicators will be incorporated into the quarterly report .

for example , the plan is mostly silent on the nature and extent of analysis to be included and on the format for displaying the additional indicators .

we also have concerns about how dod plans to report specific indicators .

for example: according to the plan , sorts will be the source of data for 4 of the 19 indicators — personnel status , equipment availability , unit training and proficiency , and prepositioned equipment .

by relying on sorts , dod may miss opportunities to provide a more comprehensive picture of readiness .

for example , the lmi study points out that sorts captures data only on major weapon systems and other critical equipment .

that study found value in monitoring the availability of equipment not reported through sorts .

in all , the lmi study identified more than 100 potential data sources outside sorts for 3 of these 4 indicators — personnel status , equipment availability , and unit training and proficiency .

 ( the lmi study did not include prepositioned equipment as a separate indicator. ) .

dod states in its implementation plan that 2 of the 19 indicators — operations tempo ( optempo ) and training funding — are not relevant indicators of readiness .

dod states further it will not include the data in its quarterly readiness reports because this data is provided to congress in budget documents , .

however , the lmi study rated these two indicators as having a high value for monitoring readiness .

the study stated , for instance , that “programmed optempo is a primary means of influencing multiple aspects of mid - term readiness” and that “a system for tracking the programming , budgeting , and execution of optempo would be a valuable management tool that may help to relate resources to readiness.” for the indicator showing equipment that is non - mission capable , the plan states that the percentage of equipment reported as non - mission capable for maintenance and non - mission capable for supply will provide insights into how parts availability , maintenance shortfalls , or funding shortfalls may be affecting equipment readiness .

according to the plan , this data will be evaluated by examining current non - mission capable levels versus the unit standards .

while this type of analysis could indicate a potential readiness problem if non - mission capable rates are increasing , it will not show why these rates are increasing .

thus , insights into equipment readiness will be limited .

mr. chairman , there are two areas where we think dod has an opportunity to take further actions to improve its readiness reporting .

the first area concerns the level of detail included in the quarterly readiness reports to congress .

in a draft report we will issue later this month , we have recommended that the secretary of defense take steps to better fulfill the legislative reporting requirements under 10 u.s.c .

482 by providing ( 1 ) supporting data on key readiness deficiencies and ( 2 ) specific information on planned remedial actions in its quarterly readiness reports .

as we discussed earlier , the quarterly reports we reviewed gave congress only a vague picture of readiness .

adding more specific detail should enhance the effectiveness of the reports as a congressional oversight tool .

dod has concurred with our recommendation .

the second area where dod can improve its readiness reporting concerns dod's plan to include additional readiness indicators in the quarterly report .

the plan would benefit from the following changes: include all 19 required indicators in the report .

make the report a stand - alone document by including data for all the indicators rather than referring to previously reported data .

further investigate sources of data outside sorts , such as those reviewed in the lmi report , that could provide insight into the 19 readiness indicators .

develop a sample format showing how the 19 indicators will be displayed in the quarterly report .

provide further information on the nature and extent of analysis to be included with the indicators .

dod recognizes in its plan that the type and quality of information included in the quarterly reports may not meet congressional expectations and will likely evolve over time .

in our view , it would make sense for dod to correct known shortcomings to the current implementation plan and present an updated implementation plan to congress prior to october 1998 .

mr. chairman , that concludes my prepared statement .

we would be glad to respond to any questions you or other members of the subcommittee may have .

the following are the additional indicators the department of defense is required , under 10 u.s.c .

482 , to include in its quarterly reports to congress beginning in october 1998 .

1 .

personnel status , including the extent to which members of the armed forces are serving in positions outside of their military occupational specialty , serving in grades other than the grades for which they are qualified , or both .

2 .

historical data and projected trends in personnel strength and status .

3 .

recruit quality .

4 .

borrowed manpower .

5 .

personnel stability .

6 .

personnel morale .

7 .

recruiting status .

8 .

training unit readiness and proficiency .

9 .

operations tempo .

10 .

training funding .

11 .

training commitments and deployments .

12 .

deployed equipment .

13 .

equipment availability .

14 .

equipment that is not mission capable .

15 .

age of equipment .

16 .

condition of nonpacing items .

17 .

maintenance backlog .

18 .

availability of ordnance and spares .

19 .

status of prepositioned equipment .

the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 37050 washington , dc 20013 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 202 ) 512-6061 , or tdd ( 202 ) 512-2537 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

