information systems are critical to the health , economy , and security of the nation .

to support these systems , the federal government plans to invest more than $89 billion on information technology ( it ) in fiscal year 2017 .

however , prior it expenditures too often have produced failed projects — that is , projects with multimillion dollar cost overruns and schedule delays measured in years , with questionable mission - related achievements .

in light of these ongoing challenges , in february 2015 we added improving the management of it acquisitions and operations to our list of high - risk areas for the federal government .

in an effort to improve federal it management , in march 2014 the general services administration ( gsa ) established 18f , a team that provides it services ( eg , develop websites and provide software development training ) to federal agencies on a reimbursable basis .

similar to 18f , in august 2014 the president established the u.s. digital service ( usds ) within the office of management and budget ( omb ) , which aims to improve the most important public - facing federal digital services .

in addition , the president's budget for fiscal year 2016 proposed funding for agencies to establish their own agency digital service teams .

you asked us to review 18f and usds , as well as agency digital service teams .

our objectives were to ( 1 ) describe 18f and usds efforts to address problems with it projects and agencies' views of services provided , ( 2 ) assess these programs' efforts against practices for performance measurement and project prioritization , and ( 3 ) assess agency plans to establish their own digital service teams .

in addressing our first objective , we reviewed 32 projects across 18 agencies for which 18f provided services to agencies , and 13 projects at 11 agencies for which usds provided services .

to identify the projects , we obtained the list of completed and ongoing projects at agencies for which 18f and usds provided services , as of august 2015 and removed projects without agency customers ( eg , internal projects and development of guides for other agencies ) .

the selected projects and associated agencies are identified in appendix ii .

we then analyzed information obtained from the projects describing the services each of the selected projects received from 18f and usds .

we also conducted a customer satisfaction survey of the managers of all selected projects to determine their level of satisfaction with the services provided by 18f and usds .

although the survey responses cannot be used to generalize the opinions and satisfaction of all customers that received services from 18f and usds programs , the responses provide data for our defined population .

to address the second objective , we compared 18f and usds policies procedures , plans , and practices to leading practices identified by federal law and gao on performance measurement and project prioritization .

to address our third objective , we administered a data collection instrument on plans to establish digital service teams to the 25 agencies with funding proposed in the president's budget for fiscal year 2016 .

additionally , we reviewed usds's plans — to include interviews with usds officials — for providing assistance to agencies that planned to establish a digital service team in fiscal year 2016 .

in addition , we selected four agencies as case studies to review the relationships between agency chief information officers ( cio ) and agency digital service teams .

to choose these agencies , we identified the three agencies that had established a charter with usds as of january 2016 — the departments of defense , homeland security , and state .

we also selected the department of veterans affairs because , as of january 2016 , it had the most staff of any agency digital service team .

for these agencies , we evaluated agency policies and procedures to determine the extent to which agencies had documented the relationships between digital service teams and agency cios .

we also conducted interviews with the cios of the departments of defense , homeland security , and state , as well as the veterans affairs principal deputy assistant secretary for the office of information and technology .

we conducted this performance audit from july 2015 to august 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

appendix i contains further details about our objectives , scope , and methodology .

investments in it can enrich people's lives and improve organizational performance .

during the last two decades the internet has matured from being a means for academics and scientists to communicate with each other to a national resource where citizens can interact with their government in many ways , such as by receiving services , supplying and obtaining information , asking questions , and providing comments on proposed rules .

however , while these investments have the potential to improve lives and organizations , some federally funded it projects can — and have — become risky , costly , unproductive mistakes .

we have previously testified that the federal government has spent billions of dollars on failed or troubled it investments , such as the office of personnel management's retirement systems modernization program , which was canceled in february 2011 , after spending approximately $231 million on the agency's third attempt to automate the processing of federal employee retirement claims ; the tri - agency national polar - orbiting operational environmental satellite system , which was stopped in february 2010 by the administration after the program spent 16 years and almost $5 billion ; the department of veterans affairs' scheduling replacement project , which was terminated in september 2009 after spending an estimated $127 million over 9 years ; and the department of health and human services' ( hhs ) healthcare.gov website and its supporting systems , which were to facilitate the establishment of a health insurance marketplace by january 2014 , encountered significant cost increases , schedule slips , and delayed functionality .

in a series of reports we identified numerous planning , oversight , security , and system development challenges faced by this program and made recommendations to address them .

in light of these failures and other challenges , last year we introduced a new government - wide high - risk area , improving the management of it acquisitions and operations .

18f and usds were formed in 2014 to help address the federal government's troubled it efforts .

both programs have similar missions of improving public - facing federal digital services .

18f was created in march 2014 by gsa with the mission of transforming the way the federal government builds and buys digital services .

agencies across the federal government have access to 18f services .

work is largely initiated by agencies seeking assistance from 18f and then the program decides how and if it will provide assistance .

according to gsa , 18f seeks to accomplish its mission by providing a team of expert designers , developers , technologists , researchers , and product specialists to help rapidly deploy tools and online services that are reusable , less costly , and are easier for people and businesses to use .

in addition , 18f has several guiding principles , to include the use of open source development , user - centered design , and agile software development .

18f is an office within the technology transformation service within gsa that was recently formed in may 2016 .

18f is led by the deputy commissioner for the technology transformation service , who reports to the service's commissioner .

prior to may 2016 , 18f was located within the office of citizen services and innovative technologies and reported to the associate administrator for citizen services and innovative technologies .

in january 2016 gsa began piloting a new organizational structure for 18f that centers around five business units .

custom partner solutions .

provides agencies with custom application solutions .

this unit also provides consulting services to assist agencies in deciding whether to build , what to build , how to build it , and who will build it .

products and platforms .

provides agencies with access to tools that address common government - wide needs .

transformation services .

aims to improve how agencies acquire and manage it by providing them with consulting services , to include new management models , modern software development practices , and hiring processes .

acquisition services .

provides acquisition services and solutions to support digital service delivery , including access to vendors specializing in agile software development , and request for proposal development consultation .

learn .

provides agencies with education , workshops , outreach , and communication tools on developing and managing digital services .

to provide the products and services offered by each business unit , 18f relied on 173 staff to carry out its mission , as of march 2016 .

the staff are assigned to different projects that are managed by the business units .

according to18f , the program used special hiring authorities for the vast majority of its staff: schedule a excepted service authorities were used to hire 162 staff .

these authorities permit the appointment of qualified personnel without the use of a competitive examination process .

gsa has appointed its staff to terms that are not to exceed 2 years .

according to the director of the 18f talent division , after the initial appointment has ended , gsa has the option of appointing staff to an additional term not to exceed 2 years .

gsa funds 18f through the acquisition services fund — a revolving fund , which operates on the revenue generated from its business units rather than an appropriation received from congress .

the federal acquisition service , with the concurrence of the administrator of general services , has used the fund to invest in the development of 18f products and services that will be resold by gsa and used by other organizations .

18f is to recover costs through the acquisition services fund reimbursement authority for work related to acquisitions and the economy act reimbursement authority for all other projects .

according to the memorandum of agreement between 18f and the federal acquisition service , 18f is required to have a plan to achieve full cost recovery .

in order to recover its costs , 18f is to establish interagency agreements with partner agencies and will charge them for actual time and material costs , as well as a fixed overhead amount .

table 1 describes 18f's revenue , expenses , and net operating results for fiscal years 2014 and 2015 .

table 2 describes 18f's projected revenue , expenses , and net operating results for fiscal years 2016 through 2019 .

as shown in table 2 , according to its projections , 18f plans to generate revenue that meets or exceeds operating expenses and cost of goods sold beginning in fiscal year 2019 .

in may 2016 the gsa inspector general reported on an information security weakness pertaining to 18f .

specifically , according to the report , 18f misconfigured a messaging and collaboration application , which resulted in the potential exposure of personally identifiable information ( pii ) .

18f officials told us that , based on the preliminary results of their ongoing review , information such as individual's first names , last names , e - mail addresses , and phone numbers were made available on the messaging and collaboration platform's databases , and could have been accessible by authorized users of the application .

those officials also stated that , based on the preliminary results of their ongoing review , more sensitive pii , such as social security numbers and protected health information , were not exposed .

they added that they are continuing a detailed review , in coordination with the gsa it organization , to confirm that more sensitive pii were not made available .

according to the administration , in 2013 it initiated an effort that brought together a group of digital and technology experts from the private sector that helped fix healthcare.gov .

in an effort to apply similar resources to additional projects , in august 2014 the administration announced the launch of usds , to be led by an administrator and deputy federal cio who reports to the federal cio .

according to omb , usds's mission is to transform the most important public - facing digital services .

usds selects which projects it will apply resources to and generally initiates the effort with agencies .

to accomplish its mission , usds aims to recruit private sector experts ( eg , it engineers and designers ) and leading civil servants , and then deploy small teams to partner them with government agencies .

with the help of these experts , omb states that usds applies best practices in product design and engineering to improve the usefulness , user experience , and reliability of the most important public - facing federal digital services .

as of november 2015 , usds staff totaled about 98 individuals .

similar to 18f , usds assigns individuals directly to projects aimed at achieving its mission .

usds has used special hiring authorities for the vast majority of its staff .

specifically: schedule a excepted service .

according to usds , as of november 2015 , 52 usds staff members were hired using the schedule a excepted service hiring authority .

according to the usds administrator , appointments made using this authority are not to exceed 2 years .

at the end of that period , staff can be appointed for an additional term of no more than 2 years .

intermittent consultants .

according to usds , as of november 2015 , 39 usds staff members were intermittent consultants — that is , individuals hired through a noncompetitive process to serve as consultants on an intermittent basis or without a regular tour of duty .

the usds administrator explained that some of these staff are eventually converted to temporary appointments under the schedule a authority .

according to its administrator , usds does not generally make permanent appointments for its staff because it allows the program to continuously bring in new staff and ensure that its ideas are continually evolving .

usds reported spending $318,778 during fiscal year 2014 and approximately $4.7 million during fiscal year 2015 .

for fiscal year 2016 , usds plans to spend approximately $14 million , and the president's fiscal year 2017 budget estimated obligations of $18 million for usds .

in an effort to make improvements to critical it services throughout the federal government , the presidents' budget for fiscal year 2016 proposed funding for the 24 chief financial officers act agencies , as well as the national archives and records administration , to establish digital service teams .

usds policy calls for these agencies to , among other things , hire or designate an executive for managing their digital service teams .

additionally , usds has established a hiring pipeline for digital service experts — that is , a unified process managed by usds for accepting and reviewing applications , performing initial interviews , and providing agencies with candidates for their digital service teams .

according to omb , before using this service , agencies must agree to a charter with the usds administrator .

over the last three decades , several laws have been enacted to assist federal agencies in managing it investments .

for example , the paperwork reduction act of 1995 requires that omb develop and oversee policies , principles , standards , and guidelines for federal agency it functions , including periodic evaluations of major information systems .

in addition , the clinger - cohen act of 1996 , among other things , requires agency heads to appoint cios and specifies many of their responsibilities .

with regard to it management , cios are responsible for implementing and enforcing applicable government - wide and agency it management principles , standards , and guidelines ; assuming responsibility and accountability for it investments ; and monitoring the performance of it programs and advising the agency head whether to continue , modify , or terminate such programs .

most recently , in december 2014 , it reform legislation ( commonly referred to as the federal information technology acquisition reform act or fitara ) was enacted , which required most major executive branch agencies to ensure that the cio had a significant role in the decision process for it budgeting , as well as the management , governance , and oversight processes related to it .

the law also required that cios review and approve ( 1 ) all contracts for it services associated with major it investments prior to executing them and ( 2 ) the appointment of any other employee with the title of cio , or who functions in the capacity of a cio , for any component organization within the agency .

omb also released guidance in june 2015 that reinforces the importance of agency cios and describes how agencies are to implement the law .

omb plays a key role in helping federal agencies address these laws and manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

within omb , the office of e - government and information technology , headed by the federal cio , directs the policy and strategic planning of federal it investments and is responsible for oversight of federal technology spending .

18f and usds have provided a variety of development and consulting services to agencies to support their technology efforts .

specifically , between march 2014 and august 2015 , 18f staff helped 18 agencies with 32 projects and generally provided six types of services to the agencies , the majority of which related to development work .

in addition , between august 2014 and august 2015 , usds provided assistance on 13 projects at 11 agencies and provided seven types of consulting services .

further , agencies were generally satisfied with the services they received from 18f and usds .

specifically , of the 26 18f survey respondents , 23 were very satisfied or moderately satisfied and 3 were moderately dissatisfied .

for usds , all 9 survey respondents were very satisfied or moderately satisfied .

between march 2014 and august 2015 , gsa's 18f staff helped 18 agencies with 32 projects , and generally provided services relating to its five business units: custom partner solutions , products and platforms , transformation services , acquisition services , and learn .

in addition , 18f also provided agency digital service team candidate qualification reviews in support of usds .

custom partner solutions .

18f helped 11 agencies with a total of 19 projects relating to developing custom software solutions .

out of the 19 projects , 12 were related to website design and development .

for example , regarding gsa's pulse project — a website that displays data about the extent to which federal websites are adopting best practices , such as hypertext transfer protocol over secure sockets layer ( ssl ) / transport layer security ( tls ) ( https ) — 18f designed , developed , and delivered the first iteration of pulse within 6 weeks of the project kick - off .

according to the gsa office responsible for managing the project , the first iteration has led to positive outcomes for government - wide adoption of best practices ; for example , between june 2015 and january 2016 , the percentage of federal websites using https increased from 27 percent to 38 percent .

as another example , officials from the department of education's college choice project stated that 18f helped develop the project's website , which the public can use to search among colleges to find schools that meet their needs ( eg , degrees offered , location , size , graduation rate , average salary after attendance ) .

18f also helped two agencies , hhs and the department of defense , on two projects to develop application programming interfaces — sets of routines , protocols , and tools for building software applications that specify how software components should interact .

acquisition services .

18f helped seven agencies on seven projects relating to acquisition services consulting .

for example , 18f provided the department of state's bureau of international information programs with cloud computing services offered under a gsa blanket purchase agreement ( bpa ) — specifically , cloud management services ( eg , developers , testing and quality assurance , cloud architect ) and infrastructure - as - a - service .

according to the department of state , the department was able to deploy its instance of the infrastructure service only 1 month after it executed an interagency agreement with 18f .

in addition , according to social security administration officials , 18f helped the agency to incorporate agile software development practices into their requests for proposals for their disability case processing system .

learn .

18f provided services to four agencies on four projects regarding training , such as educating agency officials on agile software development .

for example , 18f conducted training workshops on agile software development techniques with the social security administration and small business administration .

in addition , according to the department of labor's wage and hour division officials , 18f conducted a 3-day workshop on it modernization .

transformation services .

18f assisted two agencies on two projects to help acquire the people , processes , and technology needed to successfully deliver digital services .

for example , 18f assisted the environmental protection agency on an agency - wide technology transformation .

according to an official within the office of the cio , 18f assisted the agency with e - manifest — a system used to track toxic waste shipments .

the official noted that 18f provided user - centered design , agile coaching , prototype development services , and agile and modular acquisition services .

further , the official stated that 18f helped turn around the project and significantly decreased the time of delivery for e - manifest .

products and platforms .

18f helped two agencies on two projects related to developing software solutions that can potentially be reused at other federal agencies .

for example , according to gsa officials responsible for managing gsa's communicart project , 18f provided the agency with an e - mail - based tool for approving office supply purchases .

agency digital service team candidate qualification review .

18f worked with usds to recruit and hire team members for agency digital service teams .

according to 18f officials , it provided usds with subject matter experts to review qualifications of candidates for agency digital service teams .

of the 32 18f projects , 6 are associated with major it investments .

cumulatively , the federal government plans to spend $853 million on these investments in fiscal year 2016 .

additionally , risk evaluations performed by cios that were obtained from the it dashboard showed that three of these investments were rated as low or moderately low risk and three investments were rated medium risk .

table 3 describes the associated investments , including their primary functional areas , planned fiscal year 2016 spending , and cio rating as of may 2016 .

18f is also developing products and services — including an agile delivery service blanket purchase agreement ( bpa ) , cloud.gov , and a shared authentication platform: agile delivery service bpa .

18f established this project in order to support its need for agile delivery services , including agile software development .

in august and september 2015 , gsa awarded bpas to 17 vendors .

the bpas are for 5 years and allow gsa to place orders against them for up to 13 specific labor categories relating to agile software development ( eg , product manager , backend web developer , agile coach ) at fixed unit prices .

the bpas do not obligate any funds ; rather , they enable participating vendors to compete for follow - on task orders from gsa .

in cases where 18f determines that it should use the agile bpa to provide services to partner agencies , gsa anticipates that 18f will work with that agency to develop a request for quotations and the other documents needed for a competition with agile bpa vendors .

in june 2016 gsa issued its first task order under the agile bpa for building a web - based dashboard that would describe the status of vendors in the certification process for the federal risk and authorization management program ( fedramp ) — a government - wide program , managed by gsa , to provide joint authorizations and continuous security monitoring services for cloud computing services for all federal agencies .

the initial bpas were established under the first of three anticipated award pools — all of which are part of the “alpha” component of the agile bpa project .

18f officials stated that they planned to establish bpas for the other two pools in june 2016 .

they also anticipate a future beta version of the project that could potentially allow federal agencies beyond 18f to issue task orders directly to vendors .

officials stated that they expect to have a plan for the next steps of the beta version of this project by december of 2016 .

18f officials have also expressed interest in creating additional marketplaces , such as those relating to data management , developer productivity tools , cybersecurity , and health it .

as of march 2016 , 18f did not have time frames for when it planned to develop these additional marketplaces .

cloud.gov .

18f also developed the cloud.gov service , which is an open source platform - as - a - service that agencies can use to manage and deploy applications .

18f initially built cloud.gov in order to enable the group to use applications it developed for partner agencies .

in creating the service , 18f decided to offer it to other agencies because , according to 18f officials , cloud.gov offers a developer - friendly , secure platform , with tools that agencies can use to accelerate the process of assessing information security controls and authorizing systems to operate .

according to 18f , the goal of cloud.gov is to provide government developers and their contractor partners the ability to easily deploy systems to a cloud infrastructure with better efficiency , effectiveness , and security than current alternatives .

according to a roadmap for cloud.gov , 18f plans to receive full fedramp joint authorization board approval for this service by november 2016 .

once available , the group anticipates requiring agencies to pay for this service through an interagency agreement with 18f .

shared authentication platform .

in may 2016 18f announced that it was initiating an effort to create a platform for users who need to log into federal websites for government services .

according to 18f , this system is designed to be each citizen's “one account” with the government and allow the public to verify an identity , log into government websites , and if necessary , recover an account .

as of may 2016 , 18f plans to conduct prototyping activities through september 2016 and did not have plans beyond that time frame .

in addition to developing future products and services , 18f created a variety of guides and standards for use internally as well as by agency digital service teams .

these guides address topics such as accessibility , application programming interfaces , and agile software development .

from august 2014 through august 2015 , usds provided assistance on 13 projects across 11 agencies .

the group generally provided seven types of consulting services: quality assurance , problem identification and recommendations , website consultation , system stabilization , information security assessment , software engineering , and data management .

quality assurance .

three of the 13 projects related to providing quality assurance services .

for example , regarding the social security administration's disability case processing system , usds reviewed the quality of the software and made recommendations that , according to the agency , resulted in costs savings .

additionally , for the departments of veterans affairs and defense service treatment record project , usds provided engineers who identified and resolved errors in the process of exchanging records between the two departments , according to the department of veterans affairs .

further , for the hhs healthcare.gov system , the group performed services aimed at optimizing the reliability of the system , according to hhs .

problem identification and recommendations .

usds identified problems and made recommendations for three projects .

for all three projects , it performed a discovery sprint — a quick ( typically 2 week ) review of an agency's challenges , which is to culminate in a clear understanding of the problems and recommendations for how to address the issues .

for example , according to usds , the group performed a discovery sprint for the department of the treasury internal revenue service that focused on three areas: authentication of taxpayers , modernizing systems through event - driven architecture , and redesigning the agency's website .

usds delivered recommendations to the internal revenue service with recommendations and also suggested that work initially focus on taxpayer authentication .

consistent with these recommendations , according to usds , the group and the agency focused on authentication , to include re - opening of the online application get transcript .

for the department of justice federal bureau of investigation's national incident based reporting system , according to usds , the group performed a discovery sprint and made several recommendations for accelerating deployment of the system .

website consultation .

usds provided consultation services for three agency website projects .

for example , for the office of the u.s. trade representative's trans - pacific partnership trade agreements website , usds provided website design advice and confirmed that the agency had the necessary scalability to support the number of anticipated visitors .

additionally , it consulted with the office of personnel management ( opm ) on the design , implementation , and development of a website for providing information on reported data breaches .

system stabilization .

for the department of state's consular consolidated database , according to usds , it helped stabilize the system and return it to operational service after a multi - week outage in june 2015 .

information security assessment .

usds helped with an information security assessment regarding electronic questionnaires for investigations processing , which encompasses the electronic applications used to process federal background check investigations .

software engineering .

for the department of homeland security u.s .

citizenship and immigration services transformation project , usds's software engineering advisors provided guidance on private sector best practices in delivering modern digital services .

according to the department , the group's work has supported accomplishments such as increasing the frequency of software releases and improving adoption of agile development best practices .

data management .

for the department of homeland security office of immigration statistics , usds helped to develop monthly reports on immigration enforcement priority statistics .

according to the department , usds supported the development of processes for obtaining data from other offices within the department and generating the monthly reports .

according to the department , after 7 weeks of working with usds , it was able to develop a proof of concept that reduced the report generating process from a month to 1 day .

seven of the 13 projects are associated with major it investments .

cumulatively , the federal government plans to spend over $1.24 billion on these investments in fiscal year 2016 .

three investments were rated by their cios as low or moderately low risk and four investments were rated as being medium risk .

table 4 describes the associated investments , including their primary functional areas , planned fiscal year 2016 spending , and cio ratings as of may 2016 .

in addition to providing services to agencies , usds has developed products to help agencies improve federal it services .

for example , it developed the digital services playbook to provide government - wide recommendations on practices for building digital services .

the group also created the techfar handbook to explain how agencies can use the digital services playbook in ways that are consistent with the federal acquisition regulation .

further , usds , in collaboration with 18f , developed the draft version of u.s .

web design standards , which includes a visual style guide and a collection of common user interface components .

with this guide , usds aims to improve government website consistency and accessibility .

in addition to developing guidance , usds , in collaboration with omb's office of federal procurement policy , used challenge.gov to incentivize the public to create a digital service training program for federal contract professionals .

the challenge winner received $250,000 to develop and pilot a training program .

additionally , the deputy administrator for usds stated that 30 federal contract professionals from more than 10 agencies completed this pilot program in march 2016 .

according to omb , the program is being revised and transitioned to the federal acquisition institute , where it will be included as part of a certification for digital service contracting officers .

in response to a satisfaction survey we administered to agency managers of selected 18f and usds projects , a majority of managers were satisfied with the services they received from the groups .

specifically , the average score for services provided by 18f was 4.38 ( on a 5-point satisfaction scale , where 1 is very dissatisfied and 5 is very satisfied ) and the average score for the services provided by usds was 4.67 .

table 5 describes the survey results for 18f and usds .

in addition to providing scores , the survey respondents also provided written comments .

regarding 18f , five factors were cited by two or more respondents as contributing to their satisfaction with the services the program provided: delivering quality products and services , providing good customer service , completing tasks in a timely manner , employing staff with valuable knowledge and skills , and providing valuable education to agencies .

for example , one respondent stated that 18f has an expert staff that helped the team understand agile software development and incorporate user - centered design into the agency's development process .

with respect to usds , four factors were cited by two or more respondents as contributing to their satisfaction with its services: delivering quality services , providing good customer service , completing tasks in a timely manner , and employing staff with valuable knowledge and skills .

for instance , one respondent stated that usds responded to the agency's request in a matter of hours , quickly developed an understanding of the agency's it system , and pushed to improve the system , even in areas beyond the scope of usds's responsibility .

although the majority of agencies were satisfied , a minority of respondents provided written comments describing their dissatisfaction with services provided by 18f .

for example , six respondents cited poor customer service , four respondents cited higher than expected costs , and one respondent stated that 18f's use of open source code may not meet the agency's information security requirements .

in a written response to these comments , 18f stated that it has received a variety of feedback from its partners and has modified and updated its processes continuously over the past 2 years .

for example , with respect to higher than expected costs , 18f stated that project costs sometimes needed to be adjusted mid - project to address , among other things , higher than expected infrastructure usage or unexpected delays .

to address this issue , 18f stated that it uses the assistance of subject matter experts to estimate project costs , and wrote a guide to assist with , among other things , better managing the budgets of ongoing projects .

regarding 18f's use of open source code , it stated that it has worked with its partners to discuss the use of open source software and information security practices .

to assess actual results , prioritize limited resources , and ensure that the most critical projects receive attention , usds and 18f should establish and implement the following key practices: define outcome - oriented goals and measure performance .

our previous work and federal law stress the importance of focusing on outcome - oriented goals and performance measures to assess the actual results , effects , or impact of a program or activity compared to its intended purpose .

goals should be used to elaborate on a program's mission statement and should be aligned with performance measures .

in turn , performance measures should be tied to program goals and demonstrate the degree to which the desired results were achieved .

to do so , performance measures should have targets to help assess whether goals were achieved by comparing projected performance and actual results .

finally , goals and performance measures should be outcome - oriented — that is , they should address the results of products and services .

establish and implement procedures for prioritizing it projects .

we have reported that establishing and implementing procedures , to include criteria , for prioritizing projects can help organizations consistently select projects based on their contributions to the strategic goals of the organization .

doing so will better position agencies to effectively prioritize projects and use the best mix of limited resources to move toward its goals .

18f has developed several outcome - orientated goals , performance measures , and procedures for prioritizing projects , which it has largely implemented .

however , not all of its goals are outcome - oriented and it has not yet measured program performance .

define outcome - oriented goals and measure performance at the conclusion of our review in may 2016 , 18f provided 5 goals and 17 associated performance measures that the organization aims to achieve by september 2016 ( see table 6 ) .

to 18f's credit , several of its goals and performance measures appear to be outcome - oriented .

for example , the goal of delivering two government - wide platform services and the associated performance measures are outcome - oriented in that they address results — that is , delivering services to partner agencies .

however , not all of the goals and performance measures appear to be outcome - oriented .

for example , the goal of growing 18f to 215 staff while sustaining a healthy culture and its associated measure of hiring 47 staff do not focus on results of products or services .

further , not all of the performance measures have targets .

for example , seven of the performance measures state that 18f will establish performance indicators , but 18f has yet to do so .

moreover , 18f does not have goals and associated measures that describe how it plans to achieve its mission after september 2016 .

in addition , although 18f is required to have a plan to achieve full cost recovery , it has yet to recover costs and its projections for when this will occur have slipped over time .

specifically , in june 2015 , 18f projected that it would fully recover its costs for an entire fiscal year beginning in 2016 ; however , in may 2016 , 18f provided revised projections indicating that it would recover costs beginning in fiscal year 2019 .

those projections also indicated that , in the worst case , it would not do so through 2022 , the final year of its projections .

establishing performance measures and targets that are tied to achieving full cost recovery would help management gauge whether the program is on track to meet its projections .

however , 18f has not established such performance measures and targets .

finally , 18f has yet to fully assess the actual results of its activities .

specifically , the group has not assessed its performance in accordance with the 17 performance measures it developed .

18f's then - parent organization assessed its own performance quarterly beginning in the 4th quarter of fiscal year 2015 , including for measures that 18f was responsible for .

however , this review process did not include or make reference to the 17 measures developed to gauge 18f's performance , and thus do not provide insight into how well it is achieving its own mission .

in a written response , gsa stated that 18f performance is measured as part of the technology transformation service's goals and measures and that these goals and measures should form the basis for our review .

however , the technology transformation service's goals and measures do not describe how gsa aims to achieve the specific mission of 18f .

until it establishes goals and performance measures beyond september 2016 , ensures that all of its goals and performance measures are outcome - oriented , and that its performance measures have targets , 18f will not have a clear definition of what it wants to accomplish .

additionally , without developing performance measures and targets tied to achieving full cost recovery , gsa will lack a fully defined approach to begin recovering all costs in fiscal year 2019 .

further , until 18f fully measures actual results , it will not be positioned to assess the status of its activities and determine the areas that need improvement .

establish and implement procedures for prioritizing it projects 18f has developed procedures , including criteria , for prioritizing projects and largely implemented its procedures .

specifically , according to the director of business strategy , potential projects are discussed during weekly intake meetings .

as part of these meetings , 18f discusses project decision documents , which outline the business , technical , and design elements , as well as the schedule , scope , and resources needed to fulfill the client's needs .

using these documents , 18f determines whether proposed projects meet , among other things , the following criteria: ( 1 ) the project is aligned with the products and services offered by 18f , ( 2 ) it can be completed in a time frame that meets the agency's needs and at a cost that fits the agency's budget , and ( 3 ) the project's government transformation potential ( eg , impact on the public , cost savings ) .

these documents are used by the business unit leads to make a final decision about whether to accept the projects .

18f has largely implemented its procedures .

to its credit , with respect to the 14 projects that 18f selected since establishing its prioritization and selection process , 18f developed a decision document for 12 of the 14 projects .

however , 18f did not develop a decision document for the 2 remaining projects — the nuclear regulatory commission's master data management project and gsa's labs.usa.gov project .

with respect to the nuclear regulatory commission's master data management project , 18f officials explained that this project only required staff from one division ; as such , that division was able to independently prioritize and select this project .

additionally , regarding the gsa labs.usa.gov project , 18f officials said the associate administrator for the office of citizen services and innovative technologies directed 18f to provide assistance .

if 18f consistently follows its process for prioritizing projects , it will be better positioned to apply resources to it projects with the greatest need of improvement .

while usds has developed program goals and a process for prioritizing projects , it has not fully implemented important program management practices .

define outcome - oriented goals and measure performance in november 2015 usds developed four goals to be achieved by december 2017: ( 1 ) recruit and place over 200 digital service experts in strategic roles at agencies and cultivate a continually growing pipeline of quality technical talent through usds , ( 2 ) measurably improve five to eight of the government's most important services , ( 3 ) begin the implementation of at least one outstanding common platform , and ( 4 ) increase the quality and quantity of technical vendors working with government and cultivate better buyers within government .

additionally , usds established a performance measure with a target for one of its goals .

specifically , it has a measure for its first goal as it plans to measure the extent to which it will hire 200 digital service experts by december 2017 .

to its credit , several of the goals appear to be outcome - oriented .

for example , improving five to eight services is outcome - oriented in that it addresses results .

however , usds has not established performance measures or targets for its other goals .

in addition , the program's first goal — recruit and place over 200 digital service experts in strategic roles at agencies and cultivate a continually growing pipeline of quality technical talent through usds — does not appear to be outcome - oriented .

further , usds has only measured actual results for one of its goals .

specifically , for the goal of placing digital service experts at agencies , as of may 2016 , usds officials stated that they had 152 digital service experts .

however , usds has not measured actual results for the other three goals .

usds officials provided examples of how they informally measure performance for the other three goals .

for example , for the goal of measurably improving five to eight of the government's most important services , the usds administrator stated that approximately 1 million visitors viewed the department of education's college scorecard website in the initial days after it was deployed .

however , usds has not documented these measures or the associated results to date .

until usds ensures that all of its goals are outcome - oriented and establishes performance measures and targets for each goal , it will be difficult to hold the program accountable for results .

additionally , without an assessment of actual results , it is unclear what impact usds's actions are having relative to its mission and whether investments in agency digital service teams are justified .

establish and implement procedures for prioritizing projects usds has developed procedures and criteria for prioritizing projects .

to identify projects to be considered , usds is to use , among other sources , june 2015 and june 2016 omb reports to congress that identify the 10 highest - priority federal it projects in development .

to prioritize projects , usds has the following three criteria , which are listed in their order of importance ( 1 ) what will do the greatest good for the greatest number of people in the greatest need ? .

 ( 2 ) how cost - efficient will the usds investment be ? .

and ( 3 ) what potential exists to use or reuse a technological solution across the government ? .

using these criteria , usds intends to create a list of all potential projects , to include their descriptions and information on resources needs .

this list is to be used by usds leadership to make decisions about which projects to pursue .

to its credit , usds created a list of all potential , ongoing , and completed projects , which included project descriptions and resource needs .

additionally , usds has engaged with 6 of the 10 priority it projects identified in the june 2015 and june 2016 reports , including hhs's healthcare.gov project and the department of homeland security's u.s .

citizenship and immigration services transformation .

additionally , according to a usds staff member , usds considered the remaining 4 projects and decided not to engage with them to date .

although usds has yet to develop a quarterly report on the 10 high priority programs , which it was directed by congress to develop , it expects to issue its first report by september 2016 .

specifically , in december 2015 , congress modified its direction for the executive office of the president to develop the reports regarding the top 10 high priority programs and specifically called for usds to do so on a quarterly basis .

if usds develops its report on the 10 high priority programs within the established time frame and on a quarterly basis thereafter , and considers the programs identified in these reports as part of its prioritization process , it will have greater assurance that it will apply resources to the it projects with the greatest need of improvement .

to help agencies effectively deliver digital services , the president's budget for fiscal year 2016 proposed funding for digital service teams at 25 agencies — the 24 chief financial officers act agencies , as well as the national archives and records administration .

according to usds policy , agencies are to , among other things , hire or designate an executive for managing their digital service teams .

in addition , usds has called for the deputy head of these agencies ( or equivalent ) to , among other things , agree to a charter with the usds administrator .

after agreeing to a charter , according to usds , agencies can use usds's hiring pipeline for digital service experts .

of the 25 agencies included in the president's budget proposal to establish teams , omb has established charters with 6 agencies for their digital service teams — the departments of defense , health and human services , homeland security , the treasury , state , and veterans affairs .

the charters establish the executives for managing digital service teams and describe the reporting relationships between the team leaders and agency leadership .

in addition , according to the deputy usds administrator , usds expects to establish charters with an additional 2 agencies by the end of the fiscal year — the department of education and the small business administration .

for the remaining 16 agencies , as of april 2016 , 8 agencies reported that they plan to establish digital service teams but have yet to establish charters with usds — the department of housing and urban development , environmental protection agency , general services administration , national aeronautics and space administration , national archives and records administration , national science foundation , nuclear regulatory commission , and office of personnel management .

of the other 9 agencies , 8 reported that they do not plan to establish digital service teams by september 2016 because they did not receive requested funding — the departments of agriculture , commerce , energy , the interior , justice , labor , and transportation ; and the u.s. agency for international development .

the remaining agency , the social security administration , does not plan to establish a team because , according to officials , it does not have large , public - facing it projects that are troubled .

table 7 summarizes agency and omb efforts to establish digital service teams .

congress has recognized the importance of having a strong agency cio .

in 1996 , the clinger - cohen act established the position of agency cio and , among other things , gave these officials responsibility for it investments , including it acquisitions , monitoring the performance of it programs , and advising the agency head whether to continue , modify , or terminate such programs .

more recently , in december 2014 , fitara was enacted into law .

it required most major executive branch agencies to ensure that the cio has a significant role in the decision process for it budgeting , as well as the management , governance , and oversight processes related to it .

the law also required that cios review and approve ( 1 ) all contracts for it services associated with major it investments prior to executing them and ( 2 ) the appointment of cios for any component within the agency .

omb also released guidance in june 2015 that reinforces the importance of agency cios and describes how agencies are to implement fitara .

further , according to our prior work , leading organizations clearly define responsibilities and authorities governing the relationships between the cio and other agency components that use it .

only one of the four agencies we selected for review — the department of homeland security — defined the relationship between the executive for managing the digital service team and the agency cio .

specifically , the department of homeland security established a charter for its digital service team , signed by both the administrator of usds and the deputy secretary , which outlines the reporting structure and authorities for the digital services executive , including the relationship with the cio .

for example , according to the charter , the digital services executive will report on a day - to - day basis to the cio , but will also report directly to the deputy secretary .

however , the other three agencies we reviewed — the departments of defense , state , and veterans affairs — have not defined the role of agency cios with regard to these teams .

although they have established charters for these teams , which describe the reporting structure between the digital services executive and senior agency leadership , the charters do not describe the role of the agencies' cios and they have not documented this information elsewhere .

the department of defense cio and the department of veterans affairs principal deputy assistant secretary for the office of information and technology told us that they work closely with their agency digital service teams .

however , while these officials have coordinated with the agency digital service teams , the roles and responsibilities governing these relationships should be described to ensure that cios can carry out their statutory responsibilities .

in contrast to the departments of defense and veterans affairs , the state cio told us that he has had limited involvement in the department's digital service team .

he added that he believes it will be important for cios to be involved in agency digital service teams in order to sustain their efforts .

in written comments , omb acknowledged that the department of state's charter does not describe the role of the cio , but stated that the departments of defense and veterans affairs digital service team charters at least partially address the relationship between digital service teams and agency cios .

specifically , with respect to the department of defense , omb stated that the charter calls for senior leadership , including the department's cio , to ensure that digital service team projects proceed without delay .

additionally , according to omb , the charter for the veterans affairs digital service team calls for the team to be located in and supported by the department's cio organization .

however , these requirements do not address the specific responsibilities or authorities of the departments defense and veterans affairs' cios with regard to their digital service teams .

the lack of defined relationships is due , in large part , to the fact that usds policy on digital service teams does not describe the expected relationship between agency cios and these teams .

as previously mentioned , usds policy calls for the digital service team leader to report directly to the head of the agency or its deputy ; however , it does not describe the expected responsibilities and authorities governing the relationship of the cio .

until omb updates the usds policy to clearly define the responsibilities and authorities governing the relationships between cios and digital service teams and ensures that existing agency digital service team charters or other documentation reflect this policy , agency cios may not be effectively involved in the digital service teams .

this is inconsistent with long - standing law , as well as the recently enacted fitara , and omb's guidance on cio responsibilities , and may hinder the ability for cios to carry out their responsibilities for it management of the projects undertaken by the digital service teams .

by hiring technology and software development experts and using leading software development practices , both 18f and usds have provided a variety of useful services to federal agencies .

most surveyed agency project managers that partnered with 18f and usds were satisfied with the services provided .

it is important for usds and 18f to establish outcome - oriented goals , measure performance , and prioritize projects , particularly since these are valuable management tools that could aid in the transfer of knowledge when critical temporary staff leave these organizations and are replaced .

to their credit , both 18f and usds have developed several outcome - orientated goals and procedures for prioritizing projects .

however , the goals and associated performance measures and targets were not always outcome - oriented .

additionally , they have not fully measured program performance .

as a result , it will be difficult to hold the programs accountable for results .

moreover , without documented measures and results for usds , it is unclear whether investments in agency digital service teams are justified .

further , by delaying the date for when it projects to fully recover its costs and not having associated performance measures , 18f is at risk of not having the information necessary for gsa leadership to determine whether to continue using the acquisition services fund for 18f operations .

although omb has called for agencies to establish digital service teams , usds policy does not require agencies to define the expected responsibilities and authorities governing the relationships between cios and digital service teams .

to fulfill their statutory responsibilities , including as most recently enacted in fitara and reinforced in omb guidance , and ensure that cios have a significant role in the decision making process for projects undertaken by the digital service teams , such defined relationships are essential .

to effectively measure 18f's performance , we recommend that the administrator of gsa direct the commissioner for the technology transformation service to take the following two actions: ensure that goals and associated performance measures are outcome - oriented and that performance measures have targets , including performance measures and targets tied to fully recovering program costs ; and goals , performance measures , and targets for how the program will achieve its mission after september 2016 ; and assess actual results for each performance measure .

to effectively measure performance , prioritize usds's resources , and ensure that cios play an integral role in agency digital service teams , we recommend that the director of the office of management and budget direct the federal chief information officer to take the following three actions: ensure that all goals and associated performance measures are outcome - oriented and that performance measures have targets ; assess actual results for each performance measure ; and update usds policy to clearly define the responsibilities and authorities governing the relationships between cios and the digital service teams and require existing agency digital service teams to address this policy .

in doing so , the federal chief information officer should ensure that this policy is aligned with relevant federal law and omb guidance on cio responsibilities and authorities .

we provided a copy of a draft of this report to gsa , omb , and 27 agencies to which we did not make recommendations .

we received comments from gsa and omb , stating that they agreed with our recommendations , and from 3 agencies — the department of housing and urban development , national science foundation , and national archives and records administration — describing their plans to establish digital service teams .

the remaining 24 agencies stated that they had no comments .

the following is a discussion of each agency's comments .

in its written comments , gsa concurred with the two recommendations and described planned actions to address them .

the agency also provided technical comments , which we have incorporated in the report as appropriate .

gsa's comments are printed in appendix iii .

in its written comments , omb generally concurred with the three recommendations and described planned actions to address them .

in a draft of this report , we had included a recommendation to omb that it establish a time frame for developing the report identifying the highest priority projects , develop the report within that established time frame and on a quarterly basis thereafter , and consider the highest priority it projects as part of the established process for prioritizing projects .

subsequently , in june 2016 omb provided a second report identifying the highest priority projects and stated that the next report would be issued by september 2016 .

given these actions , we have removed this recommendation from our report .

the agency also provided technical comments , which we have incorporated in the report as appropriate .

omb's comments are reprinted in appendix iv .

in written comments , the department of housing and urban development described activities underway for establishing a digital service team .

the department's comments are reprinted in appendix v. in written comments , the national archives and records administration stated that it plans to establish a digital service team and is currently working with usds to develop a charter .

the agency's comments are reprinted in appendix vi .

in comments provided via e - mail on june 29 , 2016 , a senior advisor from the national science foundation stated that the agency plans to fund a digital service team from its fiscal year 2016 appropriation to focus on transforming its digital services with the greatest impact to citizens and businesses so they are easier to use and more cost - effective to build and maintain .

multiple agencies also provided technical comments , which we have incorporated as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to interested congressional committees , the director of the office of management and budget , the administrator of gsa , the secretaries and agency heads of the departments and agencies addressed in this report , and other interested parties .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staffs have any questions about this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vii .

our objectives were to ( 1 ) describe 18f and u.s. digital service ( usds ) efforts to identify and address problems with information technology ( it ) projects and agencies' views of services provided , ( 2 ) assess these programs' efforts against practices for performance measurement and project prioritization , and ( 3 ) assess agency plans to establish their own digital service teams .

in addressing our first objective , we reviewed 32 projects across 18 agencies for which 18f provided services to agencies , and 13 projects at 11 agencies for which usds provided services .

to identify these projects , we obtained the list of 52 completed and ongoing projects for 18f , as of august 2015 ; and the 17 completed or ongoing projects for usds , as of august 2015 .

for the 18f program , we added a project identified by the nuclear regulatory commission that it initiated with 18f in july 2015 but that was not included in general services administration's ( gsa ) list of 18f projects .

we removed 18 projects that did not have agency customers .

in addition , we removed 1 project that was terminated without substantial work performed by 18f and 2 projects that , as of march 2016 , had not yet been initiated .

regarding usds , we removed 2 projects that did not use usds staff ( eg , projects that used staff from 18f or an agency digital service team ) , and 1 project that did not have an agency customer .

we also consolidated 2 projects into 1 project because the customer agency considered them to be a single project .

the final 32 18f projects and associated 18 agencies , as well as the final 13 usds projects and associated 11 agencies are identified in appendix ii .

we administered a data collection instrument to each of the selected projects about the services they received from 18f and usds , and the extent to which the projects were associated with major it investments .

we then analyzed information obtained from the completed data collection instruments describing the services they received from 18f and usds .

we also reviewed information obtained from 18f and usds regarding key projects that did not have agency customers .

additionally , we conducted a web - based survey of the agency managers of selected 18f and usds projects .

we designed a draft questionnaire in close collaboration with our survey specialist .

we also conducted pretests with officials at the environmental protection agency , the office of management and budget ( omb ) , and gsa .

from these pretests , we made revisions as necessary to reduce the likelihood of overall and item non - response as well as reporting errors on our questions .

we sent the survey via e - mail to the managers of the selected 32 18f and 13 usds projects from january 12 , 2016 , through february 18 , 2016 .

log - in information was e - mailed to all contacts .

we contacted project managers by telephone and e - mailed those who had not completed the questionnaire at multiple points during the data collection period .

we closed the survey on march 31 , 2016 .

we received a completed questionnaire from the managers of 35 of the 43 selected projects ( 81 percent ) — 27 of the 32 selected 18f projects ( 84 percent ) and 10 of the 13 selected usds projects ( 77 percent ) .

because we surveyed all of the project managers and therefore did not conduct any sampling for our survey , our data are not subject to sampling errors .

however , the practical difficulties of conducting any survey may introduce non - sampling errors .

for example , differences in how a particular question is interpreted , the sources of information available to respondents , or the types of people who do not respond to a question can introduce errors into the survey results .

we included steps in both the data collection and data analysis stages to minimize such non - sampling errors .

our analysts resolved difficulties that respondents had in completing our survey .

although the survey responses cannot be used to generalize the opinions and satisfaction of all customers that receive services from 18f and usds programs , the responses provide data for our defined population .

in our questionnaire we asked the managers of all projects to identify the extent to which they are satisfied or dissatisfied with the services provided by 18f and usds programs .

to determine the extent to which both programs are providing satisfactory services to its customers , we described the results on a 5-point satisfaction scale , where 5 is “very satisfied” and 1 is “very dissatisfied.” to obtain additional narrative and supporting context , survey respondents were given multiple opportunities to provide additional open - ended comments throughout our survey .

using these open - ended responses , we conducted a content analysis in order to identify common factors .

to address the second objective , we reviewed federal laws and guidance on performance measurement , and gao's guidance on investment management .

we then identified the following practices relevant to entities that provide it services: define outcome - oriented goals and measure performance .

according to federal law and our previous work , outcome - oriented goals and performance measures are vital to assess the actual results , effects , or impact of a program or activity compared to its mission .

establish and implement procedures for prioritizing it projects .

according to gao's guidance on investment management , establishing procedures , to include criteria , for prioritizing projects can help organizations consistently select projects based on their contributions to the strategic goals of the organization .

we analyzed 18f and usds policies , procedures , plans , and practices and compared them to the identified areas .

to address our third objective , we administered a data collection instrument on plans to establish digital service teams to the 25 agencies with funding proposed in the president's budget for fiscal year 2016 .

additionally , we reviewed usds's plans — to include interviews with usds officials — for providing assistance to agencies that planned to establish a digital service team in fiscal year 2016 .

in addition , we selected four agencies as case studies to determine the extent to which agencies had documented the relationships between digital service teams and agency chief information officers ( cio ) .

to choose these agencies , we identified the three agencies that had established a charter with usds as of january 2016 — the departments of defense , homeland security , and state .

we also selected the department of veterans affairs because , as of january 2016 , it had the most staff of any agency digital service team .

for these agencies , we evaluated the extent to which agency policies and procedures , including digital service team charters , clearly defined responsibilities and authorities governing the relationships between the cio and other agency organizations that use it ( in the case of this report , the other agency organizations that use it were the agency digital service teams ) .

further , we conducted interviews with the cios of the departments of defense , homeland security , and state , as well as the veterans affairs principal deputy assistant secretary for the office of information and technology .

we also shared our analysis with omb officials to review , comment , and provide additional information .

we conducted this performance audit from july 2015 to august 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

between march 2014 and august 2015 , the general services administration's ( gsa ) 18f staff helped 18 agencies with 32 projects , and generally provided services relating to its five business units: custom partner solutions , products and platforms , transformation services , acquisition services , and learn .

in addition , 18f also provided agency digital service team candidate qualification reviews .

table 8 describes each project , to include the associated agency , project name , project description , and service provided .

between august 2014 and august 2015 , usds provided assistance on 13 projects across 11 agencies .

usds generally provided seven types of consulting services: quality assurance , research , website development , system stabilization , information security assessment , software engineering , and data management .

table 9 describes each project , to include the associated agency , project name , project description , and service provided .

in addition to the contact named above , individuals making contributions to this report included nick marinos ( assistant director ) , kavita daitnarayan , rebecca eyler , kaelin kuhn , jamelyn payan , and tina torabi .

