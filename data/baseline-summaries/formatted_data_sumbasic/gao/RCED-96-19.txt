this report was mandated by the small business research and development enhancement act of 1992 .

the report focuses on the implementation of the small business technology transfer ( sttr ) pilot program , which was established by the act .

in particular , the report discusses ( 1 ) the quality and commercial potential of the sttr program's research as shown by technical evaluations of the winning proposals in the first year of the sttr program , ( 2 ) how agencies addressed potential conflicts of interest resulting from the involvement of federally funded research and development centers in the program , and ( 3 ) agencies' views on the effects of and need for the sttr program in view of its close similarity to the small business innovation research program .

please contact me at ( 202 ) 512-3841 if you or your staff have any questions .

major contributors to this report are listed in appendix i .

the small business research and development enhancement act of 1992 established the small business technology transfer ( sttr ) pilot program and authorized it for 3 years , beginning in fiscal year 1994 .

the act also reauthorized the small business innovation research ( sbir ) program , which was established in 1982 .

the sttr program was modeled closely on the sbir program .

both of the programs , for example , share the same four major goals: stimulating technological innovation .

using small business to meet federal research and development ( r&d ) needs .

fostering and encouraging participation by minorities and disadvantaged persons in technological innovation .

increasing private - sector commercialization of innovations derived from federal r&d .

the two programs include many of the same agencies , and the same agency offices administer the programs .

in spite of the numerous points in common , the two programs differ in one important respect .

the sttr program requires a small business to collaborate with a u.s. nonprofit research institution , such as a university ; a contractor - operated , federally funded research and development center ; or other entity , in order to be eligible for an award .

this collaboration is permitted under the sbir program , but it is not mandatory .

agency participation in and funding for the sttr program followed the general approach established for the sbir program .

agencies having a budget of more than $1 billion annually in fiscal years 1994 , 1995 , or 1996 for external r&d are authorized to set aside a percentage of this amount for the sttr program .

the act sets the percentage at not less than 0.05 percent in fiscal year 1994 , not less than 0.1 percent in fiscal year 1995 , and not less than 0.15 percent in fiscal year 1996 .

five agencies — dod , doe , nasa , nih , and nsf — currently participate in the program .

in the sbir program , by contrast , the same five agencies and six additional agencies with smaller budgets for external r&d were authorized to set aside a significantly higher percentage and amount of money .

when the sttr program reaches its highest authorized funding percentage in fiscal year 1996 , it will receive about $60 million , according to small business administration ( sba ) calculations ; when the sbir program reaches its maximum funding percentage in fiscal year 1997 , it will receive about $1 billion .

as with sbir , the legislation required agencies to solicit proposals for r&d projects .

the solicitation lists and describes the topics to be addressed by company proposals and invites companies to submit proposals for consideration .

each agency is responsible for targeting research areas and administering its own sttr funding agreements .

such agreements include a contract , grant , or cooperative agreement entered into between a federal agency and a small business for the performance of experimental , developmental , or research work funded in whole or in part by the federal government .

the legislation also required sba to issue a policy directive for the general conduct of sttr programs within federal agencies .

the directive was to include such features as simplified , standardized , and timely sbir solicitations ; a simplified , standardized funding process ; and minimization of the regulatory burden for small businesses participating in the program .

the sttr policy directive was issued in august 1993 and remains in effect .

federal agencies are also required to report key data to sba .

to be eligible for an sttr award , small businesses must be independently owned and operated , other than the dominant firms in the field in which they are proposing to carry out sttr projects , organized for profit , the employer of 500 or fewer employees ( including its affiliates ) , and at least 51 percent owned by u.s. citizens or lawfully admitted permanent resident aliens .

the law established a three - phase structure for the program .

the first phase , not to exceed 1 year , is designed to determine the scientific , technical , and commercial merit and feasibility of a proposed idea .

the second phase , not to exceed 2 years , is designed to further develop the idea .

the statute established $100,000 and $500,000 as the general limits for phases i and ii awards , respectively .

the third phase is somewhat more flexible and difficult to define .

in general , this phase is expected to result in commercialization or further continuation of r&d .

no sttr funding is provided for phase iii .

unlike phases i and ii , phase iii has no general time limits .

in addition , phase iii can include not only federal non - sttr funds but private - sector funds .

for selection of phase i proposal awards , sba's 1993 policy directive stated that agency criteria shall give primary consideration to scientific and technical merit along with the potential for commercialization .

according to the directive , funding for phase ii shall be based upon the results of phase i and the scientific and technical merit and commercial potential of the phase ii proposal .

given the similarities between the sbir and sttr programs , the question of the rationale for the sttr program naturally presents itself .

in the 1992 report cited above , the chairman of the house committee on small business provided two basic answers to this question when the sttr program was under consideration .

the report states that the program addresses a core problem in u.s. economic competitiveness and that the existing sbir program does not provide a direct mechanism for technology transfer .

in discussing the first point , the report noted that the nation's research institutions — its universities , federal laboratories , and nonprofit research institutions — contain enormous scientific and technical resources .

the report also noted that the institutions employ one in four scientists and engineers in the nation and perform nearly $40 billion in r&d each year , or one - quarter of all r&d conducted in the united states .

in addition , the report stated that perhaps the core of the u.s. competitiveness problem is the inability to translate its worldwide leadership [in science and engineering] into technology and commercial applications that benefit the economy .

the report concluded that what is needed is an effective , systematic “technology transfer” mechanism to move new knowledge from the research institution to industry , where it can be exploited for the national good .

in discussing the second point , the report noted that sbir has turned out to be a remarkably effective mechanism for commercializing ideas in the small business community .

however , according to the report , sbir is less effective at fostering commercialization of ideas that originate in universities , federal laboratories , and nonprofit research institutions .

sttr would provide a strong incentive for small companies and researchers at universities , federal laboratories , and nonprofit research institutions to find each other and work together because the only way they can access sttr funding is by collaborating .

thus , sttr was envisioned primarily as a technology transfer program , in which promising concepts originating in the nonprofit research community would move toward commercialization .

in accomplishing this objective , researchers in nonprofit research institutions would ally themselves with small businesses and play a greater role in sttr projects than they could play in sbir projects .

table 1.1 summarizes basic information about the phase i awards made during the first year ( fiscal year 1994 ) of the sttr program .

as shown in the table , agencies made 206 phase i awards in the first year of the program , with dod accounting for about half of them .

among the small business affiliates , universities constituted the majority ( 78 percent ) of the research partners .

doe was the only agency in which the awardees formed partnerships with research and development centers in a majority of cases .

among other significant points about awardees during this period , according to sba officials , almost two - thirds of the awards went to companies that had previously received awards from the sbir program .

in addition , the officials told us that the proposed allocation of award money between the small businesses and the research institutions showed the former receiving about 61 percent of the funding and the latter receiving about 39 percent .

in addressing the quality and commercial potential of sttr research , we obtained and reviewed all of the agencies' technical evaluations for 206 proposals that received phase i awards on the basis of fiscal year 1994 solicitations .

the technical evaluations were prepared by experts in federal agencies or the private sector , if an agency relied on outside peer review .

the evaluations played the primary role in determining which proposals were selected for funding .

the number of evaluations for each award ranged from one to as many as six .

we reviewed the technical evaluations of project proposals because they were the only systematic source of information on the quality and commercial potential of the research .

at this time , the actual results of these awards cannot be assessed .

we restricted our work to the 206 awards because they were the only complete set of awards made under the pilot program at the time that we conducted our review .

the results of the first round of the phase ii award process were not available for all agencies until late in 1995 ; in fact , only a few phase ii awards were made during fiscal year 1994 .

because our conclusions about the quality of research and commercial potential were drawn from these technical evaluations of proposed projects , we gave additional attention to the evaluation process used by each of the agencies .

in our view , greater confidence could be placed in evaluations resulting from a thorough and well - documented evaluation process .

in particular , we noted the number of reviewers per proposal , the number of proposals per reviewer , and the level of analysis resulting from the reviews .

our findings were agency - specific for dod , nih , nasa , nsf , and doe .

in addition to reviewing the technical evaluations , we obtained further information about the evaluation process from discussions with program officials in each of the sttr agencies .

to address compliance by agencies and research and development centers with a requirement of the act to avoid conflicts of interest , we obtained relevant documents from and conducted interviews with sttr program officials .

the act required each federal agency with an sttr program to develop , in consultation with the office of federal procurement policy and the office of government ethics , procedures to ensure that research and development centers ( 1 ) are free from organizational conflicts of interests relative to the sttr program ; ( 2 ) do not use privileged information gained through work performed for an sttr agency or private access to sttr agency personnel in the development of an sttr proposal ; and ( 3 ) use outside peer review , as appropriate .

to address the effect , if any , of sttr on sbir and other agency r&d , we interviewed sttr program officials at sba and the five agencies that have sttr programs.as agreed with the committees , we also looked at the need for the sttr program .

the sttr program , as mentioned earlier , is modeled closely on the sbir program .

the issue then arises whether there is a need for two separate programs .

in addressing this concern , we reviewed a 1992 report by the house committee on small business .

the report helped us to identify questions that are relevant in determining the need for the sttr program .

we performed our audit work between june and december 1995 in accord with generally accepted government auditing standards .

agencies generally rated the quality of the proposed research and commercial potential in sttr proposals highly .

for example , doe rated the quality of research in all 21 of its winning proposals as being among the top 10 percent of all research in the agency .

evaluations of the commercial potential were also favorable but somewhat more cautious .

for example , in some cases there were concerns about the cost of the product that might result or the limited size of its potential market .

such reservations were understandable in view of the newness of the program and the innovation or risk associated with many of the proposed projects .

the evaluation process , upon which these findings depended , varied greatly .

in agencies other than dod , the selection relied on input and consensus among several ( generally three or four ) technical reviewers .

in dod , by contrast , a single reviewer was frequently responsible for the technical evaluation .

table 2.1 provides a brief overview of the evaluation processes used by the five agencies and the agencies' assessment of the research proposals .

the following sections summarize ( 1 ) the agencies' evaluation processes and ( 2 ) the quality of research and commercial potential identified in the technical evaluations .

in its first ( fiscal year 1994 ) solicitation for sttr proposals , nih identified seven review criteria , including the soundness and technical merit of the proposed approach and commercial potential , which were used to select proposals for funding .

these criteria were not given specific weight in the evaluation process .

in the process of evaluation , nih assigned two reviewers ( who provided written evaluations ) and two additional readers to each proposal .

a peer review panel consisting of 10 or more experts was convened .

the four reviewers and readers began the discussion of each proposal by presenting a specific , numerical score to the panel .

the two reviewers also presented their written statements .

the panel then discussed the proposal in further detail to assist in the development of a final summary statement .

each member of the panel also provided a score , which ranged between 100 for the best and 500 for the worst .

the level of quality was defined as follows: 100 to 150: outstanding .

150 to 200: excellent .

200 to 250: very good .

250 to 350: good .

350 to 500: acceptable .

for the 48 winning proposals , the average score was 165 ( or “excellent” ) .

specifically , 14 proposals were judged outstanding , 31 excellent , 2 very good , and only 1 good .

there were none in the acceptable category .

this overall result compared favorably with nih's sbir program , in which the average score for all sbir phase i projects with awards in fiscal year 1994 was 187 .

about 94 percent of sttr's awards were judged as outstanding or excellent , which compared very favorably with sbir's 66 percent .

in contrast to the practice at other agencies , no separate score was assigned for the quality of research and commercial potential .

the chief of the technology and applied sciences section said that the importance of individual criteria may vary from one proposal to another .

in his view , an overall score allows for greater flexibility than the sum of a series of specific scores , each of which represents a fixed percentage of the total potential score .

the chief added , however , that nih has organized a committee to evaluate its entire scoring system .

although the panels did not provide scores for each criterion , they provided detailed written analyses of the proposals .

the summaries for nih's winning proposals include , among other points , a resume of the research , a critique of strengths and shortcomings in terms of research , and an assessment of its commercial potential .

nih's evaluations of the quality of research were generally very favorable .

one evaluation , for example , described a proposal as addressing a well worthwhile problem involving a new method of enhancing the power of x - ray tubes .

the evaluation stated that the proposed approach was sound , had high technical merit , and was supported by the theoretical calculations presented .

statements about commercial potential were similarly favorable but sometimes contained expressions of concern about costliness or other potential drawbacks .

in one instance , involving a new approach to x - ray mammography , the evaluation found that the proposed approach likely will result in an expensive technology , but prospects are good for a result that improves on current technology and should enjoy a very high market potential .

in another case , the commercial potential for a product that would help prevent geriatric wandering was described as good if the cost of hardware and system operation could be kept low .

the evaluation raised concerns about the potential costliness and sophistication of the system .

dod identified four criteria in making its phase i awards , including ( 1 ) the soundness and technical merit of the proposed approach and ( 2 ) the potential for commercial ( government or private sector ) application and the benefits expected to accrue from this commercialization .

dod specified that each of the four criteria was worth 25 points .

only one dod agency , the ballistic missile defense organization ( bmdo ) , developed a different set of basic criteria and provided no quantitative scores .

dod officials pointed out that there was a great deal of variation from one dod agency to another in the evaluation process .

in particular , the agencies varied in the number of reviews that they require to make an award .

in the navy , air force , and advanced research projects agency ( arpa ) , 47 of the winning proposals received only one technical review ; in bmdo , by contrast , the number ranged between two and six .

several dod program managers noted that the number and quality of the reviews were time - dependent and that time pressures may lead to fewer or less thorough reviews .

in fact , of the 47 winning proposals with only one review , 37 were limited in the analysis used to support the award .

generally speaking , the dod evaluations rated the quality of research as favorable for most of the winning proposals ; as with nih , the evaluations of commercial potential were frequently positive but accompanied by caveats .

nasa identified four criteria for evaluating proposals and assigned them significantly different weights .

these criteria included scientific / technical merit ( worth 20 percent of the total score ) , the anticipated commercial applications of the technology ( worth 40 percent ) , and two other criteria ( with a combined value of 40 percent ) .

the great emphasis placed upon commercial applications in the evaluation process was unique among the agencies .

nasa developed a clear statement of its selection procedures .

according to this statement , proposals were to be evaluated by a review team consisting of three members — one from academia , one from the private sector , and one from government .

each reviewer was to independently review the proposals .

the nasa technical manager at the field center was required to resolve differences and obtain a consensus on the merit of the proposal .

in a further document providing “guidelines for evaluators,” nasa stated that a scoring range of 90 to 100 percent should be interpreted as equivalent in quality to the top 10 percent of all nasa proposals for comparable r&d .

a score between 80 and 89 percent signified an above average proposal .

of the 21 winning proposals , 11 were considered above average , and 8 were judged as being among the top 10 percent of all nasa proposals for comparable r&d .

nsf identified five criteria to which approximately equal consideration was given .

these criteria included ( 1 ) the scientific / engineering quality and ( 2 ) the potential for commercial applications and the success of past commercialization efforts .

after an initial screening to eliminate any proposals not responsive to the solicitation , nsf required three concurrent reviews of the remaining proposals .

the reviewers rated each of the above criteria on a five - point scale ( from poor to excellent ) .

a “very good” score was 20 , and “excellent” ( or maximum ) score was 25 .

the reviewers then presented their results to a panel , which developed a summary for each proposal .

of the 11 winning proposals , the quality of the research and its commercial potential were consistently rated as favorable .

among the 33 reviews of the 11 projects , all but one found the quality to range between very good and excellent .

for commercial potential , the majority of the reviews found the quality to be excellent .

only 1 of the 33 reviews evaluated the quality as merely “good. .

doe identified five criteria with approximately equal consideration given to each of them .

these criteria included ( 1 ) the scientific / technical quality of the research and ( 2 ) the anticipated technical and / or economic benefits of the proposed research , if successful , with special emphasis on the likelihood that the project will attract further funding for product or process development after the sttr support expires .

doe's evaluation process consisted of two steps .

first , each of three reviewers provided written comments addressing the five criteria but did not assign a specific score to the criteria .

second , technical managers reviewed the three evaluations and quantified the results .

of the 21 winning proposals , 16 received perfect scores .

the other five received perfect scores on four of the five criteria .

the managers expressed unanimous agreement about the quality of the research in particular .

according to doe's definition of quality as used in evaluating sttr proposals , a rating of “outstanding” for scientific / technical merit indicated that the proposal was comparable to the top 10 percent of projects in doe .

all 21 of the winners were rated as outstanding .

while doe did not specifically evaluate proposals for commercial potential , it did evaluate anticipated technical and / or economic benefits .

twenty of the 21 proposals received outstanding ratings ; only 1 was rated in the next lower category as “significant.” many of the evaluations on which the managers based their conclusions contained favorable , but qualified , statements about commercial potential .

for example , one of the reviewers wrote that , if the research is successful , the technology will be rapidly commercialized .

one reason for the excellent results , as noted by doe's sttr program manager , was the unusually large number of proposals in relation to the number of awards available .

doe received 487 proposals , meaning that less than 5 percent of the applicants were successful .

in the manager's view , the program was excessively competitive .

as a result , he took two steps to reduce the number of applicants .

first , a notice was included in the second solicitation to alert applicants to the situation .

the notice pointed out the ratio ( 1 award to 23 proposals ) in the first year and concluded that only those applications with the highest scientific / technical quality would be competitive .

second , because broad topics tend to attract more proposals , the topics were narrowed in the new solicitation to reduce the number of proposals and improve the award / proposal ratio from 1 in 23 to about 1 in 10 .

in general , technical evaluations of sttr proposals showed favorable views of the quality of proposed research and commercial potential .

for research quality , the evaluations ( 1 ) awarded perfect scores to many proposals , ( 2 ) rated proposals among the top 10 percent of research in certain agencies , ( 3 ) described some proposals as “cutting edge,” and ( 4 ) generally found the quality to be excellent .

for commercial potential , the evaluations arrived at similarly favorable conclusions , although in some cases they were somewhat more cautious because of the newness of the program or the risk associated with the proposals .

the evaluation processes varied greatly , ranging from several technical reviewers to only one per proposal and from detailed critiques to evaluation sheets that provided no analysis in support of the ratings .

almost half ( 47 ) of dod's winning proposals received only one review ; in 37 cases , the reviews were too brief to support the findings .

because of their limited analysis and lack of a broader consensus , these instances tended to reduce our confidence in the reliability of the results .

the five federal agencies with sttr programs have taken steps to avoid potential problems relating to conflict of interest with federally funded research and development centers .

in addition , dod and doe , which accounted for 29 of the 32 awards involving centers during the first year of the program , have taken steps to prevent centers from using privileged information in preparing sttr proposals .

the legislation establishing the sttr program required agencies to develop procedures to ensure that r&d centers are free from organizational conflicts of interest .

such conflicts might arise , for example , if a center formed a partnership with a company submitting an sttr proposal and helped a federal agency judge the merits of its own and other proposals .

a second requirement directs each agency to develop procedures ensuring that the centers use outside peer review , as appropriate .

under the sttr program , however , the agencies , not the centers , are responsible for decisions regarding peer review ; accordingly , we have focused on what the agencies have done .

in addition , the legislation required agencies to ensure that the centers do not use privileged information gained through work performed for an sttr agency or private access to sttr agency personnel in the development of an sttr proposal .

in general , the five agencies with sttr programs have taken steps to prevent conflicts of interest from occurring .

dod , doe , and nih have specific policies intended to prevent such conflicts while nasa and nsf have more general procedures to prevent such conflicts of interest from arising .

dod's director of defense research and engineering issued a memorandum in mid - 1994 providing policy guidance on research and development centers' participation with industry in sttr and similar programs .

the memorandum concludes that , if a center requests authorization to participate in such programs , the mission of the particular center and the potential for conflict of interest must be primary considerations in the decision process .

as a result of dod's policy , only two r&d centers are currently approved research partners for its sttr awardees .

the air force had to rescind some awards because the proposed research partners ( certain dod laboratories ) were ineligible to participate .

according to dod's sttr program director , future proposals will be evaluated on a case - by - case basis to ensure that conflicts of interest do not occur .

doe has a policy addressing conflict of interest for the sttr program .

according to the policy , doe staff members should neither request nor receive assistance from personnel in research institutions ( that are eligible to participate in the sttr program ) in the preparation of technical topics for the sttr solicitation .

in addition , no person affiliated with a research institution may serve as a reviewer of a grant application that names that research institution as a participant .

furthermore , no one affiliated with a research institution may assist technical managers with the doe review , evaluation , and selection process for phase i grant applications in a particular scientific area if that research institution is a participant on any grant application submitted to that scientific area .

nih has adopted a certification procedure to avoid conflicts of interest .

nih's solicitation for proposals requires the applicant to certify that it “ ( 1 ) is free from organizational conflicts of interests relative to the sttr program , ( 2 ) did not use privileged information gained through work performed for an sttr agency...and ( 3 ) used outside peer review , as appropriate , to evaluate the proposed project and its performance therein.” nasa relied on a peer review process for its proposals to avoid conflicts of interest .

as stated earlier , nasa's review teams consisted of three members — one from academia , one from the private sector , and one from the government .

according to nasa's sttr program manager , none of the reviewers were connected with a federally funded r&d center .

the manager also mentioned that nasa's reviewers certified that they have no conflict of interest in their evaluations .

the manager said that the only federally funded r&d center directly associated with nasa on a regular basis is the jet propulsion laboratory .

the manager said that , although the laboratory was included as a research partner in several proposals by small businesses , it was not involved in reviewing those proposals or administering the program ; in addition , no special information was provided to this center .

nsf also relied on peer review procedures to avoid conflicts of interest .

its “proposal and award manual” provides guidance on the use of peer review and states that each nsf program has one primary method for peer review which represents the minimum evaluation received by proposals in that program .

each of its sttr proposals received three reviews .

nsf's sttr program manager told us that , in fiscal year 1994 , none of nsf's 11 awards involved a small business in a partnership with a research and development center .

in general , four of the five agencies with sttr programs used peer review in evaluating sttr proposals .

nih , nasa , nsf , and doe relied on specified numbers of outside reviewers .

nih assigned four technical reviewers to each proposal and provided additional input through its peer review panels .

nsf , nasa , and doe used three .

dod was the only agency that relied mainly on technical expertise within the department rather than on outside reviewers .

this approach followed dod's usual policy in evaluating proposals .

dod and doe , which accounted for 29 of the 32 awards involving r&d centers as research partners , have policies to prevent centers from using privileged information .

dod's policy of carefully restricting participation by its own laboratories helped in preventing the centers from using inside knowledge in preparing proposals .

as mentioned earlier , only two of dod's centers were eligible to participate in the program .

the exclusion of the other centers , which would be the main sources of privileged information regarding dod's research needs , avoided the potential problem raised by this issue .

doe's policy prohibits agency staff members from requesting or receiving assistance from personnel in research institutions ( that are eligible to participate in the sttr program ) in the preparation of technical topics for the sttr solicitation .

this policy is intended to prevent research institutions from using their expertise to influence doe's choice of sttr solicitation topics .

otherwise , research institutions could acquire a significant advantage by designing topics to match their expertise and then preparing a proposal in the same area .

agency officials expressed differing views regarding the effect of sttr on sbir and other agency r&d .

furthermore , none of the officials indicated any specifically negative effects such as a competition between the two programs for quality proposals .

however , conclusive information concerning the effect , if any , of sttr on sbir and other agency r&d was not available because of the program's newness and smallness .

the similarities between the sttr and sbir programs raise a broader issue about the need for the sttr program .

the rationale for the program , which points to certain weaknesses in sbir and potential strengths in sttr , suggests some additional questions that point more directly toward evaluating the need for sttr .

the legislation establishing the sttr program required us to assess the effects of sttr on the sbir program and other agency r&d .

our discussions with agency officials provided no evidence to suggest that sttr was competing for quality proposals with sbir or reducing the quality of agency r&d in general .

instead , agency officials expressed differing views regarding the effect of sttr on sbir and other agency r&d .

a few officials noted some potentially beneficial effects .

others said that sufficient data were not yet available to determine the effect , if any , of sttr on sbir or other agency r&d .

sba officials contended that sttr was too small and too new a program to have any real effect on sbir or on the broader range of agency research at the present time .

the officials pointed out that the program represented only 0.05 percent of each agency's external r&d budget during its first year and that it was only 1 year old .

nih's sttr program manager told us that one of the main potential effects of the program is that universities can have a greater role than under sbir .

however , the manager noted that , in an sbir survey undertaken by nih several years ago , collaboration between small businesses and universities was already evident in well over half of the sbir projects .

in contrast to the view that sttr's effect was very limited , the army's sttr program manager said that sttr was influencing sbir in a beneficial way .

in his opinion , sttr is becoming known through national conferences and other channels .

as a result , small businesses are realizing that they have more credibility and chance of winning an award by collaborating with a university or other research institution .

the manager believes that the sttr program has already led to more collaboration in sbir .

in general , according to the program manager , sttr is a promising program that may be as successful as the sbir program .

the manager also said that sttr will influence agencies whose research has traditionally involved the university community to a lesser extent .

in dod , for example , the program manager believes that sttr's impact will be greater than in certain other agencies ( such as nih ) where the research program has been closely tied to the universities .

doe's sttr program manager said that he has tried to make the topics in the sttr solicitation somewhat different from sbir .

regarding the effect of sttr on doe's research program , he said that sttr projects , if successful , will help to meet the agency's r&d needs by contributing to areas of particular interest to the agency .

however , the manager added that no conclusions can be drawn in the first year of the program because the data are too limited .

the stated rationale for the sttr program and the mandatory collaboration suggest three additional questions that are relevant in determining the need for the sttr program: ( 1 ) is the technology originating primarily in the research institution as envisioned in the rationale for the program or is it originating in the small business ? .

 ( 2 ) is the mandatory collaboration between the small business and the research institution effective in transferring the technology to the market place ? .

 ( 3 ) can the sbir program accomplish the same objective without the collaboration required by the sttr program ? .

the technology may originate with the research institution , the small business , or a combination of the two .

in the sttr program , the assumption is that the research institution will be the primary originator of the new concept .

however , data to determine the extent to which research institutions are providing the core technologies are not currently available .

neither sba nor the agencies have collected this information .

doe's sbir / sttr program manager said that he would like to know whether the companies or the research institutions were drafting the proposals , but such information is not available .

the relative roles of the research institution and the small business as the source of the technology bear directly on the need for the sttr program .

if a high percentage of the ideas are originating with small businesses rather than with research institutions , this finding would raise questions about the need for the program .

on the other hand , if a high percentage of ideas are originating with research institutions , this finding would suggest that the program was achieving the first step in moving ideas from research institutions to small businesses .

if the program is effective in moving ideas from research institutions to small businesses , then the next logical question concerns whether their collaboration is effective in moving them to the market place .

this question can be approached from two directions: ( 1 ) short - term views of how well the collaboration is working in general and ( 2 ) long - term data on actual commercialization .

information on how well the collaboration is working can be obtained in the near future .

in particular , it would be useful to know how the small businesses rated the contribution made by their research partners to the research effort .

since most of the companies had not completed even the first phase of their sttr award at the time of this report , such information was not available , but it will be obtainable in the next year or two .

information on actual commercial outcomes will require a greater amount of time before it can be obtained .

generally , 5 to 9 years are needed to turn an initial concept into a marketable product .

thus , it may be several years before the commercial effectiveness of the program can be evaluated .

because one important difference between the two programs is that sttr makes a small business / research institution collaboration mandatory , the question arises whether the sbir program could accomplish the objective of transferring technology from research institutions to the private sector without the mandatory collaboration .

the rationale for the sttr program tends to assume that such collaborations were relatively rare in the sbir program .

however , as noted above , nih's program manager told us that , in an sbir survey undertaken by nih several years ago , collaboration between small businesses and universities was already evident in well over half of nih's sbir projects .

by contrast , the army's program manager believed that sttr's impact will be greater in the army than in agencies such as nih because the army sbir program has had a lesser degree of involvement with universities and other research institutions in the past .

given the apparent variation from one agency to another and the lack of current data , no definite conclusion can be drawn at present concerning the need for sttr in forging new collaborations .

certain proposals may suggest a need for the sttr program .

for example , one nih proposal involved a relationship between the developer of a new type of microscope and a company with the experience and capability to commercialize the instrument .

the principal investigator with the company told us that sttr led to a partnership between his company and the research institution that would not have existed otherwise .

according to the investigator , without sttr , his company would have been reluctant to devote a lot of time to a technology that it did not have rights to .

in such a situation , according to the investigator , the company would have provided specific components of the microscope to the research institution , let the institution develop the product , and then pursued a license to market it .

instead , under sttr , the company developed an agreement with the research institution which led to the current partnership .

however , other cases may suggest the opposite .

for example , the president of a company that has participated in both the sbir and sttr programs said that , in his experience , the sbir program gave companies a greater advantage in dealing with research institutions as potential partners .

under sttr , according to this official , the research institution has “veto power,” but under sbir , a company is in a better negotiating position if it wants research to be done .

in addition , according to the official , the sttr program will not be able to alter the research - oriented outlook of the universities in a more commercial direction .

in general , the official said that almost all sttr research could be accomplished through sbir .

bernice steinhardt , associate director robin nazzaro , assistant director dennis carroll , evaluator the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 6015 gaithersburg , md 20884-6015 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 301 ) 258-4066 , or tdd ( 301 ) 413-0006 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

