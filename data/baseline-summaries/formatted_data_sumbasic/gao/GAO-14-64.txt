in accordance with its responsibilities set forth in law , the office of management and budget ( omb ) launched the federal information technology ( it ) dashboard in june 2009 as a public web site that reports performance and supporting data for major it investments — on which the federal government plans to invest over $38 billion in fiscal year 2014 .

the dashboard is to provide transparency for these investments in order to facilitate public monitoring of government operations and accountability for investment performance by the federal chief information officers ( cio ) who oversee them .

according to omb , it began using the dashboard to identify at - risk investments with its launch in june 2009 .

these investments became the focus of joint omb - agency techstat accountability sessions ( techstats ) — evidence - based reviews intended to improve investment performance through concrete actions .

this report responds to your request that we review the consistency of investment risk information submitted to the dashboard .

our objectives for this engagement were to ( 1 ) characterize the cio ratings for selected federal agencies' it investments as reported on the dashboard over time , ( 2 ) determine the extent to which selected agencies' cio ratings are consistent with reported investment risk , and ( 3 ) determine the extent to which selected agencies are addressing at - risk investments .

to address our first objective , we selected the eight agencies with the most reported major it spending in fiscal year 2012 , after excluding agencies reviewed in our most recent report on the dashboard.reviewed publicly available data from the dashboard from its inception in june 2009 through august 2013 .

we also interviewed officials from omb and the selected agencies .

to accomplish the second objective , we selected the top 10 major investments with the highest reported it spending at each of the 8 selected agencies ( for a total of 80 investments ) .

we reviewed investment documentation ( such as executive - level briefings , operational analyses , and the results of performance reviews ) and interviewed officials from each of the agencies to understand how they rated and monitored investments .

we compared these documents to the cio ratings submitted to the dashboard in calendar year 2012 to determine whether the ratings on the dashboard were accurately portraying the risk of these investments .

we elected to review the investments' ratings during calendar year 2012 so that our assessment would span multiple fiscal years .

finally , for our third objective , we reviewed and described each of the processes used by the eight selected agencies to address the highest - risk major investments ( such as techstat guides and investment review board results ) .

we also identified investments within the 80 investments selected for the second objective which had received high or moderately high ratings of risk on the dashboard during 2012 .

we analyzed both the processes used to address these investments and the results of performance reviews .

we also interviewed officials to determine whether the agencies implemented the appropriate risk management processes to oversee and review the selected investments .

we conducted this performance audit from february to december 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are provided in appendix i .

while it can enrich people's lives and improve organizational performance , we have previously reported that federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

for example , in july 2013 , we testified that the federal government continued to spend billions of dollars on troubled it investments and we identified billions of dollars worth of failed and challenged it projects .

omb plays a key role in overseeing how federal agencies manage their it investments by working with them to better plan , justify , and determine how to manage them .

each year , omb and federal agencies work together to determine how much the government plans to spend on it projects and how these funds are to be allocated .

omb also guides agencies in developing sound business cases for it investments and establishing management processes for overseeing these investments throughout their life cycles .

the scope of this undertaking is quite large: in planning for fiscal year 2014 , 27 federal agencies reported plans to spend about $82 billion on it.investments , $37.6 billion is to be spent on non - major it investments , and $5.5 billion is to be spent on classified department of defense it investments .

of that , $38.7 billion is to be spent on major it within omb , the office of e - government and information technology , headed by the federal cio , directs the policy and strategic planning of federal it investments and is responsible for oversight of federal information technology spending .

in june 2009 , omb deployed a public web site to further improve the transparency and oversight of agencies' it investments .

known as the it dashboard,performance data for over 700 major federal it investments at 27 federal agencies , accounting for $38.7 billion of those agencies' planned $82 billion budget for fiscal year 2014 .

according to omb , these data are intended to provide a near - real - time perspective on the performance of these investments , as well as a historical perspective .

further , the public display of these data is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold the government agencies accountable for progress and results .

this site displays federal agencies' cost , schedule , and the dashboard visually presents performance ratings for agencies and for individual investments using metrics that omb has defined — cost , schedule , and cio evaluation .

the web site also provides the capability to download certain data .

figure 1 is an example of va's portfolio page as recently depicted on the dashboard .

 ( the ratings are explained in the narrative following the figure. ) .

the dashboard's data spans the period from its june 2009 inception to the present , and is based , in part , on agency assessments of individual investment performance and each agency's budget submissions to omb .

the cost and schedule data agencies are required to submit to the dashboard have changed over time , as have the related calculations .

for example , in response to our recommendations ( further discussed in the following section ) , omb changed how the dashboard calculates the cost and schedule ratings in july 2010 to include “in progress” milestones rather than just “completed” ones for a more accurate reflection of current investment status .

while the required cost and schedule data have changed , the thresholds for assigning cost and schedule ratings have remained constant over the life of the dashboard .

specifically , the dashboard assigns rating colors ( red , yellow , green ) based on agency - submitted cost and schedule variances , as shown in table 1 .

unlike the variance - based cost and schedule ratings , the dashboard's “investment evaluation by agency cio” ( also called the cio rating ) is determined by agency officials .

according to omb's instructions , each agency cio is to assess his or her it investments against a set of six pre - established evaluation factors identified by omb ( shown in table 2 ) and then assign a rating of 1 ( high risk ) to 5 ( low risk ) based on the cio's best judgment of the level of risk facing the investment .

omb recommends that cios consult with appropriate stakeholders in making their evaluation , including chief acquisition officers , program managers , and other interested parties .

according to an omb staff member , agency cios are responsible for determining appropriate thresholds for the risk levels and for applying them to investments when assigning cio ratings .

omb requires agencies to update these ratings as soon as new information becomes available which will affect an investment's assessment .

after agencies assign a level of risk to each investment , the dashboard assigns colors to cio ratings according to a five - point scale , as illustrated in table 3 .

an omb staff member from the office of e - government and information technology noted that the cio rating should be a current assessment of future performance based on historical results and is the only dashboard performance indicator that has been defined and produced the same way since the dashboard's inception .

furthermore , omb issued guidance in august 2011other things , that agency cios shall be held accountable for the performance of it program managers based on their governance process and the data reported on the it dashboard , which includes the cio rating .

according to omb , the addition of cio names and photos on dashboard investments is intended to highlight this accountability and link it to the dashboard's reporting on investment performance .

we have previously reported that omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , and by improving the accuracy of investment ratings .

we also found issues with the accuracy and data reliability of cost and schedule data , and recommended steps that omb should take to improve these data .

in july 2010 , we reportedomb's dashboard were not always accurate for the investments we that the cost and schedule ratings on reviewed , because these ratings did not take into consideration current performance .

as a result , the ratings were based on outdated information .

we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting .

omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and completed activities .

similarly , our report in march 2011 noted that omb had initiated several efforts to increase the dashboard's value as an oversight tool , and had used its data to improve federal it management .

we also reported , however , that agency practices and the dashboard's calculations contributed to inaccuracies in the reported investment performance data .

for instance , we found missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data .

as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance .

most agencies generally concurred with our recommendations and three have taken steps to address our recommendation .

omb agreed with our recommendation for improving ratings for schedule variance .

it disagreed with our recommendation to improve how it reflects current performance in cost and schedule ratings , but more recently made changes to dashboard calculations to address this while also noting challenges in comprehensively evaluating cost and schedule data for these investments .

we also reported on omb's guidance to agencies for reporting their it investments in september 2011 and found that it did not ensure complete reporting .

specifically , agencies differed on what investments they included as an it investment .

among other things , we recommended that omb clarify its guidance on reporting it investments to specify whether certain types of systems — such as space systems — are to be included .

omb did not agree that further efforts were needed to clarify reporting in regard to the types of systems .

subsequently , in november 2011 , we noted that the accuracy of investment cost and schedule ratings had improved since our july 2010 report because omb had refined the dashboard's cost and schedule calculations .

most of the ratings for the eight investments we reviewed as part of our november 2011 report were accurate , although we noted that more could be done to inform oversight and decision making by emphasizing recent performance in the ratings .

we recommended that the general services administration comply with omb's guidance for updating its ratings when new information becomes available ( including when investments are rebaselined ) .

the agency concurred and has since taken actions to address this recommendation .

since we previously recommended that omb improve how it rates investments , we did not make any further recommendations .

more recently , in october 2012 we reported that cios at six agencies rated a majority of investments listed on the federal it dashboard as low or moderately low risk from june 2009 through march 2012 .

additionally , two agencies , the department of defense and the national science foundation , rated no investments as high or moderately high risk during this time period .

agencies generally followed omb's instructions for assigning cio ratings , although the department of defense's ratings were unique in reflecting additional considerations , such as the likelihood of omb review .

most of the selected agencies reported various benefits associated with producing and reporting cio ratings , such as increased quality of their performance data and greater transparency and visibility of investments .

we recommended that omb analyze agencies' investment risk over time as reflected in the dashboard's cio ratings and present its analysis with the president's annual budget submission , and that the department of defense ensure that its cio ratings reflect available investment performance assessments and its risk management guidance .

both agencies concurred with our recommendations , and omb reported on cio rating trends in the fiscal year 2014 budget submission ; however , the department of defense has not updated any of its ratings since september 2012 .

further , we studied omb's efforts to help agencies address it projects with cost overruns , schedule delays , and performance shortfalls in june 2013 .

in particular , we reported that omb used cio ratings from the dashboard , among other sources , to select at - risk investments for reviews known as techstats .

omb initiated these reviews in january 2010 to further improve investment performance , and subsequently incorporated the techstat model into its 25-point plan for reforming federal it management .

we reported that omb and selected agencies had held multiple techstat sessions but additional omb oversight was needed to ensure that these meetings were having the appropriate impact on underperforming projects and that resulting cost savings were valid .

among other things , we recommended that omb require agencies to address their highest - risk investments and to report on how they validated the outcomes .

omb generally agreed with our recommendations , and stated that it and the agencies were taking appropriate steps to address them .

as of august 2013 , cios at the eight selected agencies rated 198 of their 244 major investments listed on the dashboard as low risk or moderately low risk .

of the remaining 46 investments , 41 were rated medium risk , and 5 were rated high risk or moderately high risk .

historically , over the life of the dashboard from june 2009 to august 2013 , low or moderately low risk investments accounted for an average of 72 percent of all ratings at the eight agencies , medium risk ratings an average of 22 percent , and high risk or moderately high risk ratings an average of 6 percent .

the cio ratings and associated number of investments at each the eight agencies as reported on the dashboard over the past 4 years are presented in figure 2 .

when comparing the investments' first and most recent ratings , the agencies we reviewed showed generally positive results .

specifically , of the 383 total investments listed on the dashboard from june 2009 to august 2013 for the selected agencies , 121 increased ( meaning risk is lower ) and 81 decreased ( meaning risk is higher ) .

additionally , a significant portion of investments had returned to their original rating ( 74 ) and about one third of investments' ratings had never changed ( 107 ) .

 ( see fig .

3. ) .

when considered individually , five of the eight selected agencies — agriculture , commerce , energy , treasury , and va — had more investments' ratings increase than decrease , whereas the other three — justice , transportation , and ssa — had more investments decrease .

figure 4 presents the net changes in investments' ratings for each agency during the reporting period of june 2009 to august 2013 .

the agencies cited additional oversight or program reviews as factors that contributed to improved ratings .

furthermore , agriculture and commerce both attributed improved ratings to enhanced timeliness and sufficiency of investment reporting .

both of these agencies factor investment reporting into cio ratings and increased ratings as better performance and risk information was provided by investment management .

this suggests that caution should be exercised when interpreting changing risk levels for investments , as rating changes by agencies may not be entirely due to investment performance .

agencies also commented that the cio ratings and dashboard reporting had spurred improved investment management and risk mitigation .

additionally , the total number of investments that the eight agencies reported on the dashboard has varied over time , which impacts the number of investments receiving cio ratings ( see fig .

5 ) .

the variation in the number of investments reported on the dashboard can be attributed , in part , to agencies' ability to add , downgrade , and remove investments throughout the annual federal budget process .

however , as we concluded in september 2011 , omb's guidance also did not ensure complete reporting of it investments by agencies .

specifically , we found that omb's definition of an it investment is broad , and the 10 agencies we evaluated differed on what systems they included as it investments .

for example , 5 of the 10 agencies we reviewed consistently considered investments in research and development systems as it , and 5 did not .

consequently , we recommended that omb clarify its guidance on reporting it investments to specify whether certain types of systems — such as space systems — were to be included .

omb did not agree that further efforts were needed to clarify reporting in regard to the types of systems .

now , 2 years later , and given the continuing lack of clarity in investment reporting guidance , agencies have elected to withdraw investments from the dashboard that are clearly it .

because we have continued to identify inappropriate reclassifications of it investments , we continue to believe this recommendation has merit .

for example , as part of the most recent budget process , energy officials reported several of energy's supercomputer investments as facilities rather than it , thus removing those investments from the dashboard and accounting for a portion of the recent decrease in investments .

energy officials also stated that omb approved this change .

these investments account for $368 million , or almost 25 percent , of energy's fiscal year 2012 it spending , and include the national nuclear security administration's sequoia system , which energy reported as the most powerful computing system in the world as of june 2012 .

according to energy officials , these investments were recategorized because they include both supercomputers and laboratory facilities ( which are not it ) .

as another example , the deputy secretary of commerce issued a directive in september 2012 which resulted in the removal of commerce's satellite investments from the dashboard .

as we found in 2011 , commerce only reported the ground systems of a spacecraft as it investments , and not the technology components on the spacecraft itself .

with this directive , commerce removed the ground - based portions from it investment reporting as well , accounting for $447 million , or 26 percent , of the department's fiscal year 2012 it spending .

in making this decision , commerce determined that it needed to refocus oversight efforts to a more appropriate level and consequently minimized the role of the cio and others in the oversight of satellites .

a commerce official stated that commerce plans to exclude all such investments from the department's fiscal year 2015 it budget submission .

clinger - cohen act of 1996 ( 40 u.s.c .

§ 11101 ( 6 ) ) .

and transmit satellite data .

a staff member from the office of e - government stated that omb could not stop agencies from making such recategorizations and that omb had no control over such agency decisions .

by gathering incomplete information on it investments , omb increases the risk of not fulfilling its oversight responsibilities , of agencies making inefficient and ineffective investment decisions , and of congress and the public being misinformed as to the performance of federal it investments .

as part of the budget process , omb is required to analyze , track , and evaluate the risks and results of all major it investments .

the dashboard gives the public access to the same tools and analysis that the government uses in conducting this performance oversight .

according to omb , dashboard data are intended to provide a near - real - time perspective on the performance of major investments .

http: / / www.itdashboard.gov / faq .

year , omb decreases the utility of the dashboard as a tool for greater it investment oversight and transparency .

of the 80 selected investments at the eight agencies we reviewed , 53 of the cio ratings were consistent with the risks portrayed in the supporting investment performance documents , 20 were partially consistent , and 7 va investments were inconsistent .

those that were partially consistent had few instances of discrepancies during the 12-month period .

for example , both of commerce's inconsistent investments had a discrepancy during 1 of the 12 months which we reviewed .

as previously mentioned , a cio rating should reflect the level of risk facing an investment on a scale from 1 ( high risk ) to 5 ( low risk ) relative to that investment's ability to accomplish its goals .

however , seven of the eight agencies we reviewed had at least one instance wherein the dashboard's cio ratings did not consistently reflect the risks identified in agency documents .

table 4 summarizes our assessment of the 10 investments at each of the selected agencies during the 12-month period from january 2012 through december 2012 and a discussion of the analysis of each agency's documentation follows the table .

agriculture: eight of the 10 investments at agriculture had some inconsistencies with reported risks .

in particular , agriculture experienced technical issues uploading its ratings to the dashboard in early 2012 , which impacted the ratings of six investments .

agriculture officials identified an issue with these investments after observing that submitted changes were not reflected on the dashboard .

they addressed this issue by working with their contractor to determine the cause of the issue , modifying their process to prevent the issue from recurring , and establishing procedures to identify future technical issues .

additionally , agriculture's financial management modernization initiative — one of the six investments that experienced the technical issue described above — and human resources line of business's october ratings should have been lower ( higher risk ) in october 2012 .

officials stated that these cio ratings were updated at the beginning of november 2012 and attributed the delays in the rating submissions to the annual budget process .

additionally , agriculture's cio rated the farm program modernization investment as moderately low risk until november 2012 , despite the investment showing indications of significantly increased risk as early as may 2012 .

for example , the number of high and very high risks increased every month in a series of briefings from may to july 2012 , at which point there was 1 high - impact risk and 2 very high - impact risks each with an estimated 70 percent probability of occurring .

further , in september 2012 a senior management oversight committee determined the investment to be off - track and unable to resolve the issues .

however , the investment remained rated on the dashboard as moderately low risk until november 2012 , when it was updated to medium risk .

as a result , the cio rating did not reflect the results of the oversight committee's review for 3 months .

the investment was ultimately given a high - risk rating in december 2012 .

commerce: two of the 10 selected commerce investments differed from risk levels identified in underlying documentation for 1 month each .

in particular , commerce's census - economic census and surveys investment should have been green rather than yellow in march 2012 , and its national oceanic and atmospheric administration it infrastructure should have been yellow rather than green in june 2012 .

in both cases commerce officials recognized that there were problems and took steps to correct them the following month .

energy: five of the 10 selected energy investments remained listed on the dashboard after the department had recategorized them as investment types that would have no longer been displayed .

specifically , in october 2012 , as part of its annual budget process , energy downgraded its sr mission support systems from major to non - major and , as discussed previously , changed four of the selected supercomputer investments from it to non - it .

an sr mission support systems official explained that energy downgraded the investment after realizing that significant portions of it could be interpreted as non - it under the federal acquisition regulation , and the remainder did not meet the department's threshold for being a major it investment .

further , although energy made these decisions in october 2012 , omb officials explained that they do not publish budgetary data such as these until the release of the president's budget , which did not happen until april 2013 at which time the dashboard reflected energy's changes .

justice: one of the 10 selected justice investments remained on the dashboard after the department recategorized it as an investment type that would have no longer been displayed , similar to energy's situation .

specifically , justice downgraded its law enforcement wireless communications investment from major to non - major as part of its annual budget process in july 2012 , but the dashboard did not reflect the change until april 2013 .

as noted above , omb officials explained that they do not publish budgetary data until the release of the president's budget , which occurred in april 2013 .

transportation: there were no inconsistencies between cio ratings and reported investments' risks .

continuing to report consistent and timely data should help ensure that transportation's dashboard's cio ratings are accurate .

treasury: we did not identify inconsistencies between cio ratings and reported investments' risks .

such attention to reporting consistent and timely data should help ensure the accuracy of the dashboard's cio ratings .

va: seven of the 10 selected investments were never updated during our evaluation period , and 3 were updated once .

va did not update its ratings because it did not have the ability to automatically submit ratings from september 2011 to march 2013 , a period of 19 months .

instead , va elected to build the capability to submit ratings to the dashboard , rather than purchase this capability .

va completed development of this tool in march 2013 , at which point it resumed making the required updates to cio ratings on the dashboard .

for the 3 that va updated during this period , va used a manual budget process to update their ratings in september 2012 .

ssa , information resources management strategic plan: fy 2012-2016 ( may 2012 ) .

and schedule variances , prevent the agency from monitoring year - to - year investment performance , and make it difficult to estimate and understand life - cycle costs .

while this only affected one investment we reviewed , it has the potential to impact ssa's entire it investment portfolio .

as described in our cost estimating and assessment guide , the purpose of such baseline changes should be to restore management's control of the remaining effort by providing a meaningful basis for performance management .

further , these changes should be infrequent , and recurrent changes may indicate that the scope is not well understood or simply that program management is unable to develop realistic estimates.ssa , frequent rebaselining increases the risk that its investments have undetected cost or schedule variances that will impact the success of the associated investment .

while agencies experienced several issues with reporting the risk of their investments , such as technical problems and delayed updates to the dashboard , the cio ratings were mostly or completely consistent with investment risk at seven of the eight selected agencies .

additionally , the agencies had already addressed several of the discrepancies that we identified .

the final agency , va , did not update 7 of its 10 selected investments because it elected to build , rather than buy , the ability to automatically update the dashboard , and has now resumed updating all investments .

to their credit , agencies' continued attention to reporting the risk of their major investments supports the dashboard's goal of providing transparency and oversight of federal it investments .

nevertheless , the rating issues that we identified with performance reporting and annual baselining , some of which are now corrected , serve to highlight the need for agencies' continued attention to the timeliness and accuracy of submitted information , in order to allow the dashboard to continue to fulfill its stated purpose .

we have previously concluded that , consistent with government and industry best practices , including our own guidance on it investment management , agencies' highest - risk investments should receive additional management attention .

in particular , agency leaders should use data - driven reviews as a leadership strategy to drive performance improvement .

correspondingly , omb requires reviews , known as techstat sessions , to enable the federal government to intervene to turnaround , halt , or terminate it projects that are failing or are not producing results .

of the 80 investments we reviewed , there were 8 investments at four of the eight selected agencies that received a red ( high or moderately high risk ) rating in 2012 .

see figure 6 for a list of those at - risk investments and the associated cio ratings .

of the eight investments receiving a red rating in 2012 , the agencies reviewed seven using tools such as techstat sessions,investment review boards , and other high - level reviews .

each of these resulted in action items intended to improve performance .

the final investment , agriculture's human resources line of business: service department center was scheduled to be reviewed using a techstat session , but in august 2013 officials from agriculture's office of the cio stated that this investment was in the process of going through a rebaseline .

as noted earlier , such changes should be infrequent and reinforce the need for agencies to review the highest - risk investments to ensure that root causes of baseline changes are effectively addressed .

each of these agencies we reviewed had similar approaches to identifying and conducting these reviews .

the agencies with red - rated investments in 2012 — agriculture , commerce , justice , and transportation — monitored investment performance , identified troubled investments using a variety of criteria , reviewed investments in need of attention , and assigned and tracked action items .

for example , both agriculture and commerce identified investments for review using the dashboard's cio ratings .

all four agencies further ensured that investments implemented corrective actions resulting from management reviews .

for example , as a result of its november 2012 techstat , agriculture leaders assigned 17 action items to the farm program modernization investment , which were to be completed by january 2013 .

additionally , the four remaining agencies without red - rated investments — energy , treasury , va , and ssa — have documented processes which provide comparable monitoring and oversight capabilities .

for example , va's process relies on missed deliverable dates as the key requirement to hold a review , after which senior management makes a determination as to the future of the investment .

alternatively , energy reviews all major it investments on a quarterly basis and requires those which fall outside expected thresholds to complete corrective actions .

as such , all of the agencies have focused management attention on troubled investments and once identified , have established and followed through on clear action items to turn around or terminate such investments .

since its inception in 2009 , the it dashboard has increased the transparency of the performance of major federal it investments .

its cio ratings , in particular , have improved visibility into changes in the risk level of investments over time , as well as agencies' ability to accomplish their goals .

to that end , over the past 4 years , the agencies we reviewed have reported lower risk for more than one quarter of all their investments .

however , the effectiveness of the dashboard depends on the quality of the information that agencies submit .

we previously recommended that omb clarify guidance on whether certain types of systems should be included in it reporting , but omb did not agree .

agencies' subsequent decisions to remove investments from the dashboard by changing investments' categorizations represent a troubling trend toward decreased transparency and accountability .

thus , we continue to believe that our prior recommendation remains valid and should be implemented .

additionally , omb's annual freeze of the dashboard for as long as 8 months negates one of the primary purposes of this valuable tool — to facilitate transparency and oversight of the government's billions of dollars in it investments .

beyond the transparency they promote , cio ratings present an opportunity to improve the data and processes agencies use to assess investment risk .

each of the agencies we examined had established such processes , and most of the resulting dashboard ratings were consistent with underlying documentation .

while two agencies , transportation and treasury , had ratings that accurately reflected their investments' supporting documentation , other agencies' dashboard ratings were inconsistent with the actual risk of the associated investment .

these inconsistencies — such as inaccurate reflection of negative performance , questionable decisions about the recategorization of investments , and too - frequent changes to performance baselines — highlight the continued need for agencies to populate the dashboard with an accurate representation of the full breadth and health of their investments .

in providing this information , agencies will help the dashboard accomplish its goal of providing transparency and oversight of millions of dollars in federal it investments .

to better ensure that the dashboard provides meaningful ratings and reliable investment data , we are recommending that the director of omb direct the federal cio to make accessible regularly updated portions of the public version of the dashboard ( such as cio ratings ) independent of the annual budget process .

in addition , to better ensure that the dashboard provides accurate ratings , we are making three recommendations to the heads of three of the selected agencies .

specifically , we are recommending that: the secretary of commerce direct the department cio to ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard .

the secretary of energy direct the department cio to ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard .

the commissioner of the ssa direct the cio to revise the agency's investment management processes to ensure that they are consistent with the baselining best practices identified in our published guidance on cost estimating and assessment .

we received comments on a draft of this report from omb and all eight departments and agencies in our review .

omb neither agreed nor disagreed with our recommendation ; agriculture agreed with the report's findings ; commerce disagreed with our recommendation ; energy concurred with our recommendation with one exception related to supercomputers ; justice , treasury , and transportation neither agreed nor disagreed with the report's findings ; va agreed with the report's findings ; and ssa agreed with our recommendation .

each agency's comments are discussed in more detail below .

in written comments , the federal cio neither agreed nor disagreed with our recommendation for omb to make accessible regularly updated portions of the public version of the dashboard ( such as cio ratings ) independent of the annual budget process .

however , omb also noted that the manner in which agencies submit dashboard information to omb makes it difficult to separate materials it believes are predecisional or deliberative .

despite this , omb agreed to explore whether it would be practical to make the dashboard more accessible , and to consider how it could separate predecisional or deliberative materials .

we support omb in its efforts to increase public transparency and accountability .

omb continued to disagree with the 2011 recommendation that we believe continues to have merit , related to the clarity of its guidance on whether certain types of systems should be included in it reporting .

omb believes that the existing guidance is appropriate and does not have plans to review it .

however , as noted earlier in this report , we believe that the recommendation is appropriate because the existing guidance does not address key categories of it investments where we continued to find inconsistencies .

omb also disputed our characterization of two issues .

first , omb noted that up - to - date dashboard information is available to the agencies and omb during the budget development period , who use it as a tool for investment oversight and decision making .

while we acknowledge that omb and federal agencies continue to have access to the dashboard during the budget process , the public display of these data is intended to allow other oversight bodies , including congress and the general public , to hold government agencies accountable for progress and results .

secondly , omb stated that it began using the it dashboard to help identify at - risk investments starting with its launch in june 2009 , rather than 2010 .

we modified the report to reflect omb's statement .

omb also provided technical comments , which we have incorporated in the report as appropriate .

omb's written comments are provided in appendix iii .

in written comments , agriculture's cio stated that the department concurred with the content of the report and had no comments .

agriculture's written comments are provided in appendix iv .

in written comments , commerce agreed with most of the report's findings but disagreed with our recommendation to ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard .

specifically , commerce stated that although it is no longer reporting publicly on its satellite ground system investments through the dashboard , neither the department cio nor omb has relinquished its oversight role .

moreover , commerce stated that it is reviewing its satellites more frequently and in more detail ; as an example , the department noted that commerce's cio conducts monthly dashboard - like assessments that cover the status of the satellite investments .

however , regardless of these additional efforts , the removal of the satellite investments from the dashboard prevents the public display of these data intended to allow omb and other oversight bodies , including congress , to hold the government agencies accountable for progress and results .

additionally , as discussed in this report , commerce's recategorization of its satellite ground system investments is contrary to the definition of it as set forth in the clinger - cohen act .

based on these facts , we continue to believe that our recommendation to commerce that its cio ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard has merit and should be implemented .

commerce's written comments are provided in appendix v. the department also provided technical comments related to our characterization of the department's rating process , which we have incorporated in the report as appropriate .

in written comments , energy concurred , but with a comment regarding our recommendation to ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard .

specifically , while agreeing that energy should ensure that its it investments are appropriately categorized , the department stated that it would continue to report investments that it believes should be categorized and managed on the dashboard , noting that supercomputers would be an exception to this policy .

however , this exception runs contrary to the finding on which this recommendation is based , namely that energy removed $368 million in investments from the dashboard , including a supercomputer reported in 2012 as the most powerful in the world .

while energy agreed to partner with omb to develop an alternate mechanism to track and report supercomputer investments to omb , it is not clear whether this information will be publicly available .

additionally , energy's recategorization of its supercomputer investments is contrary to the definition of it as set forth in the clinger - cohen act , as discussed in this report .

therefore , we disagree that the department's planned actions adequately address existing requirements for open and transparent reporting of major it investments , and we stand by our assessment that energy's cio should ensure that the department's investments are appropriately categorized in accordance with existing statutes and that major it investments are included on the dashboard .

energy's written comments are provided in appendix vi .

in comments provided via e - mail on november 6 , 2013 , a representative of justice's justice management division stated that the department had no comments .

in comments provided via e - mail on november 1 , 2013 , transportation's deputy director of audit relations stated that the department had no comments .

the department also provided technical comments , which we have incorporated in the report as appropriate .

in written comments , the treasury's cio stated that the department had no comments .

treasury's written comments are provided in appendix vii .

in written comments , va's chief of staff stated that the department generally agreed with the findings of the report and provided general comments in which it confirmed that issues identified in the report had been addressed .

specifically , va confirmed that the department has implemented the capability to automatically submit cio ratings to the dashboard and now uses more specific data to generate the cio rating , such as the number of “red flags” associated with an investment or the number of techstat reviews held .

finally , while stating that the department will continue its efforts to improve timely and accurate reporting to the dashboard , va also noted that it is important to recognize that the department manages investment delivery at the project level in comparison to the investment level found in the dashboard .

va's written comments are provided in appendix viii .

in written comments , ssa's deputy chief of staff agreed with our recommendation to revise the agency's investment management processes to ensure that they are consistent with the baselining practices , and discussed planned actions to address it .

ssa's written comments are provided in appendix ix .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to interested congressional committees ; the secretaries of agriculture , commerce , energy , transportation , treasury , veterans affairs , attorney general of the united states , acting commissioner of social security , the director of the office of management and budget ; and other interested parties .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staffs have any questions on matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix x .

our objectives for this engagement were to ( 1 ) characterize the chief information officer ( cio ) ratings for selected federal agencies' information technology ( it ) investments as reported on the dashboard over time , ( 2 ) determine the extent to which selected agencies' cio ratings are consistent with reported investment risk , and ( 3 ) determine the extent to which selected agencies are addressing at - risk investments .

to address our first objective , we selected the eight agencies with the most reported major it spending in fiscal year 2012 , after excluding agencies reviewed in our most recent dashboard report .

the eight agencies are the departments of agriculture ( agriculture ) , commerce ( commerce ) , energy ( energy ) , justice ( justice ) , transportation ( transportation ) , the treasury ( treasury ) , and veterans affairs ( va ) , and the social security administration ( ssa ) .

the results in this report represent only these agencies .

we downloaded and examined the dashboard's cio ratings for all major investments at the eight agencies ( a total of 383 investments reported by these agencies ) .

to characterize the numbers and percentages of major it investments at each risk level at each of our subject agencies , we analyzed , summarized , and — where appropriate — graphically depicted average cio ratings for investments by agencies over time during the period from june 2009 to august 2013 .

specifically , we compared each investment's first and last cio ratings ( including those investments that were not on the dashboard at its inception , and those that were downgraded or eliminated ) and summarized the data by agency .

to describe whether cio ratings indicated higher or lower investment risk over time , we calculated the numbers and percentages of investments ( by agency and collectively for all the agencies ) that maintained a constant rating over the entire performance period , and those that experienced a change to their cio rating in at least one rating period .

then we analyzed the subset of investments that experienced at least one changed rating and compared the first cio rating with the latest cio rating ( no later than august 2013 ) to determine the numbers and percentages of investments ( by agency and collectively for all the agencies ) that experienced a net rating increase , a net rating decrease , or no net change .

we also reviewed investments which had been removed from the dashboard due to recategorization and compared their definitions to the clinger - cohen act's definition of it .

we presented our results to each agency and the office of management and budget ( omb ) and solicited their input , explanations for the results , and additional corroborating documentation , where appropriate .

to accomplish the second objective , we selected the top 10 major investments at each of the eight selected agencies ( for a total of 80 investments ) , with the highest reported it spending in fiscal year 2012 .

we reviewed investment documentation ( such as executive - level briefings , operational analyses , and techstat results ) and interviewed officials from each of the agencies to understand how they rate and monitor investments .

of investment risk and performance ( such as cost and schedule variances ) , which could impact the success of the associated investment .

we then organized the results by month , and compared these results to the relevant investment dashboard cio ratings for calendar year 2012 .

within these artifacts , we identified representations we then evaluated whether each investment's monthly cio rating was consistent with reported investment risks , and categorized each investment by the number of months which were inconsistent .

investments which were consistent with underlying documentation for every month in 2012 were categorized as “consistent,” those which were inconsistent for 1 or more months were categorized as “partially inconsistent,” and those with inconsistencies in each month were “inconsistent.” because we were only evaluating the consistency of dashboard ratings with reported risk , we did not evaluate the accuracy of the investment documentation .

moderately high ) rating on the dashboard during 2012 .

we reviewed the processes used to address these investments and interviewed relevant officials .

we also examined the results of performance reviews and interviewed officials to determine whether the agencies implemented the appropriate risk management processes to oversee and review the selected investments .

we conducted this performance audit from february to december 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

below is the list of the investments that are included in this review , as well as the reported fiscal year 2012 it spending .

in addition to the contact named above , individuals making contributions to this report included dave hinchman ( assistant director ) , rebecca eyler , mike mithani , kevin walsh , and shawn ward .

