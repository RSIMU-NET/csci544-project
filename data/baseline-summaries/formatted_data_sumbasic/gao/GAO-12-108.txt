federally funded science , technology , engineering , and mathematics ( stem ) education programs can serve an important role both by helping to prepare students and teachers for careers in stem fields and by enhancing the nation's global competitiveness .

in this effort , many federal agencies administer stem education programs .

in addition to the federal effort , state and local governments , universities and colleges , and the private sector have also developed programs that provide opportunities for students to pursue stem education and occupations .

nonetheless , research continues to show that the united states lacks a strong pipeline of future workers in stem fields and that u.s. students continue to lag behind students in other highly technological nations in mathematics and science achievement .

over the decades , congress and the executive branch have continued to create new stem education programs , even though , as we reported in 2005 , there has been a general lack of assessment of how well stem programs are working .

a little more than a year after our report was issued , the academic competitiveness council ( acc ) — headed by the department of education — issued a report that outlined areas of potential overlap and recommended areas for better coordination and evaluation of stem education programs .

in this context , we were asked to examine the delivery and effectiveness of stem education programs .

specifically , our objectives were to determine ( 1 ) the number of federal agencies and programs that provided funding for stem education programs in fiscal year 2010 ; ( 2 ) the extent to which these stem programs have similar objectives , serve similar target groups , and provide similar types of services and , if necessary , what opportunities exist to increase coordination ; and ( 3 ) the extent to which federal stem education programs have measured their effectiveness .

to address our objectives , we collected and analyzed information through several methods .

we reviewed relevant federal laws and regulations as well as previous gao work on overlap , duplication , and fragmentation .

we interviewed officials from the office of management and budget ( omb ) and the office of science and technology policy ( ostp ) , and officials from other federal agencies that administer stem education programs .

we reviewed relevant literature and past reports that catalog and assess the federal investment in stem education .

to gather information on federal stem education programs and to assess the level of fragmentation , overlap , and potential duplication , we surveyed over 200 programs across 13 agencies that met our definition of a stem education program , asking questions about program objectives , target populations , services provided , interagency coordination , outcome measures and evaluations , and funding.was administered between may 2011 and august 2011 to federal agency program officials , achieved a 100 percent response rate .

to assess the reliability of data provided in our survey , we incorporated questions about the reliability of the programs' data systems , reviewed documentation for a sample of selected questions , conducted internal reliability checks , and conducted follow - up as necessary .

while we did not verify all responses , we determined that the data used in our report are sufficiently reliable for our purpose .

to gather additional perspectives about federal stem education programs , we attended several stem education conferences .

our web - based survey , which to gather information on program effectiveness , we reviewed evaluations provided by program officials as well as agencies' annual performance plans and reports .

for more information on our scope and methodology , see appendix i .

the stem survey and selected results can be found in gao - 12-110sp , an e - supplement that is a companion to this report .

we conducted this performance audit from february 2011 through january 2012 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in 2005 , we reported that 207 federal stem education programs across 13 different agencies spent $2.8 billion in federal funds in fiscal year 2004 .

we noted that before increasing investment in stem education , it is important to know the extent to which existing stem education programs are appropriately targeted and whether or not they are making the best use of available federal resources .

additionally , information about the effectiveness of these programs could help guide policymakers and program managers .

since then , several other efforts have been conducted to identify federal stem programs and provide recommendations to improve both coordination and program evaluation as well as reduce potential duplication .

for example , in 2006 , acc , led by the department of education , created an inventory and assessed the effectiveness of federal stem programs .

acc recommended further coordination among federal agencies administering stem programs , states , and local school districts .

in addition , acc recommended that agencies adjust program designs and operations so that programs can be assessed and measurable results can be achieved and that funding for federal stem education programs should not be increased unless a plan for rigorous , independent evaluation is in place .

in 2010 , the president's council of advisors on science and technology ( pcast ) , an advisory group of the nation's leading scientists and engineers housed in ostp , published a report in response to the president's request to develop specific recommendations concerning the most important actions that the administration should take to ensure that the united states is a leader in stem education in the coming decades.pcast found that approaches to kindergarten – 12th grade ( k - 12 ) stem education across agencies emerged largely without a coherent vision or careful oversight of goals and outcomes .

pcast also found that relatively little funding was targeted at efforts with the potential to transform stem education , too little attention was paid to replication efforts to disseminate proven programs widely , and too little capacity at key agencies was devoted to strategy and coordination .

our past effort to inventory stem education programs identified a multitude of agencies that administer such programs .

the primary missions of these agencies vary , but most often , they are to promote and enhance an area that is related to a stem field or enhance general education .

see table 1 for relevant agencies and their missions .

as part of this effort , we also identified the role that the national science and technology council ( nstc ) , a component of ostp , plays in coordinating stem education programs .

nstc was established in 1993 and is the principal means for the administration to coordinate science and technology with the federal government's larger research and development effort .

nstc is made up of the vice president , the director of the office of science and technology policy , and officials from other executive branch agencies with significant science and technology responsibilities .

one objective of nstc is to establish clear national goals for federal science and technology investments in areas ranging from information technologies and health research to improving transportation systems and strengthening fundamental research .

nstc is responsible for preparing research and development strategies that are coordinated across federal agencies in order to accomplish these multiple national goals .

stem education programs have been created in two ways — by congress directly in legislation or through agencies' broad statutory authority to carry out their missions .

the higher education opportunity act , the no child left behind act of 2001 , and the national science foundation act of 1950 created programs at the department of education and the national science foundation ( nsf ) — two key agencies that administer many stem education programs .

in addition , since our 2005 review of stem education programs , congress has also passed legislation to examine the overall federal effort to improve stem education .

for example , the deficit reduction act of 2005 established acc .

acc , consisted of officials from the department of education and other federal agencies with responsibility for managing mathematics and science education programs and was mandated to ( 1 ) identify all federal programs with a mathematics or science education focus , ( 2 ) identify the target populations being served by such programs , ( 3 ) determine the effectiveness of such programs , ( 4 ) identify areas of overlap or duplication in such programs , and ( 5 ) recommend processes to integrate and coordinate such programs .

while various pieces of legislation directly created some stem education programs , agencies reported using their broad statutory authority to create many programs as well .

for example , according to agency officials , nsf created 25 of its 37 programs and the department of health and human services ( hhs ) created 40 of its 46 programs in this manner .

more recently , the america competes act ( competes ) , enacted in 2007 , authorized several programs to promote stem education .

december 2010 , congress reauthorized competes .

pub .

l. no .

110-69 , 121 stat .

572 ( 2007 ) .

competes also focused on stem research programs .

reauthorization approved new funding for some stem education programs and made substantive changes to others by reducing certain nonfederal matching requirements .

additionally , it repealed many of the programs that went unfunded following the original competes passage .

the competes reauthorization also sought to address coordination and oversight issues , including those associated with the coordination and potential duplication of federal stem education efforts .

specifically , congress required the director of ostp to establish a committee under nstc to inventory , review , and coordinate federal stem education programs .

congress also directed this nstc committee to specify and prioritize annual and long - term objectives for stem education , and to ensure that federal efforts do not duplicate each other , among other things .

nstc is required to report to congress annually .

beyond stem - specific efforts , the federal government as a whole is seeking to identify programmatic areas that could be better tracked and coordinated .

one such effort revolves around the government performance and results act ( gpra ) modernization act of 2010 .

gpra modernization act established a new framework aimed at taking a more crosscutting and integrated approach to focusing on results and improving government performance .

it requires omb , in coordination with agencies , to develop — at least every 4 years — long - term priority goals , including outcome - oriented goals covering a limited number of crosscutting policy areas .

on an annual basis , omb is to provide information on how these long - term crosscutting goals will be achieved .

this approach could provide a basis for more fully integrating a wide array of federal activities as well as a cohesive perspective on the long - term goals of the federal government .

pub .

l. no .

111-352 , 124 stat .

3866 .

in 2010 , congress directed gao to conduct routine investigations to identify programs , agencies , offices , and initiatives with duplicative goals and activities within departments and governmentwide and report annually to congress .

in march 2011 , gao issued its first annual report in that report , we identified to congress in response to this requirement.81 areas for consideration — 34 areas of fragmentation , overlap , and potential duplication and 47 additional areas — where agencies or congress may wish to consider taking action in an effort to reduce the cost of government operations or enhance revenue collections .

using the framework established in the march 2011 gao report , we examine the extent to which federal stem education programs are fragmented , overlapping , and duplicative .

for the purposes of this report , the key terms are defined as follows: fragmentation occurs when more than one federal agency ( or more than one organization within an agency ) is involved in the same broad area of national need .

overlap occurs when multiple programs offer similar services to similar target groups in similar stem fields to achieve similar objectives .

duplication occurs when multiple programs offer the same services to the same target beneficiaries in the same stem fields .

thirteen agencies administered 209 stem education programs in fiscal year 2010 .

 ( see appendix i for our definition of a stem education program. ) .

agencies reported that they developed the majority ( 130 ) of these programs through their general statutory authority and that congress specifically directed agencies to create 59 of these programs.the number of programs each agency administered ranged from 3 to 46 with three agencies — hhs , the department of energy , and nsf — administering more than half of all programs — 112 of 209 .

figure 1 provides a summary of the number of programs by agency , and appendix ii contains a list of the 209 stem education programs and reported obligations for fiscal year 2010 .

having multiple agencies , with varying expertise , involved in delivering stem education can be advantageous .

one such advantage is that agencies may be better able to tailor programs to suit their specific missions and needs .

for example , energy officials said that their efforts to support students in pursuing a stem course of study are related to energy's mission and work in their labs and can be a way to attract new employees to their workforce .

however , this could also make it challenging to develop a coherent federal approach to educating stem students and creating a workforce with stem skills .

having multiple agencies involved in the delivery of stem education could also make it challenging to identify gaps and allocate resources across the federal government .

agencies obligated over $3 billion to stem education programs in fiscal year 2010 .

individual program obligations ranged from $15,000 to hundreds of millions of dollars .

nsf and the department of education programs account for over half of this funding .

almost a third of the programs had obligations of $1 million or less , with 5 programs having obligations of more than $100 million each .

see figure 2 for program obligation ranges .

agencies carried out other activities that did not fit our definition of a stem education program because stem education was their secondary or tertiary objective , rather than their primary objective .

these efforts include broad - based programs with stem components , programs that enhance the general public's knowledge of stem , and research programs that may hire students .

selected examples of agencies' efforts as reported to us by agency officials include the following: broad - based programs that include stem components several of the department of education's programs have stem components .

for example , title i of the elementary and secondary education act of 1965 , as amended , includes funding for the assessment of math for primary and secondary students , putting a renewed focus on educational attainment in these areas .

in addition , the race to the top fund , a competitive grant program , includes bonus points for states that report they will include in their grant activity , efforts to enhance stem education .

the department of transportation's state maritime academy program supports maritime training and education programs in an effort to improve the quality of the u.s. maritime industry with a secondary objective to encourage students to pursue careers in stem fields that can contribute to the maritime industry .

programs to educate the general public the national institutes of health's ( nih ) science education drug abuse partnership award provides support for the formation of partnerships among scientists and educators , media experts , community leaders , and other interested organizations for the development and evaluation of programs and materials that will enhance knowledge and understanding of science related to drug abuse .

the intended focus is on topics not well addressed in existing efforts by educational , community , or media activities .

research programs that include internships or assistantships energy's national laboratories , most of which are managed by contractors and engage in research activities on behalf of multiple federal agencies , sometimes partner with universities and offer students research opportunities in various disciplines , such as science and technology .

the primary focus of these laboratories is on research and development , which is determined by the funding institution , and there is not always a requirement that they hire students .

when research programs do hire students , this can enhance students' education and interest in stem .

the department of defense has several programs with a primary objective to further research on a specific stem topic .

for example , it has programs that fund university faculty to conduct research on stem topics and who may hire students to assist with research .

the department of homeland security receives funding for technological research in areas that support its mission , and a portion of this may go to student research activities such as hiring a student for the summer or for several weeks to assist with the research .

nonmonetary partnerships with schools or through private partnerships the department of the interior participates in the geoforce program — a precollege program that provides hands - on science learning experiences for middle and high school students ( primarily underserved minorities ) — which is mostly funded by private donations and the university of texas .

the environmental protection agency has a cooperative agreement with the hispanic association of colleges and universities that is intended to increase the diversity of students going into science and technology careers .

the agreement includes activities such as epa staff participation in lectures , conferences , and other events , as well as epa staff members serving as mentors or coaches , among other things .

dedicated funds for education programs nasa's science mission directorate ( smd ) requires each of its missions to fund smd - related education and public outreach using a small percentage of the research and development program costs , but these funds are not specifically for stem education .

as figure 3 illustrates , in fiscal year 2010 , 83 percent of stem education programs overlapped to some degree with another program in that they offered at least one similar service to at least one similar target group in at least one similar stem field to achieve at least one similar objective .

these programs ranged from being narrowly focused on a specific group or field of study to offering a range of services to students and teachers across stem fields .

this complicated patchwork of overlapping programs has largely resulted from federal efforts to both create and expand programs across many agencies in an effort to improve stem education and increase the number of students going into stem fields .

program officials reported that approximately one - third of stem education programs funded in fiscal year 2010 were first funded between 2005 and 2010 .

indeed , the creation of new programs during that time frame may have contributed to overlap and , ultimately , to inefficiencies in how stem programs across the federal government are focused and delivered .

overlap among stem education programs is not new .

in 2007 , acc identified extensive overlap among stem education programs , and , in 2009 , we identified overlap among teacher quality programs , which include several programs focused on stem education .

many programs provided services to similar target groups , such as k - 12 students , postsecondary students , k - 12 teachers , and college faculty and staff .

the vast majority of programs ( 170 ) served postsecondary students .

ninety - five programs served college faculty and staff , 75 programs served k - 12 students , and 70 programs served k - 12 teachers .

in addition , many programs served multiple target groups .

in fact , as figure 4 illustrates , 177 programs were primarily intended to serve two or more target groups .

as figure 5 illustrates , we also found many stem programs providing similar services .

to support students , 167 different programs provided research opportunities , internships , mentorships , or career guidance .

in addition , 144 programs provided short - term experiential learning opportunities and 127 long - term experiential learning opportunities .

short - term experiential learning activities include field trips , guest speakers , workshops , and summer camps .

long - term experiential learning activities last a semester in length or longer .

furthermore , 137 programs provided outreach and recognition to generate student interest , 124 provided classroom instruction , and 75 provided student scholarships or fellowships .

to support teachers , 115 programs provided curriculum development , 83 programs provided teacher in - service , professional development , or retention activities , and 52 programs provided preservice or recruitment activities .

to support stem research , 68 programs reported conducting research to enhance the quality of stem education .

to support institutions , 65 programs provided institutional support to management and administrative activities , and 46 programs provided support for expanding the facilities , classrooms , and other physical infrastructure of institutions .

many programs provided similar services to similar target groups .

for example , 39 programs that listed chemistry as a primary field of focus provided student scholarships or fellowships to postsecondary students .

many of these programs offered scholarships and fellowships to minority , disadvantaged , or underrepresented students across a broad range of stem fields .

specifically , some programs , like nasa's minority university research and education program ( murep ) and the department of commerce's dr. nancy foster scholarship program , offered scholarships , along with a range of other services , to underrepresented and underserved students in overlapping stem fields even though the programs focused on preparing students to work in fields that support the science mission of each agency .

overall , most programs provided an array of services to target groups — 150 programs provided four or more services , while only 16 programs provide one service .

similar stem fields of focus in addition to the serving multiple target groups , most programs also provided services in multiple stem fields .

twenty - three programs targeted one specific stem field , while 121 programs targeted four or more specific stem fields .

in addition , 26 programs indicated not focusing on any specific stem field , they provided services eligible for use in any stem field .

five different stem fields had over 100 programs that provided services .

biological sciences and technology were the most selected stem fields focused on by programs .

agricultural sciences , which was the least commonly selected , still had 27 programs that provided services specifically to that stem field .

while the data show that many programs had similar target groups and similar stem fields of focus , it is also important to compare programs' target groups and stem fields of focus to get a better picture of the potential target beneficiaries that could be served within a given stem discipline .

for example , both the national environmental satellite , data , and information service ( nesdis ) education and the graduate automotive technology education program provided scholarships or fellowships to postsecondary students , but one focused on students in earth , atmospheric , and ocean sciences programs , and one on students in engineering , specifically in the areas of hybrid propulsion systems , fuel cells , biofuels , energy storage systems , lightweight materials , and advanced computation ; therefore , the target beneficiaries served by these programs are quite different .

nevertheless , 72 programs provided services to postsecondary students in physics .

as table 2 illustrates , many programs offered services to similar target groups in similar stem fields of focus .

overlapping programs can lead to individuals and institutions being eligible for similar services in similar stem fields offered through multiple programs and , without information sharing , could lead to the same service being provided to the same individual or institution .

many stem education programs had similar objectives .

the vast majority ( 87 percent ) of stem education programs indicated that attracting and preparing students throughout their academic careers in stem areas was a primary objective .

in addition to attracting and preparing students throughout their academic careers in stem areas , officials also indicated the following primary program objectives: improving teacher education in stem areas ( teacher development ) — 26 percent , improving or expanding the capacity of k - 12 schools or postsecondary institutions to promote or foster education in stem fields ( institution capacity building ) — 24 percent , and conducting research to enhance the quality of stem education provided to students ( stem education research ) — 18 percent .

many programs also reported having multiple primary objectives .

while 107 programs focused solely on student education , 82 others indicated having multiple primary objectives , and 9 programs reported having 4 or more primary objectives .

few programs reported focusing solely on teacher development , institution capacity building , or stem education research .

most of these objectives were part of a larger program that also focused on attracting and preparing students in stem education .

however , even when programs overlapped , the services they provided and the populations they served may differ in meaningful ways and would therefore not necessarily be duplicative: there may be important differences between the specific field ( s ) of focus and the program's stated goals .

for example , both commerce's national estuarine research reserve system education program and the nuclear regulatory commission's integrated university program provided scholarships or fellowships to doctoral students in the field of physics .

however , the national estuarine research reserve system education program's goal was to increase environmental literacy related to estuaries and coastal watersheds by providing students with an opportunity to conduct research of local and national significance that focuses on enhancing coastal zone management ; while the integrated university program focused on supporting education in nuclear science , engineering , and related fields with the goal of developing a workforce capable of designing , constructing , operating , and regulating nuclear facilities and capable of handling nuclear materials safely .

programs may be primarily intended to serve different specific populations within a given target group .

for example , 65 programs were primarily intended to serve minority , disadvantaged , or underrepresented groups and 10 programs limited their services to students or teachers in specific geographic areas.programs providing services to k - 12 students in the field of technology , 10 were primarily intended to serve specific underrepresented , minority , or disadvantaged groups , and 2 were limited geographically to individual cities or universities .

indeed , of the 34 furthermore , individuals may receive assistance from different programs at different points throughout their academic careers that provide services that complement or build upon each other , simultaneously supporting a common goal rather than serving cross purposes .

despite past recommendations from acc and others to improve coordination among stem education programs , efforts to coordinate stem education programs across the government remain limited .

although 83 percent of stem education programs overlapped to some degree with at least one other program , only 33 percent of programs reported coordinating with other agencies that provide similar stem education services to similar program beneficiaries , not including basic governmentwide inventory efforts .

some program officials mentioned that they coordinate by employing informal mechanisms for information sharing such as conversations and meetings between program staff , sharing resources or best practices , and participating in conferences with other agency officials .

other efforts included developing memorandums of understanding , issuing joint guidance , cofunding programs , and establishing interagency working groups focused on specific science subjects or providing a specific service to a specific target group .

with the growing concern for improved federal coordination and planning in stem education , congress passed the america competes reauthorization act of 2010 , which requires the director of ostp to establish a committee under nstc to coordinate stem education activities and programs among respective federal agencies and omb .

the nstc committee on science , technology , engineering , and math education ( costem ) , comprised of representatives from 11 different federal agencies , convened its first meeting in march 2011 .

the statute requires nstc to develop a 5-year governmentwide stem education strategic plan and identify areas of duplication among federal programs .

costem provides nstc with an opportunity to improve coordination and be more strategic with the federal investment in stem education .

best practices in interagency collaboration include developing ongoing mechanisms and processes to monitor , measure , and report agency progress toward nstc's strategic planning goals and making the results publicly available to improve accountability .

according to ostp officials , a description of the 5-year strategic plan should be publicly available in early 2012 ; however , as called for in its charter , the committee will terminate no later than march 31 , 2015 , before the first 5-year plan is carried out , unless it is renewed by the director of ostp .

pursuant to requirements under the 2010 reauthorization of the competes act , nstc has implemented several initiatives to enhance coordination .

in december 2011 , costem published a report on the inventory of the federal stem education portfolio that , according to ostp officials , will be used to improve coordination and inform the strategic planning process .

specifically , ostp officials said the inventory will allow agencies to identify similar programs and share information and best practices .

without proper coordination , overlapping programs may not share information about the results of the actions taken or research conducted with other interested agencies , possibly leading to numerous programs providing assistance to address the same issue or area of research .

to the extent that costem identifies duplicative programs , it will be important that it considers the trade - offs associated with program consolidation and assist agencies in determining the most effective and efficient way to reduce duplication .

cost savings might be achieved through the consolidation of duplicative program administrative structures .

however , our past work has shown that program consolidation can be more expensive in the short term , and , in the long term , cost savings could be diminished if the workload associated with certain administrative activities remains the same , such as reviewing and assessing applications , providing technical assistance , and monitoring program recipients .

that reported on administrative costs estimated having administrative costs lower than 10 percent of their total program costs .

last , the consolidation of some programs may require congressional action because some programs may be statutorily mandated .

gao - 11-318p .

program officials varied in their ability to provide reliable information on the number of students , teachers , or institutions directly served by their programs — which is a type of output measure .

for example , among programs in our review that served postsecondary teachers and students in 2010 , about one - fifth of them did not know the number served .

however , depending on the service delivery structure of the program , it may be more difficult to track this number .

in some cases , the program's agency did not maintain databases or contracts that would track the number of students served by the program .

in other cases , programs may not have been able to provide information on the numbers of institutions they served because they provided grants to secondary recipients .

for example , one program indicated that it gives grants to institutions to provide internships or scholarships but that funding goes directly to students , so it does not have information about the number of institutions served .

programs that provide informal educational activities or online services also reported difficulty in tracking the number of individuals who benefited from their programs .

the validity and accuracy of the reported output data for some of these programs may be questionable and may hinder program planning and assessment .

programs that reported the numbers they served used varied approaches to collect this information , including annual reports from grant recipients , student enrollment counts , estimates of the expected number of participants reached , and reviews of funding proposals .

some programs had third parties track the numbers served , but did not always take steps to independently verify the data or review the process for how the information was collected .

further , the inconsistent collection of output measures across programs makes it challenging to aggregate the number of students , teachers , and institutions served and to assess the effectiveness of the overall federal effort .

output data are an important component to understanding whether programs are likely to meet their goals .

for example , if a k - 12 program has the goal of increasing the number of undergraduates pursuing coursework in stem fields , it is important to know how many k - 12 students were in the program .

without such data , it would be challenging to assess the intended outcome of the program — for example , the number of students who actually went on to pursue such coursework .

agencies in our review did not use outcome measures in a way that is clearly reflected in their performance plans and performance reports — publicly available documents they use for performance planning .

this may hinder decisionmakers' ability to assess how agencies' stem efforts contribute to agencywide performance goals and the overall federal stem effort .

in our review of fiscal year 2010 annual performance plans and reports of the 13 agencies with stem programs , we found that most agencies did not connect stem education activities to agency goals or measure and report on the progress of those activities.documents typically lay out agency performance goals that establish the level of performance to be achieved by program activities during a given fiscal year , the measures developed to track progress , and what progress has been made toward meeting those performance goals .

as figure 6 illustrates , in our review of agencies' specific references to their overall stem education initiatives , although 38 percent of agencies mentioned stem education in their performance plans and 62 percent in their performance reports , fewer cited outcome measures related to stem education .

more specifically , in reporting on their progress toward meeting their performance goals , 46 percent of the agencies mentioned stem education as contributing to one of these goals in their performance reports .

moreover , agencies that spent the most on stem education were not necessarily more likely to mention , connect to agency performance goals , or measure and report on progress of their stem efforts .

for instance , nasa , which administered 9 stem programs and obligations of about $209.6 million in fiscal year 2010 , mentioned its overall stem education efforts and connected them to agency performance goals in its planning documents and measured and reported on progress in both its performance plan and report .

on the other hand , hhs's national institutes of health , which administered the most stem education programs ( 44 ) and obligations of about $573.6 million , referred to agency performance goals and outcome measures of its stem education efforts only in some of its institutes' performance reports , but not in its nih - wide performance plan .

as figure 7 illustrates , in our review of agencies' specific references to their stem education programs , while the 13 agencies combined mentioned 38 percent of their programs in their performance plans , they connected 19 percent of their stem education programs to agency performance goals and measured and reported on progress of 9 percent of the programs .

agencies' stem education obligations and number of programs did not correlate directly with their likelihood of connecting the programs to agency performance goals or measuring and reporting on their progress in performance plans and reports .

for example , interior , through the u.s. geological survey , which administered just 3 stem education programs in fiscal year 2010 , mentioned all of its programs in its performance plan .

in contrast , nsf , which administered 37 stem education programs and obligated about $1.1 billion in fiscal year 2010 , connected only 2 of its programs to agency performance goals while measuring and reporting on progress in its performance plan and report .

the gpra modernization act of 2010 and the america competes reauthorization act of 2010 afford agencies the opportunity to better utilize performance measures for both governmentwide and agency - specific stem education efforts .

for example , the gpra modernization act will require agencies to identify program activities and other activities , which may include stem education activities that contribute to each performance goal .

it recognizes the importance of governmentwide performance goals as it requires omb to develop , in coordination with agencies , long - term , crosscutting federal government priority goals that are to be updated or revised every 4 years , which will be tracked quarterly in order to review progress to improve government performance .

according to omb guidance , it will announce interim federal government priority goals in february 2012 and finalize its goals in february 2014 .

the america competes reauthorization act of 2010 also focuses on accountability through strategic planning , and has specific requirements for agencies with stem programs .

specifically , it requires nstc to develop a stem education strategic plan with long - term objectives , metrics to assess agencies' progress , and approaches taken by participating agencies to assess the effectiveness of their stem programs and activities .

however , while ostp will be required to report on agencies' annual progress toward the long - term objectives , an ostp official said there is no mechanism to make agencies align their performance measures with the goals and objectives in the strategic plan .

little is known about the effectiveness and performance of stem education programs because the majority of them ( 66 percent ) have not conducted an evaluation of their entire program since 2005 ( as figure 8 illustrates ) .

we define “evaluation” as an individual systematic study conducted periodically or on an ad hoc basis to assess how well a program is working , typically relative to its program objectives .

some programs that reported that they did not complete an evaluation reported they had their grantees complete one ; however , in those cases , few programs used these grantee evaluations to inform a more comprehensive evaluation of the entire program that they or an external evaluator completed .

in total , since 2005 , agencies conducting 61 programs , ( representing about 61 percent of the $3.1 billion obligated in fiscal year 2010 ) responded that they had completed evaluations — all of which used a variety of methods and designs .

we reviewed evaluations for 35 of the 61 programs .

most of the 35 program evaluations we reviewed used methods and designs that appropriately assessed how well they met their stated objectives .

for instance , one evaluation selected a random sample of its former program participants and compared them with a sample of students who had applied to the same program , but had not participated .

while former participants had some statistically significant academic outcomes when compared with the nonparticipants , the evaluation also noted other factors that may have influenced the favorable outcomes of the program — for example , that participants , on average , were more interested in careers in science and math than the nonparticipants , so the true effects of program participation may be overstated .

even though most of the 35 programs we reviewed employed appropriate methods and designs to assess their programs' effectiveness , we identified several ways to improve evaluations of stem education , based on our review .

improved survey response rates: many of the evaluations we reviewed had low response rates .

without better response rates , generalizations from the results may be limited .

better alignment of the methods with other components of the evaluation: specifically , 10 of the programs used evaluation methods that were not fully aligned with the evaluation questions and the program context .

for example , 3 of these evaluations had data limitations , thus hindering the use of methods that could collect the full range of data to inform program outcomes .

robust use of criteria to measure outcomes: among the 27 programs that measured outcomes , 9 did not evaluate them against any criteria .

without criteria to evaluate the outcomes , it may be difficult to establish programmatic impact and assess performance and effectiveness .

furthermore , in order to influence program practice , the evaluation results must be disseminated widely .

while nearly all of the stem education programs that reported completing an evaluation reported using different mechanisms to disseminate results , they did not always share results in a way that facilitated knowledge sharing .

program officials reported that the most common means of dissemination of their results were through their websites or at conferences or forums , which , according to a 2006 nstc report , were methods that require practitioners to actively seek out results , so such methods may prevent the results of the research from being conveyed to them .

however , these mechanisms have limits .

for example , nstc also reported that stem education research results may not be conveyed to practitioners because the results often lack applicability , some are ambiguous , and the culture of teaching typically does not make decisions based on research findings .

nstc identified other issues with sharing information about stem education program results and suggested several actions that agencies could take to improve dissemination , such as engaging practitioners to collaborate with researchers in setting research agendas .

according to nstc officials , most agencies do not share or disseminate evaluations in a way that could be useful for coordination .

although the federal government invests billions of dollars annually in stem education programs , there remains concerns over u.s. economic and educational competitiveness , particularly with regard to the national educational system's ability to produce citizens literate in stem subjects and to produce future scientists , technologists , engineers , and mathematicians .

prior reports on stem education highlighted the lack of federal governmentwide planning and coordination .

recently , both congress and the administration called for a more strategic and effective approach to the federal government's investment in stem education .

the america competes reauthorization act of 2010 requires the director of ostp to establish a committee under nstc to develop a 5-year strategic plan and submit annual reports , including a description of the plan , to congress .

the plan is expected to include common measures to assess progress toward the plan's goals .

in addition , the gpra modernization act of 2010 requires agencies to identify program activities that contribute to each performance goal , and , as agencies implement this provision , more information about stem education efforts in performance plans and reports can be expected .

nstc's ongoing strategic planning efforts provide an opportunity to develop guidance on how to incorporate stem - and program - specific education goals and measures in agencies' performance planning and reporting process and align their stem education efforts with a governmentwide stem education strategy .

to further strengthen strategic planning and coordination efforts , an accountability and reporting framework should exist to ensure agencies are adhering to nstc's strategic plan .

while the stem education programs we reviewed in this report are fragmented and overlapping to some degree , they are not necessarily duplicative of one another .

more analysis is needed to identify areas of duplication among federal stem education programs and ensure that the federal investment in these programs advances nstc's 5-year strategic plan that is under development .

in this era of budget constraints , governmentwide strategic planning can play a critical role in addressing concerns about program fragmentation , overlap and duplication .

fragmentation and overlap can ( 1 ) frustrate federal officials' efforts to administer programs in a comprehensive manner , ( 2 ) limit the ability to determine which programs are most cost - effective , and ( 3 ) ultimately increase program administrative costs .

therefore , if nstc's 5-year strategic plan is not developed in a way that aligns agencies' efforts to achieve governmentwide goals , enhances the federal government's ability to assess what works , and concentrates resources on those programs that advance the strategy , the federal government may spend limited funds in an inefficient and ineffective manner that does not best help to improve the nation's global competitiveness .

understanding program performance and effectiveness is also key in determining where to strategically invest limited federal funds to achieve the greatest impact in developing a pipeline of future workers in stem fields .

programs need to be appropriately evaluated to determine what is working and how improvements can be made .

however , most agencies have not conducted comprehensive evaluations since 2005 to assess the effectiveness of their stem education programs .

furthermore , methods for dissemination of program evaluations , especially to practitioners , could be improved .

agency and program officials would benefit from guidance and information sharing within and across agencies about what is working and how to best evaluate programs .

this could not only help to improve individual program performance , but also inform agency and governmentwide decisions about which programs should continue to be funded .

without an understanding of what is working in some programs , it will be difficult to develop a clear strategy for how to spend limited federal funds .

the director of ostp should direct nstc to 1 .

develop guidance for how agencies can better incorporate each agency's stem education efforts and the goals from nstc's 5-year stem education strategic plan into each agency's own performance plans and reports .

2 .

develop a framework for how agencies will be monitored to ensure that they are collecting and reporting on nstc strategic plan goals .

this framework should include alternatives for a sustained focus on monitoring coordination of stem programs if the nstc committee on stem terminates in 2015 as called for in its charter .

3 .

work with agencies , through its strategic planning process , to identify programs that might be candidates for consolidation or elimination .

specifically , this could be achieved through an analysis that includes information on program overlap , similar to the analysis conducted by gao in this report , and information on program effectiveness .

as part of this effort , ostp should work with agency officials to identify and report any changes in statutory authority necessary to execute each specific program consolidation identified by nstc's strategic plan .

4 .

develop guidance to help agencies determine the types of evaluations that may be feasible and appropriate for different types of stem education programs and develop a mechanism for sharing this information across agencies .

this could include guidance and sharing of information that outlines practices for evaluating similar types of programs .

we provided a draft of this report to the office of science and technology policy ( ostp ) and the office of management and budget ( omb ) for review and comment .

ostp provided technical comments that we incorporated as appropriate .

omb had no concerns with the report .

as we agreed with your office , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

we are sending copies of this report to relevant congressional committees , ostp , omb , and other interested parties .

in addition , this report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-7215 or scottg@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iv .

the objectives of our report were to determine ( 1 ) the number of federal agencies and programs that provided funding for science , technology , engineering , and mathematics ( stem ) education programs in fiscal year 2010 ; ( 2 ) the extent to which stem programs have similar objectives , serve similar target groups , provide similar types of services , and , if necessary , what opportunities exist to increase coordination ; and ( 3 ) the extent to which stem programs have measured their effectiveness .

to inform all of our objectives , we reviewed relevant federal laws and regulations .

we also reviewed previous work that was conducted to catalog and assess the federal investment in stem education programs , including a 2005 gao study , the 2007 academic competitiveness council ( acc ) report , and the 2010 office of management and budget ( omb ) inventory .

we reviewed relevant literature and past reports on stem education , including the 2010 president's council of advisors on science and technology ( pcast ) report entitled report to the president: prepare and inspire: k - 12 education in science , technology , engineering , and math ( stem ) for america's future and the national academies press report entitled rising above the gathering storm: energizing and employing america for a brighter economic future: committee on prospering in the global economy of the 21st century: an agenda for american science and technology .

in addition , we interviewed officials from omb , the office of science and technology policy ( ostp ) , and 13 other federal agencies that administer stem education programs to gather information on their stem education efforts , the extent of coordination between programs , and the existence of program evaluations .

we attended several stem education conferences to gather additional perspectives about federal stem education programs .

finally , we reviewed evaluations provided by program officials as well as agencies' annual performance plans and reports .

to gather information on federal stem education programs and to assess the level of fragmentation , overlap , and potential duplication among them , we first reviewed past gao work on assessing the level of fragmentation , overlap , and duplication among other groups of federal programs .

next , we surveyed over 200 programs across 13 agencies that met our definition of a stem education program ( see below ) with questions about program objectives , target populations , services provided , interagency coordination , outcome measures and evaluations , and funding information .

in december 2011 , nstc's committee on stem education released its inventory of the federal stem education portfolio .

the nstc inventory differs from gao's survey in that it counts investments and allocations , whereas gao asked agencies to report on programs and obligations .

for the purposes of our study , we defined a federally funded stem education program as a program funded in fiscal year 2010 by congressional appropriation or allocation that includes one or more of the following as a primary objective: attract or prepare students to pursue classes or coursework in stem areas through formal or informal education activities ( informal education programs provide support for activities provided by a variety of organizations that offer students learning opportunities outside of formal schooling through contests , science fairs , summer programs , and other means ; outreach programs targeted to the general public should not be included ) , attract students to pursue degrees ( 2-year , 4-year , graduate , or doctoral degrees ) in stem fields through formal or informal education activities , provide training opportunities for undergraduate or graduate students in stem fields ( this can include grants , fellowships , internships , and traineeships that are targeted to students ; general research grants that are targeted to researchers that may hire a student to work in the lab should not be considered a stem education program ) , attract graduates to pursue careers in stem fields , improve teacher ( preservice or in - service ) education in stem areas , improve or expand the capacity of k - 12 schools or postsecondary institutions to promote or foster education in stem fields , or conduct research to enhance the quality of stem education programs provided to students .

in addition , we defined stem education programs to include grants , fellowships , internships , and traineeships .

while programs designed to retain current employees in stem fields were not included , programs that fund retraining of workers to pursue a degree in a stem field were included because these programs help increase the number of students and professionals in stem fields by helping retrain non - stem workers to work in stem fields .

for the purposes of this study , we defined the term “program” as an organized set of activities supported by a congressional appropriation or allocation .

further , we defined a program as a single program even when its funds were allocated to other programs as well .

we asked agency officials to provide a list of programs that received funds in fiscal year 2010 .

this included programs that received one - time , limited funds in fiscal year 2010 , such as earmarks .

we determined that a stem field should be considered any of the following broad disciplines: earth , atmospheric , and ocean sciences ; social sciences ( eg , psychology , sociology , anthropology , cognitive science , economics , behavioral sciences ) ; or technology .

in addition , we determined that our definition of stem education would include health care programs that train students for careers that are primarily in scientific research .

we did not , however , include health care programs that train students for careers that are primarily in patient care , that is , those that trained nurses , doctors , dentists , psychologists , or veterinarians .

to identify federally funded stem education programs , first we developed a combined list of programs based on the findings of three previous stem education inventory efforts completed by gao in 2005 , acc in 2007 , and omb in 2010 .

second , we shared our list with agency officials , provided our definition of stem education program , and asked officials to make an initial determination about which programs should remain on the list and which programs should be added to the list .

if agency officials indicated they wanted to remove a program from our list , we asked for additional information .

for example , programs on our initial list may have been terminated or consolidated , or did not receive federal funds in fiscal year 2010 .

in addition , we asked officials to provide program descriptions , program names , and contact information .

next , we reviewed each agency's submission and individual program information and determination .

we also gathered additional information on the program , mainly through agency websites and program materials , and held discussions with program officials to understand the program in more detail .

on the basis of this additional information , we excluded programs that we found did not meet our definition of a stem education program .

once our determinations were made , we asked each agency to confirm the list of programs and the names and contact information for the officials who would be responsible for completing the survey .

in total , we determined that 274 programs should receive a survey .

we also included several screening questions in the survey to provide an additional verification to ensure the programs met our definition of a stem education program .

nineteen programs did not pass our screening questions and therefore were excluded from our analysis .

all in all , 209 programs were included in our final analysis .

for a list of the 209 stem education programs by agency , see appendix ii .

for a summary of excluded programs and their exclusion rationales , see table 3 .

furthermore , we provide aggregate survey responses from these programs in an e - supplement ( gao - 12-110sp ) .

we developed a web - based survey to collect information on federal stem education programs .

see gao - 12-110sp for a copy of the survey's full text .

the survey included questions on program objectives , target groups served , services provided , academic fields of focus , output metrics , outcome measures , obligations , and program evaluations .

to minimize errors arising from differences in how questions might be interpreted and to reduce variability in responses that should be qualitatively the same , we conducted pretests with 14 different programs in march and april 2011 .

to ensure that we obtained a variety of perspectives on our survey , we selected 14 programs from 11 different agencies that differed in program scope , objectives , services provided , target groups served , evaluations completed , and funding sources .

we included budget staff as well as program officials in the pretests to ensure budget - related terms in the survey were understandable and available .

an independent gao reviewer also reviewed a draft of the survey prior to its administration .

on the basis of feedback from these pretests and independent review , we revised the survey in order to improve its clarity .

after completing the pretests , we administered the survey .

on may 3 , 2011 , we sent an e - mail announcement of the survey to the officials responsible for the programs selected for our review , notifying them that our online survey would be activated within a week .

on may 11 , 2011 , we sent a second e - mail message to officials that informed them that the survey was available online .

in that e - mail message , we also provided them with unique passwords and usernames .

we made telephone calls to officials and sent them follow - up e - mail messages , as necessary , to clarify their responses or obtain additional information .

we received completed surveys from 269 programs , for a 100 percent response rate .

we collected survey responses through august 31 , 2011 .

we used standard descriptive statistics to analyze responses to the survey .

because this was not a sample survey , there were no sampling errors .

to minimize other types of errors , commonly referred to as nonsampling errors , and to enhance data quality , we employed survey design practices in the development of the survey and in the collection , processing , and analysis of the survey data .

for instance , as previously mentioned , we pretested the survey with federal officials to minimize errors arising from differences in how questions might be interpreted and to reduce variability in responses that should be qualitatively the same .

we further reviewed the survey to ensure the ordering of survey sections was appropriate and that the questions within each section were clearly stated and easy to comprehend .

to reduce nonresponse bias , another source of nonsampling error , we sent out e - mail reminder messages to encourage officials to complete the survey .

in reviewing the survey data , we performed automated checks to identify inappropriate answers .

we further reviewed the data for missing or ambiguous responses and followed up with agency officials when necessary to clarify their responses .

to assess output measures , we asked a series of questions to assess the agency's procedures , policies , and internal controls to ensure the quality of data provided in the survey .

for program obligations questions , we sampled 10 percent of responses reviewing documentary evidence to corroborate survey responses .

for evaluation questions , we reviewed program evaluations provided to corroborate survey responses .

to assess the reliability of data provided in our survey , we incorporated questions about the reliability of the programs' data systems , reviewed documentation for a sample of selected questions , conducted internal reliability checks , and conducted follow - up as necessary .

while we did not verify all responses , on the basis of our application of recognized survey design practices and follow - up procedures , we determined that the data used in this report were of sufficient quality for our purposes .

we did not report on data that we found of questionable reliability based on our review of data reliability questions in the survey — such as the number of students and teachers served .

all data analysis programs were also independently verified by a gao data analyst for accuracy .

program officials who responded on their survey that an evaluation of their program had been completed in 2005 or later provided us with information about their most recent evaluations .

gao defines “evaluation” as an individual systematic study conducted periodically or on an ad hoc basis to assess how well a program is working .

studies are often conducted by experts external to the program , inside or outside the agency , as well as by program managers .

furthermore , an evaluation typically examines achievement of program objectives in the context of other aspects of program performance or in the context in which it occurs .

after ensuring that the evaluations met this definition , we reviewed them to analyze their characteristics , including their methods and designs , and the extent to which program outcomes were measured .

in addition , we examined whether the methods and designs were appropriate given the evaluation questions and program context .

in total , 61 programs responded that they had completed a program evaluation since 2005 , and we reviewed evaluations from 35 of those programs .

because we requested that officials provide us with a citation for the most recent evaluation , we selected the most recent one for our review .

we did not review evaluations from the remaining 26 programs for a variety of reasons .

specifically , they were committee of visitors reports , and other types of reports that did not have evaluation information that aligned with the criteria by which we analyzed the other evaluations .

among these reports , we were unable to obtain 6 of them .

as a result , we were unable to analyze them and determine whether they met gao's definition of evaluation .

for more details about the evaluations in our review , see appendix iii .

we reviewed agencies' fiscal year 2010 required strategic planning documents — performance plans and performance reports — to determine the extent to which they incorporated program - specific and broad - based stem goals and objectives .

the performance plans and reports were done at the agency level , while others were done at other levels , such as the institute or office level — in which case we reviewed the documents that covered the particular stem program ( s ) in our review .

when reviewing these documents , we determined the extent to which agencies made any reference to agencywide stem initiatives or particular stem education programs , in general , but not in the context of agency goals or of outcome measures ; agencies connected their stem initiatives or their individual stem programs to agency goals ; and agencies articulated outcome measures of their stem initiatives or of individual stem programs .

exploration systems directorate - stem education activities minority university research and education program nasa informal education opportunities ( nieo ) .

advanced technological education ( ate ) alliances for graduate education and the professoriate ( agep ) broadening participation in computing ( bpc ) centers for ocean science education excellence cise pathways to revitalized undergraduate computing education ( cpath ) cyberinfrastructure training , education , advancement , and mentoring for our 21st century workforce ( ci - team ) discovery research k - 12 ( dr - k12 ) east asia & pacific summer institutes for u.s. graduate students ( eapsi ) engineering education ( ee ) enhancing the mathematical sciences workforce in the 21st century ( emsw21 ) ethics education in science & engineering ( eese ) federal cyber service: scholarship for service ( sfs ) geoscience teacher training ( geo - teach ) global learning and observations to benefit the environment ( globe ) graduate research fellowship program ( grfp ) graduate stem fellows in k - 12 education program ( gk - 12 ) historically black colleges and universities undergraduate program ( hbcu - up ) informal science education ( ise ) integrative graduate education and research traineeship ( igert ) program interdisciplinary training for undergraduates in biological and mathematical sciences ( ubm ) international research experiences for students ( ires ) .

program louis stokes alliances for minority participation ( lsamp ) nanotechnology undergraduate education in engineering opportunities for enhancing diversity in the geosciences research and evaluation on education in science and engineering ( reese ) research experiences for teachers ( ret ) in engineering and computer science research experiences for undergraduates ( reu ) research in disabilities education ( rde ) research on gender in science and engineering ( gse ) robert noyce teacher scholarship program science , technology , engineering , and mathematics talent expansion program ( step ) transforming undergrad education in stem ( tues ) tribal colleges and universities program ( tcup ) undergraduate research and mentoring in the biological sciences ( urm ) .

minority serving institutions program ( msip ) .

animal and plant health inspection service ( aphis ) national institute of food and agriculture ( nifa ) .

national institute of standards and technology ( nist ) .

awards to stimulate and support undergraduate research experience ( assure ) .

army educational outreach program ( aeop ) consortium research fellows program ( crfp ) national science center ( nsc ) .

autonomous robotic manipulation ( arm ) computer science in science , technology , engineering , and mathematics education ( cs - stem ) .

national defense education program ( ndep ) k - 12 national defense education program ( ndep ) science , mathematics and research for transformation ( smart ) .

uniformed services university of the health sciences ( usuhs ) historically black college and universities / minority institutions research education partnership science and engineering apprentice program ( seap ) the naval research enterprise intern program ( nreip ) university / laboratory initiative ( uli ) .

developing hispanic - serving institutions: stem and articulation programs ( mandatory ) .

academies creating teacher scientists ( doe acts ) .

hbcu mathematics , science & technology , engineering and research workforce development program minority university research associates program ( mura ) national undergraduate fellowship program in plasma physics and fusion energy sciences office of science graduate fellowship ( scgf ) program pan american advanced studies institute summer applied geophysical experience ( sage ) .

bridges to the baccalaureate program ccr / jhu master of science in biotechnology concentration in molecular targets and drug discovery technologies community college summer enrichment program education programs for population research ( r25 ) .

initiative for maximizing student development material development for environmental health curriculum national cancer institute cancer education and career development program ncrr science education partnership award ( sepa ) nhlbi minority undergraduate biomedical education program nih summer research experience programs ninds diversity research education grants in neuroscience nlm institutional grants for research training in biomedical informatics office of science education k - 12 program post - baccalaureate intramural research training award program postbaccalaureate research education program ( prep ) recovery act limited competition: nih challenge grants in health and science research research scientist award for minority institutions research supplements to promote diversity in health - related research rise ( research initiative for scientific enhancement ) ruth l. kirschstein national research service award institutional research training grants** ( t32 , t35 ) .

u.s. geological survey ( usgs ) edmap component of the national cooperative geologic mapping program national association of geoscience teachers ( nagt ) - usgs cooperative summer field training program student intern in support of native american relations ( sisnar ) .

federal aviation administration ( faa ) national center of excellence for aviation operations research ( nextor ) .

different types of evaluation designs can provide rigorous evidence of effectiveness if designed well and implemented with a thorough understanding of their vulnerability to potential sources of bias .

there are four main types of evaluations that gao has identified: implementation evaluations ( which assess the extent to which the program is operating as intended ) , impact evaluations ( which include experimental and quasi - experimental designs ) , outcome evaluations ( which assess the extent to which a program achieves its objectives ) , and cost - benefit and cost - effectiveness analyses ( which assess a program's outputs or outcomes with the costs to produce them ) .

deciding which evaluation type to use involves a variety of different considerations , as no one evaluation is suitable for all programs .

for instance , as we have previously reported , an impact evaluation is more likely to provide useful information about what works when the intervention consists of clearly defined activities and goals and has been well implemented.experimental comparison group design — which compares outcomes for program participants with those of a similar group not in the program , is used in instances when random assignment to the participant and nonparticipant groups is not possible , ethical , or practical .

it is most successful in providing credible estimates of program effectiveness when the groups are formed in parallel ways and are not based on self - selection .

on the other hand , case studies are recommended for assessing the effectiveness of complex interventions in limited circumstances when assessing comprehensive reforms that are so deeply integrated with the context ( for example , the community ) that no truly adequate comparison case can be found .

furthermore , every research method has inherent limitations ; therefore , it is often advantageous to combine multiple measures or two or more designs in a one type of impact evaluation — the quasi - study or group of studies to obtain a more comprehensive picture of the program's effect .

as we have also previously reported , the evaluation methods literature describes a variety of issues to consider in planning which methods to use in carrying out an evaluation , including the expected use of the evaluation , the nature and implementation of program activities , and the resources available for the evaluation .

we identified the following methods and designs of evaluation in our review , which may be used to carry out one or more of the main types of evaluation listed above: committee of visitors , and other report types , which are generally external peer reviews that examine programs' managerial stewardship , compare plans with progress made , and evaluate outcomes to determine whether the research contributes to the agency's mission and goals ; experimental methods , which involve randomly assigning one group to a program and another to not participate in the program in order to compare outcomes of both groups ; mixed methods , which combine qualitative and quantitative designs ; qualitative , such as interviews or focus groups ; surveys , which involve the systematic collection of data from a respondent using a structured instrument ( i.e. , a questionnaire ) to ensure that the collected data are as accurate as possible ; and quasi - experimental comparison groups .

in addition , there were two evaluations based solely on a compilation of grantee reports .

as stated previously , other evaluations also used grantee evaluations , but these used other data sources to inform their results , and so were classified as using either mixed or qualitative methods .

the most common evaluation designs that we classified programs as using were the committee of visitors and mixed methods .

we reviewed 35 evaluations from the following agencies and programs , and determined their primary method for assessing effectiveness: the following are different types of reports , including the committee of visitors , that programs used to assess effectiveness of their stem education programs .

as stated in appendix i , we did consider these to be evaluations but did not review them because they did not align with the criteria we used to assess the evaluations .

the following staff members made key contributions to this report: bill keller , assistant director ; susan baxter ; james bennett ; karen brown ; david chrisinger ; melinda cordero ; elizabeth curda ; karen febey ; jill lacey ; ben licht ; amy radovich ; james rebbe ; nyree ryder tee ; martin scire ; ryan siegel ; and walter vance .

opportunities to reduce potential duplication in government programs , save tax dollars , and enhance revenue .

gao - 11-635t .

washington , d.c.: may 25 , 2011 .

managing for results: gpra modernization act implementation provides important opportunities to address government challenges .

gao - 11-617t .

washington , d.c.: may 10 , 2011 .

performance measurement and evaluation: definitions and relationships ( supercedes gao - 05-739sp ) .

gao - 11-646sp .

washington , d.c.: may 2011 .

opportunities to reduce potential duplication in federal teacher quality programs gao - 11-510t .

apr 13 , 2011 .

government performance: gpra modernization act provides opportunities to help address fiscal , performance , and management challenges .

gao - 11-466t .

washington , d.c.: march 16 , 2011 .

opportunities to reduce potential duplication in government programs , save tax dollars , and enhance revenue .

gao - 11-441t .

washington , d.c.: march 3 , 2011 .

opportunities to reduce potential duplication in government programs , save tax dollars , and enhance revenue , gao - 11-318sp .

washington , d.c.: mar .

1 , 2011 .

program evaluation: experienced agencies follow a similar model for prioritizing research .

gao - 11-176 .

washington , d.c.: january 14 , 2011 .

america competes act: it is too early to evaluate programs long - term effectiveness , but agencies could improve reporting of high - risk , high - reward research priorities .

gao - 11-127r .

washington , d.c.: october 7 , 2010 .

federal education funding: overview of k - 12 and early childhood education programs gao - 10-51 .

washington , d.c.: jan 27 , 2010 .

program evaluation: a variety of rigorous methods can help identify effective interventions .

gao - 10-30 .

washington , d.c.: november 23 , 2009 .

government performance: strategies for building a results - oriented and collaborative culture in the federal government .

gao - 09-1011t .

washington , d.c.: september 24 , 2009 .

teacher quality: sustained coordination among key federal education programs could enhance state efforts to improve teacher quality .

gao - 09-593 .

washington , d.c.: july 6 , 2009 .

higher education: federal science , technology , engineering , and mathematics programs and related trends .

gao - 06-114 .

washington , d.c.: oct. 12 , 2005 .

results - oriented government: practices that can help enhance and sustain collaboration among federal agencies .

gao - 06-15 .

washington , d.c.: oct. 21 , 2005 .

