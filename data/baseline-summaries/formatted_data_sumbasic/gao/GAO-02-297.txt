to assess the quality of the population data collected in the 2000 census , the u.s. census bureau conducted the accuracy and coverage evaluation ( a.c.e. ) .

survey , a sample of persons designed to estimate the number of people missed , counted more than once , or otherwise improperly counted in the census .

on the basis of uncertainty in the a.c.e .

results , in separate decisions in march and october 2001 , the acting director of the bureau decided that the 2000 census tabulations should not be adjusted for purposes of redrawing the boundaries of congressional districts or for other purposes , such as distributing billions of dollars in federal funding .

although a.c.e .

was generally implemented as planned , the bureau found that a.c.e .

overstated census undercounts due in part to error introduced during matching operations and other remaining uncertainties .

the bureau has reported that additional review and analysis on these remaining uncertainties would be necessary before any potential uses of these data can be considered .

a critical component of the a.c.e .

survey was the person matching operation , in which the bureau matched the persons counted in the a.c.e .

survey to the persons counted in the census .

the results of person matching formed the basis for statistical estimates of the proportions of the population missed or improperly counted by the census .

this report , prepared at the request of the chairman and ranking minority member of the former house subcommittee on the census , reviews the person matching operation of a.c.e .

we agreed to describe ( 1 ) the process and criteria involved in making an a.c.e .

and census person match , ( 2 ) the quality assurance procedures used in the key person matching phases and the available results of those procedures , and ( 3 ) any deviations in the matching operation from what was planned .

this report is the latest of several we have issued on lessons learned from the 2000 census that can help inform the bureau's planning efforts for the 2010 census .

to address our three objectives , we examined relevant bureau program specifications , training manuals , office manuals , memorandums , and other progress and research documents .

we also interviewed bureau officials at bureau headquarters in suitland , md. , and the bureau's national processing center in jeffersonville , ind. , which was responsible for the planning and implementation of the person matching operation .

further scope and methodological details are given in appendix i .

we performed our audit work from september 2000 through april 2001 in accordance with generally accepted government auditing standards .

on january 4 , 2002 , we requested comments on a draft of this report from the secretary of commerce .

on february 13 , 2002 , the secretary of commerce forwarded written comments from the bureau ( see appendix ii ) , which we address in the “agency comments and our evaluation” section of this report .

from april 24 through september 11 , 2000 , the u.s. census bureau surveyed a sample of about 314,000 housing units ( about 1.4 million census and a.c.e .

records in various areas of the country , including puerto rico ) to estimate the number of people and housing units missed or counted more than once in the census and to evaluate the final census counts .

temporary bureau staff conducted the surveys by telephone and in - person visits .

the a.c.e .

sample consisted of about 12,000 “clusters” or geographic areas that each contained about 20 to 30 housing units .

the bureau selected sample clusters to be representative of the nation as a whole , relying on variables such as state , race and ethnicity , owner or renter , as well as the size of each cluster and whether the cluster was on an american indian reservation .

the bureau canvassed the a.c.e .

sample area , developed an address list , and collected response data for persons living in the sample area on census day ( april 1 , 2000 ) .

although the bureau's a.c.e .

data and address list were collected and maintained separately from the bureau's census work , a.c.e .

processes were similar to those of the census .

after the census and a.c.e .

data collection operations were completed , the bureau attempted to match each person counted by a.c.e .

to the list of persons counted by the census in the sample areas to determine the number of persons who lived in the sample area on census day .

the results of the matching process , together with the characteristics of each person compared , provided the basis for statistical estimates of the number and characteristics of the population missed or improperly counted by the census .

correctly matching a.c.e .

persons with census persons is important because errors in even a small percentage of records can significantly affect the undercount or overcount estimate .

matching over 1.4 million census and a.c.e .

records was a complex and often labor - intensive process .

although several key matching tasks were automated and used prespecified decision rules , other tasks were carried out by trained bureau staff who used their judgment to match and code records .

the four phases of the person matching process were ( 1 ) computer matching , ( 2 ) clerical matching , ( 3 ) nationwide field follow - up on records requiring more information , and ( 4 ) a second phase of clerical matching after field follow - up .

each subsequent phase used additional information and matching rules in an attempt to match records that the previous phase could not link .

 ( first phase ) ( second phase ) computer matching took pairs of census and a.c.e .

records and compared various personal characteristics such as name , age , and gender .

the computer then calculated a match score for the paired records based on the extent to which the personal characteristics were aligned .

experienced bureau staff reviewed the lists of paired records , sorted by their match scores , and judgmentally assigned cutoff scores .

the cutoff scores were break points used to categorize the paired records into one of three groups so that the records could be coded as a “match,” “possible match,” or one of a number of codes that defines them as not matched .

computer matching successfully assigned a match score to nearly 1 million of the more than 1.4 million records reviewed ( about 66 percent ) .

bureau staff documented the cutoff scores for each of the match groups .

however , they did not document the criteria or rules used to determine cutoff scores , the logic of how they applied them , and examples of their application .

as a result , the bureau may not benefit from the possible lessons learned on how to apply cutoff scores .

when the computer links few records as possible matches , clerks will spend more time searching records and linking them .

in contrast , when the computer links many records as possible matches , clerks will spend less time searching for records to link and more time unlinking them .

without documentation and knowledge of the effect of cutoff scores on clerical matching productivity , future bureau staff will be less able to determine whether to set cutoff scores to link few or many records together as possible matches .

 ( first phase ) ( second phase ) during clerical matching , three levels of matchers — including over 200 clerks , about 40 technicians , and 10 experienced analysts or “expert matchers” — applied their expertise and judgment to manually match and code records .

a computer software system managed the workflow of the clerical matching stages .

the system also provided access to additional information , such as electronic images of census questionnaires that could assist matchers in applying criteria to match records .

according to a bureau official , a benefit of clerical matching was that records of entire households could be reviewed together , rather than just individually as in computer matching .

during this phase over a quarter million records ( or about 19 percent ) were assigned a final match code .

the bureau taught clerks how to code records in situations in which the a.c.e .

and census records differed because one record contained a nickname and the other contained the birth name .

the bureau also taught clerks how to code records with abbreviations , spelling differences , middle names used as first names , and first and last names reversed .

these criteria were well documented in both the bureau's procedures and operations memorandums and clerical matchers' training materials , but how the criteria were applied depended on the judgment of the matchers .

the bureau trained clerks and technicians for this complex work using as examples some of the most challenging records from the 1998 dress rehearsal person matching operation .

in addition , the analysts had extensive matching experience .

for example , the 4 analysts that we interviewed had an average of 10 years of matching experience on other decennial census surveys and were directly involved in developing the training materials for the technicians and clerks .

 ( first phase ) ( second phase ) the bureau conducted a nationwide field follow - up on over 213,000 records ( or about 15 percent ) for which the bureau needed additional information before it could accurately assign a match code .

for example , sometimes matchers needed additional information to verify that possibly matched records were actually records of the same person , that a housing unit was located in the sample area on census day , or that a person lived in the sample area on census day .

field follow - up questionnaires were printed at the national processing center and sent to the appropriate a.c.e .

regional office .

field follow - up interviewers from the bureau's regional offices were required to visit specified housing units and obtain information from a knowledgeable respondent .

if the household member for the record in question still lived at the a.c.e .

address at the time of the interview and was not available to be interviewed after six attempts , field follow - up interviewers were allowed to obtain information from one or more knowledgeable proxy respondents , such as a landlord or neighbor .

 ( first phase ) ( second phase ) the second phase of clerical matching used the information obtained during field follow - up in an attempt to assign a final match code to records .

as in the first phase of clerical matching , the criteria used to match and code records were well documented in both the bureau's procedures and operations memorandums and clerical matchers' training materials .

nevertheless , in applying those criteria , clerical matchers had to use their own judgment and expertise .

this was particularly true when matching records that contained incomplete and inconsistent information , as noted in the following examples .

different household members provided conflicting information .

the census counted one person — the field follow - up respondent .

a.c.e .

recorded four persons — including the respondent and her daughter .

the respondent , during field follow - up , reported that all four persons recorded by a.c.e .

lived at the housing unit on census day .

during the field follow - up interview , the respondent's daughter came to the house and disagreed with the respondent .

the interviewer changed the answers on the field follow - up questionnaire to reflect what the daughter said — the respondent was the only person living at the household address on census day .

the other three people were coded as not living at the household address on census day .

according to bureau staff , the daughter's response seemed more reliable .

an interviewer's notes on the field follow - up questionnaire conflicted with recorded information .

the census counted 13 people — including the respondent and 2 people not matched to a.c.e .

records .

a.c.e .

recorded 12 people — including the respondent , 10 other matched people , and the respondent's daughter who was not matched to census records .

the field follow - up interview attempted to resolve the unmatched census and a.c.e .

people .

answers to questions on the field follow - up questionnaire verified that the daughter lived at the housing address on census day .

however , the interviewer's notes indicated that the daughter and the respondent were living in a shelter on census day .

the daughter was coded as not living at the household address on census day , while the respondent remained coded as matched and living at the household address on census day .

according to bureau staff , the respondent should also have been coded as a person that did not live at the household address on census day , based on the notes on the field follow - up questionnaire .

a.c.e. , census , or both counted people at the wrong address .

the census counted two people — the respondent and her husband — twice ; once in an apartment and once in a business office that the husband worked in , both in the same apartment building .

the a.c.e .

did not record anyone at either location , as the residential apartment was not in the a.c.e .

interview sample .

the respondent , during field follow - up , reported that they lived at their apartment on census day and not at the business office .

the couple had responded to the census on a questionnaire delivered to the business office .

a census enumerator , following up on the “nonresponse” from the couple's apartment , had obtained census information from a neighbor about the couple .

the couple , as recorded by the census at the business office address , was coded as correctly counted in the census .

the couple , as recorded by the census at the apartment address , was coded as living outside the sample block .

according to bureau staff , the couple recorded at the business office address were correctly coded , but the couple recorded at the apartment should have been coded as duplicates .

an uncooperative household respondent provided partial or no information .

the census counted a family of four — the respondent , his wife , and two daughters .

a.c.e .

recorded a family of three — the same husband and wife , but a different daughter's name , “buffy.” the field follow - up interview covered the unmatched daughters — two from census and one from a.c.e .

the respondent confirmed that the four people counted by the census were his family and that “buffy” was a nickname for one of his two daughters , but he would not identify which one .

the interviewer wrote in the notes that the respondent “was upset with the number of visits” to his house .

“buffy” was coded as a match to one of the daughters ; the other daughter was coded as counted in the census but missed by a.c.e .

according to bureau staff , since the respondent confirmed that “buffy” was a match for one of his daughters — although not which one — and that four people lived at the household address on census day , they did not want one of the daughters coded so that she was possibly counted as a missed census person .

since each record had to have a code identifying whether it was a match by the end of the second clerical matching phase , records that did not contain enough information after field follow - up to be assigned any other code were coded as “unresolved.” the bureau later imputed the match code results for these records using statistical methods .

while imputation for some situations may be unavoidable , it introduces uncertainty into estimates of census over - or undercount rates .

the following are examples of situations that resulted in records coded as “unresolved.” conflicting information was provided for the same household .

the census counted four people — a woman , an “unmarried partner,” and two children .

a.c.e .

recorded three people — the same woman and two children .

during field follow - up , the woman reported to the field follow - up interviewer that the “unmarried partner” did not really live at the household address , but just came around to baby - sit , and that she did not know where he lived on census day .

according to bureau staff , probing questions during field follow - up determined that the “unmarried partner” should not have been coded as living at the housing unit on census day .

therefore , the “unmarried partner” was coded as “unresolved.” a proxy respondent provided conflicting or inaccurate information .

the census counted one person — a female renter .

a.c.e .

did not record anyone .

the apartment building manager , who was interviewed during field follow - up , reported that the woman had moved out of the household address sometime in february 2000 , but the manager did not know the woman's census day address .

the same manager had responded to an enumerator questionnaire for the census in june 2000 and had reported that the woman did live at the household address on census day .

the woman was coded as “unresolved. .

the bureau employed a series of quality assurance procedures for each phase of person matching .

the bureau reported that person matching quality assurance was successful at minimizing errors because the quality assurance procedures found error rates of less than 1 percent .

clerks were to review all of the match results to ensure , among other things , that the records linked by the computer were not duplicates and contained valid and complete names .

moreover , according to bureau officials , the software used to link records had proven itself during a similar operation conducted for the 1990 census .

the bureau did not report separately on the quality of computer matched records .

although there were no formal quality assurance results from computer matching , at our request the bureau tabulated the number of records that the computer had coded as “matched” that had subsequently been coded otherwise .

according to the bureau , the subsequent matching process resulted in a different match code for about 0.6 percent of the almost 500,000 records initially coded as matched by the computer .

of those records having their codes changed by later matching phases , over half were eventually coded as duplicates and almost all of the remainder were rematched to someone else .

technicians reviewed the work of clerks and analysts reviewed the work of technicians primarily to find clerical errors that ( 1 ) would have prevented records from being sent to field follow - up , ( 2 ) could cause a record to be incorrectly coded as either properly or erroneously counted by the census , or ( 3 ) would cause a record to be incorrectly removed from the a.c.e .

sample .

analysts' work was not reviewed .

clerks and technicians with error rates of less than 4 percent had a random sample of about 25 percent of their work reviewed , while clerks and technicians exceeding the error threshold had 100 percent of their work reviewed .

about 98 percent of clerks in the first phase of matching had only a sample of their work reviewed .

according to bureau data , less than 1 percent of match decisions were revised during quality assurance reviews , leading the bureau to conclude that clerical matching quality assurance was successful .

under certain circumstances , technicians and analysts performed additional reviews of clerks' and technicians' work .

for example , if during the first phase of clerical matching a technician had reviewed and changed more than half of a clerk's match codes in a given geographic cluster , the cluster was flagged for an analyst to review all of the clerk and technician coding for that area .

during the second phase , analysts were required to make similar reviews when only one of the records was flagged for their review .

this is one of the reasons why , as illustrated in figure 2 , these additional reviews were a much more substantial part of the clerks' and technicians' workload that was subsequently reviewed by more senior matchers .

the total percentage of workload reviewed ranged from about 20 to 60 percent across phases of clerical matching , far in excess of the 11- percent quality assurance level for the bureau's person interviewing operation .

the quality assurance plan for the field follow - up phase had two general purposes: ( 1 ) to ensure that questionnaires had been completed properly and legibly and ( 2 ) to detect falsification .

supervisors initially reviewed each questionnaire for legibility and completeness .

these reviews also checked the responses for consistency .

office staff were to conduct similar reviews of each questionnaire .

to detect falsification , the bureau was to review and edit each questionnaire at least twice and recontact a random sample of 5 percent of the respondents .

as shown in figure 3 , all 12 of the a.c.e .

regional offices exceeded the 5 percent requirement by selecting more than 7 percent of their workload for quality assurance review , and the national rate of quality assurance review was about 10 percent .

at the local level , however , there was greater variation .

there are many reasons why the quality assurance coverage can appear to vary locally .

for example , a local census area could have a low quality assurance coverage rate because interviewers in that area had their work reviewed in other areas , or the area could have had an extremely small field follow - up workload , making the difference of just one quality assurance questionnaire constitute a large percentage of the local workload .

seventeen local census office areas ( out of 520 nationally , including puerto rico ) had 20 percent or more of field follow - up interviews covered by the quality assurance program , and , at the other extreme , 5 local census areas had 5 percent or less of the work covered by the quality assurance program .

less than 1 percent of the randomly selected questionnaires failed quality assurance nationally , leading the bureau to report this quality assurance operation as successful .

when recontacting respondents to detect falsification by interviewers , quality assurance supervisors were to determine whether the household had been contacted by an interviewer , and if it had not , the record of that household failed quality assurance .

according to bureau data , about 0.8 percent of the randomly selected quality assurance questionnaires failed quality assurance nationally .

this percentage varied between 0 and about 3 percent across regions .

the bureau carried out person matching as planned , with only a few procedural deviations .

although the bureau took action to address these deviations , it has not determined how matching results were affected .

as shown in table 1 , these deviations included ( 1 ) census files that were delivered late , ( 2 ) a programming error in the clerical matching software , ( 3 ) printing errors in field follow - up forms , ( 4 ) regional offices that sent back incomplete questionnaires , and ( 5 ) the need for additional time to complete the second phase of clerical matching .

it is unknown what , if any , cumulative effect these procedural deviations may have had on the quality of matching for these records or on the resultant a.c.e .

estimates of census undercounts .

however , bureau officials believe that the effect of the deviations was small based on the timely responses taken to address them .

the bureau conducted reinterviewing and re - matching studies on samples of the 2000 a.c.e .

sample and concluded that matching quality in 2000 was improved over that in 1990 , but that error introduced during matching operations remained and contributed to an overstatement of a.c.e .

estimates of the census undercounts .

the studies provided some categorical descriptions of the types of matching errors measured , but did not identify the procedural causes , if any , for those errors .

furthermore , despite the improvement in matching reported by the bureau , a.c.e .

results were not used to adjust the census due to these errors as well as other remaining uncertainties .

the bureau has reported that additional review and analysis on these remaining uncertainties would be necessary before any potential uses of these data can be considered .

the computer matching phase started 3 days later than scheduled and finished 1 day late due to the delayed delivery of census files .

in response , bureau employees who conducted computer matching worked overtime hours to make up lost time .

furthermore , a.c.e .

regional offices did not receive clusters in the prioritized order that they had requested .

the reason for prioritizing the clusters was to provide as much time as possible for field follow - up on clusters in the most difficult areas .

examples of areas that were expected to need extra time were those with staffing difficulties , larger workloads , or expected weather problems .

based on the bureau's master activities schedule , the delay did not affect the schedule of subsequent matching phases .

also , bureau officials stated that although clusters were not received in prioritized order , field follow - up was not greatly affected because the first clerical matching phase was well staffed and sent the work to regional offices quickly .

on the first full day of clerical matching , the bureau identified a programming error in the quality assurance management system , which made some clerks and technicians who had not passed quality assurance reviews appear to have passed .

in response , bureau officials manually overrode the system .

bureau officials said the programming error was fixed within a couple of days , but could not explain how the programming error occurred .

they stated that the software system used for clerical matching was thoroughly tested , although it was not used in any prior censuses or census tests , including the dress rehearsal .

as we have previously noted , programming errors that occur during the operation of a system raise questions about the development and acquisition processes used for that system .

a programming error caused last names to be printed improperly on field follow - up forms for some households containing multiple last names .

in situations in which regional office staff may not have caught the printing error and interviewers may have been unaware of the error — such as when those questionnaires were completed before the problem was discovered — interviews may have been conducted using the wrong last name , thus recording misleading information .

according to bureau officials , in response , the bureau ( 1 ) stopped printing questionnaires on the date officials were notified about the misprinted questionnaires , ( 2 ) provided information to regional offices that listed all field follow - up housing units with multiple names that had been printed prior to the date the problem was resolved , and ( 3 ) developed procedures for clerical matchers to address any affected questionnaires being returned that had not been corrected by regional office staff .

while resolving the problem , productivity was initially slowed in the a.c.e .

regional offices for approximately 1 to 4 days , yet field follow - up was completed on time .

bureau officials inadvertently introduced this error when they addressed a separate programming problem in the software .

bureau officials stated that they tested this software system ; however , the system was not given a trial run during the census dress rehearsal in 1998 .

according to bureau officials , the problem did not affect data quality because it was caught early in the operation and follow - up forms were edited by regional staff .

however , the bureau could not determine the exact day of printing for each questionnaire and thus did not know exactly which households had been affected by the problem .

according to bureau data , the problem could have potentially affected over 56,000 persons , or about 5 percent of the a.c.e .

sample .

in addition to the problem printing last names , the bureau experienced other printing problems .

according to bureau staff , field follow - up received printed questionnaires that were ( 1 ) missing pages , ( 2 ) missing reference notes written by clerical matchers , and ( 3 ) missing names and / or having some names printed more than once for some households of about nine or more people .

according to bureau officials , these problems were not resolved during the operation because they were reported after field follow - up had started and the bureau was constrained by deadlines .

bureau officials stated that they believed that these problems would not significantly affect the quality of data collected or match code results , although bureau officials were unable to provide data that would document either the extent , effect , or cause of these problems .

the bureau's regional offices submitted questionnaires containing an incomplete “geocoding” section .

this section was to be used in instances when the bureau needed to verify whether a housing unit ( 1 ) existed on census day and ( 2 ) was correctly located in the a.c.e .

sample area .

although the bureau returned 48 questionnaires during the first 6 days of the operation to the regional offices for completion , bureau officials stated that after that they no longer returned questionnaires to the regional offices because they did not want to delay the completion of field follow - up .

a total of over 10,000 questionnaires with “geocoding” sections were initially sent to the regional offices .

the bureau did not have data on the number , if any , of questionnaires that the regional offices submitted incomplete beyond the initial 48 .

the bureau would have coded as “unresolved” the persons covered by any incomplete questionnaires .

as previously stated , the bureau later imputed the match code results for these records using statistical methods , which could introduce uncertainty into estimates of census over - or undercount rates .

according to bureau officials , this problem was caused by ( 1 ) not printing a checklist of all sections that needed to be completed by interviewers , ( 2 ) no link from any other section of the questionnaire to refer interviewers to the “geocoding” section , and ( 3 ) field supervisors following the same instructions as interviewers to complete their reviews of field follow - up forms .

however , bureau officials believed that the mistake should have been caught by regional office reviews before the questionnaires were sent back for processing .

about a week after the second clerical matching phase began , officials requested an extension , which was granted for 5 days , to complete the second clerical matching phase .

according to bureau officials , the operation could have been completed by the november 30 , 2000 , deadline as planned , but they decided to take extra steps to improve data quality that required additional time .

according to bureau officials , the delay in completing person matching had no effect on the final completion schedule , only the start of subsequent a.c.e .

processing operations .

matching a.c.e .

and census records was an inherently complex and labor - intensive process that often relied on the judgment of trained staff , and the bureau prepared itself accordingly .

for example , the bureau provided extensive training for its clerical matchers , generally provided thorough documentation of the process and criteria to be used in carrying out their work , and developed quality assurance procedures to cover its critical matching operations .

as a result , our review identified few significant operational or procedural deviations from what the bureau planned , and the bureau took timely action to address them .

nevertheless , our work identified opportunities for improvement .

these opportunities include a lack of written documentation showing how cutoff scores were determined and programming errors in the clerical matching software and software used to print field follow - up forms .

without written documentation , the bureau will be less likely to capture lessons learned on how cutoff scores should be applied , in order to determine the impact on clerical matching productivity .

moreover , the discovery of programming errors so late in the operation raises questions about the development and acquisition processes used for the affected a.c.e .

computer systems .

in addition , one lapse in procedures may have resulted in incomplete geocoding sections verifying that the person being matched was in the geographic sample area .

the collective effect that these deviations may have had on the accuracy of a.c.e .

results is unknown .

although the bureau has concluded that a.c.e .

matching quality improved compared to 1990 , the bureau has reported that error introduced during matching operations remained and contributed to an overstatement of the a.c.e .

estimate of census undercounts .

to the extent that the bureau employs an operation similar to a.c.e .

to measure the quality of the 2010 census , it will be important for the bureau to determine the impact of the deviations and explore operational improvements , in addition to the research it might carry out on other uncertainties in the a.c.e .

results .

as the bureau documents its lessons learned from the 2000 census and continues its planning efforts for 2010 , we recommend that the secretary of commerce direct the bureau to take the following actions: 1 .

document the criteria and the logic that bureau staff used during computer matching to determine the cutoff scores for matched , possibly matched , and unmatched record pairs .

2 .

examine the bureau's system development and acquisition processes to determine why the problems with a.c.e .

computer systems were not discovered prior to deployment of these systems .

3 .

determine the effect that the printing problems may have had on the quality of data collected for affected records , and thus the accuracy of a.c.e .

estimates of the population .

4 .

determine the effect that the incomplete geocoding section of the questionnaires may have had on the quality of data collected for affected records , and thus the accuracy of a.c.e .

estimates of census undercounts .

the secretary of commerce forwarded written comments from the u.s. census bureau on a draft of this report .

 ( see appendix ii. ) .

the bureau had no comments on the text of the report and agreed with , and is taking action on , two of our four recommendations .

in responding to our recommendation to document the criteria and the logic that bureau staff used during computer matching to determine cutoff scores , the bureau acknowledged that such documentation may be informative and that such documentation is under preparation .

we look forward to reviewing the documentation when it is complete .

in responding to our recommendation to examine system development and acquisition processes to determine why problems with the a.c.e .

computer systems were not discovered prior to deployment , the bureau responded that despite extensive testing of a.c.e .

computer systems , a few problems may remain undetected .

the bureau plans to review the process to avoid such problems in 2010 , and we look forward to reviewing the results of their review .

finally , in response to our two recommendations to determine the effects that printing problems and incomplete questionnaires had on the quality of data collected and the accuracy of a.c.e .

estimates , the bureau responded that it did not track the occurrence of these problems because the effects on the coding process and accuracy were considered to be minimal since all problems were identified early and corrective procedures were effectively implemented .

in our draft report we recognized that the bureau took timely corrective action in response to these and other problems that arose during person matching .

yet we also reported that bureau studies of the 2000 matching process had concluded that matching error contributed to error in a.c.e .

estimates without identifying procedural causes , if any .

again , to the extent that the bureau employs an operation similar to a.c.e .

to measure the quality of the 2010 census , it will be important for the bureau to determine the impact of the problems and explore operational improvements as we recommend .

we are sending copies of this report to other interested congressional committees .

please contact me on ( 202 ) 512-6806 if you have any questions .

key contributors to this report are included in appendix iii .

to address our three objectives , we examined relevant bureau program specifications , training manuals , office manuals , memorandums , and other progress and research documents .

we also interviewed bureau officials at bureau headquarters in suitland , md. , and the bureau's national processing center in jeffersonville , ind. , which was responsible for the planning and implementation of the person matching operation .

in addition , to review the process and criteria involved in making an a.c.e .

and census person match , we observed the match clerk training at the national processing center and a field follow - up interviewer training session in dallas , tex .

to identify the results of the quality assurance procedures used in key person matching phases , we analyzed operational data and reports provided to us by the bureau , as well as extracts from the bureau's management information system , which tracked the progress of quality assurance procedures .

other independent sources of the data were not available for us to use to test the data that we extracted , although we were able to corroborate data results with subsequent interviews of key staff .

finally , to examine how , if at all , the matching operation deviated from what was planned , we selected 11 locations in 7 of the 12 bureau census regions ( atlanta , chicago , dallas , denver , los angeles , new york , and seattle ) .

at each location we interviewed a.c.e .

workers from november through december 2000 .

the locations selected for field visits were chosen primarily for their geographic dispersion ( i.e. , urban or rural ) , variation in type of enumeration area ( eg , update / leave or list enumerate ) , and the progress of their field follow - up work .

in addition , we reviewed the match code results and field follow - up questionnaires from 48 sample clusters .

these clusters were chosen because they corresponded to the local census areas we visited and contained records reviewed during every phase of the person matching operation .

the results of our field visits and our cluster review are not generalizable nationally to the person matching operation .

we performed our audit work from september 2000 through september 2001 in accordance with generally accepted government auditing standards .

in addition to those named above , ty mitchell , lynn wasielewski , steven boyles , angela pun , j. christopher mihm , and richard hung contributed to this report .

the general accounting office , the investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents is through the internet .

gao's web site ( www.gao.gov ) contains abstracts and full - text files of current reports and testimony and an expanding archive of older products .

the web site features a search engine to help you locate documents using key words and phrases .

you can print these documents in their entirety , including charts and other graphics .

each day , gao issues a list of newly released reports , testimony , and correspondence .

gao posts this list , known as “today's reports,” on its web site daily .

the list contains links to the full - text document files .

to have gao e - mail this list to you every afternoon , go to www.gao.gov and select “subscribe to daily e - mail alert for newly released products” under the gao reports heading .

web site: www.gao.gov / fraudnet / fraudnet.htm , e - mail: fraudnet@gao.gov , or 1-800-424-5454 or ( 202 ) 512-7470 ( automated answering system ) .

