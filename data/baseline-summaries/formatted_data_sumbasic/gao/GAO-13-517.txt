in increasing recognition of the value of open government and the need to make performance information more transparent , congress took steps to improve federal performance management and reporting by updating the government performance and results act ( gpra ) through the gpra modernization act of 2010 ( gprama ) .

gprama established important changes to modernize and refine existing federal agency reporting requirements and to ensure more frequent , relevant data are available to inform decision making and the public .

the act included a requirement that the office of management and budget ( omb ) provide quarterly updates on agency and cross - agency priority goals via a central , government - wide website , performance.gov .

in march 2013 , federal agencies added the first performance updates for cross - agency and agency priority goals to the website .

omb stated that over time it plans to add more information from agency strategic plans , performance plans , and reports and produce the information in formats that allow users to see trends , look at goals contributing to common themes , see programs contributing to common goals , and cross reference other related data .

gprama required omb to establish a single , performance - related website by october 1 , 2012 , to provide program and performance information , which would be accessible to members and committees of congress and the public .

first developed by omb in 2010 for executive branch use , performance.gov was made available to the public in august 2011 .

omb's stated goals for performance.gov include providing both a public view into government performance to support transparency as well as providing executive branch management capabilities to enhance senior leadership decision making .

in addition to linking agency programs and their relationships and contributions to strategic objectives , another omb goal of the website , as a repository of federal government performance information , is to increase the utility of this information through enhanced agency comparisons , supporting both benchmarking and best practice identification .

this report is part of our response to a mandate to assess initial implementation of the gpra modernization act of 2010 .

specifically , this report examines the extent to which performance.gov incorporates leading practices for the development of federal websites .

this report is the fifth in a series that looks at how agencies are implementing various gprama requirements .

to address our objective , we analyzed information from the performance.gov website and the requirements from gprama regarding the website .

we also examined related guidance , such as omb's circular no .

a - 11 , which lays out expectations for the design , development , and implementation of performance.gov .

we reviewed relevant academic and policy literature on performance management and reporting , including our previous reports on performance management , and conducted interviews with staff at omb and the general services administration ( gsa ) , who worked on the design and development of the website .

we reviewed omb and gsa staff's description of the development of performance.gov from our interviews and compared it against criteria established by howto.gov , a source of guidance and leading practices for government websites .

we also conducted an analysis of the ease with which commercial search engines , and performance.gov's internal site search engine , can be used to find relevant information on performance.gov .

lastly , we reviewed design features from a subset of other federal open government websites , including recovery.gov , data.gov , and usaspending.gov , which like performance.gov are used to make government - wide information and data accessible .

to further address our objective , we interviewed potential users of performance.gov , including executive branch and congressional staff and a variety of nonfederal stakeholders .

we reached out to groups most likely to use the information on performance.gov because of their management , oversight , advocacy , or academic interests and asked them to review the website prior to our interviews .

we interviewed staff from five agencies — the u.s. department of agriculture , department of the interior , department of labor , department of state , and the environmental protection agency — responsible for reporting performance information to gather their perceptions on the utility of performance.gov .

we selected from agencies that have not been the subject of our gprama mandate work and grouped them together according to the number of their priority goals .

we then selected the agency in each group that used the largest number of tools of government — such as direct service , regulations , grants , and loans — to achieve their performance goals .

this process allowed us to consider a mix of agency priority goals , an example of each of the tools of government , and performance improvement officers under both career and political appointments .

we interviewed staff from 13 different majority and minority congressional oversight and authorizing committee offices of the u.s. senate and house of representatives to gather their views on performance.gov and any suggestions they had for improving it .

see appendix i for a full list of the 13 committees represented in our interviews .

we also interviewed a variety of nonfederal stakeholders including representatives from 10 academic , advocacy , transparency , and public policy organizations as potential users to solicit feedback about the website and any suggestions they had for improving it .

we selected these individuals on the basis of our literature review and recommendations from other performance management and reporting academics and practitioners .

the results of these interviews are not generalizable to all nonfederal stakeholders but provided insights into perceptions of performance.gov .

in addition , based on a review of government performance reporting websites , we selected a group of 12 websites from state and local levels , as well as one from canada , to gather lessons learned from representatives of these performance reporting websites that were relevant to the design and development of performance.gov .

to focus this review on performance reporting websites , we selected only sites that presented performance information through web pages ; we did not select transparency websites that are designed to make available information on state finances and the distribution of state expenditures .

table 1 provides a list of the state , local , and canadian performance reporting websites we selected to review .

we conducted our work from july 2012 to june 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

more detailed information on our scope and methodology appears in appendix i .

in recent years , significant improvements in data management and presentation technologies have made it possible for governments to report performance information in new ways .

prior to the development of these technologies , performance information was generally reported in static reports , such as printed annual reports .

technology now allows data to be collected from different operational units and then presented through interactive , web - based dashboards and performance - reporting websites to provide an integrated , multidimensional view of a government's performance .

this advantage can give web - based performance reporting the ability to more efficiently convey understandable information to stakeholders and citizens .

performance reporting websites are dynamic and can be updated on a more frequent basis , or in real - time if technology allows and data are available , ensuring that information is timely .

they can also be flexible enough to meet the needs of different audiences .

for example , they can be used to combine and organize , or layer , different levels of performance information , allowing users to quickly access information from an overall organizational perspective and to drill down to get a more detailed perspective at the sub - unit level .

in addition , performance reporting websites can be made more engaging for users through the use of tables and interactive graphics and maps that allow users to visualize performance information in different ways .

governments at the local and state level in the united states and in other countries have developed performance reporting websites .

the purposes of these websites include , for example , compliance with statutory reporting requirements and making information on the goals and performance of government more widely accessible and transparent for those inside and outside of government .

officials from other state governments , who have developed performance reporting websites , emphasized that making regularly - updated and understandable information on goals and performance widely accessible for potential users of information on the site , such as interested members of the general public , legislators , executive branch leaders , delivery partners , the media , key stakeholders , advocacy groups , and researchers can increase oversight and lead to a greater focus within government on the activities and efforts necessary to improve performance .

in this way , the websites can also enhance accountability within government and support a culture of performance .

these websites can also be used to facilitate communication by providing a shared source of information on goals and performance .

according to omb , the performance.gov website is intended to highlight the administration's management initiatives and address the performance reporting requirements of gprama .

figure 1 displays the performance.gov home page .

to comply with the requirements of gprama , the current version of performance.gov also provides information , which is to be updated on a quarterly basis , on agency and cross - agency priority goals .

the pages with information on agency and cross - agency priority goals can be accessed either through individual agency pages on performance.gov or through the website's “clear goals” page .

the information available on agency priority goals includes the strategies being used to achieve each goal , next steps and milestones , performance indicators used to track progress , and contributing programs .

figure 2 provides an example of how performance trends are presented graphically through an agency priority goal page .

we recently examined the extent to which 24 agencies implemented selected requirements related to 102 agency priority goals and commented on the 21 agency priority goals of 5 selected agencies .

we recommended that omb could improve agency priority goal implementation by revising its guidance to better reflect interim target , milestone , and cross - agency priority goal alignment requirements and by ensuring that agencies provide complete information about external contributors to their agency priority goals and describe congressional input on the goal's development .

omb concurred with the recommendations .

in addition , gprama requires federal government performance plan information ; agency - level strategic plans , agency performance plans , and agency performance updates ; and detailed information about each program identified by agencies to be posted on the website .

according to omb's guidance , these additional features will be added to the website over time .

implementation of performance.gov , including omb's projected dates for the inclusion of additional information .

omb circular no .

a - 11 , preparation , submission , and execution of the budget , pt .

6 ( august 2012 ) .

howto.gov is a key source of leading practices for federal website development and management .

the website is managed by gsa and designed as a resource to improve how agencies communicate and interact with customers and use innovative tools and technologies to provide services and information .

howto.gov offers best practices , guidance , and training on strategic planning ; federal web requirements and policies ; applications , data , and web infrastructure tools ; web content management , usability , and design ; and performance metrics .

according to gsa staff , to provide access to key industry practices howto.gov makes available a list of the “top 10 best practices” for federal websites and provides detailed information on how to implement each practice ( see table 2 ) .

omb has also encouraged the more widespread application of many of these practices across the federal government in its digital government strategy .

in addition to these leading practices for the development of federal websites , lessons learned from the experiences of those who have developed other government performance reporting websites and views of potential users provide useful insights that could help inform the continued development of performance.gov .

omb staff stated that , thus far , the specific legal requirements of gprama have been the primary framework used to guide efforts to develop performance.gov .

they have been focused on working to comply with these requirements by providing information on agency and cross - agency priority goals and by establishing a phased development plan for integrating additional information from agency strategic plans , performance plans , and performance reports and the inventory of federal programs .

omb and gsa staff members have said , however , that the leading practices provided by howto.gov will help guide the future development of performance.gov .

leading practices for the development of federal websites from howto.gov recommend identifying the purposes of a website and the ways in which specific audiences could use a website to accomplish various tasks , from finding relevant information to commenting on a government regulation to conducting a transaction , and then structuring information and navigation to help visitors quickly complete these tasks .

according to howto.gov , this is important because people often visit government websites with a specific task in mind , and if it is not easy to find the information they want to quickly complete that task , they will leave the site .

providing guidance about the tasks that can be accomplished on a website , along with explanations and navigation assistance , can help website users successfully achieve their objectives .

similarly , some officials we interviewed from other governments with experience developing performance reporting websites emphasized the importance of understanding and articulating the purposes that a performance reporting website is designed to achieve .

they noted that the audiences the website is designed to serve , along with the intended uses of a site , should influence its design and content .

for example , the main purpose of the state of michigan's dashboards is to provide citizens with a quick reference to see how the state is performing in key areas and whether trends are moving in the right direction .

to fulfill this purpose , according to michigan officials , the dashboards were designed to be simple , easy to access and navigate , and provide the public with an at - a - glance overview of the state's progress .

users interested in more information on a specific performance measure can click on the link for that measure and will be directed to a separate page with trend data and information on why the measure is significant .

a prominent link at the bottom of the page also allows them to provide direct feedback on the dashboard .

the dashboards are supplemented by departmental “scorecards” that include relevant performance measures to inform internal departmental management and decision making .

an example of the michigan education dashboard is shown in figure 4 .

gprama provides direction on the purposes and audiences for performance.gov .

in addition to specifying the types of information to be made available on the website , the act states that information on the website “shall be readily accessible and easily found on the internet by the public and members and committees of congress.” omb's written guidance states that performance.gov “serves as the public window on the federal government's goals and performance in key areas of focus,” and will be “the single , government - wide performance website required under the gpra modernization act.” omb's written guidance and information on the website also say that performance.gov will make information about cross - agency and agency - specific goals and performance easier to find for the public and congress , as required by gprama , as well as for delivery partners , agency employees , the media , and other stakeholders .

lastly , the guidance specifies that the website will “support coordination and decision - making to advance shared goals.” an analysis of statements from omb and gsa staff , agency officials , and feedback we obtained from potential users , however , indicates that there are varying expectations regarding the primary audiences and uses of performance.gov .

for example , omb and gsa staff emphasized that they have viewed performance.gov as a tool for agencies to support cross - agency coordination and efforts to achieve agency goals .

consistent with this , omb staff said that performance.gov has been used to facilitate conversations between omb examiners and agency managers about progress on agency priority goals .

while officials we interviewed from the five agencies said that omb has collected feedback from agencies in the development of performance.gov , officials from four of these agencies also said , however , that performance.gov is not being used as a resource by agency leadership or other staff .

according to agency officials , agencies have information sources tailored to meet their needs , and performance.gov does not contain critical indicators or the ability to display some visualizations used for internal agency performance reviews .

agency officials stressed instead that they view performance.gov primarily as an external reporting tool so that the public and other external stakeholders can get a sense for how agencies are performing on key priorities .

for example , an agency official noted that a key function of performance.gov is to make performance data more readily available in one place so that members of the public and external stakeholders do not have to go to individual agency sites to collect information on agency goals and performance .

several potential users and a practitioner we interviewed also indicated that it was unclear to them who the audience of performance.gov is , and how audiences would use the site , given its content and design .

for example , some said that , while the public is listed as an audience of the website , the detailed , technical nature of the website seemed primarily oriented toward a government , rather than a public audience .

some also commented that developers need clarity about how different audiences should be able to use a website , as developers can then ensure it is designed to present information in a way that addresses the needs of those audiences .

omb and gsa staff members have acknowledged that , given the requirements of gprama , performance.gov will need to shift to become a more publicly - oriented website in the future , which could involve a more thematic presentation to engage the public .

while the “areas of focus” section of the performance.gov home page directs visitors to pages that allow them to explore information and metrics in specific areas , omb has not yet articulated various ways that intended audiences , such as interested members of the public , congress , experts and researchers , agency staff , and delivery partners could use the website or the information available through it to accomplish other specific tasks .

for example , as mentioned previously , omb has stated that a purpose of performance.gov is to support coordination and decision making to advance shared goals .

while omb said that this intended use will help provide direction as they take a phased approach to development , the current version of the website gives no indication or examples of the ways that agency staff , or others , such as delivery partners or congressional staff , could use performance.gov to facilitate coordination or communication about goals , activities , and performance between agencies and with interested stakeholders from other sectors .

it is also unclear whether performance.gov includes all the information and design elements necessary to support coordination across agencies .

the president's budget for fiscal year 2014 indicates that , in the future , efforts will be undertaken to test the potential use of performance.gov to facilitate coordination among goal allies , enlist ideas and assistance to accelerate progress on goals , and enhance public understanding of the work of the federal government .

other federal open government websites have indicated how visitors can use them to accomplish specific tasks .

for instance , recovery.gov contains an entire page that outlines what users can do on the site , including how to use the raw data available through the website , report waste , fraud , and abuse , or find job and grant opportunities .

data.gov states that it is designed to enable “the public to participate in government by providing downloadable federal datasets to build applications , conduct analyses , and perform research,” and offers an introductory video that provides information on the purposes of the website and instructions on how to use it .

these websites have also integrated web 2.0 technologies , such as links to social media feeds , to help people share and use of the information available through these sites .

for example , recovery.gov and data.gov have integrated social - networking tools , such as facebook and twitter , blogs , and online discussion forums to facilitate the sharing of information , provide new venues for communication and participation , and enable collaboration and discussions between stakeholders and web - based communities of interest .

recovery.gov and data.gov also have dedicated pages for different audiences that compile and organize relevant resources according to the needs and interests of those audiences .

data.gov , for instance , contains individual pages that allow users interested in specific areas , such as education , health , or energy , to find data and resources on those specific topics and to communicate with others who share those interests .

figure 5 provides a page for the energy community .

to comply with the recommendations of the digital government strategy , the digital services advisory group and the federal web managers council have also encouraged the use of social media to distribute content to large and diverse audiences and more effectively engage with customers .

omb and gsa have been working to ensure that performance.gov complies with the reporting requirements of gprama .

however , if the specific intended uses of performance.gov are not clarified , while taking into consideration what the law requires , it could lead to varying ideas and expectations for how performance.gov should be developed and designed and the audiences it should serve .

the website's accessibility to visitors and its relevance to them could be increased by additional direction on how various audiences could use the information available through the website , such as explanations or navigation assistance , or by tools , such as web 2.0 technologies , that could facilitate that use .

leading practices from howto.gov on the development of federal websites recommend that developers collaborate with officials from other agencies to avoid duplication of effort , use content that already exists , and let organizations with the greatest expertise on a topic create content .

according to howto.gov , this is important because allowing the organization with the greatest expertise to create content can help ensure its accuracy and quality , and linking to existing content can help save time and resources .

howto.gov also recommends that developers engage potential users through focus groups and other outreach ; regularly conduct usability tests to gather insight into navigation , the organization of content , and the ease with which different types of users can complete specific tasks ; and collect and analyze performance , customer satisfaction , and other metrics .

howto.gov states that these efforts are important for collecting and analyzing information about audiences , their needs , and how they are using , or want to use , the website .

this information is also critical to help inform the development of a useful and usable website that meets the needs of intended audiences .

similarly , some developers of other government performance reporting websites who we interviewed reported that it is important to use a developmental approach focused on continuous improvement , providing users with opportunities to give input and feedback , so that websites can be adjusted and improved to meet their needs .

for example , following the initial creation of the state's performance reporting website , officials in maryland analyzed the website's performance metrics to get a sense for how people were accessing the site , as well as the information they were searching for .

they also employed usability testing to collect insight into the navigation and content of the website .

from these insights , they identified the need to make information on related state programs and resources more easily accessible through the website , which is now reflected in its design .

according to omb and agency officials we interviewed , to leverage the expertise of agencies in the development of content for performance.gov , omb established a content management and review process by which each agency shared information and data on their own goals and activities with omb .

omb also developed the performance reporting entry portal ( prep ) , a data input system , which allowed agencies to directly input their information and data on agency priority goals into performance.gov .

for many of the pages on cross - agency management initiatives , they also leveraged information from agencies and other federal websites , linking to existing resources and reports .

omb used other strategies to help maintain a collaborative environment and open lines of communication with federal agencies .

according to omb staff , during the initial development of performance.gov , omb used a pilot project involving several agencies to model different iterations of the website and collect feedback on potential designs from agency staff .

omb staff members also continue to hold biweekly conference calls with agency staff to provide status updates on the development of performance.gov , answer questions , and address concerns .

a gsa staff member supporting the pmlob also recently met individually with staff from 24 agencies to discuss the data requirements for performance.gov and the best way for agencies to provide information to be made available through the website .

according to omb , gsa , and agency staff , they have been able to use this ongoing communication to collect feedback on performance.gov and ideas to gradually improve the design and usability of the system agencies use to input data on agency priority goals .

for instance , the prep system has been changed in response to agency requests so that data from prior time periods will be pre - populated in the system , an improvement that was made to help reduce the burden of data collection on agencies .

according to omb and gsa staff , to gather input from other audiences as they developed performance.gov , they also held briefings on the website for congressional staff and government transparency organizations and performance experts based in washington , d.c .

since 2010 , omb staff said that they met several times with staff from the senate homeland security and governmental affairs committee , house oversight and government reform , and the senate budget committee to discuss the development of performance.gov .

they noted that they used this outreach to stakeholders to identify several specific website modifications .

for example , omb staff heard from stakeholders , including congressional staff , that it was important for performance.gov visitors to be able to understand how agency priority goals relate to broader agency strategic goals and objectives , so omb nested the priority goals within the larger agency goal framework .

omb staff also said that several stakeholders noted that they appreciated the ability to filter goals by different themes , so omb included the capability to apply this feature to all categories of goals and objectives .

of the three congressional staff we spoke with who said they had received briefings on the development of performance.gov , however , only one felt she had been consulted on website input .

also , since 2010 , omb staff reported holding no meetings on the development of performance.gov with staff from other house or senate committees who might use the website to inform their federal agency oversight .

furthermore , while a focus of gprama is to make federal performance information more accessible to the public , efforts to collect input and feedback from interested members of the public and other potential audiences have thus far been limited to the collection of suggestions through the website's “feedback” page .

according to omb staff , the submissions received through this portal are generally on topics unrelated to the website , although they do receive questions about the website or suggestions for minor corrections to content .

they also said that the suggestions received through this portal have not yet had a significant impact on the design of the site .

gsa staff said that usability testing of performance.gov is planned for september 2013 and that the tests will be managed by usability experts from gsa and will include a mixture of participants from within government and from the general public .

from this testing , they hope to inform new development and interface and navigation improvements .

other federal open government websites have used broader outreach efforts to inform their development and design .

for instance , the developers of recovery.gov used both focus groups and usability testing with interested citizens to collect feedback and recommendations and inform the development of the website from its initial stages .

the developers of data.gov also used a technology service called ideascale to collect feedback and ideas from hundreds of users during the initial development of the site .

omb and gsa monitor visitors' use of performance.gov using a variety of metrics .

for example , one metric that is tracked is the total number of visits to performance.gov , by month , as seen in figure 6 .

howto.gov recommends that agencies collect , analyze , and report on a minimum baseline set of performance , search , customer satisfaction , and other metrics , which allow officials to get a holistic view of how well online information and services are delivered .

of the 24 metrics recommended by howto.gov , 15 are currently tracked for performance.gov .

see table 3 for the list of recommended metrics and whether they are collected for performance.gov .

howto.gov also recommends setting goals for metrics , and making sure that these align with the objectives of a website , to help prioritize and guide design changes for improved performance and usability .

these goals can be identified based on prevailing practices or the desire to improve a particular metric over time .

except for customer satisfaction , which is discussed later in this report , omb has not yet established goals for any recommended metrics .

gsa staff acknowledged the need to collect additional recommended metrics for the website .

they said that as they expand the list of metrics that are collected , they are planning usability testing to identify the most appropriate metrics for analyzing the website's use and identifying improvements that will increase the usability of performance.gov .

omb staff also said that right now their primary goal is releasing the legally required information on performance.gov , but they will consider setting targets for certain metrics in the future .

in addition to performance and search metrics , howto.gov recommends that agencies collect customer satisfaction metrics , which include measures of overall customer experience , the completion rate of intended tasks , and the percentage of visitors likely to return and recommend the website .

the digital government strategy requires that agencies use customer satisfaction measurement tools for all .gov websites .

while omb has established a numerical customer satisfaction target for performance.gov , the agency has not established tools on the website , such as online customer satisfaction surveys , that would allow them to collect the data necessary to track the recommended customer service metrics .

gsa staff said that there are no public - focused customer satisfaction tools for performance.gov , outside of the “feedback” page , on the website because , at this stage in the website's lifecycle , the primary focus has been on collecting feedback from congress and agency management .

they said , however , that they have used feedback collected through congressional meetings and feedback from members of the pic , as well as comments from the “feedback” page of the website , to get a general sense of how satisfied some users are with the website .

the “feedback” page , however , does not ask users to rate their overall satisfaction or the quality of their experience .

as stated earlier , according to omb and gsa staff , complying with the time frames and requirements for public reporting in gprama has been the focus of development , although they used outreach to collect feedback from some congressional staff , government transparency organizations , and performance experts to inform changes to the website .

a lack of outreach to other potential audiences and user testing , however , means that developers have not yet had an opportunity to systematically collect input on the information needs and preferences of potential audiences of the website , including interested members of the public , other congressional committees , delivery partners , or others ; whether the content on the website meets the needs of different audiences ; or if it is presented or organized in a way that makes it easily accessible .

furthermore , without collecting all recommended search , and customer satisfaction metrics , and establishing goals or targets for metrics , it may be more difficult for the developers of performance.gov to get a holistic picture of how they are delivering information and to identify and prioritize potential improvements .

omb and gsa staff have said that as the phased development of performance.gov takes place , they expect to use outreach to a broader set of audiences , including members of the public , and usability testing with these audiences to make performance.gov more “public - facing” and “citizen - centric.” while they have developed a general time frame for conducting usability tests , they have not yet established a specific plan , a timeline for other forms of outreach , or indicated the specific audiences they plan to target for greater outreach .

leading practices from howto.gov recommend organizing website content based on audience needs and using common terminology and placement , consistent navigation , and plain language .

howto.gov states this is important because providing consistent terminology and placement across a website helps users easily find what they need as they will know what labels to look for .

consistent navigation makes websites easier to use because users are more likely to find what they need from a website if they are familiar with its navigation scheme .

using plain language also makes the content of a website more accessible by using words that the website's typical user can understand the first time .

lastly , leading practices recommend organizing and categorizing website content to help ensure that it can be found using commercial search engines or a website's internal search engine .

all of these elements factor into the overall usability of a site , which is key to ensuring an effective user experience .

similarly , developers of some government performance reporting websites we interviewed emphasized the importance of designing these websites in a way that makes them easy to access , use , and navigate .

several states have tried to increase the visibility of their performance reporting websites by integrating them with other transparency and open data websites and by including prominent links on other state government websites .

a number of state and local officials also emphasized the importance of making the information and data on a performance reporting website engaging and easily understandable .

they suggested doing this by organizing information so users can quickly locate what is of interest to them through visual displays of data , minimizing the amount of text , avoiding the use of jargon , and ensuring that data is current and timely .

for example , city of boston staff said that visitors generally stay on these sites for limited amounts of time , so they designed the boston about results website to present performance information in a way that can be easily consumed and understood , using brief descriptions of key strategies and graphics to depict performance trends .

figure 7 shows an example of a screen shot from the boston about results website .

performance.gov is accessible from some other federal websites , a key source of potential traffic .

for example , usaspending.gov includes a link to performance.gov on its home page and the “government performance” link on the “topics” page of usa.gov includes performance.gov at the top of a list of links .

other federal websites , however , including data.gov , the white house and omb home pages , and the websites of the five agencies included in our study do not have links to performance.gov .

as one potential user from a government transparency organization pointed out , if performance.gov is not connected in any meaningful way with other federal information and data websites , it may not be a site that government data users would naturally go to .

furthermore , a lack of social media integration on performance.gov could increase the effort required for users to share content from the website .

a lack of other communication tools , such as rss feeds and social media , which can be used to alert users to new or important content and enable website administrators to push out timely and relevant information , could also limit the website's ability to keep interested users informed about new content .

the various sections of performance.gov were designed with a consistent organization and navigation .

this can be seen particularly in the structure of individual agency pages .

as shown in the u.s. department of agriculture example in figure 8 , each page includes a brief overview of the agency's mission , includes links to the agencies existing performance reports , and has three tabs with information on the agency , the agency's goals , and the agency's efforts on government - wide management initiatives .

the “agency goals” tabs also use a consistent , layered structure to provide lists of agency strategic goals , objectives , and priority goals .

according to gsa staff , this platform will also be used to organize the performance.gov information on agency strategic goals and objectives that will be added to comply with the gprama requirement that the website include agency strategic plans , annual performance plans , and annual performance reports .

the use of layering is a common practice in the design of performance reporting websites , as it allows developers to structure the presentation of information in a way that meets the needs of different audiences by allowing them to access the level of information that is most relevant for them .

for example , the approach begins with the presentation of higher - level goals , highlights , or more aggregated information , while allowing those users interested in progressively more detailed goals or information to drill down and find the level of information most appropriate for their needs .

several individuals we interviewed had positive comments on the layered structure used to organize the information on agency strategic goals , objectives , and priority goals within performance.gov and said that using the same format across goals and agencies facilitated navigation .

however , others we spoke with , including some agency officials , raised concerns that the complexity of the website's navigation contributes to the difficulty of finding critical pieces of information on the site , particularly information on individual agency priority goals .

for example , if a user chooses to access information on agency priority goals through the page of an individual agency , the user would need to click through multiple drop downs and tabs , and navigate to a new page , before reaching the tab that has a graphic depicting a specific agency priority goal's progress .

if a user is unaware that this information exists , the user may not be able to easily find it , and performance.gov does not include instructions on how to locate this information .

to facilitate access to information on agency and cross - agency goals , a number of experts and potential users suggested that the performance.gov home page include a thematic index , similar to the filter on the “clear goals” page that currently allows users to search for goals by different themes , like agriculture , energy , or natural resources and environment .

other suggestions for the pages on individual agency priority goals were to make the goals and visualizations of performance trends a more central part of the presentation .

omb staff said that lessons learned through the federal government's past experience with performance reporting have emphasized presenting performance information in an appropriate context with relevant information on agency missions , goals , activities , and factors that influence performance .

according to omb staff , the use of plain language was a focus in trying to make performance.gov easy to understand , and omb held a briefing on plain language for agencies developing content for performance.gov .

however , several potential users we spoke with also noted that some pages on performance.gov include jargon and technical detail that may not be understandable or relevant to certain audiences , including members of the general public .

concerns were also expressed that some pages on performance.gov contain too much text , which can limit the accessibility of information .

our analysis of the ease with which commercial search engines , and performance.gov's internal site search engine , can be used to find relevant information on the website produced mixed results .

when using a commercial search engine to search for several general , government - wide terms , including “federal government goals,” “agency performance,” and “federal agency performance measures,” we found that a link to performance.gov was listed on the first page of results for all of these searches .

we also found that a link to performance.gov was included on the first page of results when using a variety of agency - specific search terms , including “department of agriculture performance,” “epa goals,” “department of labor goals,” “department of interior goals,” and “department of state performance.” the results of using performance.gov's internal search engine to find relevant information on the website were mixed , however .

to analyze the ease with which a user could find relevant agency priority goal pages using the website's search engine , we used topic - focused search terms taken from specific agency priority goals of the five agencies included in our study , including “rural communities,” “greenhouse gas,” “climate change,” “training programs,” and “afghanistan.” for only two of these five searches — on “rural communities” and “afghanistan” — however , did the results include a link to the relevant agency priority goal page .

the results sometimes included links to other potentially relevant pages on performance.gov , such as those with information on related cross - agency priority goals , cross - government management initiatives , or featured stories .

many of the links included in the results direct users to web pages that include agency overviews or lists of agency strategic goals , but which do not appear to be part of the normal performance.gov structure .

figure 9 is an example of one of these pages .

while the opinions of potential users on the organization and navigation of performance.gov were mixed , comments and our analysis of performance.gov's internal search engine indicated the site's overall usability could be limited for potential audiences by several factors: the complexity of the website's navigation , difficulty accessing some information , the lack of an effective internal search engine , and the level of detail and technical information provided .

actions omb and gsa are taking to develop a more public - oriented and usability - focused perspective for performance.gov are discussed later in the report .

leading practices for the development of federal websites recommend defining and documenting how a website will be governed , including the structure of the team managing the website , the roles and responsibilities of different stakeholders , and the policies and procedures that will be used to govern the creation and management of content .

howto.gov states this is important because codifying how a website will be governed , as well as its organizational structure , policies , and procedures provide involved staff with a common understanding of how things are supposed to work .

as noted previously , staff from omb , gsa , and members of the pic are considering whether to create a performance management line of business ( pmlob ) .

according to omb and gsa staff , in an effort to document the potential governance structure of the pmlob and performance.gov , as well as the roles and responsibilities of various key stakeholders and the processes that will be used for major decisions , a draft charter was developed .

omb staff said the charter was completed in may 2013 .

the proposed creation of the pmlob has also offered an opportunity to define the purposes and scope of the system that will be used to centralize the collection of performance data from agencies , as well as how it will be displayed on performance.gov .

for instance , an executive steering committee made up of staff from omb , gsa , and several members and staff from the pic , has been created .

omb staff said that the intent of the committee is to provide a forum for the discussion of potential options and to help determine the requirements that a future iteration of performance.gov and its underlying data collection system must fulfill and what purposes they should be designed to accomplish .

according to omb and gsa staff , they are also developing working groups to examine various issues .

for example , a working group on “business intelligence” has been established through the pic to help determine how to more effectively use the website's performance information to support internal management and coordination .

they are also starting a working group on the presentation and display of performance information through performance.gov .

gsa staff said that members of the pic have been asked to collect agency input on the current version of performance.gov , the reporting burden it presents , what they like and do not like about it , and what they would like to see in future iterations of the website .

potential solutions to address identified issues will then be developed and considered .

omb and gsa staff also said that the knowledge and capacity of the center for excellence in digital government , a component within gsa that provides government - wide support on the use of the web , social media , and other technologies to improve customer service , will be used to help bring a more public - oriented and usability - focused perspective to performance.gov .

a gsa representative emphasized that the development of performance.gov will happen in phases as funding becomes available .

performance.gov has been funded through the e - government fund , which consists of funding appropriated by congress every year.staff said that while funding for the development of the website has been consistent over the past 2 years , it has varied over time .

the staff added that at times funding was only sufficient to cover operational expenses with no funding available for new development .

the gsa representative also noted that while the focus of the pmlob is on making performance information more visible and transparent for congress and the public by replacing printed reports with a publicly accessible website , there is hope that the shift to the pmlob will provide improved , steady governance of performance management across government and greater stability from one administration to another .

omb's and gsa's development efforts for performance.gov have been focused on ensuring that information required by gprama is available on the website .

leading practices for the development of federal websites , however , offer insights into approaches that could be used to guide the development of performance.gov going forward .

for example , clarity about the intended uses of performance.gov could provide additional direction to ensure that the website is developed as a valuable and relevant tool for its audiences , including interested members of the public and congress , agency staff , delivery partners , and other potential audiences .

the practices could also help determine any design changes , such as navigation assistance or the integration of web 2.0 technologies , to facilitate those intended uses .

while omb collected input from some congressional staff , government transparency organizations , and performance experts , the limited outreach to a broader set of potential audiences and the lack of usability testing to date means that omb does not know whether performance.gov meets users needs or how the website could be further developed .

not tracking all recommended performance metrics , particularly those measuring user satisfaction or establishing appropriate goals for certain metrics , may also make it more difficult to analyze the effectiveness of the website and to identify and prioritize potential improvements .

to enhance the value of performance.gov for intended audiences and improve the ability to identify and prioritize potential improvements , we are recommending that the director of the office of management and budget — working with the performance improvement council and the general services administration — take the following three actions: clarify the ways that intended audiences could use the information on the performance.gov website to accomplish specific tasks and specify the design changes that would be required to facilitate that use .

seek to more systematically collect information on the needs of a broader audience , including through the use of customer satisfaction surveys and other approaches recommended by howto.gov .

seek to ensure that all performance , search , and customer satisfaction metrics , consistent with leading practices outlined in howto.gov , are tracked for the website , and , where appropriate , create goals for those metrics to help identify and prioritize potential improvements to performance.gov .

we provided a draft of this report to the director of omb and the acting administrator of gsa for review and comment .

omb staff provided oral comments , and we made technical changes as appropriate .

omb staff agreed with our recommendations .

gsa did not have comments on the report .

we also provided the draft for review and comment to the five agencies with officials we interviewed regarding their perspectives on performance.gov .

none of the five agencies had comments .

we are sending copies of this report to the director of omb and the acting administrator of gsa as well as appropriate congressional committees and other interested parties .

the report is also available at no charge on the gao website at http: / / www.gao.gov .

if you have any questions concerning this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix ii .

this report is part of our response to a mandate to assess initial implementation of the gpra modernization act of 2010 .

specifically , this report examines the extent to which performance.gov incorporates leading practices for the development of federal websites .

this report is the fifth in a series that looks at how agencies are implementing various gprama requirements .

to address our objective , we analyzed information from the performance.gov website and the requirements from gprama regarding the website .

we also examined related guidance , such as the office of management and budget's ( omb ) circular no .

a - 11 , which lays out expectations on the design , development , and implementation of performance.gov .

we reviewed relevant academic and policy literature on performance management and reporting , including our previous reports on performance management .

the literature was selected based on whether it provided relevant information on the reporting requirements in gprama , lessons learned from the development of other federal reporting systems , or insights into performance reporting best practices or lessons learned .

we conducted interviews with staff at omb and the general services administration ( gsa ) , who worked on the design and development of the website .

we reviewed omb and gsa staff's description of the development of performance.gov from our interviews and compared it against criteria established by howto.gov , a source of guidance and leading practices for government websites .

we also conducted an analysis of the ease with which commercial search engines and performance.gov's internal site search engine can be used to find relevant information on perfomance.gov .

lastly , we reviewed a subset of federal open government websites , including recovery.gov , data.gov , and usaspending.gov .

these websites were selected because , like performance.gov , they are used to make government - wide information more transparent through publicly - accessible websites .

we reviewed design features of each , including whether they identified their purposes , audiences , and potential uses , and approaches they used to facilitate access to information .

to further address our objective , we obtained the views and suggestions of potential users of performance.gov .

we interviewed executive branch and congressional staff and a variety of nonfederal stakeholders .

we reached out to groups most likely to use the information on performance.gov because of their management , oversight , advocacy , or academic interest and asked them to review the website prior to our interviews .

we selected five agencies to gather their perceptions on the utility of performance.gov — the u.s. department of agriculture , department of the interior , department of labor , department of state , and the environmental protection agency .

we selected these agencies from those that have not previously been the subject of our gprama mandate work .

we grouped agencies with the same number of priority goals together , and selected the agency in each group that uses the largest number of tools of government — such as direct service , regulations , grants , and loans — to achieve their performance goals .

this process allowed us to consider a mix of agency priority goals , an example of each of the tools of government , and performance improvement officers under both career and political appointments .

we also interviewed staff from 13 different majority and minority congressional oversight and authorizing committee offices of the u.s. senate and house of representatives to gather their views on performance.gov and any suggestions they had for improving it .

we interviewed congressional staff from the committees shown in table 4 .

we also interviewed a variety of nonfederal stakeholders including representatives from 10 academic , advocacy , transparency , and public policy organizations to solicit feedback about the use of performance reports , their use of the website , and suggestions they had for improving it .

we interviewed academics and practitioners from the california state university san bernardino ; government finance officers association of the united states and canada ; government performance coalition ; socrata ; university of victoria ; and the wilfrid laurier university .

our advocacy and transparency group representatives were from the center for effective government ; coalition for evidence - based policy ; openthegovernment.org ; and the project on government oversight .

we selected these academics and practitioners on the basis of our literature review and recommendations from other performance management and reporting academics and practitioners .

the results of these interviews are not generalizable to all nonfederal stakeholders but provided insights into perceptions of performance.gov .

in addition , based on a review of government performance reporting websites , we selected a group of 12 websites from state and local levels , as well as one from canada , to gather lessons learned and best practices from representatives of these performance reporting websites that were relevant to the design and development of performance.gov .

to identify examples of web - based performance reporting by state governments , we visited the main government websites of all 50 states , or their governor's website , and searched each one using the search term “performance” to identify any relevant agency or government - wide websites providing performance information in a web - based format .

we selected only sites that presented performance information through web pages ; we did not select transparency websites that are generally designed to make available information on state finances and the distribution of state expenditures .

to identify relevant examples of web - based performance reporting by local governments , we conducted a search similar to the state sites and talked to representatives from the local websites who agreed to be interviewed .

to identify international performance reporting websites , we visited the websites of several national governments of english - speaking countries recognized for performance reporting — canada , the united kingdom , australia , and new zealand — to examine if these governments had developed government - wide performance reporting websites .

through this approach , we identified the canadian government's “planning and performance gateway,” which consists of the canada's performance and overview of government spending and performance websites .

none of the other countries had government - wide performance reporting websites .

table 5 provides a list of the state , local , and canadian performance reporting systems we selected to review with representatives who agreed to be interviewed .

we conducted our work from july 2012 to june 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , elizabeth curda , assistant director ; judith kordahl ; and adam miles made key contributions to the report .

