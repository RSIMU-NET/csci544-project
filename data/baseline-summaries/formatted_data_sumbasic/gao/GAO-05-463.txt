complete and accurate data from the decennial census are central to our democratic system of government .

as required by the constitution , census results are used to apportion seats in the house of representatives .

census data are also used to redraw congressional districts , allocate billions of dollars in federal assistance to state and local governments , and for many other public and private sector purposes .

failure to deliver quality data could skew the equitable distribution of political power in our society , impair public and private decision making , and erode public confidence in the u.s. census bureau ( bureau ) .

to ensure it delivers accurate data , the bureau employs a number of quality assurance programs throughout the course of the census .

one such effort during the 2000 census was the count question resolution ( cqr ) program , which enabled state , local , and tribal governments to formally challenge the counts of housing units and “group quarters” ( dormitories , prisons , and other group living facilities ) , and their associated populations .

bureau personnel could initiate a review of the counts as well .

although the bureau did not design cqr with the intention of incorporating any of the corrections that resulted from it into census 2000 data products — including the numbers used for congressional apportionment and redistricting ( figures commonly referred to as “public law data” ) — governmental entities could use the updated information when applying for federal aid that uses census data as part of an allocation formula , as well as for other purposes .

because the count corrections could have political and financial implications for states and localities , it was important for the bureau to carry out cqr consistent with its protocols .

cqr began on june 30 , 2001 , and no new submissions were accepted after september 30 , 2003 .

this letter responds to your request to review the conduct of the cqr program .

as agreed with your offices , we reviewed the results of the cqr program and assessed whether the program was consistently implemented across the country .

in doing this , we paid particular attention to the extent to which the bureau reviewed the census data for errors that could have been caused by broader , more systemic problems .

we also evaluated how well the bureau transitioned to cqr from an earlier quality assurance program called full count review .

to meet these objectives , we reviewed relevant program documents and examined case files and conducted on - site inspections at four of the bureau's regional offices where some of the largest cqr corrections took place .

we also interviewed officials and staff responsible for administering the cqr program at the bureau's headquarters and 12 regional offices .

we did our audit work between february 2004 and march 2005 in accordance with generally accepted government auditing standards .

the bureau launched the cqr program on june 30 , 2001 , as the last in a series of quality assurance initiatives aimed at improving the accuracy of 2000 census data ( see fig .

1 ) .

specifically , the cqr program provided a mechanism for state , local , and tribal governments to have the bureau correct errors in certain types of census data .

the bureau referred to these challenges as “external cases.” bureau personnel could also initiate reviews of suspected count errors , independent of these challenges , for further review .

these were known as “internal cases.” many of the internal cases were unresolved issues inherited from full count review .

indeed , when the full count review program began , the bureau planned to fold unresolved issues from that program into cqr .

the bureau accepted no new submissions after the program officially ended on september 30 , 2003 , although it continued to review challenges submitted before the deadline and completed the final revisions in the summer of 2004 .

three types of corrections were permissible under the cqr program: ( 1 ) boundary corrections , where a jurisdictional boundary of a functioning governmental unit was in the wrong location ; ( 2 ) geocoding corrections , where the bureau placed the living quarters and their associated population in the wrong location ; and ( 3 ) coverage corrections , where the bureau properly enumerated specific living quarters and their corresponding population during the census but incorrectly added or deleted the information during data processing .

bureau officials were to research cases using existing bureau data gathered during the 2000 census ; they could not conduct any new fieldwork to resolve count questions .

the bureau required governmental entities to accompany their challenges with specific documentation before it would investigate their claims .

importantly , under the design of cqr , if a governmental unit had evidence that the bureau missed housing units or group quarters that existed on census day 2000 ( april 1 ) , but the bureau's records indicated that all of the bureau's boundary information , geocoding , and processing were properly implemented , the bureau would not change the data .

rather , the bureau was to address this as part of the planning process for the 2010 census .

if the cqr program corrected the population or housing unit counts of a particular entity , the bureau was to issue revised , official figures for that jurisdiction .

the governmental unit could then use the updated numbers for future programs requiring 2000 census data .

cqr corrections were also used to modify annual post - censal estimates beginning december 2002 and were publicized on the bureau's census 2000 and american factfinder web sites ( www.census.gov and www.factfinder.census.gov , respectively ) , as part of the 2000 census notes and errata .

however , cqr was not designed or publicized as a mechanism to correct the census results for purposes of apportionment and redistricting .

in compliance with legal requirements , the bureau produced apportionment data by december 31 , 2000 , and redistricting data by april 1 , 2001 ( this information is known collectively as public law data ) .

although the law does not require that states use census data to redraw the boundaries of congressional districts , most states have always done so .

nothing would preclude the states from using the corrected data for redistricting .

the general perception of the impartiality of the bureau and the great cost and administrative effort required to take a census have been strong arguments in favor of using the bureau's data .

as agreed with your offices , we assessed whether the program was consistently implemented across the country , paying particular attention to the extent to which the bureau reviewed the census data for errors that could have been caused by broader , more systemic problems , such as shortcomings with a particular census - taking procedure .

we also evaluated how well the bureau transitioned from an earlier quality assurance program used in the 2000 census , full count review .

to assess the implementation of the cqr program , we obtained a headquarters perspective by reviewing program documents and case files at the bureau's offices in suitland , maryland , as well as program results reported on the bureau's web site .

as part of this assessment , we reviewed the program's internal controls , especially those controls related to ensuring data quality .

we also interviewed bureau officials responsible for administering the program .

to determine how cqr was implemented in the field , we visited 4 of the bureau's 12 regional offices — charlotte , north carolina ; denver , colorado ; kansas city , missouri ; and los angeles , california ( see fig .

2 ) .

we selected these regions because they included the six states and 10 governmental units within those states where the largest cqr count revisions occurred .

we supplemented these cases by selecting an additional seven states and 61 places within the four regions for further examination .

the 61 localities were selected because they represented the full spectrum of cqr cases and were geographically diverse .

at each of the four regions , we reviewed regional case file information and interviewed bureau personnel responsible for implementing cqr , such as program managers and geographers .

we also made a site visit to at least one type of facility — including prisons , apartment buildings , and dormitories — in each region to understand firsthand the nature of the errors and the corrections made .

to augment these four regional visits and obtain a more complete picture of how cqr was implemented , we used a structured telephone interview to elicit information from program officials at the bureau's eight remaining regional offices that we did not visit in person .

to determine the extent to which the bureau reviewed census data for systemic errors , and its procedures for folding unresolved cases from the full count review program into cqr , we examined program manuals , memoranda , and other documents , and interviewed officials in the bureau's headquarters and all of its regional offices .

as part of this effort , we also analyzed the cqr case - tracking data in an attempt to determine the number of unresolved full count review cases that were rolled into the cqr program .

however , we were unable to do this because the tracking system did not contain information on which cqr cases originated as full count issues .

we requested comments on a draft of this report from the secretary of commerce .

on may 20 , 2005 , we received the acting deputy secretary's written comments and have reprinted them in appendix iii ; we address them in the agency comments and our evaluation section of this report .

overall , the cqr program corrected data affecting over 1,180 of the nation's more than 39,000 governmental units .

the revisions impacted a range of housing types including private homes with only a handful of residents , to college dormitories and prison cell blocks with populations in the thousands .

at the same time , however , we identified several shortcomings with the cqr program , including inconsistent handling of internal cases by the bureau's regional offices and inaccurate data being posted to the bureau's public web site .

moreover , while cqr found the counting of group quarters in their correct location — a problem known as geocoding error — to be particularly challenging , the bureau did not perform a nationwide review of these known trouble spots , and thus missed an opportunity to improve the accuracy of the data for these dwellings .

nationwide , the cqr program corrected count errors involving governmental units in 47 states , puerto rico , and the district of columbia ( see fig .

3 ) .

three states — maine , new hampshire , and rhode island — had no cqr corrections .

the corrections affected over 1,180 governmental units in the united states .

although this is a small percentage of the nation's more than 39,000 governmental units , the impact of those changes on local governments was , in some cases , substantial , and could have implications for federal assistance and state funding programs that use census numbers in their allocation formulas , as well as other applications of census data .

for example , officials in one kentucky county challenged the geocoding of a housing unit located near new precinct and congressional district boundaries .

they told the bureau that the new boundaries split the county , and they were concerned that the geocoding error would affect where the housing unit's few occupants registered to vote .

because the housing unit was improperly geocoded , the bureau corrected the data .

with respect to fiscal effects , the controller of the state of california uses population figures as the basis for refunding a portion of state taxpayer fees — including automobile licensing fees — to cities and counties .

because of an error in the 2000 census , soledad , california , officials estimated it lost more than $140,000 in state refunds when over 11,000 residents were incorrectly counted in two nearby cities' populations , according to city and state officials .

although cqr eventually corrected the error , soledad did not recover the funds that went to the other cities .

other examples of large cqr corrections include the following ( see app .

i for a complete list of state - level population changes ) : north carolina's population count was reduced by 2,828 people , largely because the bureau had to delete duplicate data on almost 2,700 students in 26 dormitories ( see fig .

4 ) at the university of north carolina ( unc ) at chapel hill .

the erroneous enumerations occurred , in large part , because of mistakes that occurred in various preparatory activities leading up to the 2000 census .

 ( see app .

ii for a more detailed discussion of this incident. ) .

the population count of morehead , kentucky , increased by more than 1,600 when cqr found that a large number of students from morehead state university's dormitories were erroneously excluded from the city's population .

during the 2000 census , the bureau had incorrectly identified the dormitories as being outside city limits and in an unincorporated area of rowan county .

the population count of cameron , missouri , was off by nearly 1,500 people when the bureau found that the prison population of the state's crossroads correctional center was inadvertently omitted from the town's headcount ( see fig .

5 ) .

the correction to the town's population accounted for the entire 1,472 person increase in missouri's total population under the cqr program .

the population of the city of waseca , minnesota , increased by more than 1,100 .

the 2000 census had mistakenly included the prison population for the waseca federal correctional institute in two surrounding townships in waseca county .

the cqr program resulted in the population being shifted to the city .

the population of colorado increased by more than 750 in large part because a processing error in counting housing units in grand junction initially excluded almost 700 people from the city's population total .

the population of denver and arapahoe counties in colorado shifted by more than 900 because the bureau had incorrectly assigned the location of two apartment complexes .

as a result of cqr , the apartment complexes were incorrectly identified and counted as being in denver but under cqr were later found to be in adjoining arapahoe county .

the bureau's 12 regional offices did not always adhere to the same set of procedures when developing internal cases , and this , in turn , produced uneven results .

importantly , the procedures used to execute public programs need to be well documented , transparent , and consistently applied in order to ensure fairness , accountability , defensible decisions , and reliable outcomes .

to do otherwise could raise equity questions .

one variation in the way internal cases were handled was evident at the bureau's los angeles regional office , which appeared to be the only region to do comprehensive methodical research to actively identify possible count errors beyond those that were submitted by governmental entities .

according to the office's senior geographer , the geography staff developed a structured approach to systematically examine census data from all the prisons and colleges within the office's jurisdiction , because the data on both types of group quarters were known to be problematic .

he added that the more problems they found , the more they were motivated to keep digging .

the geographer noted that the in - depth review was possible because the los angeles region covers only the southern half of california and the state of hawaii , and thus has fewer governmental units compared to the bureau's other regional offices .

had it not been for the los angeles region's self - initiated and systematic review , certain data errors would have gone uncorrected because they were not identified by the affected jurisdiction .

for example , regional staff found instances where college dormitories were counted in the wrong geographic location , which , in turn , affected the population counts of their surrounding locales .

such was the case with california state university monterey bay ( csumb ) and the university of california at santa barbara ( ucsb ) .

as a result , the bureau transferred a population of more than 1,400 between the towns in which they were initially counted and in which csumb is located and shifted a population of more than 2,700 between the city and the unincorporated area of the county in which ucsb is located .

the bureau's charlotte region , while also more active than the bureau's other offices that generated internal cqr cases , seemed to be less methodical and comprehensive than los angeles in its approach .

for example , although charlotte geographers detected the duplicate count of almost 2,700 students at the university of north carolina mentioned in the previous section , their research was not the result of any systematic review .

rather , it came about largely because of the curiosity of key employees in the charlotte office , who were also alumni of the school .

 ( see app .

ii for more details on the circumstances surrounding the duplicate count. ) .

vague guidance was one reason for the disparate handling of internal cases .

for example , the cqr procedural manual indicates that the bureau's 12 regional offices were to research cqr cases “as appropriate.” however , the manual did not define whether this meant that the regional offices should initiate their own data reviews or merely verify cqr cases submitted by governmental units .

more generally , numerous geographers we interviewed — the primary users of the manual — did not find it user - friendly , noting it was confusing , complex , or impractical .

for example , a geographer pointed out that the manual did not have an index covering the eight chapters and 26 appendixes , which would have helped them more quickly find information and procedures .

in addition , we found that the manual and other documents did not discuss how program staff were to address full count review issues .

the bureau's training was also problematic and likely added to the implementation disparities .

for example , geographers in five regions told us that during training they were instructed or given the impression they were not to generate additional internal cases beyond the small number of count errors that had already been identified at the beginning of the program .

also , geographers in two of these regions told us they were specifically told not to investigate any count errors they found that were outside the scope of the cases that governmental units submitted .

conversely , geographers in the other seven regions said they were not restricted in any way .

there were other training issues as well .

according to the bureau's draft cqr program assessment , the final version of which is pending , some training materials were developed at the last minute and were never finalized , and training began before needed software was in place at all the research divisions .

proper training was particularly important because , as the draft evaluation notes , staff assigned to the cqr program had census experience but limited geographic and field operations knowledge .

others had limited or no census 2000 software program experience .

federal internal control standards call on agencies to employ edit checks and other procedures to ensure their information processing systems produce accurate and complete data .

however , the bureau's internal controls in reporting cqr results were insufficient in that we found , after a partial review , a number of instances where the bureau disseminated inaccurate data on its web site where it maintains an errata report that lists the cqr revisions to the 2000 census data .

specifically , after comparing data from the errata report to the certified numbers in the cqr case files , we found errors with the reporting of cqr housing , group quarters , and population counts .

importantly , our review found that the revised , certified figures the bureau provided to affected jurisdictions were correct .

this is significant because the affected jurisdictions could use these updated numbers for revenue sharing and other programs that require census data .

however , users who obtain information from the bureau's errata report — these can be people in academia , government , and the private sector — would not have the most up - to - date information .

for example: the original state - level total housing unit count for delaware mistakenly excluded 30,000 housing units .

the revised housing unit count for the minnehaha county portion of sioux falls , south dakota , was underreported by almost 47,000 housing units .

the burlington county , new jersey , revised total housing count mistakenly excluded about 145,000 units .

the errata report excluded 8 of the 12 american indian and alaska native areas that had revisions to their housing , group quarter , or total population counts .

the bureau later corrected these errors after we brought them to its attention .

although the bureau had controls in place to ensure accurate research and reporting , the problems we found point to the need for the bureau to tighten its procedures to prevent mistakes from slipping into its data products .

for example , the cqr manual included some quality control steps , such as having headquarters divisions review field research and results .

further , field geographers told us they consulted one another about questions or procedures and checked each other's work , and bureau program managers had procedures in place to review final revisions and certify them as correct .

documents in the cqr case files we reviewed substantiate these practices .

also , managers told us they randomly checked data entered into the files that are the basis for the revisions posted to the errata report .

still , the number of errors we found after only a partial review of the errata files raises questions about how effectively the bureau implemented the quality assurance procedures , as well as the quality of the data we did not review .

it also underscores the importance of adequate control activities to prevent these problems from recurring .

data users may have encountered problems trying to access certain information from the web - based errata report .

additionally , because there was no link or cross - walk between some of the initial population data the bureau released and the cqr revisions , users may have been unaware that some of the original numbers had been revised .

as shown in figure 6 , the bureau's web - based errata report presented revised data for states , american indian and alaska native areas , and other jurisdictions at the state or similar geographic level .

although the table had embedded links that were supposed to take users to revisions at lower levels of geography , these links did not always work and produced error messages instead .

we found that unless the users' software and internet access paralleled the bureau's , users could not access the more detailed data using the embedded links .

bureau staff involved with posting the data to the web site stated in the summer of 2004 that they were aware of the problem , but as late as march 2005 , the problem had yet to be fixed .

at the same time , the cqr revisions may not be evident to users who access certain data from the 2000 census data posted elsewhere on the bureau's web site .

this is because these sites lack notes or flags informing users that updated figures are available in the census errata report .

for example , the bureau's american factfinder web site — the bureau's primary electronic source of census data — does not inform users that revised data on group quarter counts , including the number of correctional institutions , as well as data on their associated populations , exist as part of the bureau's notes and errata report .

american factfinder presents data known as summary file 1 ( sf - 1 ) , which is the first data set the bureau produces from the census , and is used for purposes of apportionment and redistricting .

while the sf - 1 data remain unchanged , other data users may find the revised numbers better suited to their needs .

figure 7 illustrates the existence of two sets of numbers without any explanation .

summary file data from american factfinder show the population for soledad , california , as 11,263 .

however , the bureau's errata report , which reflects the cqr revisions , shows the soledad population at 23,015 .

because american factfinder lacks notes or links that tell users about the revised data , users might inadvertently obtain erroneous information .

according to bureau officials , while they thought about adding notes directing users to the cqr revisions , they decided against it because they thought it would confuse more people than it would help .

they reasoned that knowledgeable users , such as county planners and state data center staff , are likely aware of the cqr information and would therefore not need to be informed about the existence of the notes and errata web site .

the errors uncovered by the cqr program highlight some of the limitations in the way in which the bureau builds its address list for the decennial census , particularly in the procedures used to identify and locate group quarters .

for the 2000 census , the bureau had three operations that were primarily designed to locate these types of dwellings .

however , given the number of prisons and other group quarters geocoded to the wrong location , refinements are needed .

moreover , the bureau's draft cqr program assessment found that the bureau's master address file had numerous data entry errors including incorrect spellings , geocoding , and zip codes .

to its credit , the bureau is planning several improvements for 2010 , including integrating its housing unit and group quarter address lists .

this could help prevent the type of duplicate counting that occurred at unc when the same dormitories appeared on both lists .

likewise , the bureau's planned use of a satellite - based navigational system could help census workers more precisely locate street addresses .

the cqr program was preceded by a quality assurance program called full count review , which ran from june 2000 through march 2001 and , like cqr , was designed to find problems with the census data .

however , although the bureau planned to fold unresolved full count issues into cqr , many full count issues were rejected from cqr because the latter program had more stringent documentation requirements .

as a result , the bureau was unable to resolve hundreds of additional data issues .

under the full count review program , analysts were to identify data discrepancies to clear census data files and products for subsequent processing or public release .

analysts did so by checking the data for their overall reasonableness , as well as for their consistency with historical and demographic census data and other census data products .

the types of issues flagged during full count review included potential discrepancies involving the counts and / or locations of group quarters , housing units , and individual households , among others .

as we noted in our july 2002 report , full count review identified 4,809 potential data anomalies .

however , of these , just five were corrected prior to the december 31 , 2000 , release of apportionment data and the april 1 , 2001 , release of redistricting data .

the corrections included a military base , a federal medical center , and multiple facilities at two prisons and a college that were counted in the wrong locations .

that the public law data were released with numerous data issues of unknown validity , magnitude , and impact , gave us cause for concern , and we noted that the bureau missed an opportunity to verify and possibly improve the quality of the information .

when the full count review program began , the bureau planned to fold unresolved issues from that program into cqr .

indeed , according to a june 2000 memo on cqr policy agreements , “a by - product of [full count review] is documentation of unresolved issues for potential use in cqr.” however , because the cqr program had more rigorous documentation requirements before it would accept a case compared to full count review , a number of issues that were deemed suitable for full count review but were unresolved , were rejected from cqr .

of the 4,804 issues remaining after full count review , 2,810 issues ( 58 percent ) , were not referred to cqr .

of the 1,994 issues ( 42 percent ) that were referred to cqr , 537 were actually accepted by the program .

the remaining 1,457 issues referred to cqr did not meet the bureau's cqr documentation requirements and , consequently , the bureau took no further action on them .

the full count training materials we examined as part of our 2002 review did not provide any specific guidance on the type of evidence analysts needed to support data issues .

rather , the materials instructed analysts to supply as much supporting information as necessary .

in contrast , the cqr program had more rigorous documentation requirements .

guidance available on the bureau's web site required governmental units to supply maps and other evidence specific to the type of correction they were requesting , or the bureau would not investigate their submissions .

simply put , full count review identified hundreds of data issues but lacked the time to investigate the vast majority of them .

then , when the remaining cases were referred to cqr , most were rejected because they could not meet cqr's higher evidentiary bar .

the bureau lacked a program specifically designed to correct individual count errors contained in the apportionment and redistricting data .

because these numbers were later found to be flawed for some jurisdictions , as the bureau proceeds with its plans for the 2010 census , it will be important for it to explore options for reviewing and correcting this essential information before it is released .

precision is critical because , in some cases , small differences in population totals could potentially impact apportionment and / or redistricting decisions .

for example , according to an analysis by the congressional research service , under the formula used to apportion seats in the u.s. house of representatives , had utah's population count been 855 persons higher , it would have gained an additional congressional seat and north carolina would have lost a seat .

however , had the duplicate unc count and other errors detected by the cqr program as of september 30 , 2003 , been uncovered prior to the release of the public law data , the already narrow margin determining whether utah gained a house seat would have dropped to 87 persons .

although in this particular instance there would not have been a change in congressional apportionment , it illustrates how the allocation of house seats can be determined by small differences in population counts .

better planning could have improved the cqr program in other ways .

for example , the bureau's draft evaluation of cqr found , among other issues , that the three teams working on the planning and development phases of cqr should have tested implementation plans earlier in the process , and training materials were not based on the bureau's experience in conducting the 1990 cqr program .

also , there was no mechanism to prioritize cases based on the magnitude of the error .

as a result , regional offices wound up expending considerable resources on cqr cases that only affected a handful of dwellings .

the draft evaluation also found that the two software applications the bureau chose to administer and track cqr cases did not appear to be up to the task .

lost cases and documentation , poor integration with other applications , and the inability to produce reports were among the issues the evaluation cited .

more generally , the integration and coordination issues that affected cqr are not unique to that program ; to the contrary , our past reports have found that other components of the 2000 census were not well planned , which unnecessarily increased the cost and risk of the entire enumeration .

the need for better strategic planning has been a consistent theme in many of our past recommendations to improve the bureau's approach to counting the nation's population and represents a significant management challenge that the bureau will need to address as it looks toward 2010 .

the bureau is beginning to develop plans for full count review and cqr for the 2010 census .

as it does so , it will be important for it to develop an initiative or consolidated program that corrects both systemic and individual issues , and does so prior to the release of apportionment and redistricting data .

granted , this effort will be no simple task given the relatively short time between the closure of the local census offices and the need to release the public law data within the legally required time frames .

still , there are steps the bureau can explore to methodically check the data for nationwide systemic errors , obtain local input , and investigate any discrepancies , and do so in an expeditious manner .

one approach might be to consolidate and leverage cqr , full count review , and certain other bureau programs .

indeed , under the full count review program , the bureau obtained local input by contracting out some of the work to members of the federal - state cooperative program for population estimates ( fscpe ) , an organization composed of state demographers that has worked with the bureau since 1973 to ensure accurate population counts .

the bureau worked with fscpe , in part , because it lacked sufficient staff to complete the review on its own , but also because the bureau believed that the members' knowledge of the demographic characteristics of their states could help the bureau examine data files and products , including public law data .

fscpe members reviewed data for 39 states and puerto rico ; bureau employees reviewed data for the remaining states and the district of columbia without fscpe representation in full count review .

both sets of analysts checked the data for their overall reasonableness , as well as for their consistency with historical and demographic data , and other census data products .

bureau staff from its regional offices reviewed the data as well .

they focused on identifying inconsistent demographic characteristics and did not necessarily concentrate on any one particular state or locality .

thus , the bureau obtained local input that focused on individual states and smaller jurisdictions , and also performed its own , broader review .

verifying any data discrepancies could be accomplished by beginning the count correction effort as local census offices complete nonresponse follow - up , when enumerators are still available to investigate issues .

in fact , the bureau is already planning to do this to some degree in 2010 under another operation called coverage improvement follow - up ( cifu ) , where the bureau is to call or visit housing units that have been designated as vacant or nonexistent but not confirmed as such by a second source .

in the 2000 census , cifu began june 26 , 2000 , and ended on august 23 , 2000 .

during that time , enumerators contacted 8.9 million housing units and counted 5.3 million people , according to the bureau .

the bureau could explore adding the count correction workload to enumerators' cifu assignments , which would enable the agency to reconcile possible data errors , as well as add any housing units and group quarters the bureau missed during the initial enumeration ( as noted in the background section , cqr could not add any residences that existed on census day but the bureau had failed to count. ) .

further , the bureau could help automate the count correction process by using computers to flag any data that exceed any predetermined tolerances .

the bureau could also develop a system to prioritize count correction issues to help manage its verification workload .

importantly , to the extent the bureau reviews and corrects census counts prior to the release of the public law data , the bureau might not need separate full count review and cqr programs ; a consolidated effort might be more cost effective .

at a minimum , to the extent a separate cqr program is needed , it may not have to be as large or last as long because presumably the earlier program would have caught the bulk of the problems .

regardless , given the possibility that similar data errors might again occur during the 2010 census , exploring options for resolving them prior to the release of public law data would be a sound investment .

reapportionment and redistricting data would be more accurate ; the bureau's credibility would be enhanced ; and the need for a large - scale count correction program along the lines of cqr could be reduced or eliminated .

the cqr program played an important role in improving the quality of data from the 2000 census .

although the net changes in housing and population counts from the program were small on a national scale , in a number of instances , they were substantial at the local level , and could affect various revenue sharing formulas and other programs that use decennial census data .

because the program functioned as a safety net — a final opportunity to catch and correct mistakes that occurred along the chain of events that led to , and extended beyond census day 2000 — the results shed light on the performance and limitations of certain upstream census operations , and areas where the bureau should focus its efforts as its plans unfold for 2010 .

in this regard , the following is clear: although the bureau puts forth tremendous effort to ensure a complete and accurate census , its numerous procedures and quality assurance operations will be challenged to stay ahead of the increasing difficulties associated with enumerating a population that is growing larger , more diverse , and increasingly hard to locate and count .

the timing of any count correction effort will also be critical .

indeed , we are concerned that key decisions using data from the 2000 census employed figures that , for a number of jurisdictions , were later found to be flawed .

as a result , it will be important for the bureau to consider developing a count correction initiative that can complete its work in time to correct the public law data before that information is released .

moreover , beyond the inherent demographic obstacles to a successful census , the results of our cqr review echo several of our past reports on other aspects of the census , which note that some of the bureau's difficulties stem from a lack of adequate strategic planning and other management challenges .

ultimately , the success of the 2010 census will hinge on the extent to which senior bureau leadership resolves these challenges .

with this in mind , resolute action is needed across three fronts .

first , it will be important for the bureau to ensure , via thorough field testing , that its planned changes to its address list development procedures help resolve the geocoding and other operational problems revealed by cqr .

second , it will be important for the bureau to improve its count correction efforts by designing a program that can systematically and consistently review the public law data and make any corrections prior to the release of those figures .

third , it will be important for the bureau to address persistent strategic planning challenges .

to help ensure the nation has the best possible data for purposes of apportionment , redistricting , and other uses of census data , we recommend that the secretary of commerce direct the bureau to improve its count correction efforts for the 2010 census by taking such actions as: 1 .

thoroughly testing improvements to the bureau's group quarters and other address list development activities to help ensure the bureau has resolved geocoding and other problems with its master address file .

2 .

consolidating full count review and cqr into a single program that systematically reviews and corrects any errors in the public law data prior to their release .

3 .

expediting count correction efforts by initiating data reviews toward the end of nonresponse follow - up , when the bureau starts getting complete data for geographic entities , and enumerators are available to help investigate any discrepancies .

as part of this effort , the bureau should consider using computers to systematically search for possible errors nationwide by checking data at the appropriate level of geography to ensure population , housing unit , and group quarter counts , as well as demographic characteristics , appear reasonable and are consistent with population estimates .

those areas that are outside of predetermined tolerances should be flagged for further review .

the bureau should also pay special attention to ensure group quarters are properly geocoded and counted .

4 .

prioritizing the investigation of errors based on the magnitude of the suspected error or similar triaging formula .

5 .

ensuring that instructions on the bureau's web site make it clear that updated information exists and that users can readily access this information .

6 .

improving the bureau's quality assurance procedures to help ensure there are no mistakes in the data the bureau posts on its web site .

7 .

enhancing the training and guidance provided to regional offices to help ensure they share the same understanding of their roles and responsibilities and will implement the program consistently .

8 .

addressing persistent strategic management challenges , in part , through early testing to help ensure information systems , training , and other activities are fully integrated .

the acting deputy secretary of commerce provided written comments on a draft of this report on may 20 , 2005 , which are reprinted in appendix iii .

commerce stated that “the report provides a good overview of program results and makes several useful observations and recommendations,” and specifically agreed with our finding that the process for conducting internal reviews was not consistently implemented .

more generally , however , commerce believes the shortcomings we describe reflect “a fundamental misunderstanding of the goals of the cqr program,” and noted that our observations and recommendations indicate we believe that cqr should have been designed to correct the public law data before they were released during the 2000 census .

our concern over the cqr program centers on the way it was implemented in 2001 , rather than the fact that the bureau did not design the program to correct the apportionment and redistricting numbers .

we agree with commerce that this was not the intent of cqr and , as commerce notes , we acknowledge this in our report .

at the same time , based on the lessons learned from the 2000 census , enumeration errors are almost inevitable .

thus , our recommendations focus on the future , and specifically , the importance of developing mechanisms for the 2010 census to review and correct errors in the public law data to the greatest extent possible before they are released .

we have clarified the report to better reflect gao's position .

commerce specifically addressed two of our eight recommendations , disagreeing with both of them .

with respect to our recommendation to consolidate full count review and cqr into a single program for the 2010 census , commerce noted that preliminary counts at the census tract or block level are needed to conduct an effective cqr program , and that information is not available until close to the deadline for releasing the apportionment data .

commerce maintains there would be little opportunity for local entities to review the counts and document potential problems and even less time for the bureau to conduct the necessary research and field work .

our report recognizes that it would be a challenge for the bureau to review and correct census figures and still release the public law data by the legally required time frames .

still , as we note in our report , we believe the bureau could expedite the process by taking such steps as ( 1 ) using computers to check census data for their overall reasonableness and flagging areas that exceed predetermined tolerances ; ( 2 ) focusing on known trouble spots such as group quarters ; and ( 3 ) beginning the review process earlier , such as , when local census offices complete their nonresponse follow - up efforts .

moreover , as we state in the report , during the 2000 census , the bureau already had programs in place that obtained local input on the census numbers before the release of the public law data ( full count review ) , and conducted extensive field operations to investigate certain discrepancies ( coverage improvement follow - up ) .

we believe that it will be important for the bureau to not simply replicate these programs for the 2010 census or make incremental improvements , but to see whether these programs could be better leveraged and be more strategically employed to improve the accuracy of the apportionment and redistricting data .

the other recommendation that commerce specifically addressed was our call for the bureau in 2010 to prioritize the investigation of errors based on the magnitude of the suspected problem .

commerce maintains that the bureau's policy in 2000 was to handle cases in the order they were received from local jurisdictions , and asserts this was a fair and reasonable practice .

while this practice is not unreasonable , we continue to believe that it would be more cost - effective for the bureau to give priority to those cases where it could achieve a greater return on its investment in resources ( especially given our findings involving group quarters such as prisons and college dormitories that affected relatively large population clusters ) .

our recommendation echoes the bureau's draft evaluation of the cqr program , which noted that regional offices expended considerable resources on cqr cases that affected only a handful of dwellings .

moreover , as we state in our report , prioritizing the bureau's workload could help expedite the count correction process .

commerce's comments also included some technical corrections and suggestions where greater clarity was needed .

we revised the report as appropriate .

we will send copies of this report to the chairman of the house committee on government reform , the secretary of commerce , and the director of the u.s. census bureau .

copies will be made available to others on request .

this report will also be available at no charge on gao's home page at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or williamso@gao.gov .

contact points for our office of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

the duplicate counting of nearly 2,700 students at the university of north carolina ( unc ) at chapel hill during the 2000 census resulted from a combination of factors .

the incident is interesting because it shows how the various safety nets the bureau has built to ensure an accurate count can be undermined by human error , the limitations of census - taking operations , and other events that in some cases occur years before census day ( april 1 , 2000 ) .

the duplicate count was discovered after cqr began when the director of the charlotte regional office ( a unc graduate ) , asked one of her geographers ( also a unc graduate ) , to see whether the unc dormitories were counted in their correct locations .

according to the geographer , the director's curiosity was aroused after the cqr program found problems with the geocoding of dormitories at other schools in the charlotte region .

the geographer told us he initiated an internal cqr case in the summer of 2001 after discovering that two unc dormitories were geocoded to the wrong census block .

upon further research , where he reviewed information from the census address file and the unc web site , the geographer concluded that , in addition to the geocoding error , a large number of dormitories and their occupants were counted in error .

ultimately , by matching census records , the bureau determined that 1,583 dormitory rooms in 26 buildings — and the 2,696 students who had resided in them — were included twice in the 2000 census .

on the basis of our interviews with bureau staff and review of pertinent documents , the following sequence of events led to these erroneous enumerations: the bureau divides the places where people live into two broad categories: group quarters , which include prisons , dormitories , and group homes ; and housing units , which consist of single family homes , apartments , and mobile homes .

during the 2000 census , the bureau had distinct procedures for building its group quarters and housing unit address lists and enumerating their residents .

for example , the bureau typically enumerates college dormitories by working with schools to distribute census questionnaires to students .

conversely , the bureau enumerates residents of housing units by delivering questionnaires directly to them through the mail .

in the unc situation , the 26 unc dormitories were listed correctly in the bureau's group quarters database and incorrectly in the bureau's housing unit database .

concerned there could be systemic issues with the bureau's address list , staff at the bureau's headquarters investigated the source of the problem following the initial discovery by charlotte employees .

the headquarters review found that the dormitories were improperly included in the u.s .

postal service's address file , which it initially shared with the bureau in november 1997 and continued to update through early 2000 .

the bureau uses this database to help build its housing unit address list .

specifically , the bureau discovered that the data field that normally contains a street address erroneously contained a unit number and the name of a unc dormitory .

the bureau had no explanation for how the dormitory names got into the u.s .

postal service's address file .

other procedures designed to verify census addresses produced conflicting results , compounding the problem .

one procedure in 1998 mistakenly confirmed the dormitories as housing units , while another procedure — called block canvassing — correctly flagged the addresses for deletion from the bureau's housing unit address list .

however , under the bureau's protocols , to ensure an address was not improperly removed from the census , an address had to be flagged twice to be deleted .

during nonresponse follow - up in 2000 , where enumerators visited housing units that failed to send back the questionnaires that were mailed to them , the bureau had a third opportunity to uncover the error .

because the enumerators involved in this operation provided inconsistent information , the bureau ultimately did not delete any housing units included in the initial census .

as part of the cqr case analysis , staff in the bureau's decennial statistical studies division checked the bureau's address file for any records that contained the word “dorm” in the address field to determine whether a similar duplication occurred at other schools .

this would have picked up the word “dormitory” and its variants .

on the basis of this search , the bureau concluded that a similar issue was not problematic elsewhere in the country .

in addition to the contact named above , robert goldenkoff , keith steck , timothy wexler , robert parker , michael volpe , andrea levine , and elena lipson made key contributions to this report .

