the government performance and results act ( results act or gpra ) seeks to shift the focus of federal management and decisionmaking from a preoccupation with the number of tasks completed or services provided to a more direct consideration of the results of programs — that is , the real differences the tasks or services make to the nation or individual taxpayer .

the results act originated in part from congress's frustration over the fact that congressional policymaking , spending decisions , and oversight and agencies' decisionmaking all had been seriously handicapped by the lack of clear goals and sound performance information .

to remedy that situation , the results act requires agencies to set multiyear strategic goals in their strategic plans and corresponding annual goals in their performance plans , measure performance toward the achievement of those goals , and report on their progress in their annual performance reports .

these reports are intended to provide important information to agency managers , policymakers , and the public on what each agency accomplished with the resources it was given .

this report responds to your request that we identify some of the challenges agencies face in producing credible performance information and how those challenges may affect performance reporting — the next phase of the results act implementation .

as agreed , our objectives were to ( 1 ) discuss whether the weaknesses we have identified in agencies' performance plans imply challenges for the performance reports , ( 2 ) illustrate some of the challenges agencies face in producing credible performance data , and ( 3 ) describe how performance reports can be used to address data credibility issues .

the results act is aimed at improving performance of government programs by requiring agencies to clarify their missions , establish goals and strategies for reaching them , measure performance , and report on their accomplishments .

beginning with fiscal year 1999 , the head of each agency is to prepare and submit to congress and the president a report on program performance .

the first of these annual reports is to be submitted no later than march 31 , 2000 .

these reports are to contain two main parts: a report on the actual performance achieved as compared with the performance goals expressed in the performance plan and the plans and schedules to achieve those goals that were not met .

if a performance goal becomes impractical or infeasible to achieve , the agency is to explain why that is the case and what legislative , regulatory , or other actions are needed to accomplish the goal , or whether the goal ought to be modified or discontinued .

finally , the reports should also relate performance measurement information to program evaluation findings , in order to give a clear picture of an agency's performance and its efforts at improvement .

to address our objectives , we relied on our large body of work on agencies' performance data problems and related issues .

we drew examples from our reviews of agencies' efforts to implement the results act , such as our reviews of agencies' fiscal years 1999 and 2000 performance plans , and products we have issued on major management challenges and risks , such as our performance and accountability series and high - risk series .

because this report is based primarily on our previously issued reports , we did not obtain agency comments .

our work on this report was done from october 1999 to january 2000 , in washington , d.c. , in accordance with generally accepted government auditing standards .

agencies need reliable information during their planning efforts to set realistic goals and later , as programs are being implemented , to gauge their progress toward achievement of those goals .

in our assessments of annual performance plans , we identified challenges that will affect agencies' abilities to reliably report on the achievement of program goals and , in cases where goals are not met , either identify opportunities for improvement or whether goals need to be adjusted .

for example , we concluded in our review of fiscal year 1999 performance plans that future plans would be more useful if they would , among other things , ( 1 ) more fully articulate how strategies and resources will lead to improved performance and ( 2 ) provide much greater confidence that performance information will be credible and useful for decisionmaking .

although , on the whole , the fiscal year 2000 plans showed moderate improvements over the fiscal year 1999 plans , we identified these two issues as continuing key weaknesses common among agencies' plans .

most of the fiscal year 2000 plans related strategies and programs to performance goals ; however , few plans indicated how the strategies would contribute to accomplishing the expected level of performance .

agencies need to understand and articulate how what they do on a day - to - day basis contributes to mission - related results .

such an understanding is important for agencies to pinpoint opportunities to improve performance and design and implement appropriate initiatives .

this information is also helpful to congressional and other decisionmakers in assessing the degree to which strategies are appropriate and reasonable .

the outcomes of many federal programs are the result of the interplay of several factors , and only some of these are within a program's control.further , on a daily basis , agencies do not produce outcomes , but rather outputs , such as activities , products , or services that are intended to contribute to outcomes .

thus , a key analytic challenge for agencies is knowing how their programmatic efforts contribute to their desired outcomes .

the inconsistent attention to this critical element undermined the value of agencies' performance plans and , unless addressed , it also will severely limit the value of their performance reports .

in 1997 , we reported that the department of justice's immigration and naturalization service ( ins ) lacked data on the overall effectiveness of its southwest border strategy .

for example , data were insufficient to indicate whether illegal aliens were deterred from entering the united states , whether there had been a decrease in attempted reentries by those who had previously been apprehended , and whether the strategy had reduced border violence .

we noted that , despite the investment of billions of dollars in the strategy , ins had amassed only a partial picture of the effects of increased border control and did not know whether the investment was producing the intended results .

we reported that a comprehensive , systematic evaluation of the agency's strategy to deter illegal entry along the southwest border would provide ins with information on whether its border enforcement strategy had produced the intended results .

after our report , ins contracted with independent research firms for an evaluation .

another weakness that we identified in our review of agencies' fiscal year 1999 and fiscal year 2000 performance plans was the limited confidence they provided in the credibility of performance information .

credible performance information is essential for accurately assessing agencies' progress towards the achievement of their goals — the cornerstone of performance reporting .

as shown in figure 1 , our analysis of agencies' fiscal year 2000 performance plans noted that most of the plans provided only limited confidence that performance information would be credible .

only the plans for the department of education , the department of justice , the department of transportation , and the social security administration provided general confidence that their performance information would be credible .

decisionmakers must have assurance that the program and financial data being used will be sufficiently timely , complete , accurate , useful , and consistent if these data are to inform decisionmaking .

however , like the fiscal year 1999 performance plans , most of the fiscal year 2000 plans lacked information on the procedures the agencies would use to verify and validate performance information .

similar to our findings with the fiscal year 1999 plans , we also found that , in general , the fiscal year 2000 plans failed to include discussions of strategies to address known data limitations .

we reported that when performance data are unavailable or of low quality , a performance plan would be more useful to decisionmakers if it briefly discussed how the agency plans to deal with such limitations .

without such a discussion , decisionmakers will have difficulty determining the implications for assessing the subsequent achievement of performance goals that agencies include in their performance reports .

in order to successfully measure and report progress toward intended results , agencies need to build the capacity to gather and use performance information .

however , our work over the past several years has identified limitations in agencies' abilities to produce credible performance data .

specifically , those limitations relate to program design issues that may make it difficult to collect timely and consistent national data , the relatively limited level of agencies' program evaluation capabilities , and long - standing weaknesses in agencies' financial management capabilities .

in several program areas , devolution of program responsibility from the federal level and consolidation of individual federal programs into more comprehensive , multipurpose grant programs have shifted both program management and accountability responsibilities toward the states .

these programs vary greatly in the kind and degree of flexibility afforded to state or local entities , distribution of accountability across levels of government , and availability of direct measures of program performance .

in our report on grant program design features , we noted that relatively few flexible programs collected uniform data on the outcomes of state or local service activities .

collecting such data requires conditions — such as uniformity of activities , objectives , and measures — that do not exist under many flexible program designs .

for instance , we reported that the block grants enacted as part of the omnibus budget reconciliation act of 1981 carried no uniform federal information and reporting requirements .

states collected a wide range of program information , but the collection efforts were designed to meet the needs of the individual states .

congress had limited information on program activities , services delivered , and clients served .

as a result , it was difficult , in many cases , to aggregate state experiences and speak from a national perspective on the block grant activities or their effects .

similarly , without uniform information definitions and collection methodologies , it was difficult to compare state efforts or draw meaningful conclusions about the relative effectiveness of different strategies .

education is one of many agencies where the interest in having enough information for accountability and federal program management continually competes with the aim of providing local agencies with the flexibility needed to implement their programs on the basis of their local needs .

the safe and drug - free schools program , for example , allows a wide range of activities , such as drug prevention instruction for students ; staff training ; general violence - prevention instruction ; and special one - time events , such as guest speakers and drug - and alcohol - free social activities.states are also permitted to define the information they collect on program activities and effectiveness .

under the safe and drug - free schools and communities act , education oversees state programs and state agencies monitor local programs .

under the act , each state may establish its own reporting requirements for local education agencies .

although these requirements have some common elements , state requirements vary widely .

with no requirements that states use consistent measures , education faces , as our work has shown , a difficult challenge in assembling the required state reports to develop a nationwide picture of the program's effectiveness .

because states , localities , or nongovernmental organizations operate many of its programs , the department of health and human services ( hhs ) experiences similar challenges .

the personal responsibility and work opportunity reconciliation act dramatically altered the nation's system for providing assistance to the poor .

among the many changes , the act replaced the existing entitlement program for poor families ( aid to families with dependent children ) with fixed block grants to the states to provide temporary assistance for needy families ( tanf ) .

under the tanf block grant , states have flexibility in designing and implementing their own assistance programs within federal guidelines .

meanwhile , hhs has a broad range of responsibilities for ensuring accountability from the states .

the welfare reform law gives hhs administrative and oversight responsibilities , the performance of which will rely on state - provided data .

hhs needs to ensure that it receives comparable and reliable data from the states to help it fulfill its oversight responsibilities , which include ensuring that states enforce the federal 5-year time limit on receiving welfare benefits , meet minimum work participation rates , and maintain a certain level of state welfare spending .

enforcing the time limit , for example , will be difficult because information on the total amount of time that someone has received tanf is not always available in individual states , let alone across states .

in addition , the law gives hhs authority to assess penalties if states fail to comply with certain requirements and provides for states to receive bonuses if they meet certain performance standards .

hhs needs to collect state data to determine performance penalties and bonuses .

in view of the increased flexibility of states in designing their programs , obtaining comparable and reliable data to assess the effect of welfare reform on children and families could be difficult for hhs .

the environmental protection agency ( epa ) provides another example of a federal agency that depends on the state and local agencies it is working with to provide the performance information that indicates whether results are being achieved .

for example , the state water quality reports required by the clean water act are a key source of information for measuring progress in cleaning up the nation's lakes , rivers , and streams .

however , epa has found that the wealth of environmental data epa and states collect are often difficult to compile in a meaningful way .

as contained in the clean water act , congress left the primary monitoring responsibility to the states for measuring progress in cleaning up the nation's lakes , rivers , and streams .

however , inconsistencies in water quality assessments and in assessment methodologies from state to state make it difficult to aggregate the data and to use the information to conclusively determine whether the quality of rivers , lakes , and streams is getting better or worse over time .

absent this information , it has been difficult for epa to set priorities , evaluate the success of its programs and activities , and report on its accomplishments in a credible and informed way .

the unavailability of reliable performance information can also be traced to a lack of standards and of common definitions for terms used to evaluate programs .

for example , we reported that the agencies involved in wetlands - related activities inconsistently used terms such as protection , restoration , rehabilitation , improvement , and enhancement in describing and reporting on their accomplishments .

even when the same terms are used , the agencies do not define them in the same way .

as a result , the consistency and reliability of data on the status of wetlands acreage are questionable .

thus , neither the progress made toward achieving the governmentwide goal of no net loss of the nation's remaining wetlands nor the contributions made by the agencies in achieving this goal can be accurately measured .

weaknesses in the availability of direct measures of performance can be overcome by drawing on information from other sources , such as program evaluation studies , research on the effectiveness of service delivery , or aggregate data such as vital statistics that describe the general status of a population .

in this regard , our report on grant program design features noted that 13 of the 21 flexible grant programs reviewed used such sources along with , or as a substitute for , performance measures collected from program operations .

we found that agencies that made use of multiple sources had information that covered more aspects of program performance than those that relied upon a single source .

program evaluation studies are important for assessing how well programs are working , determining factors affecting performance , and identifying improvement opportunities .

in our report on the analytic challenges facing agencies in measuring performance , we stated that supplementing performance data with impact evaluations might help provide agencies with a more complete picture of program effectiveness .

evaluations can play a critical role in helping to address those measurement and analysis difficulties agencies face that stem from two features common to many federal programs: the interplay of federal , state , and local government activities , and objectives and the aim to influence complex systems or phenomena whose outcomes are largely outside government control .

furthermore , systematic evaluation of how a program was implemented can provide important information about why a program did or did not succeed as well as suggest ways to improve it .

however , as we reported in our assessment of agencies' fiscal year 1999 performance plans , we continue to be concerned about the lack in many federal agencies of the capacity to undertake the program evaluations that will be vital to the success of the results act .

in our earlier review of agencies' strategic plans , we found that many agencies had not given sufficient attention to how program evaluations will be used in implementing the results act and improving performance .

in another report , we noted that agencies' program evaluation capabilities would be challenged to meet the new demands for information on program results.we found that the resources allocated for conducting program evaluations were small and unevenly distributed across the 13 departments and 10 independent agencies we surveyed for that report .

good evaluation information about program effects is difficult to obtain .

each of the tasks involved — measuring outcomes , ensuring the consistency and quality of data collected , establishing the causal connection between outcomes and program activities , and separating out the influence of extraneous factors — raises formidable technical or logistical problems that are not easily resolved .

thus , evaluating program impact generally requires a planned study and , often , considerable time and expense .

the experiences of the head start program illustrate the importance — and difficulty — of systematic program evaluation .

head start , administered by hhs' administration for children and families , is one of the most popular federal early childhood programs and has long enjoyed both congressional and public support .

between fiscal years 1990 and 1998 , annual head start funding nearly tripled , from $1.5 billion to almost $4.4 billion .

head start's purpose is to improve the social competence of children in low - income families , and in the past 33 years , the program has provided a comprehensive set of services to about 16 million low - income children .

educational , medical , nutritional , mental health , dental , social , and other services have been provided to low - income children and their families in all 50 states , the district of columbia , puerto rico , and the u.s. territories , as well as to migrant and native american populations .

given the size of the head start program and the efforts to expand the program's annual enrollment to one million children by 2002 , investing in studies that will assess its impact is important .

specifically , the challenge for hhs is to determine whether the same outcomes would have occurred if children and families were in other kinds of early childhood programs , or none at all .

hhs has substantially strengthened its emphasis on determining whether head start has achieved its purpose .

in part in response to the direction of congress , hhs has new initiatives that will , in the next few years , provide information not previously available on outcomes , such as gains made by children and their families while in the program .

in addition , the program is currently designing an impact study to assess whether children and their families would have achieved these gains without participating in head start .

congress has required that hhs submit a final report on the impact of the head start program by september 30 , 2003 .

the long - standing inability of many federal agencies to accurately record and report financial management data on both a year - end and an ongoing basis for decisionmaking and oversight purposes continues to be a serious weakness .

without reliable data on costs , decisionmakers cannot effectively evaluate programs' financial performance or control and reduce costs .

under the chief financial officers ( cfo ) act , agencies are expected to develop and deploy modern financial management systems and to routinely produce sound cost and operating performance information , among other things .

further , the federal financial management improvement act ( ffmia ) focuses on ensuring greater attention to making much needed improvements in financial management systems .

the primary purpose of ffmia is to ensure that agency financial management systems routinely provide reliable , useful , and timely financial information .

with such information , government leaders will be better positioned to invest scarce resources , reduce costs , oversee programs , and hold agency managers accountable for the way they run government programs .

table 1 shows the financial statement audit results for fiscal year 1998 for the 24 cfo act agencies .

in addition , financial management systems for 21 of the 24 agencies were found by auditors not to comply substantially with ffmia's requirements for fiscal year 1998 .

the three agencies in compliance were the department of energy , national aeronautics and space administration , and the national science foundation .

for some agencies , the preparation of financial statements requires considerable reliance on ad hoc programming and analysis of data produced by inadequate financial management systems that are not integrated or reconciled , and that often require significant audit adjustments .

the key for agencies is to take steps to continuously improve internal controls and underlying financial and management information systems .

these systems must generate timely , accurate , and useful information on an ongoing basis .

the overhauling of financial and related management information systems is the overarching challenge for agencies in generating timely , reliable data throughout the year .

the following examples illustrate serious financial management weaknesses and systems problems .

while the department of defense ( dod ) is responsible for vast operations — with an estimated $1 trillion in assets , nearly $1 trillion in liabilities , and a net cost of operations of $280 billion in fiscal year 1998 — no major part of the department has been able to pass the test of an independent audit because of pervasive financial management weaknesses .

such weaknesses led us in 1995 to put dod financial management on our list of high - risk areas vulnerable to waste , fraud , and mismanagement — a designation that continued unchanged in our more recent high - risk update .

these financial management weaknesses limit the reliability and timeliness of dod's currently available financial information .

dod management and / or auditors have repeatedly found dod systems to be inadequate for measuring the cost of operations and programs .

for example: dod has acknowledged that the lack of a cost accounting system is the single largest impediment to controlling and managing weapon systems costs , including costs of acquiring , managing , and disposing of weapon systems .

dod is unable to provide actual data on the cost associated with functions to be considered for a - 76 outsourcing competitions , including the capital costs associated with operations .

dod has long - standing problems accumulating and reporting the full costs associated with working capital fund operations , which provide goods and services in support of the military services .

as a result of our assessment of dod's fiscal year 2000 performance plan , we noted that the lack of adequate cost information impairs the development of cost - based performance measures and indicators across virtually the entire spectrum of dod's program operations .

while dod developed 43 unclassified performance measures and indicators to measure a wide variety of activities — from force levels to asset visibility — these measures and indicators contained few efficiency measures based on cost .

in our most recent testimony on dod financial management , we reported that dod has started to devote additional resources to correcting its financial management weaknesses .

dod's financial management improvement plans represent an ambitious undertaking and are an important step toward long - term improvements in the department's accountability .

however , eliminating dod's financial management weaknesses represents a major challenge because they are pervasive and entrenched in an extremely large decentralized organization .

as another example , in january 1999 , we designated the federal aviation administration's ( faa ) financial management a high - risk area because of serious and long - standing accounting and financial reporting weaknesses.these weaknesses render faa vulnerable to waste , fraud , and abuse ; undermine its ability to manage its operations ; and limit the reliability of financial information it provides to congress .

beginning with fiscal year 1994 , the department of transportation's office of the inspector general has audited faa's financial statements and has consistently been unable to determine whether the financial information is reliable .

this pattern of negative financial audit results has continued with its most recent report — a disclaimer of opinion — on faa's fiscal year 1998 financial statements , citing as a primary reason the inability to verify property , plant , and equipment ( pp&e ) reported at a cost of $11.9 billion .

we previously reported that many problems in the pp&e accounts affect faa's ability to efficiently and effectively manage programs that use these assets .

we also reported that many problems in these accounts result from the lack of a reliable system for accumulating project cost accounting information .

the inadequacy of faa's cost accounting system is a weakness that prevents the agency from having reliable and timely information about the full cost of program activities .

the lack of cost accounting information also limits faa's ability to , among other things , meaningfully evaluate performance in terms of efficiency and cost - effectiveness .

faa senior management have indicated that they recognize the urgency of correcting their financial management deficiencies and have taken steps to address them , including efforts to continue to develop a cost accounting system , which faa expects will be fully operational in 2001 .

however , as we reported in july 1999 , while faa has taken steps that are likely to lead to or already have resulted in improved accountability for ffa assets , much still remains to be done .

agencies' march 2000 performance reports will provide them with an opportunity to show the progress they have made in addressing data credibility issues .

as far back as our earliest assessment of agencies' efforts to implement the results act , and more recently in our reviews of agencies' strategic and performance plans , we identified data credibility issues as a persistent and continuing challenge for agencies .

in passing the results act , however , congress emphasized that the usefulness of agencies' performance information depends , to a large degree , on the reliability and validity of their data .

during this past year , we issued several reports on practices and approaches that agencies have proposed or adopted that address data credibility issues .

for example , we reported that applied practices , such as identifying actions to compensate for unavailable or low - quality data and discussing implications of data limitations for assessing performance , can help agencies describe their capacity to gather and use performance information .

to illustrate , the department of transportation stated in its fiscal year 1999 performance plan that one of the most significant limitations of both internal and external data is timeliness .

one way the department plans to deal with this limitation is to compile preliminary estimates from the portion of data that is available in time to report on the performance measures .

according to the plan , fatality data from the first 6 months of the year could be compared with data from the first 6 months of the previous year for an initial performance measurement .

in our report on reasonable approaches to verify and validate performance information , we identified a wide range of possible approaches that can be organized into four general strategies , as follows: management can seek to improve the quality of performance data by fostering an organizational commitment and capacity for data quality .

verification and validation can include assessing the quality of existing performance data .

assessments of data quality are of little value unless agencies are responding to identified data limitations .

building quality into the development of performance data may help prevent future errors and minimize the need to continually fix existing data .

these approaches can help agencies improve the quality , usefulness , and credibility of performance information .

however , as noted earlier , making stakeholders aware of significant data limitations allows them to judge the data's credibility for their intended use and to use the data in appropriate ways .

all data have limitations that may hinder their use for certain purposes but still allow them to be used for others .

stakeholders may not have enough familiarity with the data to recognize the significance of their shortcomings .

therefore , appropriate use of performance data may be fostered by clearly communicating how and to what extent data limitations affect assessments of performance .

for example , we noted that when the department of the treasury's customs service field staff realized management was using performance data to make decisions , the staff began providing explanations for any incorrect data .

customs also said it required each office to establish a data quality function , responsible for verification and validation , which would be inspected annually .

a federal environment that focuses on results relies on new types of information that are different from those that have traditionally been collected by federal agencies .

obtaining more credible results - oriented performance information is essential for agencies to plan their efforts and gauge progress toward the achievement of their goals .

however , as we previously reported , agencies have encountered some difficult analytic and technical challenges in obtaining timely and reliable results - oriented performance information and in ensuring that program evaluations that allow for the informed use of that information are undertaken .

the results act requires agencies to describe in their annual performance plans how they will verify and validate the performance information that will be collected .

including such information in performance reports can be equally important in helping to assure report users of the quality of the performance data .

discussing data credibility and related issues in performance reports can provide important contextual information to congress and agencies to help them address the weaknesses in this area .

for example , this sort of discussion in an agency's performance report can alert congress to the problems the agency has had in collecting needed results - oriented performance information .

agencies can also alert congress to the cost and data quality trade - offs associated with various collection strategies , such as relying on sources outside the agency to provide performance data and the degree to which those data are expected to be reliable .

finally , in order to give a clear picture of the agency's performance and its efforts at improvement , annual reports on performance can also relate performance measurement information to program evaluation findings .

we are sending copies of this report to senator joseph i. lieberman , ranking minority member , senate governmental affairs committee ; representative richard a. gephardt , minority leader , house of representatives ; representative henry a. waxman , ranking minority member , house government reform committee ; representative john m. spratt , jr. , ranking minority member , house budget committee ; and the honorable jacob j. lew , director , office of management and budget .

copies will be made available to others on request .

please contact me at ( 202 ) 512-8676 if you have any questions .

dottie self was the key contributor to this report .

j. christopher mihm associate director , federal management and workforce issues the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 37050 washington , dc 20013 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 202 ) 512-6061 , or tdd ( 202 ) 512-2537 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touch - tone phone .

a recorded menu will provide information on how to obtain these lists .

