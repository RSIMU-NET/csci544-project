taxpayers expect timely and accurate assistance from the internal revenue service ( irs ) when they have tax law questions or have tax returns prepared .

the quality of irs's assistance can reduce the time and aggravation of preparing tax returns and increase taxpayers' compliance with the tax laws .

in 2004 , irs answered almost 9 million tax law questions by telephone and prepared almost half a million tax returns for low income taxpayers .

irs's past performance has shown that taxpayers cannot always rely on it to provide accurate information .

while the accuracy of irs's taxpayer assistance has improved in some cases , it has been inconsistent or below expectations in others .

after 2 years of decline , in the first weeks of the 2005 filing season irs telephone assistance accuracy was estimated at 87 , percent compared to 76 percent for the same time period in 2004 .

although data on the accuracy of assistance at irs's walk - in sites are limited by not being representative , treasury inspector general for tax administration ( tigta ) reports have raised concerns about the accuracy of the returns irs prepares and that irs had not met its annual tax law accuracy goal .

this performance has raised questions about the adequacy of the training irs provides its taxpayer assistance staff .

although a number of factors can affect the accuracy of assistance irs provides taxpayers , effective training and development programs can enhance an organization's ability to achieve its mission and goals , such as improving accuracy .

at the same time training and developing staff is costly , making it important that such investments are targeted strategically and not wasted on efforts that are irrelevant , duplicative , or ineffective .

because of your interest in ensuring that taxpayers receive accurate information when they contact irs for assistance , and the contribution that training makes to that end , you asked that we review how irs plans and evaluates its primary taxpayer assistance training and development efforts .

we focused on planning and evaluation from a strategic perspective , that is , how planning and evaluation of training can help improve accuracy .

as discussed with your offices , our objectives were to assess whether irs's processes for planning and evaluating the training and development of taxpayer assistance staff conform to published guidance .

in conducting our work we developed detailed criteria to assess how irs plans and evaluates its training and development of taxpayer assistance staff based on guidance we published for assessing strategic training and development efforts in the federal government .

we developed 27 separate criteria for planning and evaluation , such as establishing goals , conducting knowledge and skills needs analyses , benchmarking against other organizations , systematically collecting data , and comparing benefits and costs .

we shared these criteria with officials in irs's human capital office and they said the criteria are appropriate and consistent with their policy guidance .

we assessed training and development for four types of assistance: less complex tax law questions answered by phone , more complex tax law questions answered by phone , tax law questions answered at irs walk - in sites , and return preparation at irs walk - in sites .

we collected documents describing irs's planning and evaluation process for the four taxpayer assistance programs , interviewed officials in irs's wage and investment ( w&i ) and small business / self - employed ( sb / se ) divisions to get more detail , and compared irs's practices to our criteria .

the scope and methodology section provides more details .

we conducted our work from may 2004 through may 2005 in accordance with generally accepted government auditing standards .

irs provides tax law assistance to taxpayers through irs's toll - free telephone lines and tax law and return preparation assistance in person at irs taxpayer assistance centers ( formerly known as “walk - in sites” ) nationwide , in addition to other means .

the four taxpayer assistance programs we reviewed were: w&i customer accounts services ( cas ) general tax law assistance by telephone .

many taxpayers contact irs by calling its toll - free telephone number which is operated and staffed by cas .

according to irs fiscal year 2004 data , 3,420 cas staff handled more than 8.7 million telephone calls from taxpayers with general , less complex tax law questions .

we did not verify these irs data or other irs data on taxpayer assistance programs' workload and staffing .

sb / se taxpayer education communication ( tec ) complex tax law assistance by phone .

according to irs officials , during the 2005 filing season and in previous years , tec staff supported telephone service by answering taxpayers' questions on selected , more complex tax topics .

when taxpayers called irs's toll - free number about these topics , w&i cas staff recorded the taxpayers' contact information and questions so that tec staff could call the taxpayers back later to provide answers .

according to irs , in the 2004 filing season , tec handled about 320,000 telephone calls for complex tax law assistance .

according to irs , tec trained approximately 400 staff and used 272,212 staff hours to provide telephone assistance in 2004 .

in 2006 these calls will be handled by w&i cas staff .

w&i customer assistance , relationships , and education ( care ) walk - in tax law assistance .

according to irs , about 1.4 million staff hours were devoted to providing walk - in assistance in fiscal year 2004 .

also according to irs , 1,654 staff were trained to provide tax law assistance in 2004 .

in 2004 , there were about 7.7 million contacts with taxpayers at irs's approximately 400 taxpayer assistance centers .

irs did not have information on how many of these contacts were for tax law assistance .

many of the contacts were for other services , such as tax forms , publications , or accounts issues .

w&i customer assistance , relationships , and education ( care ) walk - in return preparation assistance .

according to irs data , irs staff prepared 476,813 tax returns in fiscal year 2004 .

irs did not have data on the number of staff that prepared tax returns at taxpayer assistance centers .

irs does not have data on the amount of time its assistors spend annually being trained on tax law .

however , all staff providing tax law assistance receive some training each year .

irs has continuing professional education and refresher training requirements for all its taxpayer assistance staff .

tax law assistance staff receive a variety of training .

according to an irs official , the staff get primarily classroom training but also receive computer - based training .

on - the - job training , managerial coaching , and workshops are also part of training .

taxpayer assistance staff receive training on technical tax law topics , how to use irs systems and guidance to answer questions , and communication .

responsibility for training and developing irs's tax law assistance staff is decentralized .

irs's human capital office provides guidance and sets policy and standards on training and development for the agency .

w&i and sb / se each have a human resources office with learning and education ( l&e ) staff who are assigned to the taxpayer assistance programs .

l&e staff provide program staff advice and analysis on related policies and issues and formulate strategies , procedures , and practices to address the programs' human capital needs , including training .

generally speaking , the taxpayer assistance program offices identify training needs , l&e staff work with program staff to develop and fund annual training plans , and the program offices administer training .

according to irs policy , l&e staff are responsible for evaluating training .

according to fiscal year 2004 irs data , irs invested about $7 million in training w&i cas and care staff to provide taxpayer assistance , including such expenses as travel , supplies , contractor fees , and development costs .

irs could not separate these costs into amounts spent on tax law training and other topics .

according to sb / se tec fiscal year 2004 data , training costs were $325,072 in student and instructor travel .

however , the cost data irs provided did not include the costs of assistors' time associated with training .

as with the staffing and workload data , we did not verify the accuracy of irs's training cost data .

in march 2004 , we issued an assessment guide that introduced a framework for evaluating a federal agency's training and development efforts .

this assessment guide consists of a set of principles and key questions that federal agencies can use to ensure that their training and development investments are targeted strategically and are not wasted on efforts that are irrelevant , duplicative , or ineffective .

as detailed in our assessment guide , the training and development process can loosely be segmented into four broad , interrelated components: ( 1 ) planning / front - end analysis , ( 2 ) design / development , ( 3 ) implementation , and ( 4 ) evaluation .

figure 1 depicts the general relationships between the four components , including the feedback loop between evaluation and planning .

planning and evaluation are highlighted because they are the focus of this report .

although these components can be discussed separately , they are not mutually exclusive and encompass subcomponents that may blend with one another .

for instance , evaluation is part of the planning as organizations should reach agreement up front on how the success of training and development efforts will be assessed .

tigta is conducting a review that covers some aspects of the design and implementation of training .

our work examined the training and development of employees who provide tax law and return preparation assistance to taxpayers over the telephone and at walk - in centers , and covered both seasonal and full - time employees in irs's w&i and sb / se divisions .

our assessment of irs's training and development program for taxpayer assistance employees was based on analyses of irs data and interviews with irs officials .

we also obtained background information from sources outside irs such as tigta , the irs oversight board , and the irs national taxpayer advocate .

to assess how irs plans for and evaluates the training and development of irs employees who provide tax law and return preparation assistance to taxpayers over the telephone and at walk - in centers , we used our guide for assessing training and development programs in the federal government as a framework .

we used the parts of the gao guide dealing with the planning and evaluation components , along with supplemental guidance from the office of personnel management , to identify the detailed strategic training and development criteria applicable to irs and organized them into chronological phases .

we focused our criteria on strategic planning and evaluation , that is , planning and evaluation intended to help achieve irs's accuracy goals .

we developed 27 criteria , listed in appendices i and ii .

we also developed examples of evidence that would demonstrate conformance with each of the criteria .

for example , for the criterion “conduct a knowledge and skills inventory to identify employees' current skills and knowledge,” we looked for evidence such as a completed knowledge and skills surveys and proficiency tests ( see app .

i for the planning phase criteria and further evidence examples and app .

ii for the same information on evaluation. ) .

we shared the criteria with officials in irs's human capital office and the taxpayer assistance programs and assigned l&e staff .

the officials from the human capital office said the criteria are appropriate and consistent with their policy guidance .

some program and l&e officials expressed concerns about their need and ability to satisfy the criteria .

their concerns are discussed in the planning and evaluation sections of this report .

in applying the criteria to irs , we collected documents describing irs's planning and evaluation processes for the four types of assistance .

we also interviewed officials responsible for the taxpayer assistance programs in w&i cas and care units and sb / se tec unit and associated l&e staff responsible for training planning and evaluation , to get more details where needed .

we reviewed all the evidence and made a judgment about the extent to which irs's practices conformed to the criteria .

we then discussed our initial assessments with irs officials responsible for planning and evaluating the taxpayer assistance training and development programs who , for some of our preliminary assessments , provided additional written evidence for us to consider in making our final assessments .

we then revised our initial assessments based on the evidence they provided .

we made our assessments in two steps .

first , an analyst reviewed all the evidence and made a judgment about the extent to which it conformed to the criteria .

then a second analyst independently reviewed the assessments .

the evidence supporting our assessments is provided in appendices iii through x .

much of the information we relied on was descriptive .

we determined that the information was reliable for our purposes .

to the extent possible we corroborated interview evidence with documentation .

where not possible , the description is attributable to irs officials .

where relevant we corroborated that policy guidance , such as internal revenue manual guidance , was being implemented by collecting documentation and reports showing implementation .

with respect to controls over databases , we reviewed documentation of the controls but did not assess their adequacy or test data in the databases .

we conducted our work at the wage and investment division headquarters in atlanta , ga. ; the small business / self - employed division offices in philadelphia , pa. ; and irs headquarters human capital office in washington , d.c. from may 2004 through may 2005 in accordance with generally accepted government auditing standards .

summarizing table 1 , irs's planning for taxpayer assistance training primarily focuses on meeting short - term needs , such as the challenge of training staff about tax law changes in preparation for the next year's filing season .

planning could be enhanced by long - term goals and analyses of the relative importance of the factors that affect accuracy , other organizations' experiences , and gaps between long - term needs and existing skills .

table 1 shows our assessments of planning for training by irs's four primary taxpayer assistance programs for each phase of the planning process: goals and priorities , information gathering and assessments , skills and knowledge analysis , and strategy development and selection .

the evidence supporting our assessments is shown in appendices iii through vi .

in the goals and priorities phase , the four units were relatively strong in involving key stakeholders and communicating the importance of training .

the programs involved key stakeholders in annual training planning decisions , shown by half - circles in table 1 .

however , they did not have long - term planning processes in which to involve stakeholders .

all four programs had some upper - level management efforts to communicate to staff the importance of training and development in improving accuracy .

care's was a model of communicating to all levels of staff the importance of training .

for example , care's management guidance included managers responsible , communication vehicles , key dates , and the message to be conveyed .

the other programs communicated with managers , but did not communicate more widely through the organization .

with respect to goals , irs does not have long - term goals , as opposed to annual goals , for accuracy ; nor do the four programs have goals for training and development or measures of the impact of training on accuracy .

this observation about the lack of goals for accuracy is consistent with other recent reports where we cited a lack of irs long - term goals , as well as reports and other assessments by omb and tigta .

two of the programs had annual goals , shown by half - circles .

some taxpayer assistance officials that we talked with questioned the need for long - term accuracy goals or training and development goals and measures .

for example , w&i cas telephone assistance officials said setting long - term accuracy goals is not necessary because their annual accuracy goals would move irs toward improved performance .

program officials also said that since staff training is considered a part of doing business and is not managed as a program , training goals are unnecessary .

further , some said that , given the number of factors that affect accuracy , developing measures of training effectiveness would not be possible .

however , without long - term accuracy goals , irs , congress , and others are hampered in evaluating whether irs is making satisfactory progress improving taxpayers' service .

as we said in a prior report on irs's telephone assistance program , a long - term , results - oriented goal would provide a meaningful sense of direction as well as a yardstick for measuring the results of operations and evaluating the extent of improvements resulting from changes in resources , new technology , or management of human capital .

similarly , goals and performance measures for training would provide direction and help measure progress .

we recognize that collecting performance data can sometimes be challenging , as discussed in the evaluation section .

one irs taxpayer assistance program has been collecting performance data suitable for determining the effectiveness of training .

in this phase , the four programs determined and tracked selected factors that affect accuracy , but would benefit from a more complete analysis as well as benchmarking .

irs has identified , and tracked on an individual basis , selected factors that affect accuracy .

one nontraining factor that irs has tracked and analyzed is the use and quality of the probe and response guide , a manual provided to taxpayer assistance staff intended to guide them in responding to taxpayers' questions .

irs blamed decreases in accuracy in 2003 and 2004 on problems with the probe and response guide .

according to irs officials , they used their analysis of the probe and response guide to make the guidance more usable by taxpayer assistance staff .

officials attribute improvements in accuracy in 2005 , at least in part , to this effort .

however , irs has not conducted an analysis of the factors that affect accuracy , including training — shown by half - circles in table 1 .

specifically , irs has not done an analysis to determine the relative importance of the various factors , including training , that affect accuracy .

determining the relative importance of the factors that affect accuracy is important because there are multiple factors .

some factors that affect accuracy are strategic and outside of irs's direct control such as tax law changes .

other factors are operational and subject to irs control such as manuals , information systems , and training .

without an understanding of the relative importance each factor has on accuracy , it is difficult for irs to make informed decisions about a strategy for improving accuracy , including training's role in that strategy .

because of the purpose of the analysis and the number of factors involved , the analysis might not give a precise measure of each factor's impact .

furthermore , such analyses may be conducted on an occasional basis .

none of the taxpayer assistance programs collected another type of information — best practices of other organizations learned through benchmarking .

some officials told us it would not be possible to benchmark training programs because irs has a unique mission .

however , the telephone taxpayer assistance programs have benchmarked other aspects of their operations such as performance measures .

also , as we have stated in another report , many processes that seem unique to the government actually have counterparts in the private sector .

looking at processes in dissimilar organizations can lead to rewarding improvements because it stimulates new thinking about traditional approaches to doing work .

for example , in the report cited above , we noted that xerox benchmarked l.l .

bean to improve order fulfillment .

still another type of information useful for planning can be obtained from evaluations of training efforts .

as we discuss in the background , evaluation can provide useful feedback about the impact of existing training to help plan improvements to training .

evaluation of irs's training efforts is the subject of our second section .

all four assistance taxpayer assistance programs conducted annual needs assessments .

these assessments identified the knowledge and skills assistors needed for the following year .

none of them had longer term analyses to project future needs .

determining skills and knowledge needed is challenging for irs , especially when irs must react quickly to tax law changes .

for example , congress passed a law in early january 2005 allowing taxpayers to deduct on either their 2004 or 2005 tax returns contributions made during january 2005 for relief of the victims of the december 2004 indian ocean tsunami .

according to an irs official , irs had to react to this change , which took place after the filing season had already begun , by quickly alerting staff and providing them the information necessary to assist taxpayers who had questions about the deduction .

however , none of the four programs had analyses of long - term needs , such as improved proficiency in doing research , which is the basis for the half - circles in table 1 .

longer - term analyses of needs could help the four programs better plan strategies to meet future needs .

such planning could help ensure that the programs acquire , train , and retain the needed staff .

three of the four programs had knowledge and skills inventories based on actual testing to determine employees' proficiency and knowledge levels .

tec was the exception .

tec assumes the employees have an underlying technical foundation because of their background in irs .

however , in a recent tec survey of 133 employees , the percentage of employees who self - reported having pre - existing skills or being fully proficient in several technical categories , such as depreciation , sale of property , and trusts and fiduciaries , ranged from 31 percent to 62 percent .

three of the four programs performed gap analyses to determine the difference between current and needed skills and knowledge .

because of the above limitations in needs analyses , the gap analysis was necessarily annual not long term .

again , tec was the exception .

because tec had not done a needs analysis , it could not do a gap analysis .

weaknesses in analyses , described in the previous sections , hampered the four programs' ability to develop criteria for when to use training , as opposed to other strategies , for filling knowledge and skills gaps , such as hiring and retention efforts .

in addition , the four programs lacked information about the benefits and costs of their training efforts .

as noted in the background section , irs trains several thousand tax law assistance staff annually but does not have data on the cost of this significant effort .

without an understanding of the usefulness of other strategies , and without an understanding of the benefits and costs of training , irs lacked information that would be useful for making resource allocation decisions .

this matters because the resources devoted to training are significant .

with better information to select and develop a strategy , irs might be able to improve accuracy and save resources .

given the importance of accurate answers to taxpayers' questions and the resources spent on training , the four assistance programs would benefit from more sophisticated evaluations of the effectiveness of training .

the four programs all conduct some evaluations of their training efforts .

however , only tec attempted an evaluation of the impact of training on accuracy .

in table 2 , the lack of evaluation plans in the other three programs had a cascading effect that left the programs generally unable to fully satisfy our assessment criteria in subsequent evaluation phases — data collection , data analysis , and application of evaluation results .

appendices vii through x show the evidence supporting our assessments in table 2 .

irs adopted a four - level model based on the widely accepted kirkpatrick model for evaluating training .

under this model , the sophistication of analysis increases as the numerical level of analysis increases: level 1 — reaction .

the goal is to measure participants' reaction to training , usually through questionnaires .

level 2 – learning .

the goal is to determine what the training participants learned through various kinds of tests administered immediately after the training is completed .

level 3 – behavior .

the goal is to determine if the job performance of the training participants changed in the aftermath of the training .

the most common means for making this determination is the administration of a survey of trained staff and their supervisors 3 months , on average , after training is complete .

level 4 – results .

the goal is to determine if the training led to the desired results , in this case , improved accuracy of taxpayer assistance .

options for level 4 analyses might include statistical correlations between training and accuracy and controlled experiments where some staff receive new training and others do not .

such roi analyses are not a part of irs's evaluation approach .

this level is sometimes split into two levels with the fifth level — often referred to as return on investment ( roi ) — representing a comparison of costs and benefits quantified in dollars .

not all training and development programs are suitable for evaluations of their effect on organizational results .

the difficulty and costs of analyzing the impact of training on accuracy need to be weighed against the benefits of such an evaluation .

however , the more significant the activity targeted by the training and development program , the greater the need for level 4 analysis .

factors to consider when deciding the appropriate level of evaluation include estimated costs of the training effort , size of the training audience , management interest , program visibility , and anticipated life span of the effort .

as noted in the background , irs devotes significant resources to assisting taxpayers and training assistance staff .

congress has also expressed great interest in improving taxpayer service , such as accuracy .

for example , much of the focus of the internal revenue service reform and restructuring act of 1998 was on in improving taxpayer service .

irs's human capital office recognizes the need to do level 4 evaluations .

the office has a policy statement stating that level 4 evaluations should be done for all mission - critical training .

despite the large investment of resources , significant congressional attention , and human capital office guidance , irs officials involved in three of the four taxpayer assistance training programs had not agreed on whether to conduct a level 4 evaluation .

they conducted analyses only at lower levels , usually levels 1 or 2 .

by contrast , officials at l&e and tec agreed to conduct a pilot test of a level 4 analysis in 2004 for more complex tax law questions answered by telephone .

although the pilot test was not successful in 2004 — for reasons discussed below in the data analysis phase — l&e and tec intend to use the data collection and analysis plan created for 2004 to conduct a similar evaluation in 2005 .

for three of the four taxpayer assistance programs , as shown in table 2 , the lack of level 4 analyses had a cascading effect that left the programs unable to fully satisfy our assessment criteria in the subsequent evaluation phases .

all four programs had controls in place to help ensure systematic , timely data collection for level 1 and level 3 evaluations .

tec also had such controls in place for its level 4 evaluation .

however , the database used to store level 2 data — the administrative corporate education system ( aces ) — is no longer in place , and there is no replacement system planned .

with the exception of tec , the programs did not attempt to collect level 4 data .

database controls and data collection plans for tec's level 4 pilot evaluation helped ensure the systematic and timely collection of data .

all four units had controls in place to help ensure that level 1 and level 3 data were accurate , valid , reliable , and complete .

because of the lack of a level 2 database , none of the four units had controls in place for that data .

tec was concerned that the accuracy data , which were based on test telephone calls , did not reflect the types of calls that taxpayers actually made .

tec and irs's quality review staff are working together to try to address these concerns .

all four assistance programs conducted some analyses of their training efforts and analyzed stakeholders' assessments of training to identify potential improvements to individual courses .

a level 1 analysis , usually surveys of trainees' opinions about a course , was conducted for all courses .

the officials also said level 2 analyses , often testing , are generally done while level 3 analyses are infrequently done .

none of the four assistance programs successfully completed a level 4 evaluation of the impact of training on accuracy , compared benefits to costs , benchmarked training evaluation , or analyzed stakeholders' assessments of the impact of training on accuracy .

the 2004 tec level 4 pilot test was not completed because the level of proficiency of tec staff in the five technical categories reviewed was insufficient .

all the business units applied the results of their data analysis to individual training courses , not their training efforts as a whole .

for example , they used the results of their level 1 and level 3 analyses to make improvements in individual training courses and to identify training needs for the upcoming year .

however , because there had been no successful evaluations of the impact of training on accuracy , evaluation results could not be used to plan a strategy to improve accuracy .

training has the potential to improve the service received from irs by millions of taxpayers .

to its credit , irs has planning processes in place to address the challenges of training staff for each year's filing season .

the challenges include training staff to answer questions about annual tax law changes — changes that are often very complex .

although we do not know how much training has contributed , irs's taxpayer assistance accuracy has improved in recent years .

on the other hand , irs's current level of accuracy remains a concern , especially the accuracy of walk - in assistance and return preparation .

in addition , irs devotes significant resources to training its tax law assistance staff .

a more strategic approach to planning and evaluation would have several benefits .

strategic planning could help managers better understand the extent to which training , as opposed to other factors that affect accuracy , could be used to improve accuracy .

evaluations of training's impact on accuracy could help managers better understand which specific training techniques are effective and which are not .

while potentially difficult to design and costly to conduct , irs's human capital office has policy guidance urging more such analyses .

in addition , one taxpayer assistance program is pilot testing such an analysis .

gap analyses , evaluations , and cost - benefit comparisons might also contribute to providing training at lower cost by distinguishing between effective and ineffective training .

we recommend that the commissioner of internal revenue take appropriate action to ensure that irs's planning and evaluations of its taxpayer assistance training and development efforts better conform to guidance for strategically planning and evaluating the training efforts .

specifically , in the area of planning , we recommend that irs: establish a long - term goal for the accuracy of taxpayer assistance .

establish goals and measures for training and development logically linked to accuracy .

determine and track the relative importance of the various factors , including training , that affect accuracy .

benchmark training and development programs against high - performing organizations .

conduct skills and knowledge gap analyses for all taxpayer assistance programs , to include identifying and comparing current skills to long - term skill needs .

consider costs , benefits , ways to mitigate risks , and the appropriate level of investment for training and development efforts .

in the area of evaluation , we recommend that irs continue to pursue the level 4 pilot in tec and , if that analysis is shown to be feasible , conduct level 4 evaluations for its other taxpayer assistance training and development programs .

the evaluations should include the following: an analysis of the feasibility and cost effectiveness of alternative level 4 methodologies and a data collection and analysis plan , a comparison of the accuracy benefits to the costs of the training , benchmarking of the analytical methods and the results of the data analysis against high - performing organizations , and an analysis of stakeholder assessments of the impact of training on accuracy .

we also recommend that irs replace the defunct aces database , which had been used to store level 2 data , with another database for this purpose .

the commissioner of internal revenue provided written comments on a draft of this report in a letter dated july 6 , 2005 ( see app .

xii ) .

he said the report offers valuable insight , is timely , and has been shared with the project manager for irs's recently - initiated effort to reengineer its learning and education processes .

of the eight recommendations we made , the commissioner agreed with five recommendations: ( 1 ) establish long - term accuracy goals , ( 2 ) determine and track the relative importance of factors that affect accuracy , ( 3 ) benchmark training and development programs , ( 4 ) conduct level 4 evaluations , and ( 5 ) replace irs's system for storing level 2 evaluation data .

the commissioner partially responded to the remaining three recommendations .

in commenting on our recommendation to establish training goals and measures linked to accuracy , the commissioner focused on evaluation .

although related to after - the - fact evaluation , setting clear training goals and measures to ascertain progress toward those goals — consistent with agency goals which , in the case of taxpayer assistance , would include accuracy — is an important up - front planning step by which key stakeholders should agree on what training success is and how it will be assessed .

the commissioner said that irs recognizes the value of conducting skills and knowledge gap analyses .

he summarized the programs' efforts to identify short - term skills needs , which we recognized in this report .

however , he did not identify how short - term gap analyses would be conducted for the program for responding to more complex tax questions .

nor did the commissioner discuss gap analyses to identify long - term skills needed to reach accuracy goals .

failing to conduct gap analyses , including analyses of strategic changes — such as economic , technological , and demographic developments — can hinder performance and the development of strategies that integrate new capabilities and provide flexibility to meet new challenges and improve service .

in commenting on our recommendation that irs consider the costs and benefits of training efforts , the commissioner's comments did not specifically mention benefits and costs but did mention unfunded needs .

given the resources dedicated to training staff and providing taxpayer assistance , and the impact that assistance accuracy can have on taxpayers , the taxpayer assistance programs would benefit from analytically - based assurance that training efforts focus in a cost beneficial way on achieving accuracy goals .

as we agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution of it until 30 days from the date of this letter .

we will then send copies of this report to the chairman of the senate committee on finance , the chairman and ranking minority member of the house committee on ways and means , and the chairman and ranking minority member , subcommittee on oversight , house committee on ways and means .

we will also send copies to the secretary of the treasury ; the commissioner of internal revenue ; the director , office of management and budget ; and other interested parties .

we will also make copies available to others on request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-9110 or whitej@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix xiii .

as part of its process for deciding which training and development programs to implement , steps to ensure efforts will target needed improvements and enhance needed skills , which would yield such items as a logical or explicit link between the training and development programs offered and ( 1 ) the knowledge and skills identified in the skill needs assessment or ( 2 ) the strategic and operational factors identified as affecting accuracy ( to the extent that the needs and the identified factors could be addressed through training ) .

steps to ensure that analyses will help determine the impact of training and development on accuracy performance , such as analyses have considered and / or used an array of approaches ( both qualitative and quantitative ) analyses that separate training from other strategic and operational factors that might affect accuracy ( to the extent that it is cost - effective and / or feasible ) .

cas reported an annual accuracy goal linked to irs's strategic goal of improving taxpayer service .

the accuracy goal was not long - term .

cas had accuracy projections through 2010 , but according to irs officials , these projections were only internal and subject to change .

in addition , omb stated that irs did not have long - term goals .

cas involved stakeholders in its annual planning decisions .

for example , cas sought input from employees and managers on training priorities and needed changes to training .

however , cas did not have a long - term training planning process for involving key stakeholders in such strategic issues as establishing training program goals and long - term employee developmental needs .

cas had no such goals or measures .

cas management had efforts that conveyed the importance of training , such as an annual letter sent to field office directors that referred to the need for specific types of training .

however , management did not communicate more widely throughout the organization the strategic value of training and development and its importance in achieving long - term accuracy goals .

cas collected and analyzed accuracy data by using a quality review system to identify operational factors affecting accuracy , such as the proper use of irs guidance to assistors , and the use of a pareto analysis to determine the correlation between the causes and effects of key errors .

however , cas did not analyze and track the strategic factors such as tax law changes .

as a result , cas did not have information on the impact of training on accuracy , while holding other factors such as the quality of assistors' guidance or tax law changes , constant .

for our assessment , see “application of evaluation results” phase in appendix vii .

cas had not benchmarked its training and development program .

irs had benchmarked other nontraining practices related to customer service such as performance measures .

it was too early for an assessment because the enterprise learning management system ( elms ) , which would be cas's primary system used for providing human resource information , is being implemented in fiscal year 2005 .

cas identified the knowledge and skills assistors needed for the following year .

for example , cas projected the number of questions expected by tax topic and took into account recent tax law changes .

however , cas had not identified the specific knowledge and skills needed to achieve long - term accuracy goals .

cas had an inventory of assistors' current knowledge and skills based on testing .

cas did an annual gap analysis , but because cas had not performed a needs assessment for a period that extended beyond the next year , cas was unable to determine the differences between current skills and knowledge and skills and knowledge needed in the future .

cas had no criteria to guide decisions on using training and development strategies , as opposed to other strategies , such as improving guidance or hiring practices , to address skills and knowledge gaps .

cas had practices such as employee testing , and studies conducted to identify the most frequent errors made on an annual basis , that allowed cas to link training efforts to identified short - term skills and knowledge gaps .

however , because of limitations in long - term gap analyses , cas had limited ability to ensure training and development efforts were linked to long - term needed skills and knowledge .

cas did not have these analyses .

for our assessment , see “evaluation plan” phase in appendix vii .

cas had efforts to anticipate and react to potential short - term changes that could affect training , such as tax law changes or lack of experienced assistors due to staff turnover .

however , cas did not have a training planning process to identify potential long - term changes such as technological innovations or changes in the economy that might impact training and development .

tec did not have such goals .

tec involved stakeholders in its annual training planning .

for example , tec surveyed employees to obtain input on training needs for the upcoming year .

however , tec did not have a long - term training planning process for involving key stakeholders in such strategic issues as establishing training program goals and long - term employee developmental needs .

tec had no such goals or measures .

tec management had efforts that conveyed the importance of training , such as an annual letter sent to field office directors that referred to the need for specific types of training .

however , management did not communicate more widely throughout the organization the strategic value of training and development and its importance in achieving long - term accuracy goals .

tec collected and analyzed accuracy data to identify factors , such as an analysis of quality review system data .

however , tec did not analyze and track the strategic factors such as changes to tax law .

as a result , tec did not have information on the impact of training on accuracy , while holding other factors such as the quality of assistors' guidance or tax law changes , constant .

for our assessment , see “application of evaluation results” phase in appendix viii .

tec had not benchmarked its training and development program .

it was too early for an assessment because the enterprise learning management system ( elms ) , which would be tec's primary system used for providing human resource information , is being implemented in fiscal year 2005 .

annually , tec identified the types of tax law topics it would be handling and identified the knowledge and skills assistors needed .

this process included taking account of recent tax law changes .

however , tec had not identified the specific knowledge and skills needed to achieve long - term accuracy goals .

in 2006 , w&i will be handling these calls and training their staff to provide answers to these more complex tax law questions .

tec did not conduct an inventory of its employees' current skills and knowledge .

instead , tec assumed that its employees had an underlying technical foundation because of their prior irs experience , and the annual training they received was to supplement previously acquired skills .

because tec had not done a knowledge and skills inventory , a gap analysis could not be done .

tec had no criteria to guide decisions on using training and development strategies , as opposed to other strategies such as improving guidance or hiring practices , to address skills and knowledge gaps .

in 2004 , tec matched its training courses to the general skills and knowledge assistors needed .

however , because of limitations in long - term gap analyses , tec had limited ability to ensure training and development efforts were linked to long - term needed skills and knowledge .

tec did not have these analyses .

for our assessment , see “evaluation plan” phase in appendix viil .

tec had informal processes for anticipating and reacting to potential short - term changes that could affect training .

however , tec did not have a training planning process to identify potential long - term changes such as technological innovations or changes in the economy that might impact training and development .

appendix v: assessments of planning: tax law questions walk - in service by w&i care care reported an annual accuracy goal linked to irs's strategic goal of improving taxpayer service .

the goal was not long - term .

care had accuracy projections through 2010 , but according to irs officials , these projections were internal and subject to change .

in addition , omb stated that irs did not have long - term goals .

care involved stakeholders in its annual planning .

for example , human capital training staff developed an annual training plan based on needs identified by program staff .

however , care did not have a long - term training planning process in which to involve stakeholders in such strategic issues as identifying measures to assess training progress or identifying long - term employee developmental needs .

care had no such goals or measures .

care implemented a plan for communicating to various levels of staff , including front - line employees , management's vision for training .

the plan included the vehicle and product to be used to communicate this message , along with key dates activities should occur to deliver the message of how training and development plays a role in providing quality taxpayer service and enhancing employee development to achieve future goals .

care collected and analyzed data , including quality review system data , to identify operational factors affecting accuracy to identify the most frequent errors .

care also had studies to determine why those errors occurred , such as why staff did not properly use irs guidance to answer questions .

however , care did not analyze and track the strategic factors such as changes to the tax law or attrition .

as a result , care did not have information on the impact these factors had on accuracy holding other factors such as changes in the tax law or change in guidance constant .

___ for our assessment see the “application of evaluation results” phase in appendix ix .

care had not benchmarked its training and development program .

irs had benchmarked other nontraining practices related to customer service such as performance measures .

it was too early for an assessment because the enterprise learning management system ( elms ) , which would be care's primary system used for providing human resource information , is being implemented in fiscal year 2005 .

care had identified the knowledge and skills assistors needed for the following year .

one example is ensuring that assistors have the skills and knowledge needed to respond to questions about recent tax law changes .

however , care had not identified the specific knowledge and skills needed to achieve long - term accuracy goals .

care had an inventory of assistors' current knowledge and skills based on a prescreening testing process .

care did an annual gap analysis , but because care had not performed a needs assessment for a period that extended beyond the next year , it had not performed a gap analysis to determine the differences between current skills and knowledge and skills and knowledge needed in the future .

care had no criteria to guide decisions on using training and development strategies , as opposed to other strategies , such as improving guidance , or hiring practices to address skill gaps .

care has practices such as employee testing , and studies conducted to identify the most frequent errors made on an annual basis , that allow care to link training efforts to identified short - term skills and knowledge gaps .

however because of limitations in long term gap analyses , staff have limited ability to ensure training and development efforts are linked to long term needed skills and knowledge .

care did not have these analyses .

for our assessment , see “evaluation plan” phase in appendix ix .

care had efforts to anticipate and react to potential short - term changes that could affect training , such as tax law changes or lack of experienced assistors due to staff turnover .

however , care did not have a training planning process to identify potential long - term changes , such as technological innovations or changes in the economy , that might impact training and development .

care did not have quantitative long - term accuracy goals for return preparation .

fiscal year 2005 will be the baseline year for a new measure of tax preparation accuracy , so therefore there are no annual or long - term goals .

care involves stakeholders in annual planning .

for example , human capital training staff developed an annual training plan based on needs identified by program staff .

however , care did not have a long - term training planning process in which to involve stakeholders in such strategic issues as identifying measures to assess training progress or identifying long - term employee developmental needs .

care had no such goals or measures .

care collected and analyzed accuracy data , including quality review system data , to identify operational factors affecting accuracy to identify the most frequent errors .

care also had studies to determine why those errors occurred , such as why staff did not properly use irs guidance to answer questions .

however , care did not analyze and track the strategic factors such as changes to the tax law or attrition .

as a result , care did not have information on the impact these factors will have on accuracy holding other factors such as changes in the tax law or change in guidance constant .

for our assessment see the “application of evaluation results” phase in appendix x .

care had not benchmarked its training and development program .

irs had benchmarked other nontraining practices related to customer service such as performance measures .

it was too early for an assessment because the enterprise learning management system ( elms ) , which would be care's primary system used for providing human resource information , is being implemented in fiscal year 2005 .

care identified the knowledge and skills assistors needed for the following year .

one example is ensuring that assistors have the skills and knowledge needed to respond to questions about recent tax law changes .

however , care had not identified the specific knowledge and skills enhancements needed to achieve long - term accuracy goals .

care had an inventory of assistors' current knowledge and skills based on a prescreening testing process .

care did an annual gap analysis , but because care had not performed a needs assessment for a period that extended beyond the next year , it had not performed a gap analysis to determine the differences between current skills and knowledge and skills and knowledge needed in the future .

care had no criteria to guide decisions on using training and development strategies , as opposed to other strategies , such as improving guidance , or hiring practices to address skill gaps .

care had practices such as employee testing , and studies conducted to identify the most frequent errors made on an annual basis , that allowed care to link training efforts to identified short - term skills and knowledge gaps .

however , because of limitations in long - term gap analyses , care had limited ability to ensure training and development efforts were linked to long - term needed skills and knowledge .

care did not have these analyses .

for our assessment , see “evaluation plan” phase in appendix x .

care had efforts to anticipate and react to potential short - term changes that could affect training , such as tax law changes or lack of experienced assistors due to staff turnover .

however , care did not have a training process to identify potential long - term changes such as technological innovations or changes in the economy that might impact training and development .

l&e had adopted a four - level model for evaluating training , the evaluation monitoring system - integrated training evaluation and measurement services ( ems - items ) , which was based on the widely - accepted kirkpatrick model .

irs's human capital office l&e officials concluded that level 4 evaluations were appropriate .

however , cas and l&e officials had not agreed on whether to conduct level 4 evaluations .

in addition , l&e officials had not documented an analysis of what level of evaluation was appropriate .

because l&e and cas officials had not agreed to do a level 4 evaluation , l&e had not analyzed the feasibility and cost effectiveness of alternative level 4 methodologies .

because l&e and cas officials had not agreed to do a level 4 evaluation , l&e did not have a level 4 data analysis plan .

however , ems - items did provide general guidance for different levels of evaluation .

ems - items had controls to help ensure systematic and timely collection of data for level 1 and 3 evaluations .

ems - items had guidance on the collection of data including responsible parties and timing .

however , the administrative corporate education system ( aces ) database used to collect level 2 data was no longer in place , and as a result , there was no system to consistently collect and store level 2 data .

in addition , because l&e and cas officials had not agreed to do a level 4 evaluation , l&e had not collected the data necessary to do a level 4 evaluation .

ems - items had level 1 and some level 3 data and the system had controls in place to help ensure that data were accurate , valid , and reliable .

however , because l&e and cas had not agreed to do a level 4 evaluation , l&e did not collect level 4 data .

the aces database used to collect level 2 data no longer exists and there is no replacement system planned .

l&e conducted some level 1 , 2 , and 3 evaluations of participants' opinions , learning , and subsequent job performance .

for example , l&e analyzed course evaluations , tests , and supervisory evaluations after employees completed coursework to identify needed improvements to training .

however , l&e had done no level 4 evaluations .

because l&e and cas had not agreed to do a level 4 evaluation , l&e did not do a comparison of benefits to cost .

none .

l&e and cas analyzed stakeholder assessments of training to identify needed changes .

for example , management councils and focus groups were used to determine if changes were needed to improve course material , training environment , timing , and objectives of training .

however , because l&e and cas had not agreed to do a level 4 evaluation , there were no efforts to collect and analyze assessments from internal and external stakeholders to assess training in terms of its impact on accuracy .

evaluations were for individual courses , not for the program as a whole .

l&e and cas reported using evaluation results in planning to make improvements to training courses or to identify training needs for the upcoming year .

l&e and cas used the available level 1 through 3 evaluations to make planned improvements on individual courses .

however , because l&e and cas had not agreed to do a level 4 evaluation or cost - benefit comparisons , l&e and cas's ability to make informed decisions on improving training to improve accuracy was limited .

l&e had adopted a four - level model for evaluating training ( ems - items ) that was based on the widely accepted kirkpatrick model .

in 2004 , l&e and tec planned a pilot test of a level 4 evaluation .

l&e and tec officials stated in an official document that they intend to conduct a level 4 evaluation in 2005 .

l&e had not analyzed the feasibility and cost effectiveness of alternative methodologies .

l&e had a plan to conduct a level 4 evaluation in 2004 .

l&e planned to conduct a level 4 evaluation in 2005 similar to that done in 2004 .

officials said they would use the 2004 plan as the basis for the 2005 evaluation effort .

l&e had controls to ensure data were collected systematically and in a timely manner .

data collection practices for all four levels of evaluation in the level 4 pilot test evaluation included specific steps to be accomplished , status updates , and due dates .

in addition , for the 2005 pilot , data on tec accuracy in five technical categories and staff proficiency in those technical categories were available before the end of 2004 .

in 2004 , tec was concerned that the accuracy data in the five technical categories , based on test calls , did not reflect the types of calls taxpayers actually made .

tec and irs's quality review staff subsequently worked together in an effort to fix the problem .

ems - items had level 1 and some level 3 data , and the system had controls in place to ensure that data were accurate , valid , and complete .

however , the database used to collect level 2 data no longer existed , and there was no replacement system planned .

in the 2004 pilot level 4 evaluation , l&e did an analysis but was unable to assess the impact of training on accuracy , because the level of proficiency of tec staff in the five technical categories ( capital gains , depreciation and sale of business property , rentals , trust and fiduciaries , and international and alien ) was insufficient .

l&e decided not to measure the impact of training on accuracy until 80 percent of the employees were proficient in the five technical categories .

in a survey of 133 tec employees , the percent reported having preexisting skills or being fully proficient in the five technical categories ranged from 31 to 62 percent .

comparing the value of training to the costs of training was to be the third phase of the uncompleted 2004 pilot test .

none .

l&e and tec analyzed stakeholder assessments of training to identify needed changes .

for example , tec used information from employee surveys to target training materials on the types of calls assistors reported receiving .

however , they did not successfully collect and analyze assessments from internal and external stakeholders to assess training in terms of its impact on accuracy .

as discussed above , although l&e attempted to do a pilot test of a level 4 evaluation of the training program as a whole , the evaluation was unsuccessful .

l&e and tec reported using evaluation results in planning to make improvements to training courses or to identify training needs for the upcoming year .

however , because there was no successful level 4 evaluation or benefit cost comparison , l&e and tec's ability to make informed decisions on improving training to improve accuracy was limited .

l&e had adopted a four - level model for evaluating training , the evaluation monitoring system - integrated training evaluation and measurement services ( ems - items ) , based on the widely - accepted kirkpatrick model .

irs's human capital office l&e officials concluded that level 4 evaluations were appropriate .

however , care and l&e officials had not agreed on whether to conduct level 4 evaluations .

in addition , l&e officials had not documented an analysis of what level of evaluation was appropriate .

because l&e and care officials had not agreed to do a level 4 evaluation , l&e had not analyzed the feasibility and cost effectiveness of alternative level 4 methodologies .

in addition , l&e and care officials did not look at methodologies specific to questions on tax law assistance .

the approach for training and evaluating staff who provided assistance by answering walk - in customers' tax law questions and staff preparing returns were the same .

because l&e and care officials had not agreed to do a level 4 evaluation , l&e did not have a level 4 data analysis plan .

however , ems - items did provide general guidance for different levels of evaluation .

ems - items had controls to help ensure systematic and timely collection of data for level 1 and 3 evaluations .

ems - items has guidance on the collection of data including responsible parties and timing .

however , the administrative corporate education system ( aces ) database used to collect level 2 data was no longer in place , and as a result , there was no system to consistently collect and store level 2 data .

in addition , because l&e and care officials had not agreed to do a level 4 evaluation , l&e had not collected the data necessary to do a level 4 evaluation .

ems - items had level 1 and some level 3 data and the system had controls in place to help ensure that data were accurate , valid , and reliable .

however , because l&e and care had not agreed to do a level 4 evaluation , l&e did not collect level 4 data .

the aces database used to collect level 2 data no longer exists and there was no replacement system planned .

l&e conducted some level 1 , 2 , and 3 evaluations of participants' opinions , learning , and subsequent job performance .

for example , l&e analyzed course evaluations , tests , and supervisory evaluations after employees completed coursework to identify needed improvements to training .

however , l&e had done no level 4 evaluations .

in addition , there was no distinction between analysis of walk - in tax law and return preparation assistance .

because l&e and care had not agreed to do a level 4 evaluation , l&e did not do a comparison of benefits to cost .

none .

l&e and care analyzed stakeholder assessments of training to identify needed changes .

for example , focus groups were used to determine if changes were needed to improve course material , training environment , timing , and objectives of training .

however , because l&e and care had not agreed to do a level 4 evaluation , there were no efforts to collect and analyze assessments from internal and external stakeholders to assess training in terms of its impact on accuracy .

evaluations were for individual courses , not for the program as a whole .

l&e and care reported using evaluation results in planning to make improvements to training courses or to identify training needs for the upcoming year .

l&e and care used the available level 1 through 3 evaluations to make planned improvements on individual courses .

however , because l&e and care had not agreed to do a level 4 evaluation or cost - benefit comparisons , l&e and care's ability to make informed decisions on improving training to improve accuracy was limited .

l&e has adopted a four - level model for evaluating training , the evaluation monitoring system - integrated training evaluation and measurement services ( ems - items ) , based on the widely - accepted kirkpatrick model .

irs's human capital office l&e officials had concluded that level 4 evaluations were appropriate .

however , care and l&e officials had not agreed on whether to conduct a level 4 evaluation .

in addition , l&e officials had not documented an analysis of what level of evaluation was appropriate .

because l&e and care officials had not agreed to do a level 4 evaluation , l&e had not analyzed the feasibility and cost effectiveness of alternative level 4 methodologies .

in addition , l&e and care officials did not look at methodologies specific to questions on return preparation .

the approaches for training and evaluating staff that provided assistance by preparing returns and staff answering walk - in customers' tax law questions were the same .

because l&e and care officials had not agreed to do a level 4 evaluation , l&e did not have a level 4 data analysis plan .

however , ems - items did provide general guidance for different levels of evaluation .

ems - items had controls to help ensure systematic and timely collection of data for level 1 and 3 evaluations .

ems - items had guidance on the collection of data including responsible parties and timing .

however , the administrative corporate education system ( aces ) database used to collect level 2 data was no longer in place , and as a result , there was no system to consistently collect and store level 2 data .

in addition , because l&e and care officials had not agreed to do a level 4 evaluation , l&e had not collected the data necessary to do a level 4 evaluation .

ems - items had level 1 and some level 3 data and the system had controls in place to help ensure that data were accurate , valid , and reliable .

however , because l&e and care had not agreed to do a level 4 evaluation , l&e did not collect level 4 data .

the aces database used to collect level 2 data no longer existed and there was no replacement system planned .

l&e conducted some level 1 , 2 , and 3 evaluations of participants' opinions , learning , and subsequent job performance .

for example , l&e analyzed course evaluations , tests , and supervisory evaluations after employees completed coursework to identify needed improvements to training .

however , l&e had done no level 4 evaluations .

in addition , there was no distinction between analysis of walk - in tax law and return preparation assistance .

because l&e and care had not agreed to do a level 4 evaluation , l&e did not do a comparison of benefits to cost .

none .

l&e and care analyzed stakeholder assessments of training to identify needed changes .

for example , focus groups were used to determine if changes were needed to improve course material , training environment , timing , and objectives of training .

however , because l&e and care had not agreed to do a level 4 evaluation , there were no efforts to collect and analyze assessments from internal and external stakeholders to assess training in terms of its impact on accuracy .

evaluations were for individual courses , not for the program as a whole .

l&e and care reported using evaluation results in planning to make improvements to training courses or to identify training needs for the upcoming year .

l&e and care used the available levels 1 through 3 evaluations to make planned improvements on individual courses .

however , because l&e and care had not agreed to do a level 4 evaluation or cost - benefit comparisons , l&e and care's ability to make informed decisions on improving training to improve accuracy was limited .

in recent years , a growing number of organizations have adopted a balanced , multilevel approach to evaluating their training and development efforts .

such an approach can help provide varied data and perspectives on the effect that training efforts have on the organization .

one commonly accepted model consists of four levels of assessment .

the first level measures the training participants' reaction to , and satisfaction with , the training program or planned actions to use new or enhanced competencies .

the second level measures the extent to which learning has occurred because of the training effort .

the third level measures the application of this learning to the work environment through changes in behavior that trainees exhibit on the job because of the training or development program .

the fourth level measures the impact of the training program on the agency's program or organizational results .

the fourth level is sometimes split into two levels with the fifth level representing a comparison of costs and benefits quantified in dollars .

this fifth level — often referred to as return on investment ( roi ) — compares the benefits ( quantified in dollars ) to the costs of the training and development program .

not all training and development programs require , or are suitable for , higher levels of evaluation .

indeed , higher levels of evaluation can be challenging to conduct because of the difficulty and costs associated with data collection and the complexity in directly linking training and development programs to improved individual and organizational performance .

figure 2 depicts an example gradation of the extent to which an agency could use the various levels of evaluation to assess its training and development programs .

for example , an agency may decide to evaluate participants' reactions for all ( 100 percent ) of its programs , while conducting an roi analysis for 5 percent of its programs .

in addition to the contact named above , charlie daniel , david dornisch , chad gorman , jason jackson , ronald jones , veronica mayhand , and katrina taylor made key contributions to this report .

