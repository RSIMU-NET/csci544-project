in the 5 years since the enactment of the digital accountability and transparency act of 2014 ( data act ) , much progress has been made to improve the transparency of federal spending data , which was roughly $4.45 trillion in fiscal year 2019 .

the office of management and budget ( omb ) and the department of the treasury ( treasury ) established a set of data standards to enable the reporting and tracking of federal spending data displayed on usaspending.gov .

treasury , in collaboration with omb , issued additional guidance and improved the technical architecture used by federal agencies to facilitate their efforts to report spending data .

with these improvements and improvements made in reporting data at the agency level , more agencies are reporting more data to treasury and thus making more information available to the public .

the ongoing implementation of the data act is one of several government - wide initiatives under way focused on improving the transparency and quality of federal data assets .

recent initiatives that extend beyond the data act include the cross - agency priority ( cap ) goal leveraging data as a strategic asset under the 2019 president's management agenda .

this cap goal includes the development of a federal data strategy .

in addition , the foundations for evidence - based policy making act of 2018 ( evidence act ) , enacted in january 2019 , requires , among other things , that agencies designate a chief data officer to help improve data quality across government .

while these more recent initiatives provide opportunities for continued improvement , our prior work examining the quality of the data made available under the data act has found significant data quality challenges that limit their usefulness .

the data act contains a provision requiring us and agency inspectors general ( ig ) to report on the completeness , timeliness , quality , and accuracy of the data — in 2017 , 2019 , and 2021 .

this is our second assessment of the quality — defined as encompassing the concepts of timeliness , completeness , and accuracy — of data agencies were required to report pursuant to the data act .

more specifically , this report addresses: ( 1 ) the timeliness , completeness , and accuracy of the data , and the implementation and use of data standards ; and ( 2 ) progress made to develop a data governance structure consistent with leading practices , and how it affects data quality .

we also update the status of our previous recommendations related to implementation of the data act and data transparency .

to assess the timeliness , completeness , and accuracy of the data submitted and the implementation and use of data standards , we analyzed agency submission files for the fourth quarter of fiscal year 2018 ( q4 fy2018 ) on usaspending.gov and reviewed a representative stratified random sample of transactions selected from the usaspending.gov database containing spending data for q4 fy2018 .

we designed our stratified random sample to estimate rates within each of the three data files: ( 1 ) procurement award transactions , ( 2 ) assistance award transactions , and ( 3 ) budgetary records .

estimates for the results of the procurement , assistance , and budgetary samples have sampling errors of + / - 7.8 , 8 , and 10 percentage points or less , respectively , at the 95 percent level of confidence .

see table 1 for a listing of the six budgetary data elements and the 38 procurement and financial assistance award data elements and subelements that we tested in our review .

we compared the results of our review of q4 fy2018 data to those of our second quarter of fiscal year 2017 ( q2 fy2017 ) data that we reviewed in our first mandated assessment of data quality .

for both reviews , we examined a projectable sample of budgetary and award transactions from a database that according to treasury is partly used to display data on usaspending.gov .

however , there were the following differences: ( 1 ) our 2017 sampling frame was confined to the 24 chief financial officers act of 1990 ( cfo act ) agencies ( which represented 99 percent of obligations in our data set at that time ) , while our sampling frame for this review included all agencies that submitted q4 fy2018 data files as of february 11 , 2019 ; ( 2 ) more agencies and their components reported data in q4 fy2018 than in q2 fy2017 ; ( 3 ) in 2017 , our estimated error rate calculations included elements of certain sampled transactions that were determined to be not applicable to the transaction , and were classified as consistent with agency sources in both the numerator and denominator , while in this review , we excluded not - applicable elements from both the numerator and denominator of the estimated rate calculations ; ( 4 ) our sampling frame for this review included more data elements and subelements than were in our q2 fy2017 sampling frame ; ( 5 ) in this review , since three data elements we reviewed were derived by federal procurement data system - next generation ( fpds - ng ) and financial assistance broker submission ( fabs ) rather than provided by agencies , we compared the information in the sample to other sources rather than agency documents and therefore did not compare those results to q2 fy2017 ; ( 6 ) agencies' q4 fy2018 data were submitted under policies and procedures outlined in the data act information model schema ( daims ) v1.3 which reflects changes in validation rules and reporting requirements from the daims v1.0 that was in effect in 2017 ; ( 7 ) omb issued additional guidance on data act reporting since we reported in 2017 ; and ( 8 ) changes were made to the treasury broker — the system that collects and validates agency data — since our last report .

to evaluate how the current data governance structure affects data quality , we compared data quality challenges we identified during our review to key practices for data governance identified in our prior work .

to assess progress made to develop a data governance structure consistent with key practices , we reviewed policy and other documentation related to ongoing efforts to develop a government - wide structure for governing the standards established under the act , and interviewed omb staff about these efforts .

for the agencies selected in our sample , we also reviewed agency data quality plans and agency guidance intended to facilitate agency efforts to establish data governance programs , and interviewed agency officials on their data governance efforts .

to update the status of our prior recommendations related to the implementation of the data act , we reviewed new guidance and other related documentation , and interviewed omb staff and treasury officials .

see app .

iv for an update on our open recommendations related to data transparency and data act implementation .

additional details regarding our objectives , scope , and methodology along with information about data reliability are provided in app .

ii .

we conducted this performance audit from november 2018 to november 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

signed into law on may 9 , 2014 , the data act expands on previous federal transparency legislation .

it requires a greater variety of data related to federal spending by agencies , such as budget and financial information , to be disclosed and agency spending information to be linked to federal program activities so that policymakers and the public can more effectively track federal spending through its life cycle .

the act gives omb and treasury responsibility for establishing government - wide financial data standards for any federal funds made available to , or expended by , federal agencies .

as treasury and omb implemented the data act's requirement to create and apply data standards , the overall data standardization effort has been divided into two distinct , but related , components: ( 1 ) establishing definitions which describe what is included in each data element with the aim of ensuring that information will be consistent and comparable and ( 2 ) creating a data exchange standard with technical specifications that describe the format , structure , tagging , and transmission of each data element .

accordingly , omb took principal responsibility for developing policies and defining data standards .

treasury took principal responsibility for the technical standards that express these definitions , which federal agencies use to report spending data for publication on usaspending.gov .

under the act , agencies are required to submit complete and accurate data to usaspending.gov , and agency - reported award and financial information is required to comply with the data standards established by omb and treasury .

see app .

v for more information on the sources of data and process for submitting data under the data act .

since the data act's enactment in 2014 , we have issued a series of reports and made recommendations based on our ongoing monitoring of data act implementation .

in november 2017 , we issued our first report on data quality , which identified issues with , and made related recommendations about , the completeness and accuracy of the q2 fy2017 data that agencies submitted , agencies' use of data elements , and treasury's presentation of the data on beta.usaspending.gov .

in addition , as part of our ongoing monitoring of data act implementation , and in response to provisions in the data act that call for us to review ig reports and issue reports assessing and comparing the quality of agency data submitted under the act and agencies' implementation and use of data standards , we issued a report in july 2018 , based on our review of the ig reports of the quality of agencies' data for q2 fy2017 .

our prior reports identified significant data quality issues and challenges that may limit the usefulness of the data for congress and the public .

these data quality challenges underscore the need for omb and treasury to make further progress on addressing our 2015 recommendation that they establish clear policies and processes for developing and maintaining data standards that are consistent with key practices for data governance .

such policies and processes are needed to promote data quality and ensure that the integrity of data standards is maintained over time .

in march 2019 , we reported on the status of omb's and treasury's efforts to establish policies and procedures for governing data standards .

we found that omb and treasury have established some procedures for governing the data standards established under the data act , but a formal governance structure has yet to be fully developed .

therefore , we made recommendations to omb to clarify and document its procedure for changing data definition standards , and to ensure that related policy changes are clearly identified and explained .

for q4 fy2018 , 107 agencies , including all 24 cfo act agencies and 83 non - cfo act agencies , determined they were required to submit data , or they would voluntarily submit data , under the data act .

of these 107 agencies , 96 submitted data for q4 fy2018 .

this is an increase over the initial submissions for q2 fy2017 when 78 agencies submitted data that covered 91 federal entities .

this represents an improvement in the number of agencies reporting .

however , not all the required files submitted by agencies were complete , and the data submitted were not always accurate ( i.e. , consistent with agency source records and other authoritative sources and applicable laws and reporting standards ) .

in addition , we found that some cfo act agencies did not include certain financial assistance programs that made awards during fiscal year 2018 in their submissions .

finally , some agencies continued to have challenges in reporting some data elements in accordance with standards .

while the total number of agencies that submitted data for q4 fy2018 increased compared to q2 fy2017 , more agencies submitted their data for q4 fy2018 after the due date compared to q2 fy2017 .

in addition , the data for q4 fy2018 available on usaspending.gov are not complete because some agencies failed to submit data or submitted partial data .

fourteen agencies submitted late .

agencies were required to submit their data act files for q4 fy2018 by november 14 , 2018 — 45 days after the end of the quarter .

eighty - two agencies submitted their data on time .

these 82 agencies represented about 84 percent of the total obligations government - wide reported to treasury on the sf 133 for q4 fy2018 .

fourteen agencies submitted their data after the november 14 , 2018 due date .

our prior review of data submitted for q2 fy2017 found that one agency submitted data after the due date .

eleven agencies did not submit data .

eleven non - cfo act agencies did not submit any data act files for q4 fy2018 .

by contrast , in reviewing q2 fy2017 data , we identified 28 agencies that determined they should have reported data under the data act , but did not .

agencies told us that they did not submit data for q4 fy2018 because ( 1 ) there was confusion or miscommunication between the agency and its shared service provider about who was responsible for reporting the data ; ( 2 ) their officials had determined the agency was not required to report ; ( 3 ) new staff were unfamiliar with data act requirements ; and ( 4 ) technical or systems issues , such as a financial system upgrade in process , prevented them from reporting their data .

multiple agencies submitted blank files .

of the 96 agencies that submitted data act files for q4 fy2018 , 35 non - cfo act agencies submitted the file that links budget and award information ( i.e. , file c ) or the file containing procurement data ( i.e. , file d1 ) that did not contain any data ( i.e. , files were blank ) .

specifically , 34 non - cfo act agencies submitted a blank file d1 , which contains procurement data , and 16 of those 34 also submitted a blank file c. another non - cfo act agency submitted a blank file c only .

file c data are particularly important to oversight and transparency because they link budget and award information , as required by the data act .

without this linkage , policymakers and the public may be unable to effectively track federal spending because they would be unable to see obligations at the award and object class level .

agencies told us they submitted files without data for reasons including: ( 1 ) their data was submitted by and comingled with their shared service provider's data act submissions ; ( 2 ) they did not have award activity to report or award activity was below the micro - purchase threshold for reporting ; and ( 3 ) they do not use the federal procurement data system - next generation or their systems were unable to produce the data necessary to create the files .

we did not assess the completeness of file d1 in 2017 , but we found that 13 agencies submitted a blank file c in q2 fy2017 .

of these 13 agencies , two were cfo act agencies with large amounts of award activity — the departments of defense ( dod ) and agriculture ( usda ) — both of which did submit a file c with data for q4 fy2018 .

two agencies submitted incomplete files .

dod and treasury submitted all seven required data act files for q4 fy2018 , but the data in some of those files were not complete .

according to dod officials , its file c submission for q4 fy2018 included data from six of its 18 accounting systems .

dod officials said they are working to report data from all 18 systems in file c by the fourth quarter of fiscal year 2019 .

they said prior to q4 fy2018 , omb granted dod extensions for reporting financial and payment information in file c , as permitted by the act .

dod officials said the extensions allowed dod to focus on financial statement audit readiness , build a single source tool from which file c obligation data could be aligned with procurement and grant data , and coordinate with the intelligence community on concerns over increased transparency .

according to treasury officials , the agency's data submission did not include the spending of one of its component organizations — the treasury executive office for asset forfeiture , equitable sharing program — because omb guidance does not allow for reporting aggregate transactions when primary place of performance , a required data element , is at a multistate or nationwide level .

according to treasury officials , treasury is working with omb and the treasury data act program management office to allow for these types of transactions to be reported .

in our 2017 review , we identified similar challenges with the completeness of agencies' data act submissions for q2 fy2017 and made recommendations to treasury and omb to improve the completeness of data on usaspending.gov .

we recommended that treasury reasonably assure that ongoing monitoring controls to help ensure the completeness and accuracy of agency submissions are designed , implemented , and operating as intended .

treasury agreed with this recommendation .

in september 2019 , treasury officials told us that they are working to formalize a process for monitoring agency submissions that will include emailing reminders to agencies prior to submission deadlines , following up with agencies that do not submit required data on time , and forwarding a list of non - compliant agencies to omb .

we also recommended that omb continue to provide ongoing technical assistance that significantly contributes to agencies making their own determinations about their data act reporting requirements and that it monitor agency submissions .

while omb generally agreed with our recommendation , it has not yet taken steps to monitor agency submissions to help ensure their completeness .

in october 2019 , omb staff told us that they believe monitoring agency submissions is not their responsibility .

during this review we asked agencies why they did not submit data for q4 fy2018 .

subsequently , five of them submitted their data late ( out of the initial 18 agencies that had not submitted data ) , demonstrating that simple monitoring tasks such as a follow up call or email can result in actions taken by the agencies .

to address ongoing challenges with the completeness of agencies' data act submissions , we continue to maintain that treasury and omb should monitor agencies' submissions to help ensure the completeness and accuracy of those data submissions .

see app .

iv for more information on the status of these recommendations .

agencies did not report awards made to 39 financial assistance programs .

seven of the 24 cfo act agencies did not report spending for at least one financial assistance program that made awards during fiscal year 2018 .

file d2 contains detailed information about individual financial assistance awards .

we compared the spending data reported by the 24 cfo act agencies in file d2 against the assistance listings , formerly known as the catalog of federal domestic assistance ( cfda ) , a government - wide compendium of federal programs , projects , services , and activities that provide assistance or benefits to the american public .

as of march 2019 , the assistance listings website contained 2,926 programs for the cfo act agencies .

of these , 39 programs ( approximately 1 percent ) were not included in the q4 fy2018 data act submissions , even though these agencies stated that they made reportable awards during fiscal year 2018 .

in comparison , in july 2017 , the cfda listed 2,219 programs for the cfo act agencies .

of these 2,219 programs , 160 programs ( approximately 7 percent ) were not included in the q2 fy2017 data act submissions even though they made reportable awards .

the remaining programs either reported at least one award or did not make awards that were subject to reporting .

to provide a sense of magnitude of the underreporting , we obtained estimates of the total projected annual spending for these programs for fiscal year 2018 from the assistance listings website and applicable agencies .

based on the estimated obligations , the 39 programs account for approximately $11.5 billion in estimated annual obligations in fiscal year 2018 .

the omitted amounts largely resulted from usda's failure to report 27 programs representing more than 99 percent of the estimated annual obligations .

according to usda officials , usda did not submit awards for some of these programs because it maintains that the information in legacy reporting systems is incompatible with the treasury broker .

usda is working on solutions to resolve identified reporting challenges with its financial and awards systems .

treasury took steps to address findings on completeness issues for financial assistance programs we reported in 2017 .

at treasury's request , we provided details regarding the programs that were omitted from the usaspending.gov database for fiscal year 2017 , which treasury shared with the appropriate agencies .

in our review of fiscal year 2018 data , we found that only nine of these programs did not report .

based on the results of testing performed on a sample of budgetary and award transactions , we found that the overall completeness within individual transactions and accuracy of the reported data was high .

we selected a projectable government - wide sample of 405 transactions and tested 41 data elements and subelements associated with them for completeness and accuracy .

we determined data completeness within the transaction based on whether the element included a value and whether the value was appropriate .

we determined accuracy of data elements by determining consistency with agency source records as well as applicable laws and reporting standards .

specifically , based upon our sample we estimate with a 95 percent confidence level that all the data in the population were between 99 and 100 percent complete and between 90 and 93 percent accurate .

we further analyzed accuracy at the transaction and individual data element levels as follows: 1 .

transaction level , which describes the extent to which all applicable data elements within an individual transaction are complete and consistent with agency source records , and applicable laws and reporting standards .

2 .

data element level , which describes the extent to which the data elements and subelements used for reporting budgetary and award information were consistent with agency source records and applicable laws and reporting standards .

consistency of transactions .

for data submitted in q4 fy2018 , we found that the level of consistency differed between budgetary and award transactions , but both improved compared to the data we sampled for our review of q2 fy2017 data .

based on our projectable government - wide sample of q4 fy2018 data , we estimate with 95 percent confidence that between 84 and 96 percent of the budgetary transactions and between 24 and 34 percent of the award transactions in the usaspending.gov database were fully consistent with agency sources .

we considered a transaction to be “fully consistent” if the information contained in the transaction was consistent with agency records for every applicable data element .

this result represents an increase in consistency from what we reported in 2017 , when we estimated that between 56 and 75 percent of budgetary transactions were fully consistent , and between 0 and 1 percent of award transactions were fully consistent .

in addition to the transactions that were fully consistent , we estimate that 94 to 100 percent of budgetary transactions and 62 to 72 percent of award transactions in the population were significantly consistent .

we considered a transaction significantly consistent if 90 percent or more of the data elements and subelements in the transaction were consistent with agency source records and applicable laws and reporting standards .

consistency of data elements .

we also found improvements in the consistency of budgetary and award data elements with agency records , and applicable laws and reporting standards .

as shown in figure 1 , more data elements were significantly consistent and fewer were significantly inconsistent in q4 fy2018 than q2 fy2017 .

we considered a data element to be “significantly consistent” if the estimated consistency rate was at least 90 percent .

five of six of the budgetary data elements were significantly consistent in q4 fy2018 , compared to four of seven data elements in our 2017 review .

we also found improvements in the consistency of award data elements and subelements compared to our 2017 review .

eighteen of the 35 award data elements and subelements in our sample were significantly consistent in q4 fy2018 , compared to only one of 26 data elements and subelements we tested in our 2017 review .

see figure 2 for the data elements and subelements in our sample that were significantly consistent .

we considered a data element “significantly inconsistent” if it was either not consistent with agency records or incomplete at least 10 percent of the time .

we found that no budgetary data elements were significantly inconsistent , which is an improvement from our 2017 review where we found one budgetary data element — obligation — significantly inconsistent .

similarly , we found fewer significantly inconsistent award data elements compared to our 2017 review .

specifically , we found five of 35 award data elements and subelements significantly inconsistent in q4 fy2018 , compared to 11 of 26 in our 2017 review .

see figure 3 for the data elements and subelements in our sample that were significantly inconsistent .

unverifiable data elements .

we found no data elements that exhibited a significant amount of unverifiable information — incomplete or inadequate agency source records that prevented us from determining whether the data element was significantly consistent or inconsistent .

see app .

iii for details .

while we tested the consistency of agency records and applicable laws and reporting standards for the 41 data elements and subelements previously discussed , we performed a different test for three other data elements that contained a value derived by fpds - ng and fabs .

these data elements and subelements — legal entity county name , primary place of performance county name , and primary place of performance congressional district — were assessed against the other sources from which they were derived , such as data from the u.s. census bureau and house.gov , rather than agency records .

we found that each were neither significantly consistent nor significantly inconsistent with their sources .

see appendix iii , table 5 for details .

the data act requires omb and treasury to establish data standards to produce consistent and comparable reporting of federal spending data .

while we found improvements in the overall completeness and accuracy of the data when compared with the results of our 2017 review , we identified persistent challenges with the implementation and use of two award data elements — award description and primary place of performance address that limit the usefulness of these data .

we previously reported that these data elements are particularly important to achieving the transparency goals envisioned by the data act because they inform the public what the federal government spends money on and where it is spent .

in our sample results , we found agencies reported values for award description that were significantly inconsistent with agency sources and with the established standard for reporting this data element which is defined by the data act data standard as a “brief description of the purpose of the award.” based on our testing of a representative sample of q4 fy2018 transactions , we estimate that the award description data element was inconsistent with agency source records or contained information that was inconsistent with the established standard in 24 to 35 percent of awards .

while this represents an improvement over the results we reported for this data element in 2017 , we found in our testing that agencies continue to face challenges in reporting award description consistent with the established standard .

see figure 4 for several examples of the award description data submitted by agencies in our sample , which illustrates the range of agency interpretations of this data element from understandable to incomprehensible .

lengthy , technical description .

for example , the national aeronautics and space administration ( nasa ) included several paragraphs for the description of procurement and financial assistance award transactions in our sample that were long and highly technical .

these descriptions did not meet the data standard because they contained acronyms , jargon , and other technical terminology that might be challenging for others outside the agency to understand .

nasa officials said they use the award description field internally to search for vendors when making awards for similar services .

thus , they instructed contract officers to include as much information as possible to maximize the award description field for later use .

as of june 2019 , the general services administration decreased the character limit for reporting award description in fpds - ng for procurement awards from 4,000 characters to 250 characters to discourage agencies from copying and pasting sizeable portions of a contract's contents rather than thoughtfully including a brief description of what is being procured .

nasa officials said that the new maximum will limit the flexibility to search for contractors .

they are seeking alternatives for these searches .

no description provided .

the department of education reported “unknown title” for the award description for the majority of the financial assistance award transactions in our sample .

this does not meet the data standard because it does not provide any information about the award .

agency officials said the award description is provided by the applicant and if one is not provided , their system automatically will populate it as “unknown title.” geographic information .

dod reported location information for the award description in several transactions in our sample .

the locations reported in the description field were not understandable except to agency officials .

for example , one field contained the text “4542874050 ! trbo region 1.” dod officials explained that this description includes the part number for a medical supply item and the region of the country and is auto populated by an agency system .

while the description is consistent with agency sources , it is not easily understood by the public .

the defense federal acquisition regulation supplement procedures , guidance , and information provides instructions to use plain english as much as possible , and to explain numbers and acronyms .

dod officials said the agency is investigating methods to improve how similar transactions are auto - populated .

description of modification .

the department of homeland security ( dhs ) used the award description field to describe modifications to contracts instead of the good or service being procured .

specifically , dhs reported “de - obligate excess funds and closeout” for a modification to a contract that procured information technology products and services .

dhs officials said reporting the nature of the modification , rather than the original purpose of the award , is consistent with practices used in contract writing systems across the federal government and is intended to inform the public of changes made to the contract by the modification .

dhs is working with treasury to clarify how this information is displayed on usaspending.gov and suggested that additional information on how award descriptions for modifications are to be reported would be beneficial and should be provided in the daims .

we found that some individual agencies have taken steps to provide additional guidance on award description to ensure agency personnel are providing information that is consistent with the standard .

four agencies in our sample had additional guidance for their contracting officers .

for example , officials from the department of veterans affairs ( va ) said that in june 2019 , va trained hundreds of members of its contracting workforce with curriculum that included an interactive game to illustrate how to provide a brief description of an award that meets the standard for reporting this information .

officials from 11 agencies said additional guidance on award description could help ensure those entering the data understand the standard definition and report appropriate information , for example , by providing examples of award definitions that meet the standard .

in the absence of government - wide guidance , agencies have reported values that are inconsistent with the data standard and not comparable between agencies .

agencies also reported several challenges with reporting primary place of performance address for nonroutine locations , which omb and treasury defined as “where the predominant performance of the award will be accomplished.” taking into account each of its subelements , we found the information regarding primary place of performance address had higher rates of inconsistency than the majority of the data elements in our review .

multiple subrecipients .

agency officials reported challenges with identifying primary place of performance address in cases where an award is made to a recipient that further distributes the funding to subrecipients .

for example , the u.s. agency for global media ( usagm ) awards radio free europe / radio liberty a grant that funds work globally .

officials from usagm said that as a u.s. not - for - profit organization , radio free europe / radio liberty , maintains corporate headquarters in washington , d.c. , but , as an international media organization , maintains many offices abroad .

usagm reports the primary place of performance address as washington , d.c. because it is where the organization maintains its corporate office , but much of the performance takes place in other locations .

in another example , the department of health and human services' ( hhs ) centers for medicare and medicaid services ( cms ) reports the primary place of performance address for medicare payment data as the county of its payment processing centers , even though each processing center makes payments to recipients in multiple states and counties .

cms contracts with medicare administrative contractors ( mac ) to process and pay medicare fee - for - service claims .

for each type of medicare claim , the number of jurisdictions and the number of macs that handle that type of claim vary .

at the time of our review , there were 12 jurisdictions for medicare part a and b claims handled by macs .

as shown in figure 5 , the jurisdictions are made up of multiple states .

in addition to the mac jurisdictions for medicare part a and part b claims , there were four home health and hospice jurisdictions and four durable medical equipment jurisdictions .

thus , there are 20 mac jurisdictions , almost all of which covered multiple states .

as a result , the spending for medicare payments is reported in a small number of counties instead of where the beneficiaries of medicare services are located .

software .

officials from three agencies in our review said that it is challenging to determine primary place of performance address for software licenses when purchased as a service .

for example , there could be multiple performance locations , but none of these locations are predominant .

large or undefined locations .

officials from the agencies in our review reported challenges in meeting the standard for reporting large or undefined performance locations .

for example , officials from the delta regional authority said that it was difficult , at times , to determine the primary place of performance address for watersheds because they can cover a large area and cross multiple jurisdictions .

officials from the national science foundation ( nsf ) said that for projects that may not have a single location , they report the location that corresponds to the research asset's physical location or the primary site .

for example , for a research vessel , nsf officials report the awardee's address , which is generally the vessel's homeport as the primary place of performance address .

in another example , nasa officials said that when they let contracts for services performed on the international space center , they report the command center in houston as the primary place of performance address .

for some of these non - routine locations , the fpds - ng data dictionary provides guidance for procurement transactions .

for example , for services being performed in oceans and seas , it directs agencies to report the closest u.s. city .

for services being performed in the atmosphere or space , the fpds - ng data dictionary directs agencies to report the location from which the equipment conducting the services was launched .

however , the data act information model schema ( daims ) data dictionary does not include the same level of detailed guidance for reporting financial assistance awards and directs agency officials to report the location where the predominant performance of the award will be accomplished .

officials from several agencies said it would be helpful for omb and treasury to issue guidance on primary place of performance address for financial assistance awards to help agencies report this information consistent with the established standard .

in the absence of more specific guidance , agencies are using different decision rules to identify the primary place of performance address for financial assistance awards which could limit the usefulness of this information to the public .

we previously identified similar issues with award description and primary place of performance address on usaspending.gov .

we recommended that omb and treasury provide agencies with additional guidance to address potential clarity , consistency , or quality issues with the definitions for specific data elements including award description and primary place of performance address and that they clearly document and communicate these actions to agencies providing these data as well as to end - users .

omb issued guidance in june 2018 which provides clarification on reporting requirements for some data element definitions .

however , additional guidance is needed to clarify how agencies are to report spending data using standardized data element definitions that may be open to more than one interpretation , and then broadly communicate this information to agencies and the public .

we continue to believe additional guidance is needed to facilitate agency implementation of certain data definitions to produce consistent and comparable information .

given the challenges we identified in this report and in previous reports with award description and primary place of performance address , we have concerns about whether the guidance omb issued provides sufficient detail for agencies to consistently interpret and implement the definitions .

see app .

iv for more information on the status of this recommendation .

treasury does not fully disclose all known data limitations on usaspending.gov .

according to omb guidance , federal agencies should be transparent about the quality of information and identify the limitations of the data they disseminate to the public .

further , treasury's information quality guidelines state that , when disseminating information to the public , information should be presented within the proper context to disseminate information in an accurate , clear , complete , and unbiased manner .

in november 2017 , we identified data quality limitations that were not disclosed on usaspending.gov .

we recommended that treasury disclose known data quality issues and limitations on usaspending.gov .

treasury agreed with this recommendation and has taken steps to better disclose some of these limitations , but many of the issues we identified in 2017 continue to present challenges .

some of these challenges apply widely , while others were specific to particular agencies .

they include the following: data not submitted or incomplete .

one step taken by treasury to improve disclosure was to create a webpage in usaspending.gov that provides information on unreported data .

however , it is unclear exactly what this information covers .

for example , it is unclear whether the information on unreported data includes financing accounts , agencies that should have reported but did not submit data , missing data for agencies that did submit , or spending that was not reported because obligation amounts fell below $25,000 and was therefore not required to be reported .

as a result , users do not clearly know what data are unreported or the amount that was required to be reported .

optional data elements and subelements .

another issue we identified in 2017 and found again in our current review was that key information about the reporting requirements for some data elements and subelements was not adequately disclosed to the public .

specifically , for q4 fy2018 certain data elements were listed in guidance as optional for agencies to report .

according to treasury officials , agencies were not required to report these data elements because the data standard was not fully implemented .

for example , prior to fiscal year 2019 , the data element funding office name was optional for financial assistance awards .

additionally , as of september 2019 , period of performance start date and period of performance current end date remained optional for reporting pending government - wide agreement on the standard .

usaspending.gov does offer some information regarding optional data elements by providing a link to the daims reporting submission specifications document .

however , this document is not labeled in a way that would make it clear to the user what information can be found there .

moreover , some agencies may voluntarily submit data for optional fields so only partial information for optional data elements may be displayed on usaspending.gov .

because data limitations related to optional data elements are not prominently displayed on usaspending.gov , users may not know which data elements or subelements are potentially incomplete .

a more systematic approach for identifying and disclosing known data limitations on usaspending.gov — including procedures for addressing wide ranging issues such as communicating changes in the reporting requirements for certain data elements and information about data that may be unreported or incomplete — could help users of the data better understand potential quality issues with particular data elements and sources , and how to appropriately interpret the data .

while treasury has taken steps to better disclose data limitations , it needs to take further action to implement a more systematic approach , in line with our 2017 recommendation .

in addition to such broader challenges , we identified two specific data limitations involving dod and hhs: delay in availability of dod procurement data .

a third issue we identified in our 2017 review , and again in our current review , concerns how information on dod procurement data is presented on usaspending.gov .

specifically , information related to a 90-day delay in data availability for dod procurement awards is not posted on usaspending.gov .

fpds - ng — which collects information on contract actions for display on usaspending.gov — releases dod - reported procurement data to the public after a 90-day waiting period to help ensure the security of these data before they are released to the public .

this also results in a 90-day delay in reporting these data to usaspending.gov .

fpds - ng clearly states that dod data are subject to a 90-day delay as seen in figure 6 .

while dod reports this data limitation in its senior accountable official certification statement , it is not presented prominently to users who are viewing dod's spending data .

for example , dod's delay in data availability is not presented on dod's agency profile page or with queries on specific transactions associated with dod .

until such information is transparently communicated , users of usaspending.gov who access dod procurement data directly or as a result of broader government - wide searches are likely unaware that the information may be incomplete or not comparable .

medicare payment data .

additionally , in this review we found limitations in how medicare payment data are made available to the public .

according to hhs officials , cms reports the primary place of performance address for medicare payment data as the county for the applicable medicare administrative contractor ( mac ) because the mac is the direct recipient of the agency's contract award .

as a result , medicare spending data on usaspending.gov are not reported in the county where the medicare beneficiaries are located .

there are more than 3,200 counties and county equivalents in the united states and puerto rico , but only 20 medicare mac jurisdictions .

although medicare payments may reach every county in the country , the users of usaspending.gov will only see this spending in the counties in which a mac is located .

we found that this information is not described on usaspending.gov .

hhs officials said that they identified this limitation to the transparency of medicare payment data to treasury in 2016 .

they suggested that treasury add information about how medicare payments are reported on usaspending.gov to avoid confusion for users of the data .

however , at that time , treasury determined that it was unnecessary to provide this additional information on usaspending.gov .

until such information is transparently communicated , it will be unclear to the user that medicare payments are consolidated in the counties where macs are located .

one of the purposes of the data act is to establish government - wide data standards to provide consistent and comparable data that are displayed accurately for taxpayers and policymakers on usaspending.gov .

as we have reported previously , establishing a data governance structure — an institutionalized set of policies and procedures for providing data governance throughout the life cycle of developing and implementing data standards — is critical for ensuring that the integrity of data standards is maintained over time .

such a structure , if properly implemented , would greatly increase the likelihood that the data made available to the public will be accurate .

accordingly , in 2015 , we recommended that omb , in collaboration with treasury , establish a set of clear policies and procedures for developing and maintaining data standards that are consistent with leading practices for data governance .

this recommendation has not been implemented .

having formalized policies and procedures in place for one of these key practices — managing , controlling , monitoring , and enforcing the consistent application of data standards once they are established — could help address some of the data quality challenges we identified in this and previous reviews .

as described earlier , agencies experience challenges reporting award description and primary place of performance address .

we continue to believe that having a robust data governance structure that includes policies and procedures for enforcing the consistent application of the established standards would lead to greater consistency and comparability of reporting for data elements , such as award description and primary place of performance address .

omb and treasury have established some procedures for governing the data standards established under the data act , but a robust governance structure has yet to be fully developed and operational .

since the enactment of the data act in 2014 , omb has relied on a shifting array of advisory bodies to obtain input on data standards .

in march 2019 , we reported that the governing bodies involved in initial implementation efforts had been disbanded , and that their data governance functions were to be accomplished within the broader context of the cross - agency priority ( cap ) goals established under the 2018 president's management agenda ( pma ) .

since we issued our report , omb has taken additional steps to develop a government - wide data structure and to establish data governance programs at each agency .

omb staff told us that they envision agencies as incubators of data governance where they can learn lessons on data governance .

toward that end , omb , in collaboration with other interagency groups , has taken a number of steps to further develop data governance at both the agency and government - wide levels: in october 2019 , omb issued a set of grants management data standards under the results oriented accountability for grants cap goal .

according to omb staff , they received more than 1,100 public comments on draft standard data elements which were released for public comment in november 2018 .

omb issued a memorandum in april 2019 that outlines approaches to shared services and the governance structure established to support shared services used for data reporting .

in june 2019 , as part of the cap goal leveraging data as a strategic asset , omb issued the draft 2019-2020 federal data strategy action plan ( action plan ) .

this document identifies both government - wide and agency - level action steps for improving data governance .

to address government - wide data governance , the action plan calls for improvement in the standards for financial management data and geospatial data .

the action plan directs agencies to establish a body of internal stakeholders responsible for data governance .

these bodies will be made up of senior level staff and be responsible for assessing agency capability and ensuring monitoring and compliance with policies and standards related to data .

agencies are also instructed to assess data and related infrastructure maturity , identify opportunities to increase staff data skills , and identify data needs to answer key agency questions .

omb also issued initial guidance in july 2019 to support agency efforts to implement the first phase of the evidence act .

for example , the evidence act requires , among other things , agencies to designate a chief data officer by july 13 , 2019 .

omb also guidance directs agencies to establish a data governance body , chaired by the chief data officer , with participation from relevant senior - level staff from agency business units , data functions , and financial management by september 30 , 2019 .

in july 2019 , the federal data strategy team issued a data governance playbook .

according to omb officials , this playbook is not guidance , but is meant to be a framework for agency - level data governance accompanied by forthcoming resources .

omb staff told us that updates to the playbook would come relatively quickly , but also said they had no planned time frames for doing so .

agencies have taken initial steps to establish data governance programs and develop data quality plans .

as of september 2019 , seven of the 30 agencies included in our review reported that they have taken steps to designate a chief data officer as required by the evidence act .

twenty reported establishing internal bodies similar to the data governance bodies as directed by omb guidance .

the make - up and function of data governance bodies varies across agencies .

the department of labor reported its data board was formalized and that the acting chief data officer had become the official chief data officer .

the u.s. agency for international development reported establishing a data act governance council to facilitate the effective implementation of the data act .

other agencies reported similarly structured bodies referred to as working groups , steering committees , and consortiums .

as of september 2019 , 19 agencies reported that they have completed a data quality plan as required by omb memorandum , m - 18-16 .

nine agencies that do not have a data quality plan will have one completed by september 30 , 2019 .

the data quality plans from the agencies in our sample varied in scope and content .

features of data quality plans we reviewed included a description of a data governance board , an assessment of existing and planned internal controls for data quality , and determination of priority data elements based on assessments of risk of data quality issues .

for example , the departments of commerce and the interior each conducted a risk assessment on the likelihood and consequence of improper reporting for assistance and procurement data .

they will employ strategies or controls to mitigate risks related to the highest risk elements .

similarly , treasury named targeted data elements based on their relevancy and further assessed the risk of improper reporting of each element based on existing internal controls .

agencies in our review reported using a variety of sources of guidance in developing their data quality plans , including the data quality playbook issued by the leveraging data as a strategic asset working group in november 2018 , omb circular m - 18-16 , and guidance on conducting required reviews under the data act from the council of inspector general for integrity and efficiency .

while some agencies in our review reported that the information from these sources was helpful , they also noted the need for additional guidance , including help understanding the reporting requirements for certain data elements .

in the 5 years since enactment , omb , treasury , and federal agencies have made significant strides to address many of the policy , technical , and reporting challenges presented by the data act's requirements .

we found improvements in the overall quality of the data on usaspending.gov compared to our 2017 review of data quality .

to continue moving forward with this progress and to fully realize the data act's promise of helping to improve data accuracy and transparency , more needs to be done to address continued challenges with the completeness and accuracy of key data elements .

for example , omb and treasury have not fully addressed our recommendations to monitor agency submissions and ensure agencies are accountable for the completeness and accuracy of their data submissions .

in addition , without the transparent disclosure of known data limitations , users may view , download , or analyze data made available on the website without full knowledge of the extent to which the data are timely , complete , accurate , or comparable over time .

this could lead users to inadvertently draw inaccurate information or conclusions from the data .

we have previously recommended that treasury disclose known data limitations on usaspending.gov .

the agency has taken some steps toward this goal .

however , as we have shown , work remains for treasury to develop a more systematic approach for disclosing known data limitations on its website .

in the meantime , we believe it is important to address the specific data limitations we identify in this report .

these include the need to provide users with information about the delay in the availability of dod procurement data , and how medicare payment data are reported .

finally , the challenges we have found with data completeness and accuracy , and the transparency around data limitations also demonstrate the importance of continued progress by omb and treasury in addressing our previous open recommendations to develop a robust and transparent data governance structure , and implement controls for monitoring agency compliance with data act requirements .

we maintain that omb and treasury should address our prior recommendations on data act implementation , including recommendations on monitoring agency submissions , providing additional guidance on reporting established data standards , implementing a systematic approach to facilitate the disclosure of known data limitations on usaspending.gov , and developing a robust and transparent governance structure .

we are making a total of two new recommendations to treasury regarding the disclosure on usaspending.gov of specific known data limitations: the secretary of the treasury should ensure that information about the 90-day delay for displaying dod procurement data on usaspending.gov is transparently communicated to users of the site .

approaches for doing this could include prominently displaying this information on the dod agency profile page , in the unreported data section , and in search results that include dod data .

 ( recommendation 1 ) the secretary of the treasury should ensure that information regarding how the primary place of performance address for medicare payment data are reported is transparently communicated to the users of usaspending.gov .

 ( recommendation 2 ) .

we provided a draft of this report to the departments of agriculture ( usda ) , defense ( dod ) , commerce , education , health and human services ( hhs ) , homeland security , the interior ( doi ) , labor ( dol ) , the treasury , and veterans affairs ( va ) ; the office of management and budget ( omb ) ; the national science foundation ( nsf ) ; the national aeronautics and space administration ( nasa ) ; the small business administration ( sba ) ; the u.s. agency for international development ( usaid ) ; the u.s. agency for global media ( usagm ) ; and the delta regional authority ( dra ) for review and comment .

usaid and treasury provided written responses , which are summarized below and reproduced in appendixes vii and viii , respectively .

dhs and omb provided technical comments , which we incorporated as appropriate .

usda , dod , commerce , education , hhs , doi , dol , va , nsf , nasa , sba , usagm , and dra had no comments on the draft report .

in its written comments , usaid stated that it is committed to data act reporting and the accessibility and transparency of its spending data .

in its written comments , treasury stated its commitment to fully realizing the data act's promise of helping to improve data accuracy and transparency .

treasury agreed with our two recommendations on the disclosure of specific known data limitations and stated that it will work with hhs and dod to implement them in the coming months .

treasury also stated that it remains committed to fully implementing our prior recommendations on data act implementation .

we are sending copies of this report to the relevant congressional committees ; the secretaries of agriculture , defense , commerce , education , homeland security , the interior , labor , the treasury , and veterans affairs ; the directors of the office of management and budget and the national science foundation ; the administrators of national aeronautics and space administration , the small business administration , and u.s. agency for international development ; the chief executive officer of the u.s. agency for global media ; the chairman of the delta regional authority ; and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff has any questions about this report , please contact michelle sager at ( 202 ) 512-6806 or sagerm@gao.gov or paula m. rascona at ( 202 ) 512-9816 or rasconap@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of our report .

key contributors to this report are listed in app .

ix .

national science foundation nuclear regulatory commission ( nrc ) file b ( budgetary ) file d1 ( procurement ) the broadcasting board of governors changed its name to the u.s. agency for global media in august 2018 .

the digital accountability and transparency act of 2014 ( data act ) requires that we report on the timeliness , completeness , accuracy , and quality of the data submitted under the act and the implementation and use of data standards .

this review responds to the act's requirement by addressing the following: ( 1 ) the timeliness , completeness , accuracy , and quality of the data and the implementation and use of data standards ; and ( 2 ) the extent to which progress has been made to develop a data governance structure consistent with key practices , and how it affects data quality .

we also update the status of select implementation issues and our previous recommendations related to implementing the data act and data transparency .

to assess the timeliness , completeness , accuracy , and quality of the data submitted and the implementation and use of data standards , we analyzed agency submission files for the fourth quarter of fiscal year 2018 ( q4 fy2018 ) on usaspending.gov and reviewed a representative stratified random sample from the department of the treasury's ( treasury ) usaspending.gov database download for q4 fy2018 .

specifically , to assess timeliness , we accessed agency submission files on usaspending.gov for q4 fy2018 and determined whether agencies submitted their data by the established deadline — 45 days after the end of the quarter or november 14 , 2018 — based on the date agencies certified their submissions .

to help understand the proportion of spending that agencies reported by the due date , we obtained and analyzed a file from treasury containing sf 133 report on budget execution and budgetary resources ( sf 133 ) data — which includes unaudited balances reported by agencies — for q4 fy2018 .

these obligation balances are only used for illustrative purposes in our report .

they include financing accounts , among other things , which are not required to be reported under the data act .

to assess completeness , we determined whether ( 1 ) all agencies that determined they are required to or would voluntarily submit data act files did so , ( 2 ) the transactions reported in the files submitted by agencies contained all required data for that transaction , and ( 3 ) the database contained required assistance award data from the 24 chief financial officers act of 1990 ( cfo act ) agencies .

to determine whether all agencies that should have reported q4 fy2018 data did so , we compared treasury's list of agencies that determined they were required to or would voluntarily report data to the agency file submissions on usaspending.gov for q4 fy2018 .

we followed up with agencies that had not reported to find out the reasons for not reporting , but we did not verify the accuracy of their responses .

to assess the completeness of files submitted by agencies , we accessed the agency submission files for q4 fy2018 available on usaspending.gov and determined whether all files for each agency contained data ( i.e. , were not blank ) .

we followed up with agencies that submitted a blank file c and / or file d1 that did not contain any data to find out why the files were blank , but we did not verify the accuracy of their responses .

we also made inquiries of agencies to determine whether any agency components or systems did not submit data .

finally , we tested completeness of agency submissions through our sample testing , described in detail below .

to assess the completeness of assistance data in the usaspending.gov database , we determined the extent to which federal agencies were reporting required award data based on a list of potential award - making agencies / programs from assistance listings on beta.sam.gov , formerly the catalog of federal domestic assistance .

we identified all programs listed in the assistance listings , as of september 2018 .

for the 24 cfo act agencies only , we compared programs listed in the assistance listings to data in the usaspending.gov database to determine which programs reported information on at least one assistance award for fiscal year 2018 .

for any program reporting no assistance award information for the year , we asked agency officials why information was not reported .

for all programs that agency officials determined either made an award but did not report it , or reported awards late to usaspending.gov , we extracted the agencies' obligation estimates for fiscal year 2018 as reported in the assistance listings .

to further assess completeness of the data and to assess accuracy of the data and the implementation and use of data standards , we extracted all records included in the scope of our review from a database used to display data on usaspending.gov .

the records covered activity during q4 fy2018 ( july through september 2018 ) .

to extract all records from the database , we mapped the database fields to the data elements within the scope of our audit .

once we had the data within the scope of our audit for q4 fy2018 , we performed the following steps: sampling data to determine completeness and accuracy: from the database we extracted , we selected a stratified random probability sample of 405 records for q4 fy2018 .

data records were stratified into procurement award transactions , assistance award transactions , and budgetary records .

we randomly selected 158 procurement awards , 150 financial awards , and 97 budgetary records .

estimates for the results of the procurement , assistance , and budgetary samples have sampling errors of + / - 7.8 , 8 , and 10 percentage points or less , respectively , at the 95 percent level of confidence .

the probability sample was designed to estimate the overall rate of reporting errors for a data element with a sampling error of no greater than plus or minus 5.3 percentage points at the 95 percent level of confidence .

because we followed a probability procedure based on random selections , our sample is only one of a large number of samples that we might have drawn .

since each sample could have provided different estimates , we express our confidence in the precision of our particular sample's results as a 95 percent confidence interval ( eg , + / - 7 percentage points ) .

this is the interval that would contain the actual population value for 95 percent of the samples we could have drawn .

for 41 data elements and subelements required by ffata or the data act , we first assessed the extent to which a data element was complete — whether there was a value and if that value was appropriate .

if the data element was not complete , then we also considered that data element to not be accurate .

for those elements that were complete , we then assessed the extent to which the data were accurate by comparing the information in our sample to the information contained in the originating agency's underlying source documents , where available , and determining whether the data were consistent with applicable laws and reporting standards , as applicable .

therefore we determined an element was inconsistent if it was either inconsistent with the agency documents , applicable laws or reporting standards , or incomplete .

for three data elements that contained values derived by federal procurement data system - next generation ( fpds - ng ) and financial assistance broker submission ( fabs ) based on other values provided by agencies , we compared the information in the sample to other sources , such as data from the u.s. census bureau and house.gov .

this allowed us to verify whether the values in our sample were consistent with the systems from which they were derived .

we then interviewed agency officials to discuss differences between the information in our sample and information in agency or other sources .

data element and subelement testing: table 3 shows the 44 data elements and subelements tested in the statistical sample — including six budgetary data elements and 38 award data elements and subelements .

individual data elements may vary with their representation in the sample ( e.g .

legal entity address lines 1 and 2 ) because the data element was not required for all of the sampled data records .

specific error rates by category can be found in app .

iii .

the government - wide results are a weighted total of the three strata of our sample: ( 1 ) procurement award transactions , ( 2 ) assistance award transactions , and ( 3 ) budgetary records .

for reporting purposes , we combined some of the results for the award strata because some data elements appear in both files d1 ( procurement ) and d2 ( financial assistance ) .

see app .

i for the list of agencies and number of records randomly selected and tested in each strata .

if we determined , after reviewing agency source documents , that a data element was not applicable to the sampled record , we did not factor the data element into our evaluation of completeness and accuracy .

we determined an element to be unverifiable if no agency source records were provided or the records provided did not meet our audit standards .

to test the controls over the reliability of agency data , we obtained supporting documentation to confirm that the agency provided only official agency source documents , such as a system of records notice .

when such a supporting document was unavailable , we reviewed agency transparency policy documentation , data verification and validation plans or procedures , or system source code information to ensure the reliability of the data .

we did not assess the accuracy of the data contained in sources provided by agencies .

for the purposes of our review , we defined data quality as encompassing the concepts of timeliness , completeness , and accuracy .

therefore , our assessment of overall data quality is reflected in our specific assessments of these components .

we also reviewed omb , treasury , and agency documents related to data act implementation .

we interviewed omb and treasury officials on their role in data act implementation and interviewed officials from the agencies in our sample to discuss their test results and efforts to submit data under the data act .

to describe changes in data quality since our prior work , we compared the results of our review of q4 fy2018 data to the results of our review of quarter two fiscal year 2017 ( q2 fy2017 ) data performed in our first assessment of data quality .

for both reviews , we examined a projectable sample of budgetary and award transactions from a database that , according to treasury , is partly used to display data on usaspending.gov .

however , there were the following differences: ( 1 ) our 2017 sampling frame was confined to the 24 cfo act agencies ( which represented 99 percent of obligations in our data set at that time ) , while our sampling frame for this review included all agencies that submitted q4 fy2018 data files as of february 11 , 2019 ; ( 2 ) more agencies and their components reported data in q4 fy2018 than in q2 fy2017 ; ( 3 ) in 2017 our estimated error rate calculations included elements of certain sampled transactions that were determined to be not applicable to the transaction and were classified as consistent with agency sources in both the numerator and denominator while in this review , we excluded not applicable elements from both the numerator and denominator of the estimated rate calculations ; ( 4 ) our sampling frame for this review included more data elements and subelements than were in our q2 fy2017 sampling frame ; ( 5 ) in this review , since three data elements we reviewed were derived by fpds - ng and fabs rather than provided by agencies , we compared the information in the sample to other sources rather than agency documents and therefore did not include those results in our comparisons to q2 fy2017 ; ( 6 ) agencies' q4 fy2018 data were submitted under policies and procedures outlined in daims v1.3 which reflects changes in validation rules and reporting requirements from the daims v1.0 that was in effect in 2017 ; ( 7 ) omb issued additional guidance on data act reporting since we reported in 2017 ; and ( 8 ) changes were made to the treasury broker since our last report .

to evaluate how the current data governance structure affects data quality , we compared data quality challenges we identified during our review to key practices for data governance identified in our prior work to underscore the need for a more robust structure consistent with key practices .

to assess progress made to develop a data governance structure consistent with key practices , we reviewed policy and other documentation related to ongoing efforts to develop a government - wide structure for governing the standards established under the act and interviewed omb staff about these efforts .

we also reviewed agency data quality plans — guidance intended to facilitate agency efforts to establish data governance programs — and interviewed agency officials on their data governance efforts .

to update the status of our recommendations related to the implementation of the data act , we reviewed new guidance and other related documentation , and interviewed omb staff and treasury officials .

see app .

iv for an update on our recommendations related to data act implementation .

we conducted this performance audit from november 2018 to november 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

appendix iii: estimates of consistency rates for award transactions and budgetary accounts / balances accurate / consistent ( % ) q4 fy2018 q2 fy2017 q4 fy2018 q2 fy2017 q4 fy2018 q2 fy2017 3-8 97-100 83-91 0-1 5-11 0-3 inconsistent ( % ) catalog of federal domestic assistance number ( cfda ) inconsistent ( % ) data element primary place of performance address ( all subelements ) accurate / consistent ( % ) unverifiable ( % ) legal entity address city name refers to two subelements under daims v.1.3 ( legal entity address city name and foreign city name ) , which we combined for reporting purposes .

legal entity address state name refers to three subelements under daims v.1.3 ( legal entity address state description for procurement awards and legal entity address state name and foreign province name for financial assistance awards ) , which we combined for reporting purposes .

legal entity address zip code refers to four subelements under daims v.1.3 ( legal entity address zip+4 for procurement awards , legal entity address zip 5 and last 4 for financial assistance awards , and legal entity address foreign postal code for foreign financial assistance awards ) , which we combined for reporting purposes .

primary place of performance address zip code is one subelement under daims v.1.3 ( primary place of performance address zip+4 ) , which contains both the first five digits from the zip code and the last 4 .

however , the usaspending.gov database we obtained our sample from contained the zip code information for this element in two parts: 5 digit zip code and +4 .

therefore , we present these subelements separately for reporting purposes .

element was optional for fourth quarter of fiscal year 2018 .

unverifiable includes data elements rates as inaccurate because agency records were insufficient to complete the test or because the agency did not provide supporting documentation .

accurate / consistent ( % ) estimated ranges inconsistent ( % ) unverifiable ( % ) in our prior digital accountability and transparency act of 2014 ( data act ) reports , we have made recommendations to both the department of the treasury ( treasury ) and the office of management and budget ( omb ) on a range of topics .

treasury and omb have collectively taken action that resulted in closure of nine prior recommendations on the data transparency and implementation of the data act .

table 7 provides a listing of open data act recommendations at the time this report was issued as well as a short discussion of their status .

full and effective implementation of the open recommendations listed below will contribute to more reliable and consistent federal data to measure the cost and magnitude of federal investments as well as facilitate efforts to share data across agencies to improve transparency , accountability , decision - making , and oversight .

the digital accountability and transparency act of 2014 ( data act ) requires the office of management and budget ( omb ) and the department of the treasury ( treasury ) to establish government - wide data standards that to the extent reasonable and practicable produce consistent , comparable , and searchable spending data for any federal funds made available to or expended by federal agencies .

these standards specify the data elements to be reported under the data act and define and describe what is to be included in each data element , with the aim of ensuring that data will be consistent and comparable .

the data act requires omb and treasury to ensure that the standards are applied to the data made available on usaspending.gov which has many sources of data .

some data are from agency systems , while other data are pulled or derived from government - wide reporting systems .

key award systems that generate data files that are linked to agency submitted files include the federal procurement data system - next generation ( fpds - ng ) , which collects information on contract actions ; the financial assistance broker submission ( fabs ) which collects information on financial assistance awards ; the system for award management which is the primary database for information on entities that do business with the federal government ( i.e. , contractors and grantees ) , and in which such entities must register ; and the federal funding accountability and transparency act of 2006 ( ffata ) subaward reporting system ( fsrs ) , which provides data on first - tier subawards reported by prime award recipients .

agencies submit procurement award information to fpds - ng daily and financial assistance award information ( grants , loans , insurance and other financial assistance ) to fabs at least twice monthly .

these award data are reflected in usaspending.gov daily .

as depicted in figure 7 , agencies are expected to submit financial data linked to award data and certified on a quarterly basis , 45 days after the close of the quarter .

they submit three data files with specific details and data elements to treasury's data act broker ( broker ) from their financial management systems quarterly ( files a , b , c ) .

in february 2019 , to reduce agency burden , treasury made updates including an optional new broker feature that agencies can use to generate a provisional file a which agencies can choose to upload and submit as their file a in the regular submission process .

the new feature produces an agency's provisional file a based on budget and financial information reported by the agency to the government - wide treasury account symbol adjusted trial balance system for the creation of the sf 133 report on budget execution and budgetary resources .

the broker then extracts award and subaward information from existing government - wide reporting systems to build four files that include procurement information , information on federal assistance awards such as grants and loans , and recipient information ( files d1 , d2 , e , and f ) .

each agency's data must pass a series of validations in the broker and then be certified by the agency's senior accountable official ( sao ) before they are submitted for display on usaspending.gov .

according to omb guidance , the purpose of the sao certification is to provide reasonable assurance that the agency's internal controls support the reliability and validity of the data submitted to treasury for publication on the website .

the sao assurance means that , at a minimum , the data reported are based on appropriate controls and risk management strategies as described in omb circular a - 123 , management's responsibility for enterprise risk management and internal control .

in addition , agencies should include information about any data limitations in their sao certification statements .

committee for purchase from people who are blind or severely disabled ( abilityone commission ) district of columbia courts ( dc courts ) .

in addition to the above contacts , peter del toro ( assistant director ) , kathleen drennan ( assistant director ) , michael laforge ( assistant director ) , maria c. belaval ( auditor - in - charge ) , barbara lancaster ( analyst - in - charge ) , diane morris ( auditor - in - charge ) , carl barden , daniel berg , mark canter , jenny chanley , shelby clark , tracy davis ross , tabitha fitzgibbon , valerie freeman , jamaika hawthorne , michael kany , roy kilgore , peter kramer , seraé lafache - brazier , krista loose , tonyita muschette , quang nguyen , kristine papa , joseph raymond , lisa rowland , susan sato , john a. schaefer , sara shore , james skornicki , andrew j. stephens , james sweetman , jr. , silvia symber , and lisa zhao made key contributions to this report .

additional members of gao's data act internal working group also contributed to the development of this report .

data act: customer agencies' experiences working with shared service providers for data submissions .

gao - 19-537 .

washington , d.c.: july 18 , 2019 .

data act: pilot effectively tested approaches for reducing reporting burden for grants but not for contracts .

gao - 19-299 .

washington , d.c.: april 30 , 2019 .

data act: omb needs to formalize data governance for reporting federal spending .

gao - 19-284 .

washington , d.c.: march 22 , 2019 .

open data: treasury could better align usaspending.gov with key practices and search requirements .

gao - 19-72 .

washington , d.c.: december 13 , 2018 .

data act: reported quality of agencies' spending data reviewed by oigs varied because of government - wide and agency issues .

gao - 18-546 .

washington , d.c.: july 23 , 2018 .

data act: omb , treasury , and agencies need to improve completeness and accuracy of spending data and disclose limitations .

gao - 18-138 .

washington , d.c.: november 8 , 2017 .

data act: as reporting deadline nears , challenges remain that will affect data quality .

gao - 17-496 .

washington , d.c.: april 28 , 2017 .

data act: office of inspector general reports help identify agencies' implementation challenges .

gao - 17-460 .

washington , d.c.: april 26 , 2017 .

data act: implementation progresses but challenges remain .

gao - 17-282t .

washington , d.c.: december 8 , 2016 .

data act: omb and treasury have issued additional guidance and have improved pilot design but implementation challenges remain .

gao - 17-156 .

washington , d.c.: december 8 , 2016 .

data act: initial observations on technical implementation .

gao - 16-824r .

washington , d.c.: august 3 , 2016 .

data act: improvements needed in reviewing agency implementation plans and monitoring progress .

gao - 16-698 .

washington , d.c.: july 29 , 2016 .

data act: section 5 pilot design issues need to be addressed to meet goal of reducing recipient reporting burden .

gao - 16-438 .

washington , d.c.: april 19 , 2016 .

data act: progress made but significant challenges must be addressed to ensure full and effective implementation .

gao - 16-556t .

washington , d.c.: april 19 , 2016 .

data act: data standards established , but more complete and timely guidance is needed to ensure effective implementation .

gao - 16-261 .

washington , d.c.: january 29 , 2016 .

federal spending accountability: preserving capabilities of recovery operations center could help sustain oversight of federal expenditures .

gao - 15-814 .

washington , d.c.: september 14 , 2015 .

data act: progress made in initial implementation but challenges must be addressed as efforts proceed .

gao - 15-752t .

washington , d.c.: july 29 , 2015 .

federal data transparency: effective implementation of the data act would help address government - wide management challenges and improve oversight .

gao - 15-241t .

washington , d.c.: december 3 , 2014 .

government efficiency and effectiveness: inconsistent definitions and information limit the usefulness of federal program inventories .

gao - 15-83 .

washington , d.c.: october 31 , 2014 .

data transparency: oversight needed to address underreporting and inconsistencies on federal award website .

gao - 14-476 .

washington , d.c.: june 30 , 2014 .

federal data transparency: opportunities remain to incorporate lessons learned as availability of spending data increases .

gao - 13-758 .

washington , d.c.: september 12 , 2013 .

government transparency: efforts to improve information on federal spending .

gao - 12-913t .

washington , d.c.: july 18 , 2012 .

electronic government: implementation of the federal funding accountability and transparency act of 2006 .

gao - 10-365 .

washington , d.c.: march 12 , 2010 .

