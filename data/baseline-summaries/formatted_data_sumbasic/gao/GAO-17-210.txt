the threat posed by the proliferation of nuclear weapons is one of the most pressing national security challenges facing the united states .

to address this challenge , the national nuclear security administration ( nnsa ) — a separately organized agency within the department of energy ( doe ) — implements nuclear nonproliferation programs worldwide under its office of defense nuclear nonproliferation ( dnn ) .

dnn's mission is to develop and implement policy and technical solutions to eliminate proliferation - sensitive materials and limit or prevent the spread of materials , technology , and expertise related to nuclear and radiological weapons and programs around the world .

as part of this mission , dnn supports programs that conduct research and develop technologies for use in treaty monitoring , safeguarding nuclear materials , and detecting proliferation - related activities and nuclear detonations .

dnn's nuclear nonproliferation research and development ( r&d ) activities have been implemented under different program names since the mid - 1960s .

we have previously identified challenges affecting nnsa's nuclear nonproliferation r&d activities .

for example , in august 2002 , we found that nnsa did not have a formal process to identify the needs of technology users in the u.s. government's nonproliferation community for two of the three research areas managed at that time by nnsa's nonproliferation and verification r&d program — the predecessor of dnn's current nuclear nonproliferation r&d program .

additionally , we found then that nnsa had inadequate tools to monitor project progress .

nnsa agreed with our recommendations that the agency offer technology users opportunities to provide input through all phases of r&d projects and that it upgrade the r&d program's project management information system to track progress in meeting projects' milestones .

more recently , we found in a december 2011 report that some dnn program performance measures did not include key attributes of successful measures , such as being clear , reliable , and balanced .

for instance , we found that performance measures for dnn's nonproliferation and verification r&d program were unclear because the measures assessed progress using unspecified criteria and milestones from a classified r&d requirements document .

because these performance measures were linked to an unstated secondary set of criteria , we found that a third party unfamiliar with the classified requirements would have difficulty interpreting the measures and discerning whether the criteria for measuring progress were appropriate , sufficient , and up to date .

nnsa neither agreed nor disagreed with our recommendation that the agency clarify its performance measures for evaluating the nonproliferation and verification r&d program .

we discuss this recommendation later in this report .

the house armed services committee report 114-102 , which accompanied h.r .

1735 , the national defense authorization act for fiscal year 2016 , included a provision for us to review nnsa's nonproliferation r&d programs .

this report ( 1 ) evaluates the extent to which projects managed by nnsa's research and technology development programs have resulted in advanced , transitioned , or deployed nonproliferation technologies , ( 2 ) evaluates how nnsa measures the performance of its research and technology development programs and projects , and ( 3 ) describes how dnn programs decide which research and technology development projects to pursue .

to evaluate the extent to which nnsa's research and technology development projects have resulted in advanced , transitioned , or deployed nonproliferation technologies , we obtained and analyzed information on a nongeneralizable sample of 91 research and technology development projects managed by the two dnn programs responsible for such projects .

the information included project data from a program database .

we selected these 91 projects — which were undertaken between fiscal years 2012 and 2015 — for the sample based on funding level and project technical area , among other factors , to cover a broad range of program activities .

we also interviewed officials in nnsa headquarters , project managers at doe and nnsa national laboratories , and officials in other agencies that are end users of technologies and other outcomes that result from dnn's projects .

specifically , we interviewed officials at the departments of defense ( dod ) , homeland security ( dhs ) , and state , as well as officials at the preparatory commission for the comprehensive nuclear - test - ban treaty organization ( ctbto ) and the international atomic energy agency ( iaea ) .

we also reviewed documents that establish program goals and federal internal control standards .

to evaluate how nnsa measures the performance of its research and technology development programs and projects , we reviewed externally reported and internal program – and project - level measures that nnsa uses to assess performance in dnn's research and technology development programs .

while we also reviewed internal program - level measures , we focused our evaluation on external , publicly reported performance measures .

specifically , we reviewed measures developed for and reported on in nnsa's budget materials in response to the gpra modernization act of 2010 ( gprama ) , which updated the government performance and results act of 1993 .

for project - level measures , we reviewed documentation on the 91 projects we selected for our nongeneralizable sample ; specifically , we reviewed documents that establish baseline targets for projects' scope , cost , and completion date , as well as documents that contain information about project results .

we also interviewed dnn officials to obtain their views about program and project management and performance .

in addition , we reviewed documents that establish criteria for successful performance measures and federal internal control standards .

to describe how dnn decides which projects to pursue , we reviewed documents that establish the nonproliferation mission needs of doe , interagency , and iaea stakeholders ; those that establish program requirements and project selection criteria ; and project proposals .

we also interviewed nnsa and other agency officials and project managers at the national laboratories to understand the decision - making processes that dnn uses to define broad program areas and select projects for funding .

to assess the reliability of the data we analyzed on the projects in our nongeneralizable sample , we interviewed nnsa officials and reviewed documentation , and we determined that the data were sufficiently reliable for presenting the information contained in this report on nnsa project characteristics and funding .

additional details on our scope and methodology can be found in appendix i .

we conducted this performance audit from november 2015 to february 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

within dnn , two programs manage research and technology development projects: the dnn research and development ( dnn r&d ) program and the nonproliferation and arms control ( npac ) program .

these two programs have established program areas to focus their research and technology development activities .

dnn r&d program areas .

within the dnn r&d program , two program areas pursue research and technology development projects: nuclear detonation detection and proliferation detection .

the nuclear detonation detection program area develops and provides global monitoring capabilities for detecting foreign nuclear weapon detonations , in order to meet both treaty monitoring and military needs .

the proliferation detection program area develops capabilities to detect special nuclear materials ( which include plutonium and highly enriched uranium ) , to detect weapons production and movement , and to enhance transparent nuclear reductions and monitoring .

these two program areas each are divided into three functional areas .

the nuclear detonation detection program area's three functional space - based nuclear detonation detection , which develops and builds space - based sensors for nuclear explosion detection ; ground - based nuclear detonation detection , which produces and updates modeling and analysis capabilities — such as software codes and algorithms that help distinguish underground nuclear detonations from natural seismic events — and provides other technical support for the nation's ground - based nuclear explosion monitoring networks ; and forensics - based nuclear detonation detection , which conducts r&d to advance capabilities in the field of nuclear forensics analysis .

the proliferation detection program area's three functional areas nuclear weaponization and material production detection , which supports the development of technology to detect and characterize the production of nuclear weapons and related materials by foreign entities ; nuclear weapons and material security , which , among other things , develops tools for nuclear security , treaty monitoring and verification , and operational interdiction and nuclear security efforts across nnsa ; and enabling capabilities , which develops cross - cutting technologies applicable to multiple nnsa and interagency missions .

npac program areas .

within the npac program , two program areas pursue technology development projects: international nuclear safeguards ( safeguards ) , among other things , develops technologies to detect and deter undeclared nuclear materials and activities .

specifically , safeguards' activities include developing and transferring tools , technologies , and approaches to improve u.s. , iaea , and iaea member states' capabilities in undertaking iaea safeguards activities , such as monitoring uranium enrichment levels in nuclear facilities .

nuclear verification ( verification ) , among other things , develops and deploys technologies to maintain the united states' ability to monitor and verify nuclear reduction agreements , detect treaty violations , and verify other nuclear nonproliferation commitments .

the goals of the verification program area include developing technologies to verifiably and irreversibly disable nuclear facilities in countries of concern .

figure 1 illustrates the structure of the dnn programs and functional areas that manage research and technology development projects .

from fiscal year 2012 through fiscal year 2015 , dnn r&d and npac obligated a total of more than $1.1 billion on 511 research and technology development projects .

the dnn r&d program obligated more than $1 billion on 420 projects , and the npac program obligated about $73 million on 91 projects .

according to nnsa officials we interviewed , the space - based nuclear detonation detection functional area expenditures were mostly for production activities that resulted in technologies deployed to end users , rather than for basic and applied r&d , in contrast with the other dnn r&d program functional areas .

dnn r&d and npac projects are undertaken at doe and nnsa national laboratories and sites ; these projects generally last about 3 years from beginning to end , according to program officials .

table 1 shows the number of projects undertaken and funding obligated in each program and functional area from fiscal years 2012 through 2015 .

according to nnsa officials , the end users of technologies resulting from dnn's research and technology development programs include dod , including its defense threat reduction agency , u.s. strategic command , and air force technical applications center ; dhs ; state ; iaea and other international organizations , including the ctbto ; international governments ; universities ; and private industry , including small business .

nnsa itself also uses some of the technologies that result from dnn's research and technology development programs .

the full extent to which research and technology development projects managed by nnsa's dnn r&d and npac programs have resulted in advanced , transitioned , or deployed technologies is unclear because nnsa does not consistently track and document all of these project outcomes .

however , by reviewing a range of information on a nongeneralizable sample of 91 dnn research and technology development projects , as well as interviewing end users of technologies that resulted from some of the projects , we were able to determine the outcomes of these projects .

more specifically , we found that 88 of the 91 projects in the sample resulted in technologies being advanced — that is , the project progressed the technology itself or the scientific knowledge behind the technology .

additionally , we found that , among these 88 projects , 33 resulted in technologies being transitioned — that is , provided to users outside of the project team for further development or deployment .

finally , we found that 17 of these 33 projects resulted in deployed technologies — that is , we were able to confirm that a technology was being actively used in the field by a federal agency or foreign partner .

the full extent to which nnsa's research and technology development projects result in advanced , transitioned , or deployed technologies is unclear because nnsa does not consistently track and document all of these project outcomes .

both the dnn r&d and npac programs track and document projects' results in advancing technologies .

for instance , the dnn r&d program uses an online system , webpmis , to track project costs and maintain an archive of project - related documentation .

this documentation includes project reports that provide information on the scientific and technological advancements that result from a project .

similarly , the npac program provided documentation of technology advancement for our review .

however , the dnn r&d program , which is by far the larger program , does not consistently track and document projects that result in technologies being transitioned or deployed .

nnsa officials acknowledged that the dnn r&d program does not use the webpmis system , or otherwise collect and maintain information , to track and document whether its projects result in technologies that have been transitioned to or deployed by end users .

as a result , the dnn r&d program was not able to readily provide us with information on the extent to which its projects result in transitioned or deployed technologies .

in contrast , we found that the npac program tracks and documents the extent to which its safeguards – and verification - related projects have resulted in technology transition or deployment , as well as tracking scientific and technological advancements .

for example , nnsa officials provided us with information on all npac projects for the past 5 fiscal years that showed whether technologies that projects developed have been deployed ; they also provided documentation of technologies that have been transitioned to end users .

both the dnn r&d and npac programs list technology transition — which includes providing technologies to end users , who may then deploy them — as a goal in strategic planning documents .

in addition , npac program officials told us that their program evaluates its success in part based on the extent to which end users deploy technologies that the npac program develops .

similarly , dnn r&d program officials said that meeting end users' technology needs constitutes an important part of their program mission .

moreover , under federal internal control standards , management should design control activities to achieve objectives and respond to risks , and should record , communicate , and use quality information to enable managers to carry out internal control responsibilities and evaluate program performance in achieving key objectives .

differing program emphases — both between the dnn r&d and npac programs and among the dnn r&d program's functional areas — may account for the inconsistencies we found in nnsa's tracking and documentation of transition – and deployment - related project outcomes .

for instance , the npac program generally focuses on developing technologies at higher readiness levels to prepare them for transition and deployment in the field , according to nnsa officials .

conversely , most of the dnn r&d program's functional areas conduct r&d on technologies that are further from being deployment - ready because they require more basic work at lower technology readiness levels , with some exceptions , nnsa officials told us .

notably , the dnn r&d program's space - based nuclear detonation detection functional area has a history , dating to the 1960s , of researching , developing , and producing successive upgrades to technologies used by the u.s. atomic energy detection system and its space - based element , the u.s. nuclear detonation detection system , to monitor and detect nuclear detonations , nnsa officials stated .

officials in the dnn r&d program office said that well - defined dod technology requirements and delivery schedules drive these research , development , and production activities ; as a result , the space - based nuclear detonation detection functional area has a stronger focus on technology production and deployment than the other dnn r&d program functional areas , which typically do not have similarly specific requirements and schedules .

relevant data from reliable internal and external sources , processing the data into quality information , and then considering the data to make informed decisions and evaluate program performance .

providing end users with tools or knowledge that are sufficiently ready for the end user to take the final steps necessary for the technology's use in mission - specific purposes , such as adapting a detection algorithm or producing a field - ready detector array .

it can be difficult to trace a link between a project and a given result — for example , when another agency builds on a project's work ; when a discrete project does not lead directly to an outcome , but results in a “spin - off” project or is part of a related portfolio of projects that does lead to the desired outcome ; or when there is a significant period of time between the end of a project and the deployment of any resulting technology .

project managers at doe and nnsa national laboratories know the outcomes of projects and can provide information to nnsa officials when needed .

information about some nonproliferation technologies is classified , with the result that nnsa is not informed about their uses and users .

these factors notwithstanding , by not consistently tracking and documenting all relevant outcomes of projects across dnn's research and technology development program areas , nnsa is unable to demonstrate to congress and the public the full extent to which its projects result in technologies that are transitioned , deployed , or otherwise used toward fulfilling the nation's nuclear nonproliferation goals .

moreover , not maintaining documentary information about project results may affect control activities by limiting the quality of information that nnsa has available to evaluate project performance ; it also may reduce opportunities for sharing knowledge , and may make dnn vulnerable to the loss of institutional memory when personnel involved with a project retire or leave the agency .

by gathering information from various sources about our sample of 91 selected dnn research and technology development projects , we were able to determine that some projects resulted in technologies being advanced , transitioned , or deployed , or that they resulted in more than one of these outcomes .

more specifically , we found that nearly all of the projects we selected ( 88 of 91 ) advanced the technology itself or the scientific knowledge behind the technology .

of those 88 projects , we found that 33 also resulted in the advanced technology being transitioned to users outside of the project team for further development or deployment .

among the 33 projects that resulted in a transitioned technology , 17 also resulted in active technology deployment — that is , field use by a federal agency or foreign partner .

figure 2 below illustrates these results .

almost all of the selected projects in our sample ( 88 of 91 ) advanced technologies in some way .

examples of projects that advanced technologies include projects in which nnsa scientists built instrument hardware , made progress in developing new technology combinations or applications , developed an algorithm or model to further data analysis , or made scientific advances that improved nonproliferation analysis and operations .

some of the projects in our sample that have resulted in technologies being advanced have not resulted in technologies being transitioned to end users ( 55 of 88 such projects ) .

according to nnsa officials and scientists we interviewed , technology transition has not occurred on these projects for a variety of reasons , including: the technology is not developed enough to be transitioned .

for example , a dnn r&d proliferation detection project developed a technology to provide new capabilities in imaging gamma rays .

the imager is a laboratory prototype and needs further development , according to a scientist we interviewed at oak ridge national laboratory .

the project has not connected with an end user who wants to receive the technology .

for example , a dnn r&d proliferation detection project developed a hand - held fast - neutron generator , but nnsa has yet to find an end user interested in the technology , according to the manager of the nuclear threat reduction program at lawrence livermore national laboratory .

the project developed a technology to meet a future nonproliferation need rather than an existing requirement .

for example , a project within the npac verification program developed a fast - neutron imaging technology that could be used in stockpile warhead verification , should a future arms control agreement call for warhead inspections .

the technology currently is not being used in a verification context , but nnsa officials said it is in use at nnsa sites to compile benchmark data on nuclear weapons ; these data may later be used in verification and other national security missions .

we determined that 33 of the 91 projects in our sample resulted in technologies that were transitioned to end users .

these 33 projects include 17 projects that resulted in deployed technologies , since a technology is transitioned to an end user before it is deployed .

for the remaining 16 projects , we found that either the technologies had not been deployed by an end user ( 13 of the 16 ) or we could not confirm with end users that the technologies had been deployed ( 3 of the 16 ) .

specifically: we counted 13 of the 16 projects and their resulting technologies as transitioned but not deployed because the end users need to take certain steps before the technologies can be deployed , according to nnsa and end user agency officials .

for example , one dnn r&d nuclear detonation detection project transitioned an instrument to a dod end user .

however , the instrument has not been deployed because dod needs to find funds to further develop the technology , which , according to dod officials , was at a low level of technology readiness when transitioned .

in another case , a dnn r&d ground - based nuclear detonation detection project transitioned software to a dod end user , but the user is still evaluating the software before integrating it into its systems .

for the remaining 3 of the 16 projects , we could not confirm whether technologies that resulted from the projects and were transitioned to end users are being used .

we were able to confirm that 17 of the 91 projects in our sample resulted in actively deployed technologies .

more specifically , we found that 13 of the 63 dnn r&d projects and 4 of the 28 npac projects in our sample resulted in deployed technologies .

examples of the projects that resulted in deployed technologies include: an npac safeguards project that developed an online enrichment monitor ( olem ) for use in uranium enrichment monitoring .

as we reported in june 2016 , the iaea is using the olem in the natanz fuel enrichment plant in iran to confirm that enrichment levels are at or below 3.67 percent , per iran's commitment under the agreement known as the joint comprehensive plan of action ( jcpoa ) .

iaea has previously used enrichment monitors , but the olem is a newer technology that improves upon older monitoring systems , as we found in our june 2016 report .

dnn r&d space - based nuclear detonation detection projects that resulted in global burst detection sensors that are in use by the air force on satellites to detect , identify , and precisely locate nuclear explosions .

dnn r&d ground - based nuclear detonation detection projects that resulted in enhanced understanding of waves produced by underground nuclear explosions .

the air force , the ctbto , and international scientists analyze these waves to locate underground nuclear explosions and establish their yield .

of the 17 projects that resulted in deployed technology , 9 were active or ongoing projects and 8 were retired or in closeout , meaning that all of the research work was completed .

similarly , of the 33 projects that resulted in transitioned technology , 15 were active or ongoing projects and 18 were retired or in closeout .

projects resulting in deployed and transitioned technologies can continue to be active for the following reasons: in some cases , project officials provide maintenance and support to the technology after it has been deployed .

for example , the olem unit has been deployed , but the project is still active because nnsa is continuing to address end user maintenance needs , according to agency officials .

in other cases , the project has deployed or transitioned a technology but has yet to accomplish research goals .

for example , a nuclear detonation detection project has deployed a technology to a dod end user , but research continues because the project has not met all of the research objectives .

see appendix ii for a list of the projects in our sample that resulted in deployed technologies .

appendix iii provides information on the number of advanced , transitioned , and deployed technologies that resulted from the 91 projects we selected for our sample , subdivided by dnn program and functional areas .

nnsa's dnn r&d and npac programs use publicly reported measures to assess program - level performance , and both programs review project documentation and communicate with project managers to assess project - level performance .

clarity limitations with the dnn r&d program's publicly reported measures , however , make them difficult to interpret , and improvements in nnsa's final project documentation could enhance assessment of project performance .

nnsa reports publicly on the dnn r&d and npac programs' performance using measures published in doe's annual budget requests , in response to requirements in gprama ; however , limitations with the clarity of the dnn r&d program's measures may make it difficult for users , such as congress , to understand the targets that nnsa has established for its research and technology development programs , as well as how nnsa has measured performance against these targets .

the nnsa budget materials we reviewed present a total of six gprama measures associated with the dnn r&d and npac programs .

specifically , the dnn r&d program reports on five gprama measures covering both the nuclear detonation detection and proliferation detection program areas .

the npac program reports on one measure for its safeguards program area , but does not have a gprama measure for its verification program area .

the six gprama measures identify specific performance timeframes and endpoint targets , and the publicly reported information summarizes nnsa's assessment of the programs' performance against the targets .

appendix iv shows the npac and dnn r&d performance measures as presented in nnsa's budget request for fiscal year 2017 , including endpoint targets and nnsa's assessment of the programs' performance against these targets .

we found that the dnn r&d program's performance measures are unclear because the program does not define targets or explain its assessments of performance against the targets in sufficient context to allow users to interpret the measures or performance assessments .

for example , regarding three of the dnn r&d program's measures: the nuclear detonation detection measure tracks percentage of progress against an annual index summarizing the status of all nnsa nuclear detonation detection r&d deliveries that improve the nation's ability to detect nuclear detonations .

the nuclear weapons and material security measure tracks cumulative percentage of progress toward demonstrating improvements in special nuclear material detection , warhead monitoring , chain - of - custody monitoring , safeguards , and characterization capabilities .

the nuclear weaponization and material production detection measure tracks cumulative percentage of progress toward demonstrating improvements in detection and characterization of nuclear weapons production activities .

for these three measures , the baseline criteria for assessing performance — the annual index and the cumulative percentages of progress — are not defined , and no justification is presented to clarify how nnsa concluded that it had met its performance targets .

without such context , the measures provide statements of nnsa's assessment of their own performance , but do not provide users with information they can use to evaluate whether nnsa's assessments are valid and justified .

previous gao reports have identified key attributes of successful performance measures , including that they be clearly stated to enable entities to assess their own performance and to enable stakeholders to determine whether a program is achieving its goals .

in addition , our december 2011 report on nnsa program management and coordination challenges in the area of nuclear nonproliferation identified concerns with the clarity of performance measures used by the dnn programs we review in this report .

specifically , we found that several of the dnn r&d program's performance measures were linked to secondary criteria that are classified , official use only , or otherwise not publicly stated , making it difficult for third - party users to interpret the measures and discern whether the criteria are appropriate , sufficient , and up to date for tracking purposes .

we recommended in our 2011 report that nnsa clarify these measures .

nnsa neither agreed nor disagreed with this recommendation .

nnsa officials told us , when we interviewed them for this report , that they do not agree that the dnn r&d program's performance measures are unclear , for two reasons .

first , according to nnsa officials , nnsa provides supplemental information for office of management and budget ( omb ) users of the performance information that explains the targets and how performance is measured .

specifically , nnsa officials told us that the dnn r&d program provides additional documentation to omb — some of which the officials also provided to us for review — that includes information explaining the progress against annual targets under each of the program's performance measures .

however , most of the information that nnsa provided on the dnn r&d program is classified or official use only .

second , an nnsa official told us that the dnn r&d program performance measures are not unclear because the measures are used primarily to inform internal program managers who are familiar with classified requirements against which progress is assessed , rather than to provide information to third - party users .

however , there may be potential external users of the performance information — such as congressional decision makers and other external parties — who are not familiar with the baseline requirements and may have difficulty interpreting the level of performance achieved by the dnn r&d program .

in the current review , the clarity issues we found with the dnn r&d program's performance measures were similar to those we identified in december 2011 .

we continue to believe that nnsa should clarify these publicly reported measures , so that users of the performance information — including congress and the public — have a sufficient basis to judge the programs' performance in meeting the united states' nuclear nonproliferation technology needs .

nnsa assesses the performance of the dnn r&d and npac programs' projects by reviewing project documentation and communicating with project managers , but improvements in nnsa's final project documentation could enhance assessment of project performance .

to assess project performance , nnsa officials said that both programs track progress during the project by communicating with project managers at the national laboratories and by reviewing documents , such as quarterly project reports , that provide information on project performance .

according to nnsa documentation we reviewed and nnsa officials we interviewed , the dnn r&d and npac programs establish baseline targets for projects' scope of work and completion date in project plans .

in some cases , initial project plans are updated — for example , to incorporate new or modified project objectives .

at the end of the project , project managers at the national laboratories produce final project reports for program managers to review .

nnsa officials stated that these final project reports describe a project's technical results .

however , the officials said that the final reports do not document their assessment of project performance against the baseline targets established in the initial project plans .

they also said that there is no common template for final project reports .

we confirmed this in our review of several final project reports .

nnsa officials provided several reasons why they do not document their assessment of project performance against baseline targets or use a common template for final project reports .

for example , according to nnsa officials: program managers communicate with project managers at the laboratories to ensure that projects are proceeding as planned .

restating the baseline targets in final project reports would be duplicative of information contained in other sources , such as the webpmis database .

the varying formats of the laboratories' final reports do not affect program managers' ability to understand the technical outcomes of the projects .

according to nnsa officials , initial baselines may change through “progressive baselining” — which entails changing the initial project plans when discoveries warrant or course corrections are needed , subject to a defined change control process and the approval of the federal program manager .

federal internal control standards specify that managers should use quality information to make informed decisions and evaluate performance ; that managers need to compare actual performance to planned or expected results ; and that entities should record and communicate information to managers who need it to carry out their internal control responsibilities .

moreover , the standards specify that program managers need data to determine whether their programs are meeting their goals for accountability for effective and efficient use of resources .

documenting assessments that compare final project performance results against baseline targets for scope of work and completion date — whether through a common template for final project reports or by other means — could enhance nnsa's ability to manage its programs in accordance with these standards .

both the dnn r&d and npac programs use similar five - step processes to decide which research and technology development projects to pursue .

according to nnsa officials and stakeholders we interviewed at technology end user agencies , nnsa collaborates throughout the decision - making and project selection process with the end users .

in our review of program documents and interviews with agency officials , we found that the dnn r&d and npac programs generally take the following steps in their project selection processes: step 1: dnn establishes project requirements .

according to nnsa officials , dnn r&d and npac project requirements flow from national policy documents , such as the nuclear posture review ; additionally , npac project requirements flow from international sources such as the treaty on the non - proliferation of nuclear weapons ( npt ) , comprehensive safeguards agreements ; additional protocols ; and the iaea department of safeguards long term r&d plan .

to identify the technical capabilities that are required to meet the requirements established in these documents and sources , nnsa officials develop more detailed plans , such as the dnn r&d program's goals , objectives , and requirements ( gor ) documents .

the gor documents specify requirements for five of the dnn r&d program's six functional areas — all but the cross - cutting enabling capabilities functional area .

additionally , each gor document has a corresponding technology roadmap , which establishes detailed technical requirements to meet the needs identified in the gor document .

similarly , the npac program develops needs documents that identify specific needs and corresponding capabilities to develop ; npac uses these documents to guide the proposal selection process .

nnsa consults with interagency stakeholders to inform the requirements specified in the gor documents and needs documents .

step 2: the dnn programs issue annual calls for proposals .

both programs issue annual calls for proposals , which specify the technical requirements proposals should address .

the programs also require that those who submit proposals clearly explain how the proposed project relates to identified project requirements .

according to nnsa officials , the dnn r&d program typically issues its calls for proposals in mid - october and npac issues its calls for proposals during the spring .

the dnn r&d program's calls for proposals specify weighted criteria against which proposals are reviewed , including mission relevance , scientific and technical merit , and budget .

the npac program's calls for proposals differ between its two program areas .

the safeguards program area provides explicit evaluation criteria including cost effectiveness , technical maturity , and deployment potential .

calls for proposals in the verification program area do not provide explicit evaluation criteria , but they provide information on program goals and areas of technical consideration for the current budget year .

they also require applicants to describe the proposed work , including activities , milestones , and deliverables .

step 3: national laboratories submit informal project proposals .

scientists at the national laboratories respond to the call for proposals by submitting informal project proposals , known as white papers , to the nnsa program offices .

the white papers describe proposed projects to demonstrate new capabilities or technologies sought in the calls for proposals .

as specified in the dnn r&d program's calls for proposals , the program uses a three - tiered system to rank proposals — specifically , by designating proposals as hot , warm , or cold — and provides comments to improve the proposals or future white paper submissions .

according to an nnsa official , the dnn r&d program returns the informal project proposals to the original submitters .

all hot and warm proposals may be amended in response to program officials' comments and then resubmitted in the next step of the process as formal project proposals .

nnsa officials stated that cold proposals receive no further consideration .

nnsa officials within the npac program stated that they use a ranking system that is similar to the dnn r&d program's system ; the officials also provided documentation that shows their rationale for selecting or not selecting projects for further consideration .

both programs seek to provide feedback within a general time frame .

the dnn r&d program aims to provide feedback within 2 weeks of receiving the white papers .

according to nnsa officials , npac's safeguards program area provides feedback on the white papers about 3 to 3 1 / 2 months from the date of submission , and the verification program area provides feedback about 2 1 / 2 months from the date of submission .

according to nnsa officials and interagency stakeholders we interviewed , interagency stakeholders are consulted during this step of the process , as appropriate .

step 4: laboratories submit formal project proposals .

scientists at the national laboratories amend hot and warm white papers and resubmit them as formal proposals for nnsa's consideration .

step 5: dnn selects projects in consultation with laboratories and end users .

nnsa officials identify the formal proposals that best meet the criteria and requirements specified in the calls for proposals , considering input provided by laboratory officials , interagency stakeholders , and potential end users on each proposal .

according to nnsa officials , after discussing budgetary considerations and current , prioritized needs with interagency stakeholders and potential end users , program officials develop a final , ranked list of projects selected to receive funding and support .

program officials then circulate this final list to interagency stakeholders and potential end users .

figure 3 shows the general five - step selection process used by dnn's programs .

nnsa's research and technology development programs make vital contributions to national security , and the projects they support address a range of important proliferation detection and monitoring needs , including improving abilities to detect covert nuclear material production , nuclear explosions , and potential violations of iaea safeguards agreements .

it is not clear , however , how often nnsa's research and technology development projects — especially those supported by the dnn r&d program — result in transitioned or deployed technologies because nnsa does not consistently track and document such information .

we acknowledge that dnn's research and technology development programs pursue a broad range of scientific tools and capabilities , with the goal of ensuring that the united states remains prepared to deal with unanticipated nonproliferation challenges as they emerge .

therefore , we recognize that not all r&d projects , such as projects conducted at low levels of technology readiness , are intended to result in a deployed technology , and that a higher deployment rate does not necessarily indicate a more successful r&d program , or vice versa .

moreover , we recognize that in cases where technology transition and deployment are project goals , certain challenges complicate dnn's efforts to track the deployed technologies that result from its projects .

nevertheless , we were able , as part of our review of selected projects , to track projects' end results , including end users' deployment of technologies that result from the projects .

moreover , not tracking such end results may affect control activities by limiting the quality of information that nnsa has available to evaluate project performance .

more consistently tracking and documenting the transitioned and deployed technologies that result from its projects would help ensure that nnsa maintains and uses quality information to evaluate its performance and achieve its objectives , in keeping with federal internal control standards .

doing so would also facilitate knowledge sharing within dnn , and it would provide a means by which to present valuable information to congress and other decision makers about the programs' results and their overall value .

in addition , better tracking of project results by the dnn r&d program may have the salutary benefit of providing information to the program that could allow it to develop clearer program performance measures , as we recommended in december 2011 .

the dnn r&d program's performance measures continue to have clarity limitations similar to the ones we identified in the december 2011 report , because understanding the publicly reported performance information depends on users of the information having access to or being familiar with criteria in official use only or classified documents .

we continue to believe that it is important for nnsa to implement our december 2011 recommendation that nnsa clarify the dnn r&d program's publicly reported performance measures , so that congress and other users of these measures have a sufficient understanding of dnn programs' status and progress .

by documenting assessments that compare the final results of a project against the baseline targets for scope of work and completion date that are established in initial plans for each project , nnsa could improve the quality of the information it needs to evaluate performance at the end of the project .

specifically , taking these steps could enhance managers' ability to use quality data to assess actual project performance against planned or expected results , consistent with federal internal control standards .

such assessments are essential to ensure that the dnn r&d and npac programs' research and technology development projects are effectively and efficiently using resources , another key aspect of federal internal control standards .

if changes to the initial baselines are necessary — which may be the case in some instances — we believe that they should be made in such a way that initial project targets are still documented and available for review in evaluating the full progression of a project .

we recommend that the nnsa administrator take these two actions: direct the dnn r&d program to track and document the transitioned and deployed technologies that result from its research and technology development projects , to the extent practicable .

direct the dnn r&d and npac programs to document , using a common template or other means , their assessment that compares the final results of each project against the baseline targets established in each project's initial project plan .

we provided drafts of this report to nnsa , dod , dhs , and state for review and comment .

in emails , dod , dhs , and state stated that they had no comments on the report .

in nnsa's written comments , which are summarized below and reproduced in appendix v , nnsa neither agreed nor disagreed with our first recommendation and partially agreed with our second recommendation ; however , nnsa stated that it plans to take actions in response to both recommendations .

nnsa also provided technical comments , which we incorporated as appropriate .

in its written comments , nnsa did not state whether it agreed or disagreed with the recommendation that , to the extent practicable , the dnn r&d program track and document transitioned and deployed technologies resulting from the program's projects , but it described planned actions consistent with the recommendation .

nnsa stated that our report did not clearly or fully disclose reasons that the dnn r&d program does not track and document transition and deployment of technologies resulting from its projects .

specifically , nnsa commented that the dnn r&d program's space - based activities — unlike the other dnn r&d program areas — are production and deployment oriented , and that it would therefore be misleading to track individual projects in this area that are transitioned and deployed .

regarding the other dnn r&d program areas , nnsa commented that it does not have ongoing insight into transition and deployment outcomes for various reasons — for example , due to classification barriers regarding the transition and deployment of certain technologies .

nnsa also expressed concern that information on such outcomes may not be readily available or may not be cost - effective to obtain .

our report discusses a number of reasons why the agency may not consistently track the deployment – and transition - related outcomes of the dnn r&d program's projects and notes the production and deployment orientation of the space - based project portfolio .

however , we disagree that tracking the outcomes of these projects would be misleading .

to the contrary , it is vital to track these outcomes , as the space - based program area receives the largest share of the dnn r&d program's funding .

moreover , we believe that information on project outcomes should be readily available to nnsa program officials in the course of their regular interactions with the project managers at the national laboratories and the interagency end users of nonproliferation technologies — as was the case with projects in our sample .

nnsa agreed that , where such information is readily available , reporting of transitioned and deployed technologies can provide information on program successes , and it identified actions it will take consistent with the recommendation .

specifically , nnsa stated it will complete an assessment by june 2017 of the dnn r&d program's portfolio to determine to what extent transition and deployment data are readily available for specific projects and include additional information on those projects as part of its performance reporting .

if implemented as planned , such an assessment would be a step toward ensuring that nnsa maintains and uses quality information to evaluate its performance and achieve its objectives , in keeping with federal internal control standards .

nnsa stated that it agrees in part with our second recommendation — that nnsa document , using a common template or other means , its assessment that compares the final results of each dnn r&d and npac project against the baseline targets established in each project's initial project plan .

nnsa stated that it supports implementation of a common template for documenting project closeout to provide consistency in information reporting , but it noted that r&d projects are dissimilar to acquisition activities and that comparing final project results to an initial project baseline would be misleading .

specifically , for exploratory r&d projects , nnsa commented that project outcomes can be “significantly indeterminate” at the beginning of a project and stated that it uses an approach known as “progressive baselining” to adjust plans and benchmarks as research matures , where appropriate .

nnsa stated that final project results should be compared to the final approved plans to ensure consistency in project documentation and to provide a meaningful assessment of project performance .

nnsa agreed that information on the project's initial baseline is important to understand the progression of the project .

we acknowledge that adjustments to initial project plans may be appropriate in some cases , provided that the initial plans remain available and transparent and that decisions behind such revisions are well - documented , consistent with federal internal control standards .

nnsa stated it will establish a common template by june 2017 and use it to compare projects' final results to the final approved plans .

it also stated that the template will require a brief description of significant changes between the initial and final approved project plans .

if implemented as planned , the template could help nnsa to ensure that the dnn r&d and npac programs' research and technology development projects are effectively and efficiently using resources , in keeping with federal internal control standards .

we are sending copies of this report to the appropriate congressional committees ; the administrator of nnsa ; the secretaries of defense , energy , homeland security , and state ; and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff members have questions about this report , please contact me at ( 202 ) 512-3841 or oakleys@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix vi .

this report ( 1 ) evaluates the extent to which projects managed by the national nuclear security administration's ( nnsa ) research and technology development programs have resulted in advanced , transitioned , or deployed nonproliferation technologies , ( 2 ) evaluates how nnsa measures the performance of its research and technology development programs and projects , and ( 3 ) describes how programs in nnsa's office of defense nuclear nonproliferation ( dnn ) decide which research and technology development projects to pursue .

to determine the extent to which nnsa's research and technology development programs have resulted in advanced , transitioned , or deployed nonproliferation technologies , we selected a nongeneralizable sample from research and technology development projects undertaken between fiscal years 2012 and 2015 by the two dnn programs responsible for such projects: the office of dnn research and development ( dnn r&d ) and the office of nonproliferation and arms control ( npac ) .

as detailed below , we selected the projects in the sample based on funding level and project technical area of focus and status in order to cover a broad range of program activities .

in total , the dnn r&d and npac programs provided information on 511 projects .

specifically , the dnn r&d program provided us with a list of 420 projects executed between fiscal years 2012 and 2015 , and the two npac program areas that manage technology development projects — international nuclear safeguards ( npac safeguards ) and nuclear verification ( npac verification ) — provided us with a list of 91 projects that were executed between fiscal years 2012 and 2015 .

from this universe , we developed selection criteria to include projects from five of the major department of energy ( doe ) and nnsa laboratories — los alamos national laboratory , oak ridge national laboratory , sandia national laboratories , lawrence livermore national laboratory , and pacific northwest national laboratory .

our selection criteria ensured variations in program areas and project outcomes .

we generally limited our selection to the top 25 percent of projects in terms of total funding between fiscal years 2012 and 2015 , and excluded all projects that were not led by one of the five national laboratories listed above .

we also excluded projects in certain categories , such as projects under the program directors and university accounts .

after these exclusions , 134 projects remained in the sample .

the next step in our selection process was to group together similar projects , based on project status , focus area or type , age , and cost .

we generally selected 2 projects from each group , to ensure that each combination of these characteristics was represented in our final sample .

in cases in which a group contained only one project , we included only that project .

for the dnn r&d program , we treated projects with the same laboratory , focus area , and status as a single group , resulting in 40 groups .

for each group , we generally chose the 2 projects with the lowest project number ( signifying the earliest start date ) , resulting in the inclusion of 63 projects .

for the npac safeguards program area , we treated projects with the same technology category ( comparable to focus area ) and project status as a single group , resulting in 13 groups .

because we did not have data to determine earliest start date , we generally chose the 2 projects with the highest total expenditures from each group , resulting in the inclusion of 17 projects .

for the npac verification program area , we treated projects with the same r&d category ( comparable to focus area ) and project status as a single group , resulting in 8 groups .

because we did not have data to determine earliest start date , we chose the 2 projects with the highest total expenditures from each group , resulting in the inclusion of 11 projects .

this resulted in a final sample of 91 projects covering the dnn r&d and npac programs and all of their program and functional areas .

table 2 , below , shows the number of projects we selected from each program and functional area .

to determine the extent to which the 91 projects in our sample resulted in advanced , transitioned , or deployed technologies , we gathered project information from interviews and documentary sources .

specifically , we interviewed project officials , including the project manager , at the national laboratory that was the lead on the project , as well as interagency officials that received technologies transitioned from the dnn r&d and npac programs .

our interviews included officials from the departments of defense ( dod ) , homeland security , and state ; the international atomic energy agency ( iaea ) ; and the preparatory commission for the comprehensive test ban treaty organization .

we also reviewed program and project data and documentation , including basic project data — such as project names and funding information — stored in dnn r&d's webpmis database ; program funding information ; project plans ; annual , quarterly , and final reports ; and research presentations and publications .

we reviewed the evidence to determine whether each project in our sample resulted in advanced , transitioned , or deployed technology or other outcomes .

the outcomes that we identified are not mutually exclusive for any project .

for example , one project could transition a technology to an end user and also result in a publication , or a project may have resulted in a research outcome that we did not identify because the outcome was not indicated in the evidence we collected .

finally , we reviewed documents that establish program goals , including strategic planning documents , as well as standards for internal control in the federal government , which establishes federal standards for the use of information in achieving an entity's objectives .

to evaluate how nnsa measures the performance of its research and technology development programs and projects , we reviewed externally reported and internal program – and project - level measures that nnsa uses to assess performance in dnn's research and technology development efforts .

for program - level measures , we focused our review on the publicly reported measures developed and reported on in nnsa's budget materials in response to the gpra modernization act of 2010 ( gprama ) .

specifically , we reviewed the gprama measures for the dnn r&d and npac programs that nnsa presented in its budget materials for fiscal years 2015 through 2017 .

we also reviewed classified and official use only materials that , according to nnsa officials , nnsa shares with the office of management and budget ( omb ) in their discussions of the gprama measures .

the materials we reviewed included technology roadmaps , which define the technology pathways that nnsa follows to address portfolio requirements and develop funding priorities , and final reports that nnsa presents to omb on the completion of activities it undertakes in connection with the gprama measures .

we also reviewed doe's annual performance reports for fiscal years 2014 and 2015 .

in addition , we reviewed documents and interviewed nnsa officials regarding internal measures that dnn r&d and npac use to monitor program performance , including independent project performance reviews .

to further evaluate dnn's project - level performance measures , we reviewed information on the 91 selected projects in our nongeneralizable sample .

specifically , we reviewed documents that establish baseline targets for projects' scope , cost , and completion date and documents that contain information on project results .

we also interviewed dnn officials to obtain their views on program and project management and performance .

finally , we reviewed documents that establish criteria for successful performance measures , including our past reports on the key attributes of such measures , as well as federal internal control standards .

to describe how dnn decides which projects to pursue , we reviewed documents that establish the nonproliferation mission needs of doe and interagency and iaea stakeholders in nonproliferation - related research and technology development .

the documents we reviewed include dod's 2010 nuclear posture review report ; the executive office of the president's national science and technology council's nuclear defense research and development roadmap , fiscal years 2013-2017 ; the white house's national security strategy of february 2015 ; doe and nnsa strategic plans ; the iaea department of safeguards' long - term r&d plan , 2012-2023 ; and iaea's development and implementation support programme for nuclear verification 2016-2017 .

in addition , we reviewed documents that establish program requirements , such as the dnn r&d program's goals , objectives , and requirements documents ; the dnn r&d and npac programs' calls for project proposals , including the project selection criteria provided in the proposal calls ; and project proposals .

we also interviewed nnsa and other agency officials and project managers at the national laboratories to obtain their views on program priorities and the decision - making processes used to define broad program areas and select projects for funding .

to assess the reliability of the data we analyzed on the projects in our nongeneralizable sample , we interviewed nnsa officials and reviewed documentation , and we determined that the data were sufficiently reliable for presenting the information contained in this report on nnsa project characteristics and funding .

we conducted this performance audit from november 2015 to february 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

nuclear detonation detection ( ndd ) program area / forensics - based nuclear detonation detection functional area ( 5 projects ) ndd program area / ground - based nuclear detonation detection functional area ( 8 projects ) ndd program area / space - based nuclear detonation detection functional area ( 10 projects ) dnn r&d ndd program area subtotal ( 23 projects ) proliferation detection program area / nuclear weaponization and material production detection functional area ( 17 projects ) proliferation detection program area / enabling capabilities functional area ( 10 projects ) proliferation detection program area / nuclear weapons and material security functional area ( 13 projects ) dnn r&d proliferation detection program area subtotal ( 40 projects ) dnn r&d subtotal ( 63 projects ) total ( 91 projects ) .

in addition to the contact named above , william hoehn ( assistant director ) , danny baez , antoinette c. capaccio , penney harwell caramia , tara congdon , john delicath , rob grace , ben licht , alexandria palmer , timothy m. persons , steven putansu , ron schwenn , kiki theodoropoulos , jack wang , and tonya woodbury made key contributions to this report .

