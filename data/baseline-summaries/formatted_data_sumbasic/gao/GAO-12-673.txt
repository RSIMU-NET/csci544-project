through the multibillion - dollar president's emergency plan for aids relief ( pepfar ) , the united states has supported significant advances in global hiv / aids prevention , treatment , and care .

since the program was first authorized in 2003 , the estimated number of new hiv infections and aids - related deaths has steadily declined while millions of people in low - and middle - income countries have received antiretroviral treatment .

yet for every person placed on treatment , an estimated two people are newly infected with hiv , and the number of people living with hiv expanded from about 28 million in 2001 to 34 million in 2010 .

congress reauthorized pepfar in 2008 through passage of the tom lantos and henry j. hyde united states global leadership against hiv / aids , tuberculosis , and malaria reauthorization act of 2008 ( 2008 leadership act ) , which sets multiyear targets for prevention , treatment , care , and health systems strengthening programs supported through pepfar through fiscal year 2013 .

the 2008 leadership act stated , among other things , that assistance provided to combat hiv / aids shall expand impact evaluation and other research and analysis efforts to improve accountability , increase transparency , measure the outcomes and impacts of interventions , ensure the delivery of evidence - based services , and identify and replicate effective models .

the government performance and results act of 1993 , amended in 2010 as the government performance and results modernization act , also encourages evaluation of federal programs .

moreover , since 2002 , the office of management and budget has set expectations for agencies to conduct program evaluations as essential tools for improving program design and operations , determining whether intended outcomes are achieved effectively , and informing decision making .

responding to requirements in the consolidated appropriations act of 2008 and the 2008 leadership act to review global hiv / aids program monitoring , this report ( 1 ) identifies pepfar evaluation activities and examines the extent to which evaluation findings , conclusions , and recommendations are supported and ( 2 ) examines the extent to which pepfar policies and procedures adhere to established general principles for the evaluation of u.s. government programs .

to address these objectives , we reviewed the american evaluation association's ( aea ) an evaluation roadmap for a more effective government ( aea roadmap ) as well as policies and guidance developed by the department of state ( state ) , state's office of the u.s .

global aids coordinator ( ogac ) , the department of health and human services' ( hhs ) centers for disease control and prevention ( cdc ) , and the u.s. agency for international development ( usaid ) .

we conducted interviews with officials at ogac , usaid , and cdc .

we also surveyed cdc and usaid headquarters officials as well as cdc and usaid officials in the 31 countries and 3 regions that had pepfar annual operational plans in fiscal year 2010 about which of their pepfar - funded activities operating in fiscal years 2008 through 2010 had ongoing or completed evaluations .

in addition , we obtained electronic copies of completed evaluations for programs operating during this time period from cdc and usaid officials at headquarters and in the pepfar countries and regions .

using a standard assessment tool , we systematically assessed the level of support for findings , conclusions , and recommendations in samples of these evaluations , as indicated by the degree to which they were conducted in adherence with selected common evaluation standards .

we assessed judgmental samples of evaluations submitted by ogac and by cdc and usaid headquarters .

we assessed a randomly selected sample of the evaluations submitted by pepfar country and regional teams , in order to generalize our assessment results to all of the submitted evaluations .

finally , we assessed state , ogac , cdc , and usaid policies and practices against selected general principles of evaluation defined in the aea roadmap .

 ( see app .

i for a detailed description of our objectives , scope , and methodology. ) .

we conducted this performance audit from october 2011 to may 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

ogac establishes overall pepfar policy and program strategies and coordinates pepfar program activities .

in addition , ogac allocates pepfar resources from the global health and child survival account to pepfar implementing agencies , primarily cdc and usaid .

the agencies execute pepfar program activities through agency headquarters offices and interagency teams consisting of pepfar implementing agency officials in the countries and regions with pepfar - funded programs ( pepfar country and regional teams ) .

ogac coordinates these activities through its approval of operational plans , which serve as annual work plans and document planned investments in , and the anticipated results of , hiv / aids - related programs .

ogac provides annual guidance on how to develop and submit operational plans .

in fiscal years 2009 through 2011 , ogac approved operational plans representing $11.7 billion in pepfar program activities .

these activities fall primarily in three broad program areas — prevention , treatment , and care — and 18 related program areas .

program activities aimed at preventing hiv infection and at treating those infected each represented about 30 percent of approved pepfar funding , while activities aimed at caring for aids patients represented about 20 percent .

the remaining approximately 20 percent funded a variety of other program areas , such as health systems strengthening and building laboratory infrastructure .

figure 1 summarizes approved funding for these program areas in fiscal years 2009 through 2011 .

to carry out activities in these program areas , cdc and usaid use implementing mechanisms — grants , cooperative agreements , and contracts — with a variety of implementing partners .

these partners include partner country governments , nongovernmental and international organizations , and academic institutions .

cdc and usaid used more than 3,000 implementing mechanisms in fiscal years 2008 through 2010 .

cdc and usaid offices employ a wide variety of individuals and organizations to conduct pepfar evaluations , including implementing agency officials , consultants , and academic institutions as well as partner government organizations and implementing partners .

evaluation teams sometimes comprise representatives from several of these organizations .

ogac coordinates , and pepfar implementing agencies also engage in , several related activities that support evaluation , such as oversight of implementing partners , routine performance planning and reporting , biological and behavioral health surveillance , baseline studies and needs assessments , and development of health management information systems .

pepfar evaluations are subject to common evaluation standards defined in various agency - specific and governmentwide guidance .

this guidance includes cdc's framework for program evaluation in public health and usaid's evaluation policy and automated directives system guidance .

in addition , gao published guidance on designing evaluations and assessing social program impact evaluations .

also , in september 2010 , the aea published a framework to guide the development and implementation of federal agency evaluation programs and policies .

the framework offers a set of general principles intended to facilitate the integration of evaluation activities with program management .

these principles include developing evaluation policies and procedures ; developing evaluation plans ; ensuring independence of evaluators in designing , conducting , and determining findings of their evaluations ; ensuring professional competence of evaluators ; and disseminating evaluation results publicly and in a timely fashion .

ogac , cdc , and usaid managed and conducted evaluations of a wide variety of pepfar programs that were ongoing during fiscal years 2008 through 2010 .

however , we found that many of these evaluations — particularly evaluations managed by pepfar country and regional teams — did not consistently adhere to common evaluation standards , in many cases calling into question the evaluations' support for their findings , conclusions , and recommendations .

ogac , cdc , and usaid provided 496 evaluations addressing programs ongoing during fiscal years 2008 to 2010 in all pepfar program areas relating to hiv / aids treatment , prevention , and care .

of these 496 evaluations , 18 were public health evaluations ( phe ) , managed by ogac ; 42 were program evaluations provided by cdc and usaid headquarters officials ; and 436 were program evaluations provided by cdc and usaid country and regional team officials .

 ( for more information about these evaluations , see app .

iii. ) .

 ogac - managed evaluations .

ogac provided 18 phes that cdc and usaid had completed as of november 2011 under an ogac - managed approval , implementation review , and reporting process .

the completed phes addressed the following program areas: prevention of mother - to - child transmission , testing and counseling , adult care and support , adult treatment , sexual prevention , and pediatric care and support .

in addition , ogac indicated that 82 other phes had been initiated as of november 2011 .

according to ogac , phes are intended to assess the effectiveness and impact of pepfar programs ; compare evidence - based program models in complex health , social , and economic contexts ; and address operational questions related to program implementation within existing and developing health systems infrastructures .

ogac guidance states that these evaluations focus on strategies to increase program efficiency and impact to guide program development and inform the public , using rigorous quantitative or qualitative methods that permit broad generalization .

for all phes , ogac requires pepfar country and regional teams to submit evaluation concepts or protocols for approval by an interagency subcommittee and requires periodic progress and closeout reports .

 cdc and usaid headquarters - managed evaluations .

cdc headquarters officials provided 20 evaluations in the following program areas: blood safety , injection safety , adult treatment , pediatric treatment , and strategic information .

usaid headquarters officials provided 22 evaluations in the following program areas: abstinence / be faithful , sexual prevention , orphans and vulnerable children , strategic information , and health systems strengthening programs .

four cdc and usaid headquarters evaluations addressed more than one program area .

 country and regional team - managed evaluations .

cdc and usaid officials representing 31 pepfar country and 3 regional teams provided a total of 436 evaluations ; cdc officials provided 185 evaluations , and usaid officials provided 251 evaluations .

the evaluations addressed 18 program areas related to pepfar prevention , treatment , and care , with about one - fifth of the evaluations addressing activities in more than one program area ( see fig .

2 ) .

cdc and usaid officials also provided copies of evaluation protocols and statements of work , indicating that additional evaluations had been initiated .

further , based on our analysis of a randomly selected sample of 78 evaluations , we estimate that 51 percent of the evaluations used qualitative methods , 35 percent used quantitative methods , and 14 percent used a mix of quantitative and qualitative methods .

in addition , evaluations provided by usaid tended to employ qualitative methods ( 32 of 48 evaluations ) , while those provided by cdc tended to use quantitative methods ( 20 of 30 evaluations ) .

 ( see app .

iii for additional results of our analysis. ) .

our assessments of judgmental and randomly selected samples of pepfar evaluations indicate that many — particularly those managed by pepfar country and regional teams — contain findings , conclusions , and recommendations that are not fully supported .

to determine the extent to which these elements are supported , we synthesized our assessments of the extent to which evaluations generally adhered to several common evaluation standards defined in guidance issued by cdc , usaid , and gao .

specifically , we considered whether the evaluations describe the program to be evaluated and its objectives , the purpose of the evaluation , and the criteria used to reach conclusions about the achievement of the program's objectives .

we also considered the extent to which evaluations incorporate appropriate designs , sample selection methods , measures , and data collection and analysis methods .

all ogac - managed phes that we reviewed generally adhered to these standards and thus their findings , conclusions , and recommendations were fully supported .

we found similar results for most cdc and usaid headquarters' program evaluations we reviewed .

however , pepfar country and regional teams' evaluations did not consistently adhere to common evaluation standards , and thus , in most cases , their findings , conclusions , and recommendations were not fully supported .

ogac - managed evaluations .

our assessment of seven ogac - managed pepfar phes indicates that they all generally adhered to common evaluation standards , and thus their findings , conclusions , and recommendations were fully supported .

all of the evaluations that we reviewed identified program and evaluation objectives and used appropriate measures , and most used appropriate evaluation designs and data collection and analysis methods .

three of the evaluations employed fully appropriate sampling methods .

table 1 summarizes our assessments of these evaluations .

cdc and usaid headquarters - managed evaluations .

our assessment of 15 cdc and usaid headquarters - managed evaluations indicates that most generally adhere to common evaluation standards .

as a result , we found that findings , conclusions , and recommendations were fully supported in 9 evaluations and partially supported in 6 evaluations .

most of the evaluations employed appropriate evaluation designs , measures , and data collection and analysis methods .

however , 7 evaluations did not fully identify the evaluation criteria , and 8 did not employ fully appropriate sampling methods .

table 2 summarizes our assessments of these evaluations .

country and regional team - managed evaluations .

we found that evaluations managed by country and regional teams , which make up the bulk of all pepfar program evaluations , did not consistently adhere to common evaluation standards .

based on our analysis of a randomly selected sample of country and regional team evaluations , we estimate that findings , conclusions , and recommendations were fully supported in 41 percent of all evaluations provided to us by country and regional teams , partially supported in 44 percent of these evaluations , and not supported in 15 percent of these evaluations .

we estimate that 24 percent of these evaluations did not identify any evaluation criteria , and more than half did not employ evaluation designs , sampling methods , measures , or data collection and analysis methods that were fully appropriate .

for example , an evaluation of activities for providing care to orphans and vulnerable children drew conclusions about results and made recommendations , based almost exclusively on favorable anecdotal information collected from selected program participants and beneficiaries .

as a result , the objectivity and credibility of these evaluations' findings , conclusions , and recommendations are in question .

table 3 summarizes our assessments of these evaluations .

further analysis of the results of our assessments showed that evaluations using qualitative methods were more likely to contain results that were partially supported or not supported than evaluations using quantitative methods .

 ( see app .

iii for additional results of our analysis. ) .

state , ogac , cdc , and usaid have developed policies and procedures that apply to evaluations of pepfar programs , as called for in the aea roadmap .

however , they have not fully adhered to other aea roadmap principles regarding evaluation planning , independence and competence of evaluators , and dissemination of evaluation results .

first , ogac has not developed pepfar evaluation plans at the program level or required the development of such plans in individual countries and regions , limiting its own ability to ensure that evaluation resources are appropriately targeted .

second , state , ogac , cdc , and usaid guidance does not specify how to document the independence and competency of evaluators , and almost half of the evaluations we reviewed did not provide sufficient information to fully determine whether evaluators were free of conflicts of interest .

finally , not all evaluation reports are available online , thus limiting their accessibility and usefulness to pepfar decision makers and other stakeholders .

in accordance with aea principles , state , ogac , cdc , and usaid have issued policies and procedures that are applicable to pepfar program evaluation .

 state evaluation policy .

in february 2012 , state's bureau of resource management issued an evaluation policy that applies to all state bureaus and ogac .

the policy provides a framework for implementing evaluations of state's various programs and projects and encourages evaluations for programs and projects at all funding levels .

 ogac operational plan guidance .

according to ogac officials , ogac generally has deferred to implementing agency policies .

ogac also issues annual guidance to pepfar implementing agencies for preparation of their operational plans .

ogac's fiscal year 2012 operational plan guidance to pepfar country and regional teams , issued in august 2011 , addresses some elements of evaluation .

the guidance differentiates three types of evaluation and research: basic program evaluation , which focuses on descriptive and normative evaluation questions ; operations research , which focuses on program delivery and optimal allocation of resources ; and impact evaluation , which measures the change in an outcome attributable to a particular program .

 cdc evaluation framework .

in september 1999 , the program evaluation unit at cdc's office of the associate director for program issued an evaluation framework for cdc programs .

the framework summarizes essential elements of program evaluation , clarifies program evaluation steps , and reviews standards for effective program evaluation , among other things .

according to cdc's chief evaluation officer , as of may 2012 , cdc plans to issue evaluation guidelines and recommendations as well as additional guidance for using the evaluation framework .

 usaid evaluation policy .

in january 2011 , usaid's bureau for policy , planning , and learning revised evaluation policy to supplement existing evaluation guidance in usaid's automated directive system .

according to usaid , this revised policy was intended to address a decline in the quantity and quality of evaluation practice within the agency in the recent past .

the policy clarifies for usaid staff , partners , and stakeholders the purposes of evaluation ; the types of evaluations that are required and recommended ; and usaid's approach for conducting , disseminating , and using evaluations .

among other things , the policy sets forth the purposes of evaluation , the roles and responsibilities of usaid operating units , and evaluation requirements and practices for all usaid programs and projects .

the policy requires all usaid operating units to consult with program office experts to ensure that scopes of work for external evaluations meet evaluation standards .

the policy also states that operating units , in collaboration with the program office , must ensure that evaluation draft reports are assessed for quality by management and through an in - house peer technical review .

ogac has not yet developed a program - level pepfar evaluation plan or required implementing agencies or country and regional teams to develop evaluation plans as called for by the aea roadmap .

 ogac .

state's recently issued evaluation policy requires that each state bureau , including ogac , develop and submit a bureauwide evaluation plan that encompasses major policy initiatives and new programs as well as existing programs and projects .

according to a senior ogac official , at the time of our review , ogac was discussing with state's bureau of resource management how it will comply with this new requirement .

 cdc and usaid headquarters .

ogac defers to implementing agencies to plan evaluations of their headquarters - managed pepfar program activities , but cdc and usaid have not developed evaluation plans for such activities included in recent headquarters operational plans .

ogac's 2011 guidance for developing the headquarters operational plan requires a plan for technical area program priorities but does not address evaluation planning .

similarly , the fiscal year 2012 guidance does not include a requirement for an evaluation plan .

 country and regional teams .

ogac defers to pepfar country and regional teams to plan evaluations of their program activities , but does not require that the teams develop and submit annual evaluation plans .

ogac's 2011 guidance on developing country and regional operational plans urges country and regional teams to prioritize program evaluation in order to make pepfar programs more effective and sustainable .

in addition , ogac's fiscal year 2012 guidance calls for country and regional teams to address monitoring and evaluation in describing individual implementing partners' activities .

however , neither the 2011 guidance nor the 2012 guidance instructs all country teams to develop evaluation plans .

we reviewed pepfar country and regional operational plans for fiscal year 2011 and found that they did not include evaluation plans .

instead , these documents generally included ( 1 ) descriptions of ongoing or planned evaluations and related activities ( eg , surveillance ) in program area narrative summaries and ( 2 ) descriptions of monitoring and evaluation activities in implementing partner activity narratives .

in our analysis of information provided by country and regional teams , as well as cdc and usaid headquarters , we did not detect an evaluation rationale or strategy .

based on responses to our survey of cdc and usaid officials in 31 pepfar country and 3 regional teams , we calculated that evaluations had been conducted or were ongoing for about one - third of these countries' program activities in fiscal years 2008 through 2010 .

in addition , based on these officials' responses , we found similar percentages of ongoing and completed evaluations across the broad program areas of prevention , treatment , and care .

we also analyzed cdc and usaid headquarters officials' responses to our survey and found that evaluations had been conducted or were ongoing for about half of the pepfar program activities managed by agencies' headquarters and implemented during fiscal years 2008 to 2010 .

however , we found no relationships between the percentages of program activities with ongoing or completed evaluations and budgets at the country , program area ( i.e. , prevention , treatment , or care ) , or program activity levels .

state , cdc , and usaid policies and procedures address the independence of evaluators but do not consistently require that evaluation reports identify the evaluation team or address whether there are any potential conflicts of interest .

in addition , some agency policies and procedures address the need to ensure that evaluators have appropriate qualifications , but none require that evaluations document those qualifications or certify that they are adequate .

 state .

state's recently issued evaluation policy addresses evaluator independence and integrity , stating that evaluators should be free from program managers and not subject to their influence .

this policy does not address evaluator qualifications .

 ogac .

ogac's operational plan guidance to country and regional teams does not address the independence or professional qualifications of evaluators .

according to ogac officials , ogac defers to implementing agency evaluation policies .

 cdc .

cdc's evaluation framework addresses the need to assemble an evaluation team with the needed competencies , highlighting the importance of ensuring that evaluators have no particular stake in the results of the evaluation .

the cdc evaluation framework also discusses appropriate ways to assemble an evaluation team .

 usaid .

usaid's evaluation policy recommends that most evaluations be external and requires a disclosure of conflicts of interest for all evaluation team members .

in addition , usaid's evaluation policy requires that evaluation - related competencies be included in staffing selection policies .

our analysis of a randomly selected sample of evaluations submitted by 31 pepfar country and 3 regional teams found that the evaluations often did not address whether evaluators have potential conflicts of interest , as called for by the aea roadmap .

we estimate that 27 percent of the evaluations fully addressed potential conflicts of interest , 59 percent partially addressed the issue , and 14 percent did not address the issue .

in addition , while we were unable to determine whether potential conflicts of interest existed with the information provided in some of the evaluation reports , it appeared that there were evaluations in which potential conflicts of interest existed but were not addressed .

for example , one evaluation report , relating to strengthening a partner country's nongovernmental hiv / aids organizations , indicated that the evaluation team was employed by the program activity's implementing partner , but the report did not address potential conflict of interest .

furthermore , some country and regional program evaluations sometimes did not provide enough identifying information about evaluators to allow an assessment of evaluator independence or qualifications .

we estimate that 86 percent of the evaluations fully identified the evaluators , while 14 percent provided either partial or no information .

for example , an evaluation report we reviewed relating to hiv prevention program activities in one region named the organization that conducted the evaluation but did not provide any information on the evaluation team members .

moreover , we were unable to find any information about this organization in an online search based on the limited information available in the report .

agency policies and procedures generally support dissemination of evaluation results , but ogac , cdc , and usaid have not ensured that evaluation methods , data , and evaluation results are made fully and easily accessible to the public .

 state .

state's newly released evaluation policy requires bureaus to submit evaluations to a central repository .

 ogac .

ogac officials told us that the office supports dissemination of the results of important global hiv / aids research and evaluations to a variety of stakeholders .

for example , ogac officials noted that the pepfar website contains information on pepfar results as well as monitoring and evaluation guides .

ogac officials also noted that dissemination strategies are a common component of evaluation protocols and the procurement mechanisms that fund them .

in addition , ogac maintains an intranet site , which is accessible to pepfar implementing agency officials and contains information about evaluation .

however , ogac does not have a mechanism for publicly and systematically disseminating evaluation results .

 cdc .

cdc policy advises that effort is needed to ensure that evaluation findings are disseminated appropriately but does not require online dissemination of evaluation reports .

cdc officials told us that they recently made changes to cdc's public website , which , as of april 2012 , includes some information on program evaluations .

in addition , cdc's division of global hiv / aids ( dgha ) science office maintains a catalog of published journal articles coauthored by dgha officials .

however , cdc does not maintain a complete online inventory of evaluations .

 usaid .

usaid's policy states that evaluation findings should be shared as widely as possible with a commitment to full and active disclosure .

usaid requires submission of completed evaluations to the development experience clearinghouse ( dec ) , the agency's online repository of research documentation , but does not enforce this requirement .

in 2010 , usaid reported that practices for disseminating evaluation results were generally limited , that dissemination practices varied across the agency , and that the requirement to submit completed evaluations to the dec had not been fully enforced .

additionally , usaid found that documents in the dec were sometimes difficult to find .

in february 2012 , usaid also found that missions had reported submitting only 20 percent of their evaluations to the dec in fiscal year 2009 .

although documents submitted by 31 pepfar country and 3 regional teams showed that cdc and usaid have disseminated evaluation findings within these countries and regions in several ways , we found no publicly accessible and easily searchable internet source for pepfar program evaluations .

we received abstracts from annual meetings and conferences , presentations to partner government officials and stakeholders , published journal articles , and periodic agency reports , which may be publicly accessible via the internet .

however , as of the time of our review , our searches of five key websites generated far fewer pepfar evaluations than the 496 evaluations we received from country teams , cdc and usaid headquarters , and ogac .

we searched pubmed , the u.s. national library of medicine's online database , but a search using “pepfar” and “evaluation” as search terms generated seven results .

likewise , as of april 2012 , our search of usaid's dec , using “hiv / aids” and “evaluation” as search terms , generated 87 results , including some that were not evaluations , but usaid officials , in response to our request , later provided us nearly 300 evaluations .

we also found some evaluations at two usaid - maintained websites , ovcsupport.net and aidstar - one , but neither site was comprehensive or fully searchable .

in addition , a website called global hiv m&e information provides a repository of voluntarily submitted monitoring and evaluation resources ; however , we found few evaluations of pepfar programs .

pepfar's authorizing legislation emphasizes the importance of program evaluation as a tool for ogac to ensure , among other things , that funds are spent on programs that show evidence of success .

state , cdc , and usaid have demonstrated a clear commitment to program evaluation by conducting a wide variety of program evaluations that address at least one activity in each pepfar program area .

however , many evaluations managed by pepfar country and regional teams lack fully supported findings , conclusions , and recommendations , evidenced by a lack of general adherence to common evaluation standards .

without fully supported findings , conclusions , and recommendations , these pepfar program evaluations have limited usefulness as a basis for decision making and may supply incomplete or misleading information for managers' and stakeholders' efforts to direct pepfar funding to programs that produce the desired outcomes and impacts .

state , cdc , and usaid have demonstrated their commitment to program evaluation by developing policies and procedures that apply to evaluations , in accordance with established general principles .

however , without a requirement that country and regional teams prepare and submit annual evaluation plans — for example , as a component of operational plans — ogac is unable to ensure that program activities are subject to appropriate levels of evaluation .

moreover , without documentation of the independence and competence of pepfar program evaluators , ogac , agency program managers , and other stakeholders have limited assurance that evaluation results are unbiased and credible .

finally , unless evaluation results are publicly and systematically disseminated and made easily searchable online , program officials and public health researchers may be unable to assess the credibility of their findings or use them for program decision making .

we recommend that the secretary of state direct the u.s .

global aids coordinator to take the following four actions in collaboration with cdc and usaid to enhance pepfar evaluations: 1. develop a strategy to improve pepfar implementing agencies' and country and regional teams' adherence to common evaluation standards ; 2. require implementing agency headquarters and country and regional teams to include evaluation plans in their annual operational plans ; 3. provide detailed guidance for implementing agencies and country and regional teams on assessing , ensuring , and documenting the independence and competence of pepfar program evaluators ; and 4. increase the online accessibility of pepfar program evaluation results .

we provided a draft of this report to state , hhs's cdc , and usaid .

responding jointly with cdc and usaid , state ogac provided written comments ( see app .

iv ) .

cdc and usaid also provided technical comments , which we incorporated as appropriate .

in its written comments , state agreed with our recommendations and , emphasizing the interagency nature of the pepfar program , indicated that it will coordinate with pepfar agencies to implement our recommendations .

first , state explained that it will work with pepfar implementing agencies to carry out the agencies' evaluation policies and practices , which state deemed generally consistent with aea principles , and will develop strategies to ensure the appropriate application of common evaluation standards .

second , state responded that it will work through pepfar interagency processes to develop pepfar program evaluation plans , which it noted could be included in annual pepfar operational plans .

third , state will work with pepfar implementing agencies to put in place guidance to document program evaluators' independence and qualifications .

fourth , state affirmed that ogac will collaborate with pepfar implementing agencies to develop strategies for improving dissemination of evaluation results and will use pepfar's public website to link to agencies' online resources .

we are sending copies of this report to the secretary of state , the office of the u.s .

global aids coordinator , u.s. agency for international development's office of hiv / aids , the department of health and human services' office of global affairs , the centers for disease control and prevention's division of global hiv / aids , and appropriate congressional committees .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staffs have any questions about this report , please contact me at ( 202 ) 512-3149 or gootnickd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vi .

this report ( 1 ) identifies president's emergency plan for aids relief ( pepfar ) evaluation activities and examines the extent to which evaluation results are supported and ( 2 ) examines the extent to which pepfar policies and procedures adhere to established principles for the evaluation of u.s. government programs .

to identify pepfar program evaluations and examine the extent to which they generated supported evaluation results , we collected and analyzed program evaluation documents provided by centers for disease control and prevention ( cdc ) and u.s. agency for international development ( usaid ) officials in the 31 pepfar countries and 3 regions with pepfar country or regional operational plans in fiscal year 2010 , as well as the department of state's ( state ) office of the u.s .

global aids coordinator ( ogac ) and cdc and usaid headquarters officials .

to examine the extent to which pepfar program evaluation policies and procedures adhered to principles in the american evaluation association's ( aea ) an evaluation roadmap for a more effective government ( aea roadmap ) , we reviewed the general principles for conducting federal government program evaluations , as well as ogac , state , usaid , and cdc policies and guidance .

in addition , we surveyed cdc and usaid officials in the 31 pepfar countries and 3 regions with pepfar annual country or regional operational plans in fiscal year 2010 , as well as cdc and usaid headquarters officials , regarding ongoing and completed evaluations .

finally , we conducted interviews with ogac , cdc , and usaid officials in washington , d.c. , and atlanta , georgia .

to survey pepfar country and regional team officials , we took the following steps: 1 .

we consulted with ogac and cdc and usaid headquarters officials and decided to use implementing mechanism as a proxy for a program activity .

we determined that using implementing mechanisms was the only viable unit of analysis to estimate the percentage of pepfar programs with evaluations because ( 1 ) ogac officials maintained updated data on implementing mechanisms and ( 2 ) pepfar officials regularly used and understood data on implementing mechanisms .

however , in some of these cases , if the broader program was evaluated , not all implementing mechanisms under the larger program were necessarily evaluated .

we also recognized that evaluations may not be appropriate for all implementing mechanisms ( such as those that provide funding for staffing costs ) .

to the extent possible , we eliminated these implementing mechanisms from our analysis .

2 .

we obtained lists of program activities for fiscal years 2008 through 2010 from ogac for each country and region .

we then analyzed program activities by country ( or region ) and agency ; the lists included identification numbers , names , and partner names for each of the program activities .

each survey tool then contained a list of program activities relevant to the country or regional team .

3 .

based on gao and ogac guidance , we developed the following working definition of evaluation: evaluations are systematic studies to assess how well a program is working .

evaluations are often conducted by experts external to the program , either inside or outside the agency .

types of evaluations include process , outcome , impact , or cost - benefit analysis .

4 .

we developed a survey tool for ongoing and completed evaluations of pepfar programs .

we consulted with ogac and cdc and usaid headquarters officials about the survey tool and made revisions as appropriate .

for example , based on input from cdc and usaid headquarters officials , we determined that some pepfar evaluations could address several implementing mechanisms .

in addition , in some of these cases , if a broader program ( eg , national treatment program ) was evaluated , not all implementing mechanisms under the broader program were necessarily evaluated .

in response , we included questions in our survey prompting pepfar officials to indicate whether an implementing mechanism has been evaluated as part of a broader evaluation of several implementing mechanisms .

5 .

we tested the survey tool with officials in two pepfar countries — angola and ethiopia — and finalized the survey tool based on discussions with these officials .

6 .

we sent the final survey tool to pepfar country contacts ( pepfar coordinators and cdc and usaid officials ) identified by ogac and cdc and usaid headquarters .

the survey tool instructed cdc and usaid country or regional team officials to provide “yes” or “no” responses to the following questions for each implementing mechanism in the country's ( or region's ) agency - specific lists: is this one of your agency's fiscal year 2008-2010 country or regional operational plan program activities ? .

 has at least one evaluation specific to this implementing mechanism been completed ? .

is at least one evaluation specific to this implementing mechanism ongoing ? .

 has at least one evaluation covering , but broader than , this implementing mechanism been completed ? .

is at least one evaluation covering , but broader than , this implementing mechanism ongoing ? .

we also prompted the country or regional officials to provide additional information for each implementing mechanism , such as explanations for program activities that do not belong to the agency and identification of duplicate program activities .

officials were instructed to either e - mail the completed surveys to gao or upload them to a website regularly used by ogac and country and regional teams for submitting and sharing planning and reporting documents .

in some cases , we met with country or regional team officials via telephone , or corresponded via e - mail , to clarify the purpose of the survey , the questions themselves , and the evaluation document request as well as to correct anomalies and ask follow - up questions .

one gao analyst also attended the may 2011 pepfar implementing agency annual meeting in johannesburg , south africa , to provide information about the survey and evaluation document request to pepfar country and regional team officials also attending the annual meeting .

we received responses from all 31 pepfar countries and 3 regions with fiscal year 2010 operational plans .

using a similar survey tool , we also conducted surveys of cdc and usaid headquarters officials regarding program activities managed by agency headquarters and listed in pepfar headquarters operational plans for 2008 through 2010 .

to analyze country and regional teams' survey responses , we made the following assumptions regarding the survey responses: if officials did not provide a response to the question “is this one of your agency's fiscal year 2008-2010 country or regional operational plans program activities ? ” we included that implementing mechanism in the analysis .

program activities with responses of “no” or “duplicate” were eliminated from the analysis .

if officials did not respond to any of the four questions regarding ongoing or completed evaluations , we assumed that there were no ongoing or completed evaluations for that implementing mechanism .

in addition , we reviewed narrative comments provided by country and regional team officials .

we recognized that evaluations may not be appropriate for all implementing mechanisms ( such as those that provide funding for staffing costs ) .

to the extent possible , we eliminated these implementing mechanisms from our analysis .

based in part on our review of the narrative comments , we flagged and eliminated implementing mechanisms with evidence indicating that the implementing mechanism was either “to be determined” ( i.e. , the agency had yet to make an award to an implementing partner ) , related to staffing costs , related to strategic information and monitoring and evaluation , recently begun , a duplicate of another implementing mechanism , or listed in error .

once the survey responses were ready for analysis , we calculated the summary statistics that are reported in the body of the report .

we also included the survey responses provided by officials in cdc and usaid headquarters in the analysis .

to check the reliability of the data analysis , a second independent analyst reviewed the statistical programs used to analyze the data for accuracy .

in addition to our survey of cdc and usaid officials in the 31 countries and 3 regions with fiscal year 2011 operational plans , we requested program evaluation documents .

to do this , the survey tool instructions prompted cdc and usaid officials to provide documentation of completed and ongoing evaluations .

specifically , for implementing mechanisms where officials indicated that at least one evaluation had been completed , we requested documentation — such as an evaluation report — of all such completed evaluations .

for implementing mechanisms where officials indicated that at least one evaluation was ongoing , we requested documentation — such as terms of work or an evaluation plan .

we generally advised country and regional team officials to err on the side of inclusion when in doubt about whether to submit documentation of ongoing and completed evaluations .

we instructed these officials to e - mail , or , in some cases , mail electronic versions of the program evaluation documents to gao , or to upload them to a website regularly used by ogac and country and regional teams for submitting and sharing planning and reporting documents .

in response to this document request , we received more than 1,350 documents .

for example , we received documentation of ongoing or planned evaluations , such as statements of work or evaluation protocols and protocol approval forms .

we also received meeting minutes , trip reports , financial review and audit documents , presentation slides , abstracts , and conference posters .

to determine which documents met our definition of evaluation , we reviewed each of these documents and categorized them as meeting the definition of evaluation or not , following a set of decision rules .

for example , we included data quality assessments , costing studies that compared costs and explained cost differences , and analyses of surveillance data pre - and postintervention .

we excluded surveillance studies that simply reported the results of a surveillance activity ( but did not link it to a specific program or intervention ) ; needs assessments , baseline studies , and situation analyses ; trip and site visit reports ; and pre - and postevent ( eg , workshop ) questionnaires or surveys .

we identified and eliminated duplicate documents .

this categorization was checked by a second analyst and yielded 436 program evaluations .

we believe that this final set of evaluations constitutes an essentially full universe of pepfar country and regional program evaluation documents .

in addition to the program evaluation documents collected from cdc and usaid officials in pepfar countries and regions , we requested documents from ogac related to pepfar public health evaluations .

we also requested evaluation documents related to pepfar program managed by cdc and usaid headquarters from officials at each agency's headquarters .

ogac provided copies of 18 completed public health evaluations , cdc headquarters provided copies of 22 completed evaluations , and usaid headquarters provided copies of 24 completed evaluations .

we reviewed the program evaluation documents submitted by pepfar country and regional teams as well as cdc and usaid headquarters officials .

we identified whether each program evaluation was ongoing or completed as well as which program area or areas ( eg , prevention , treatment , care , or other ) were evaluated .

to do this , we used program categories defined by ogac's fiscal years 2011 operational plan guidance , resulting in the program areas and related areas reported in the report .

this categorization was checked by a second analyst .

table 4 provided descriptions of the pepfar program areas .

to determine the degree to which these evaluations were conducted in adherence with common evaluation standards , we used an assessment tool to systematically conduct in - depth analyses of a probability sample of the evaluations submitted by the pepfar country and regional teams and a nonprobability sample of the evaluations submitted by ogac and cdc and usaid headquarters officials .

our pepfar evaluation assessment tool was based on an assessment tool used for a prior gao report , which we updated using guidance on evaluation from usaid , cdc , the organization for economic cooperation and development ( oecd ) , and gao .

we piloted the assessment tool with three pepfar program evaluation documents provided by cdc and usaid headquarters officials and revised the evaluation assessment as appropriate .

after piloting and revising the tool , we finalized the tool and used it to conduct the in - depth analyses of program evaluation documents .

table 5 lists the questions and supporting questions included in the assessment tool .

to allow us to generalize to the entire set of evaluations provided by pepfar country and regional teams , we randomly selected a sample of 84 of 436 evaluations submitted by cdc and usaid officials in 31 pepfar countries and 3 regions .

the list of all evaluations was sorted by total approved operational plan budgets for each country or region for fiscal years 2008 through 2010 , so that a systematic sample would ensure representation of countries with relatively large , medium , and small budgets for fiscal years 2008 through 2011 .

after sampling , 6 evaluations — including , for example , baseline and feasibility studies — were found to be out of scope , resulting in a final sample of 78 .

results based on random probability samples are subject to sampling error .

the sample we drew for our survey is only one of a large number of samples we might have drawn .

because different samples could have provided different estimates , we express our confidence in the precision of our particular sample results as a 95 percent confidence interval .

this is the interval that would contain the actual population values for 95 percent of the samples we could have drawn .

the margin of error associated with proportion estimates is no more than plus or minus 11 percentage points at the 95 percent level of confidence and estimates of totals have a margin of error no larger than 44 evaluations .

for the 18 public health evaluations submitted by ogac , as well as the 20 and 22 evaluations submitted by cdc and usaid headquarters , respectively , we selected a nonprobability sample based on the type of program ( eg , prevention , treatment , care , or other ) evaluated as well as country or countries addressed by each evaluation .

because this is a nonprobability sample , the results of our assessments of these evaluations cannot be used to make inferences about all evaluations managed by ogac and cdc and usaid headquarters .

however , they do represent a mix of the types of evaluations managed by ogac and cdc and usaid headquarters .

using our evaluation assessment tool , we conducted in - depth analyses of the evaluation documents submitted by the pepfar country and regional teams and also those submitted by ogac , usaid , and cdc headquarters .

to do so , one analyst conducted an initial review of the evaluation document and then completed the evaluation assessment tool .

the analyst also recorded basic information about each evaluation , including title , author , date of publication , and the country or countries included in the evaluation .

for each of the questions in the assessment tool ( see table 1 ) , analysts were instructed to ( 1 ) respond using “yes,” “no,” “partial,” “not sure,” or “not applicable” and ( 2 ) summarize or cite relevant information from the evaluation documents .

analysts then were instructed to weigh the evidence and answers to these questions and provide “yes,” “no,” “partial,” , “not sure,” or “not applicable” responses for each category .

based on the analysis of the elements addressed in the assessment tool , analysts determined the extent to which each evaluation's findings , conclusions , and recommendations were supported using “yes,” “no,” “partial,” or “not sure” as their responses .

this overall determination was not based on a tally of responses to individual elements in the evaluation assessment tool , but rather a synthesis of these responses and an assessment of the contribution of each element to the overall support for the evaluation's findings , conclusions , and recommendations .

to help ensure consistency in the application of the standards and questions , the assessors met weekly during the assessment period to clarify the instructions and discuss their observations .

after each assessment was complete , a second analyst independently verified the results of the analysis by reviewing the program evaluation document and the completed evaluation assessment tool .

in cases where the two analysts did not concur on the results , or where there was a “not sure” response , they met to discuss the evidence and documented a final determination .

all the results for the evaluation assessment tools were then entered into a spreadsheet and analyzed .

to assess potential associations between key attributes of the sample of 78 evaluations we randomly selected , we calculated chi - square tests and the associated odds ratios for all pairs of the following variables: agency , methods used , evaluation type , and program type .

key results from these analyses are presented in the report .

additional results can be found in appendix iii .

we also employed logistic regressions to assess which of these variables ( i.e. , agency , methods used , evaluation type , and program type ) had the strongest effects on the extent to which sampled evaluations contained support for findings , conclusions , and recommendations .

to assess state , ogac , cdc , and usaid evaluation policies , we developed an assessment tool based on nine aea roadmap principles .

for each principle , we developed a question or series of questions asking how the policies addressed the aea roadmap principles .

one analyst reviewed each agency's policy and filled out the tool by citing evidence that would support the policy's consistency with the aea roadmap principle , or a conclusion that no evidence could be found to support adherence to the principle .

the analyst then concluded whether the policy was consistent with each principle assessed .

a second analyst conducted a review of the completed assessment tools and either concurred with or disputed the conclusion for each principle .

in cases where the two analysts did not concur , they met to discuss the evidence and made a final determination .

to determine the extent to which operational plans contained evaluation plans , we reviewed ogac's fiscal year 2011 and 2012 annual guidance to implementing agency headquarters regarding development of the annual pepfar headquarters operational plan .

we documented instances where the guidance addressed program evaluation and determined whether it constituted instructions to develop an evaluation plan .

we conducted similar analysis of ogac's fiscal year 2011 and 2012 annual guidance to pepfar country and regional teams to identify instances where the guidance addressed evaluation and , finally , to determine whether the guidance constituted instructions for developing evaluation plans .

in addition , we assessed 11 of the 33 country operational plans and 2 of the 3 regional operational plans submitted to ogac for fiscal year 2011 , the most recent year in which plans were available .

we documented instances where these operational plans discussed evaluation and whether they contained evaluation plans .

to determine the extent to which the program evaluations documented potential conflicts of interest and the identity of evaluators , we included questions on these two elements in our evaluation assessment tool .

analysts were instructed to respond using “yes,” “no,” or “partial” to these questions and to cite relevant evidence .

after each assessment was complete , a second analyst verified the results of the analysis by reviewing the program evaluation document and the completed evaluation assessment tool .

in cases where the two analysts did not concur on the results , they met to discuss the evidence and documented a final determination .

all the results for the evaluation assessment tools were then entered into a spreadsheet and analyzed .

we searched five internet databases referenced by ogac , cdc , and usaid officials to determine the public accessibility of pepfar program evaluations .

these five sites included the development experience clearinghouse ( http: / / dec.usaid.gov / index.cfm ) , pubmed ( http: / / www.ncbi.nlm.nih.gov / pubmed / ) , ovcsupport.net ( http: / / www.ovcsupport.net / s / ) , aidstar - one ( http: / / www.aidstar - one.com / ) , and global hiv m&e info ( https: / / www.globalhivmeinfo.org / pages / homepage.aspx ) .

for each of these websites , we conducted searches using keywords that would capture any pepfar - related program evaluations or documentation , such as “pepfar,” “evaluation,” and “hiv / aids.” where applicable , we then captured the results and counted the number of documents that could reasonably be considered documentation of a pepfar program evaluation .

we conducted this performance audit from august 2011 to may 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

past gao work has emphasized evaluation as a key source of information to help agency officials and congress make decisions about the programs they oversee .

gao distinguishes performance measurement — the ongoing monitoring and reporting of program accomplishments — from evaluation , which is defined as individual , systematic studies conducted periodically or on an ad hoc basis to assess how well a program is working .

further , according to gao guidance , experts external to the program , program managers , or both conduct evaluations to examine the performance of a program within a given context to understand not only whether a program works but also how to improve results .

gao guidance identifies four types of evaluation:  process evaluation .

this type of evaluation assesses the degree to which a program is operating as it was intended .

it typically assesses program activities' conformance to statutory or regulatory requirements , program design , and professional standards or customer expectations .

 outcome evaluation .

this type of evaluation assesses the degree to which a program achieves its outcome - oriented objectives .

it focuses on outputs and outcomes ( including unintended effects ) to judge program effectiveness , but may also assess program process to understand how outcomes are produced .

impact evaluation .

this is a form of outcome evaluation that assesses the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of the program .

impact evaluation is used when external factors are known to influence the program's outcomes , in order to isolate the program's contribution to achievement of its objectives .

 cost - benefit or cost - effectiveness analysis .

this type of evaluation compares a program's outputs or outcomes with the costs to produce them .

cost - effectiveness analysis assesses the cost of meeting a single objective and can be used to identify the least costly alternative for meeting that goal .

in addition , gao guidance provides basic information about the more commonly used evaluation methods ; introduces key issues in planning evaluation studies of federal programs to best meet decision makers' needs ; and describes different types of evaluations for answering varied questions about program performance , the process of designing evaluation studies , and key issues to consider in ensuring overall study quality .

further , the guidance recommends standards for evaluation design , including establishing evaluation objectives , identifying constraints , and assessing the appropriateness of the evaluation design .

we conducted a statistical analysis of the adequacy of support for findings in evaluations provided to us by cdc and usaid , to determine whether the adequacy of support differed by agency , by methods used , or by type of evaluation .

our analysis indicated that fully supported findings were more likely in cdc's evaluations than in usaid's evaluations ; in evaluations that used quantitative methods than in evaluations that used qualitative or mixed methods ; and in cost - benefit or impact evaluations , as well as outcome evaluations , than in process evaluations .

however , while cdc's evaluations' findings were more likely to be fully supported than usaid's evaluations' findings , the difference was not statistically significant after we accounted for the method used in the evaluations .

this lack of statistical significance suggests that the difference was driven partly by the agencies' choice of evaluation method .

table 6 shows technical details of our statistical analysis of the level of support for findings in cdc and usaid evaluations .

in table 6 , the chi - square statistics at the base of each of the three panels show that the adequacy of support for findings varied significantly between the two agencies and differed significantly based on the methods used and type of evaluations .

the odds ratios in the far - right column show that the odds of evaluations' being fully supported were 3.6 times greater for cdc than for usaid ; 18 times greater for quantitative evaluations than for qualitative or mixed - methods evaluations ; 23 times greater for cost - benefit or impact evaluations than for process evaluations ; and 3.7 times greater for outcome evaluations than for process evaluations .

in addition , we estimated binary logistic regression models to determine whether the difference in adequacy of support for findings in cdc's and usaid's evaluations resulted from differences in the methods used or differences in the types of evaluations conducted .

table 7 shows the odds ratios that result from fitting logistic regression models to estimate the effects of the three different factors ( agency , methods used , and type of evaluation ) on the adequacy of support for findings .

models 1 , 2 , and 3 are bivariate models , which regress “support” on dummy variables for agency , methods used , and type of evaluation , with each variable considered one at a time .

these produce the same odds ratios that we obtained from the observed data in table 6 .

in contrast , model 4 estimates the effects of agency and methods simultaneously , and model 5 estimates the effects of agency and type of evaluation .

in comparing these models , we found that controlling for the methods used ( model 4 ) rendered insignificant the differences between agencies in adequacy of support for findings , whereas controlling for type of evaluation ( model 5 ) did not .

in addition to the contact named above , jim michels , assistant director ; todd m. anderson ; chad davenport ; david dornisch ; lorraine ettaro ; justin fisher ; brian hackney ; kay halpern ; fang he ; reid lowe ; grace lui ; and erika navarro made key contributions to this report .

in addition to these staff , the following gao staff assisted by conducting in - depth assessments of selected evaluations: sada aksartova , gergana danailova - trainor , leah dewolf , rachel girshick , jordan holt , kara marshall , jeff miller , steven putansu , mona sehgal , and doug sloane .

sushmita srikanth and katy crosby assisted with quality assurance reviews .

president's emergency plan for aids relief: program planning and reporting .

gao - 11-785 .

washington , d.c.: july 29 , 2011 .

global health: trends in u.s .

spending for global hiv / aids and other health assistance in fiscal years 2001-2008 .

gao - 11-64 .

washington , d.c.: october 8 , 2010 .

president's emergency plan for aids relief: efforts to align programs with partner countries' hiv / aids strategies and promote partner country ownership .

gao - 10-836 .

washington , d.c.: september 20 , 2010 .

president's emergency plan for aids relief: partner selection and oversight follow accepted practices but would benefit from enhanced planning and accountability .

gao - 09-666 .

washington , d.c.: july 15 , 2009 .

global hiv / aids: a more country - based approach could improve allocation of pepfar funding .

gao - 08-480 .

washington , d.c.: april 2 , 2008 .

global health: global fund to fight aids , tb and malaria has improved its documentation of funding decisions but needs standardized oversight expectations and assessments .

gao - 07-627 .

washington , d.c.: may 7 , 2007 .

global health: spending requirement presents challenges for allocating prevention funding under the president's emergency plan for aids relief .

gao - 06-395 .

washington , d.c.: april 4 , 2006 .

global health: the global fund to fight aids , tb and malaria is responding to challenges but needs better information and documentation for performance - based funding .

gao - 05-639 .

washington , d.c.: june 10 , 2005 .

global hiv / aids epidemic: selection of antiretroviral medications provided under u.s .

emergency plan is limited .

gao - 05-133 .

washington , d.c.: january 11 , 2005 .

global health: u.s. aids coordinator addressing some key challenges to expanding treatment , but others remain .

gao - 04-784 .

washington , d.c.: june 12 , 2004 .

global health: global fund to fight aids , tb and malaria has advanced in key areas , but difficult challenges remain .

gao - 03-601 .

washington , d.c.: may 7 , 2003 .

