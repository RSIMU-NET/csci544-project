to enhance border security and reduce illegal immigration , the department of homeland security ( dhs ) launched its multiyear , multibillion dollar secure border initiative ( sbi ) program in november 2005 .

through sbi , dhs intends to enhance surveillance technologies , increase staffing levels , enforce immigration laws , and improve the physical infrastructure along our borders .

within sbi , the secure border initiative network ( sbinet ) is a multibillion - dollar program that involves the acquisition , development , integration , deployment , and operation and maintenance of surveillance technologies to create a “virtual fence” along the border as well as command , control , communications , and intelligence ( c3i ) technologies to create a picture of the border in command centers and vehicles .

dhs's strategy is to deliver sbinet capabilities incrementally .

in doing so , the department has relied heavily on its prime contractor — the boeing company .

because of the importance of effective contractor management and oversight to the successful deployment and operation of sbinet capabilities , you asked us to determine to what extent dhs ( 1 ) has defined and implemented effective controls for managing and overseeing the sbinet prime contractor and ( 2 ) is effectively monitoring the prime contractor's progress in meeting cost and schedule expectations .

to accomplish our objectives , we focused on dhs's management and oversight of four key task orders associated with designing , developing , and deploying the first increment of sbinet , known as block 1 , to its initial deployment sites in arizona — the tucson border patrol station ( tus - 1 ) and the ajo border patrol station ( ajo - 1 ) .

in doing so , we reviewed dhs and program guidance , policies , and plans for managing and overseeing the contractor ; task orders and associated modifications ; contract deliverable review comments ; contractor schedules ; the work breakdown structure governing all task orders ; integrated baseline review documents for each task order ; and task order contract cost performance reports .

we also analyzed a nonprobability , random sample of 28 contract deliverables ; a nonprobability , random sample of 8 technical reviews conducted with the contractor ; and a nonprobability sample of 11 action items identified during program management reviews .

in addition , we interviewed program officials about contractor management and oversight activities , such as verifying and accepting contract deliverables and conducting technical and management reviews .

we also interviewed program officials about the prime contractor's cost and schedule performance .

we then compared this information to relevant guidance and leading industry practices on contractor management and oversight , schedule estimating , and earned value management ( evm ) , such as the software engineering institute's ( sei ) capability maturity model integration® ( cmmi® ) for acquisition , gao's cost estimating and assessment guide , and the american national standards institute ( ansi ) standard .

we conducted this performance audit from june 2009 to october 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are in appendix i .

managed by u.s. customs and border protection ( cbp ) , sbinet is to strengthen dhs's ability to detect , identify , classify , track , and respond to illegal breaches at and between ports of entry .

it includes the acquisition , development , integration , deployment , and operations and maintenance of a mix of surveillance technologies , such as cameras , radars , sensors , and c3i technologies .

surveillance technologies include unattended ground sensors that can detect heat and vibrations associated with foot traffic and metal associated with vehicles , radar mounted on fixed towers that can detect movement , and cameras mounted on fixed towers that can be used to identify and classify items of interest detected and tracked by ground sensors and radar .

these technologies are generally commercial off - the - shelf products .

c3i technologies include customized software development to produce a common operating picture ( cop ) — a uniform presentation of activities within specific areas along the border — as well as the use of cbp network capabilities .

together , the surveillance technologies are to gather information along the border and transmit this information to cop terminals located in command centers , which are to assemble it to provide cbp agents with border situational awareness , to , among other things , enhance agents' tactical decisionmaking regarding potential apprehensions .

since fiscal year 2006 , dhs has received about $4.4 billion in appropriations for sbi , including about $2.5 billion for physical fencing and related infrastructure , about $1.5 billion for virtual fencing ( eg , surveillance systems ) and related infrastructure ( eg , towers ) , and about $300 million for program management .

as of may 2010 , dhs had obligated about $1.02 billion for sbinet .

the sbi program management office , which is organizationally within cbp , is responsible for managing key acquisition functions associated with sbinet , including prime contractor management and oversight .

it is organized into four components: the sbinet system program office ( spo ) , operational integration division , business operations division , and systems engineering division .

the spo is responsible for such key contractor oversight activities as verifying and accepting contract deliverables and conducting contractor management and technical reviews .

in addition , the business operations division has primary responsibility for monitoring the prime contractor's cost and schedule performance .

as of may 2010 , the sbi program management office was staffed with 194 people — 105 government employees , 78 contractor staff , and 11 detailees .

 ( see figure 1 for a partial sbi program organization organization chart. ) .

chart. ) .

in addition , cbp has engaged the defense contract management agency to , among other things , perform surveillance of the prime contractor's evm , systems engineering , hardware and software quality assurance , and risk management .

further , the sbi contracting division , which is organizationally within the cbp office of administration procurement directorate's enterprise contracting office , is responsible for performing contract administration activities , such as maintaining the contract file and notifying the contractor in writing of whether deliverables are accepted or rejected .

the sbi contracting division is organized into three branches: sbi contracting , sbinet system contracting , and sbinet deployment contracting .

as of may 2010 , the sbi contracting division was staffed with 22 people — 18 government employees and 4 contractor staff .

 ( see figure 2 for a partial sbi contracting division organization chart. ) .

in september 2006 , cbp awarded an indefinite delivery / indefinite quantity ( idiq ) prime contract to the boeing company .

the contract was for 3 base years with three additional 1-year options for designing , producing , testing , deploying , and sustaining sbi .

in september 2009 , cbp exercised the first option year .

under the prime contract , cbp has issued 11 task orders that relate to sbinet , covering for example , cop design and development , system deployment , and system maintenance and logistics support .

as of may 2010 , 5 of the 11 task orders were complete and 6 were ongoing .

 ( see table 1 for a summary of the sbinet task orders. ) .

through the task orders , cbp's strategy is to deliver sbinet capabilities incrementally .

to accomplish this , the program office adopted an evolutionary system life cycle management approach in which system capabilities are to be delivered to designated locations in a series of discrete subsets of system functional and performance capabilities that are referred to as blocks .

the first block ( block 1 ) includes the purchase of commercially available surveillance systems , development of customized cop systems and software , and use of existing cbp communications and network capabilities .

according to program officials , as of july 2010 , the block 1 design was deployed to tus - 1 and was being deployed to ajo - 1 , both of which are located in cbp's tucson sector of the southwest border .

together , these two deployments cover 53 miles of the 1,989-mile - long southern border .

also , according to program officials , as of july 2010 , tus - 1 and ajo - 1 were to be accepted in september 2010 and december 2010 , respectively .

in january 2010 , the secretary of homeland security ordered an assessment of the sbi program .

in addition , on march 16 , 2010 , the secretary froze fiscal year 2010 funding for any work on sbinet beyond tus - 1 and ajo - 1 until the assessment is completed .

also at that time , the secretary redirected $35 million that had been allocated to sbinet block 1 deployment to other tested and commercially - available technologies , such as truck - mounted cameras and radar , called mobile surveillance systems , to meet near - term needs .

according to the sbi executive director , the department's assessment would consist of a comprehensive and science - based assessment to determine if there are alternatives to sbinet that may more efficiently , effectively , and economically meet u.s. border security needs .

further , the executive director stated that if the assessment suggests that the sbinet capabilities are worth the cost , dhs will extend its deployment to sites beyond tus - 1 and ajo - 1 .

however , if the assessment suggests that alternative technology options represent the best balance of capability and cost - effectiveness , dhs will redirect resources to these other options .

according to program officials , the initial phase of the assessment , which addresses the arizona border , was completed in july 2010 , and the results are currently being reviewed by senior dhs management .

officials were unable to provide a date for completion of the review .

our research and evaluations of information technology programs have shown that delivering promised system capabilities and benefits on time and within budget largely depends on the extent to which key acquisition management disciplines are employed .

these disciplines include , among others , requirements management , risk management , and test management .

as is discussed in the following section , we have previously reported on the extent to which these disciplines have been employed on sbinet .

contractor management and oversight , which is the focus of this report , is another acquisition management discipline .

among other things , this discipline helps to ensure that the contractor performs the requirements of the contract and the government receives the service or product intended .

dhs acquisition policies and guidance , along with other relevant guidance , recognize the importance of these management and oversight disciplines .

as we have previously reported , not employing them increases the risk that system acquisitions will not perform as intended and will require expensive and time - consuming rework .

since 2007 , we have identified a range of management weaknesses and risks facing sbinet , and we have made a number of recommendations to address them that dhs has largely agreed with , and to varying degrees , taken actions to address .

for example , in february 2007 , we reported that dhs had not fully defined activities , milestones , and costs for implementing the program or demonstrated how program activities would further the strategic goals and objectives of sbi .

further , we reported that the program's schedule contained a high level of concurrency among related tasks and activities , which introduced considerable risk .

accordingly , we recommended that dhs define explicit and measurable commitments relative to , among other things , program capabilities , schedules , and costs , and re - examine the level of concurrency in the schedule and adjust the acquisition strategy appropriately .

in september 2008 , we reported that a number of program management weaknesses put sbinet at risk of not performing as intended and taking longer and costing more to deliver than was necessary .

specifically , we reported the following: important aspects of sbinet were ambiguous and in a continued state of flux , making it unclear and uncertain what technology capabilities were to be delivered when .

for example , the scope and timing of planned sbinet deployments and capabilities had continued to change since the program began and remained unclear .

further , the spo did not have an approved integrated master schedule to guide the execution of the program , and our assimilation of available information indicated that key milestones continued to slip .

this schedule - related risk was exacerbated by the continuous change in and the absence of a clear definition of the approach used to define , develop , acquire , test , and deploy sbinet .

accordingly , we concluded that the absence of clarity and stability in these key aspects of sbinet impaired the ability of congress to oversee the program and hold dhs accountable for results , and it hampered dhs's ability to measure the program's progress .

sbinet requirements had not been effectively defined and managed .

while the spo had issued guidance that defined key practices associated with effectively developing and managing requirements , the guidance was developed after several key activities had been completed .

in the absence of this guidance , the spo had not effectively performed key requirements development and management practices , such as ensuring that different levels of requirements were properly aligned .

as a result , we concluded that the risk of sbinet not meeting mission needs and performing as intended was increased , as were the chances of the system needing expensive and time - consuming rework .

sbinet testing was not being effectively managed .

for example , the program office had not tested the individual system components to be deployed to initial locations , even though the contractor had initiated integration testing of these components .

further , its test management strategy did not contain , among other things , a clear definition of testing roles and responsibilities or sufficient detail to effectively guide planning for specific test events .

to address these issues , we recommended that dhs assess and disclose the risks associated with its planned sbinet development , testing , and deployment activities and that it address the system deployment , requirements management , and testing weaknesses that we had identified .

dhs largely agreed to implement our recommendations .

in september 2009 , we reported that sbinet had continued to experience delays .

for example , deployment to the entire southwest border had slipped from early 2009 to 2016 , and final acceptance of tus - 1 and ajo - 1 had slipped from november 2009 and march 2010 to december 2009 and june 2010 , respectively .

we did not make additional recommendations at that time .

in january 2010 , we reported that sbinet testing was not being effectively managed .

specifically , we reported the following: test plans , cases , and procedures for the most recent test events were not defined in accordance with important elements of relevant guidance .

further , a large percentage of the test cases for these events were changed extemporaneously during execution .

while some of the changes were minor , others were more significant , such as rewriting entire procedures and changing the mapping of requirements to test cases .

moreover , these changes to procedures were not made in accordance with documented quality assurance processes , but rather were based on an undocumented understanding that program officials said they established with the contractor .

compounding the number and significance of changes were questions raised by the spo and a support contractor about the appropriateness of some changes , noting that some changes made to system qualification test cases and procedures appeared to be designed to pass the test instead of being designed to qualify the system .

about 1,300 sbinet defects had been found from march 2008 through july 2009 , with the number of new defects identified during this time generally increasing faster than the number being fixed — a trend that is not indicative of a system that is maturing and ready for deployment .

while the full magnitude of these unresolved defects was unclear because the majority were not assigned a priority for resolution , some of the defects that had been found were significant .

although dhs reported that these defects had been resolved , they had nevertheless caused program delays , and related problems had surfaced that continued to impact the program's schedule .

in light of these weaknesses , we recommended that dhs ( 1 ) revise the program's overall test plan ; ( 2 ) ensure that test schedules , plans , cases , and procedures are adequately reviewed and approved consistent with the revised test plan ; ( 3 ) ensure that sufficient time is provided for reviewing and approving test documents prior to beginning a given test event ; and ( 4 ) triage the full inventory of unresolved system problems , including identified user concerns , and periodically report on their status to cbp and dhs leadership .

dhs fully agreed with the last three recommendations and partially agreed with the first .

in may 2010 , we raised further concerns about the program and called for dhs to reconsider its planned investment in sbinet .

specifically , we reported the following: dhs had defined the scope of the first incremental block of sbinet capabilities that it intended to deploy and make operational ; however , these capabilities and the number of geographic locations to which they were to be deployed continued to shrink .

for example , the geographic “footprint” of the initially deployed capability has been reduced from three border sectors spanning about 655 miles to two sectors spanning about 387 miles .

dhs had not developed a reliable integrated master schedule for delivering the first block of sbinet .

specifically , the schedule did not sufficiently comply with seven of nine key practices that relevant guidance states are important to having a reliable schedule .

for example , the schedule did not adequately capture all necessary activities , assign resources to them , and reflect schedule risks .

as a result , it was unclear when the first block was to be completed , and continued delays were likely .

dhs had not demonstrated the cost - effectiveness of block 1 .

in particular , it had not reliably estimated the costs of this block over its entire life cycle .

further , dhs had yet to identify expected quantifiable and qualitative benefits from this block and analyze them relative to costs .

without a meaningful understanding of sbinet costs and benefits , dhs lacks an adequate basis for knowing whether the initial system solution is cost effective .

dhs had not acquired the initial sbinet block in accordance with key life cycle management processes .

while processes associated with , among other things , requirements development and management and risk management , were adequately defined they were not adequately implemented .

for example , key risks had not been captured in the risk management repository and thus had not been proactively mitigated .

as a result , dhs is at increased risk of delivering a system that does not perform as intended .

we concluded that it remains unclear whether the department's pursuit of sbinet is a cost - effective course of action , and if it is , that it will produce expected results on time and within budget .

accordingly , we recommended that dhs ( 1 ) limit near - term investment in the first incremental block of sbinet , ( 2 ) economically justify any longer - term investment in sbinet , and ( 3 ) improve key program management disciplines .

dhs largely agreed with our recommendations , and noted ongoing and planned actions to address each of them .

in june 2010 , we reported several technical , cost , and schedule challenges facing sbinet , such as problems with radar functionality , and noted that the program office was staffed substantially below planned levels for government positions .

we did not make any additional recommendations at that time .

federal regulations and relevant guidance recognize effective contractor management and oversight as a key acquisition management discipline .

in addition , they describe a number of practices associated with it , including training persons who are responsible for contractor management and oversight , verifying and deciding whether to accept contract deliverables , conducting technical reviews with the contractor to ensure that products and services will satisfy user needs , and holding management reviews with the contractor to monitor contractor performance .

to dhs's credit , it has largely defined key practices aimed at employing each of these contractor management and oversight controls .

moreover , it has implemented some of them , such as training key contractor management and oversight officials and holding management reviews with the prime contractor .

however , it has not effectively implemented others , such as documenting contract deliverable reviews and using entry and exit criteria when conducting technical reviews .

reasons for these weaknesses include limitations in the defined verification and acceptance deliverable process and a spo decision to exclude some deliverables from the process , and insufficient time to review technical review documentation .

without employing the full range of practices needed to effectively manage and oversee the prime contractor , dhs is limited in its ability to know whether the contractor is meeting performance and product expectations .

moreover , it increases the chances that sbinet will not function as intended and will take more time and resources than necessary to deliver , which we have previously reported is the case for block 1 .

training supports the successful performance of relevant activities and tasks by helping to ensure that the responsible people have the necessary skills and expertise to perform the tasks .

according to relevant guidance , organizations should define training expectations and should ensure that these expectations are met for individuals responsible for contractor management and oversight .

dhs's acquisition management directives define training requirements for , among others , program managers , contracting officers , and contracting officer's technical representatives ( cotr ) .

specifically: program managers must be certified at a level commensurate with their acquisition responsibilities .

for a level 1 information technology program , like sbinet , the designated program manager must be certified at a program management level 3 .

contracting officers must be certified at the appropriate level to support their respective warrant authority .

cotrs must be trained and certified within 60 days of appointment to a contract or task order .

the minimum mandatory requirements are the completion of 40 hours of cotr training and a 1-hour procurement ethics training course .

for sbinet , cbp has ensured that people in each of these key positions have been trained in accordance with dhs requirements .

specifically , the two program managers associated with the development and deployment of sbinet block 1 — the sbi executive director and the spo executive director — were issued training certifications that qualify each as an acquisition professional .

further , each contracting officer responsible for executing actions on the arizona deployment task order ( adto ) , the design task order ( dto ) , the c3i / cop task order , and the system task order ( sto ) between june 2008 and february 2010 was certified commensurate with his or her respective warrant authority .

in addition , for the same time period , each cotr assigned to each of the four task orders received dhs - issued certificates indicating that each had met the minimum training requirements before being assigned to a task order .

according to cbp officials , dhs leadership has made contractor management and oversight training a high priority , which helped to ensure that key officials were trained .

by doing so , cbp has established one of the key controls to help ensure that prime contractor products and services meet expectations .

effectively implementing a well - defined process for verifying and accepting prime contractor deliverables is vital to sbinet's success .

dhs has a defined process for verifying and accepting contract deliverables , but this process does not ensure that deliverable reviews are sufficiently documented .

further , while the spo has followed key aspects of this process , it has not effectively documented its review of certain deliverables and has not effectively communicated to the prime contractor the basis for rejecting all deliverables .

reasons for not doing so include limitations in the defined verification and acceptance process and a spo decision to exclude some deliverables from the process .

without documenting all its reviews and effectively communicating the review results to the contractor , the spo has increased the chances of accepting deliverables that do not meet requirements and having the contractor repeat work to correct deliverable problems .

the purpose of contractor deliverable verification and acceptance is to ensure that contractor - provided products and services meet specified requirements and otherwise satisfy the terms of the contract .

according to relevant guidance , organizations should have written policies and procedures for verifying and accepting deliverables that , among other things , ( 1 ) assign roles and responsibilities for performing verification and acceptance tasks and ( 2 ) provide for conducting and documenting deliverable reviews and for effectively communicating to the contractor deliverable acceptance and rejection decisions .

to its credit , cbp has defined policies and procedures for verifying and accepting sbinet deliverables .

specifically , it issued its deliverable review and approval process in july 2007 , which specifies how the spo is to receive , review , and respond to all contract deliverables .

among other things , this guide assigns verification and acceptance roles and responsibilities to key program management positions .

for example , it assigns the project manager responsibility for overseeing the review process and determining the acceptability of the deliverables , designates reviewers responsibilities for examining the deliverable , and assigns the contracting officer responsibility for communicating the decision to accept or reject the deliverable to the contractor .

in addition , it provides for conducting deliverable reviews , which according to program officials , involves comparing the deliverable to the requirements enumerated in the applicable task order statement of work ( typically within the data item description ) .

the process further specifies that the decision to accept or reject the deliverable is to be communicated in writing to the contractor .

however , the process does not state that the results of all reviews are to be documented .

instead , its states that a deliverable review comment form is to be prepared only when deficiencies or problems exist .

if the deliverable is acceptable , the form does not need to be prepared .

program officials could not explain why review documentation was not required for acceptable deliverables .

as a result , and as discussed in the next section of this report , the spo cannot demonstrate its basis for accepting a number of deliverables , which in turn has increased the risk of , and actually resulted in , deliverables being accepted that do not meet requirements .

the spo followed its process for verifying and accepting sbinet - related deliverables about 62 percent of the time , based on the 29 deliverables that we reviewed .

specifically , the process was fully followed for 18 of the deliverables: ( 1 ) 6 that were accepted without documented review comments , ( 2 ) 5 that were accepted with documented review comments , and ( 3 ) 7 that were rejected with documented review comments .

in addition , the acceptance or rejection of all 18 deliverables was communicated in writing to the contractor .

for example , the adto security plan addendum was delivered to the spo in august 2008 .

the spo reviewed the plan and documented its review comments , which included a determination that the plan did not address all required items specified in the task order's data item description .

the cbp contracting officer subsequently notified the contractor in writing that the plan was rejected for this and other reasons .

in february 2009 , the contractor resubmitted the plan , the spo reviewed the plan and documented its comments , and the contracting officer again notified the contractor in writing that the plan was rejected .

the contractor resubmitted the plan in may 2009 , and program officials documented their review of the deliverable .

the contracting officer subsequently communicated the deliverable's acceptance in writing to the contractor .

 ( figure 3 summarizes the number of contract deliverables that did and did not follow the defined process. ) .

the remaining 11 deliverables , however , did not fully adhere to the defined process .

of these , five were accepted without any documented review comments and without communicating the acceptance in writing to the contractor .

the following are examples of these five and reasons for not adhering to the process .

three of the five deliverables related to the c3i / cop task order did not require government approval .

however , the deliverable review and approval process document does not provide for such treatment of these deliverables , and thus this practice is not in accordance with the process .

program officials told us that they have since recognized that this was a poor practice , and they have modified the task order to now require approval of all c3i / cop task order deliverables .

one of the five deliverables relates to the sto and is for the c2 component qualification test package , which includes , among other things , the test plan and test cases and procedures for conducting the c2 qualification test event .

in this case , the spo accepted the test plan because , according to program officials , several days had passed since the deliverable was received , and they had not received any comments from the reviewers .

they said that they therefore accepted the deliverable on the basis of not receiving any review comments to the contrary , but did not notify the contractor in writing of the acceptance .

the 11 deliverables also include 3 that were rejected without documented review comments and without the rejection being communicated to the contractor in writing .

the following are examples of these three and reasons for not adhering to the process are discussed below .

one of the three deliverables relates to the c3i / cop task order and is for the network operations center / security operations center ( noc / soc ) test plan / procedures / description .

according to program officials , the contractor did not submit the plan on time , thus requiring them to review it during the readiness review .

based on this review , the plan was rejected , which was communicated verbally to the contractor during the review .

despite rejecting the plan , the program office began testing the noc / soc component on the day of the review , without a revised plan being submitted , reviewed , and approved .

according to program officials , this occurred in part because of insufficient time and resources to review contractor test - related deliverables .

another one of the three deliverables also relates to the c3i / cop task order , and is for the release 0.5 software test plan / procedures / description .

according to program officials , the contractor submitted the plan late .

the program office rejected the plan and provided oral comments during a teleconference prior to the review .

nevertheless , the test event again occurred without a revised plan being submitted , reviewed , and accepted .

according to program officials , this was also due to insufficient time and resources to review the test plan .

in this case , the plan was approved in late april 2009 , which was 5 months after the test event was conducted .

the 11 deliverables also include 3 for which a decision to accept or reject the deliverable was not made .

see the following examples: one of the three relates to the c3i / cop task order , and is for the noc / soc interface control document .

for this deliverable , review comments were not documented and no written communication with the contractor occurred .

the deliverable was subsequently submitted three times and ultimately accepted .

however , program officials could not explain whether the initial deliverable was accepted or rejected , or why the deliverable was submitted multiple times .

another one of these relates to sto , and is for the dynamic object - oriented requirements system .

for this deliverable , review comments were not documented , but cbp communicated in writing to the contractor that it was withholding comment on this submission of the deliverable and was to provide a consolidated set of comments on the subsequent submission .

subsequently , the contractor resubmitted the deliverable , and because it was accepted , the review was not documented .

the contracting officer communicated the deliverable's acceptance in writing to the contractor .

by not effectively verifying and accepting contractor deliverables , the spo cannot ensure that the deliverables will satisfy stated requirements , thus increasing the risk of costly and time - consuming rework .

for example , we recently reported that contractor - delivered test plans were poorly defined and resulted in problems during testing .

in particular , noc / soc testing was hampered by requirements incorrectly mapped to test cases , did not provide for testing all requirements , and required significant extemporaneous changes to test cases during the test events .

as a result of the testing problems , the spo had to conduct multiple test events .

technical reviews are performed throughout the project life cycle to confirm that products and services being produced by the contractor provide the desired capability and ultimately satisfy user needs .

to its credit , dhs has defined a process for conducting technical reviews , but it has not effectively implemented it .

in particular , the spo did not ensure that all key documentation was reviewed and relevant criteria were satisfied before concluding key technical reviews .

program officials attributed these limitations to the program's aggressive schedule , which resulted in insufficient time to review relevant documentation .

concluding technical reviews without adequate justification has resulted in schedule delays and costly rework .

according to relevant guidance , organizations should have written policies and procedures for conducting technical reviews that , among other things , ( 1 ) assign roles and responsibilities for performing the specific technical review tasks and ( 2 ) establish entry and exit criteria to determine the readiness of the technical solution to proceed to the technical review and to demonstrate and confirm completion of required accomplishments .

to its credit , dhs has policies and guidance for conducting technical reviews .

specifically , dhs's systems engineering life cycle outlines the key reviews to be performed as well as how these reviews are aligned with the department's governance process .

in addition , dhs guidance defines expectations for technical review exit criteria , stating that compliance with exit criteria is based upon the satisfaction of the content of the criteria , and not upon only the delivery of specified documents .

to augment dhs policy and guidance , the sbinet systems engineering plan ( sep ) , dated november 2008 , identifies and describes the technical reviews to be conducted .

they include , for example: requirements review , which is to ensure that requirements have been completely and properly identified and are understood by the spo and the contractor .

documentation associated with this review is to include , among other things , a requirements traceability matrix ( i.e. , a tool for demonstrating that component - level requirements are traceable to higher - level system level requirements ) .

critical design review ( cdr ) , which is to ( 1 ) demonstrate that the designs are complete and baselined and ( 2 ) ensure that the solution is ready for fabrication , coding , assembly , and integration .

documentation for this review is to include , among other things , ( 1 ) baselined requirements , ( 2 ) interface descriptions , and ( 3 ) identified risks and mitigation plans .

test readiness review , which is to assess the readiness of the system solution to begin f include , among other things , ( 1 ) test plans that include test cases and procedures and ( 2 ) a traceability matrix that maps each requirement to tested to a corresponding test case .

ormal testing .

documentation for this review is to in addition , the sbinet sep describes high - level roles and responsibilities for performing these reviews , and establishes entry and exit criteria for each .

for example , it states that the spo program manager and the chief engineer are responsible for leading the reviews .

further , the sep defines entry and exit criteria for the cdr .

for example: entry .

system - level requirements should be traceable to component - level requirements ; system internal and external interfaces should be defined .

exit .

design baseline should be established and balanced across cost , schedule , performance , and risk considerations over the investment's lifecycle ; system risks should be identified and mitigation plans should be in place .

the spo did not follow the defined process for conducting technical reviews defined in the respective task orders to guide each review .

however , the task orders do not define entry and exit criteria .

rather , they list a set of documents that the contractor is to provide and the spo is to review .

for example , for the block 1 cdr , the relevant task order requires that the contractor deliver , among other documents , ( 1 ) baselined component and system requirements , ( 2 ) interface descriptions ( i.e. , descriptions of the data to be exchanged and the protocols used to exchange the data ) , and .

instead , program officials told us that they used the requirements ( 3 ) all identified risks and mitigation plans for those risks .

however , the task orders do not associate these documents with either entry or exit criteria , and they do not specify characteristics or qualities that the documents are to satisfy .

without explicit entry and exit criteria , the basis for beginning and e the technical reviews is unclear , thus increasing the risk that a program will be allowed to proceed and begin the next phase of development before it is ready to do so .

in fact , this risk was realized for sbinet .

technical reviews were concluded without adequate justification , which ultimately resulted in problems that required additional time and resources to fix .

for example: nding noc / soc requirements review .

at this review , the contractor did not deliver a requirements traceability matrix , as required by the relevant task order , until almost a month after the review was completed .

nonetheless , program officials stated that they concluded the review in june 2008 , without knowing whether the applicable higher - level system requirements were fully satisfied .

block 1 cdr .

for this review , the contractor delivered ( 1 ) the baselined component and syste ( 3 ) all identified risks and mitigation plans for those risks .

m requirements , ( 2 ) the interface descriptions , and however , these deliverables did not demonstrate that all component - leve requirements were baselined and interface descriptions we as we previously reported , baselined requirements associated with the noc / soc were not adequately defined at the time of the cdr , as evidenced by the fact that they were significantly changed 2 months later.program officials stated that while they knew that requirements were not adequately baselined at this review , they believed that the interface requirements were understood well enough to begin system development .

however , this was also not the case .

specifically , 39 of 90 noc / soc interface requirements were removed from the baseline , and 2 new interface requirements were added after cdr .

l re understood .

further , all relevant risks were not identified , and not all identified r had mitigation plans .

specifically , 7 of 31 identi mitigation plans , including risks associated with poorly established requirements traceability and inadequately defined requirements for integration suppliers .

moreover , the risks identified were as of may 200 prior to the beginning of cdr , and did not include four risks identifi ed between june and october 2008 , when cdr was concluded .

for exam a risk associated with the instability of the c3i / cop software was not addressed during cdr .

fied risks did not have without properly baselined requirements ( including interfaces ) and proactive mitigation of known risks , system performance shortfalls are likely .

to illustrate , we previously reported that ambiguities in requirements actually forced testers to rewrite test steps during exec based on interpretations of what they thought the requirements meant , ution and they required the spo to incur the time and expense of conducting multiple events to test noc / soc requirements .

noc / soc component qualification test rea spo did not ensure that a well - defined test plan was in place , to include , among other things , test cases and procedures and a traceability matrix that maps each requirement to be tested to a corresponding test case .

specifically , the contractor delivered the test plan on the day of the review , rather than 10 days prior to the review , as required by the relev task order .

nevertheless , the spo concluded the review based on its review of the plan during the test readiness review .

in this regard , we previously reported problems with the noc / soc test plan , noting tha plan mapped 28 out of 100 requirements to incorrect test cases .

progra officials attributed the test plan limitations to , among other things , insufficient time and resources to review the deliverables .

the sbinet independent verification and validation ( iv&v ) also identified weaknesses within technical reviews .

specifically , the iv&v contractor reported that the spo was not provided with documentation , including the test plan , early enough for the noc / soc test readiness review to allow sufficient time for review .

moreover , in december 2009 , the program identified technical oversight of technical review milestones as a major risk to the cost , schedule , and performance goals of the program .

according to program officials , they are developing a technical review manual that is to supplement the sep and provide detailed guidance for conducting technical reviews .

in commenting on a draft of this report , dhs stated that it plans to complete and implement its technical review guide by december 2010 .

management reviews help to ensure that the contractor's interpretation and implementation of the requirements are consistent with those of the program office .

according to relevant guidance , organizations should have written policies and procedures for conducting management reviews tha t , among other things , ( 1 ) involve relevant stakeholders ; ( 2 ) assign roles andresponsibilities for performing management review tasks ; ( 3 ) communicate project status information , including cost and schedule information , and risks ; and ( 4 ) identify , document , and track action items to closure .

cbp policy also recognizes the importance of these reviews by requiring the conduct of management reviews .

management plan identifies the types of management reviews that are to be conducted with the contractor .

for sbinet , the primary management the review is known as the joint program management review ( jpmr ) .

plan also identifies , for example , the stakeholders that are to participate in the reviews , including the program manager , project managers , program control staff , and the risk management team ; and it specifies the topics that are to be discussed at the reviews , such as project status , cost and schedule performance , and risks .

cbp , system life cycle handbook , version 1.2 ( sept. 30 , 2008 ) .

risk management process and tool , including reviewing lessons learnedfrom other programs .

the results of the review were presented during a february 2010 briefing , and the action item was closed .

effectively conducting management reviews has helped program leadership with an understanding of the contractor's progress and the program's exposure to risks so that appropriate corrective a can be taken and the chances of delivering a system solution that meets mission needs within budget are enhanced .

however , as discussed in the next section , the evm performance data presented at these management reviews were not reliable , thus rendering those reviews , at best , limited in the extent to which they disclosed the true status of the program .

measuring and reporting progress against cost and schedule expectations ( i.e. , baselines ) is a vital element of effective contractor management and oversight .

as noted earlier , evm provides a proven means for measuring progress against cost and schedule commitments and thereby identifying potential cost overruns and schedule delays early , when the impact can be minimized .

however , dh s has not ensured that its prime contractor's evm system , which was certified as meeting relevant standards , has been effectively implemented on sbinet .

in particular , it has not ensured that performanc measurement baselines were validated in a timely manner , that establish baselines were complete and realistic , and that contractor - provided cost and schedule data were reliable .

reasons cited by program officials for these weaknesses include the instability in the scope of the work to be performed , an unexpected temporary stop in block 1 design and deployment work when sbinet funding was redirected , and the contractor's use of estimated , rather than actual , costs for subcon work , which are subsequently adjusted when actual costs are rec without effectively implementing evm , dhs has not been positioned to identify potential cost and schedule problems early , and thus has not bee able to take timely actions to correct problems and avoid program schedule delays and cost increases .

eived .

in august 2005 , the office of management and budget issued guidance that , among other things , directs agencies to ensure that evm systems are compliant with the american national standards institute ( ansi ) standard .

the ansi standard consists of 32 guidelines associated with a sound evm system that are intended to ensure that data are reliable and can be used for informed decision - making .

the program office relies on the prime contractor's evm system to provide cost and schedule performance data .

this system was certified in april 2005 by dcma as being compliant with the ansi standard .

dcma certified the contractor's evm system again in february 2009 .

notwithstanding these certifications , dcma identified a number of issues with the contractor's implementation of its evm system .

in particular , in january 2010 , dcma reported that the sbinet prime contractor's implementation of evm was not consistent with all of the 32 ansi guidelines .

specifically , dcma identified concerns with the quality of scheduling and reporting , and the identification of significant differences between planned and actual cost and schedule performance , as well as reasons for those differences .

according to relevant guidance , the performance measurement baseline , which is the foundation of an evm system and the estimated cumulative value of planned work , serves as the value against which performance is measured for the life of the program or task order .

as such , it should be established as early as possible after contract or task order award , or whenever a major contract modification or baseline change occurs .

dhs guidance further states that a baseline should be validated within 90 days of the contract or task order award .

however , the program office validated a performance measurement baseline within 90 days for only two of the six baselines that we reviewed ( see figure 4 ) .

for the other four , the length of time to establish a validated baseline ranged from 5 to 10 months .

for example , the program office issued the adto in june 2008 , and it did not establish a validated baseline until 10 months later in april 2009 .

similarly , in february 2009 , the program office modified the scope of the sto and extended the period of performance , but it did not validate the revised baseline to include the additional scope and time until 7 months later in september 2009 .

figure 4 summarizes the periods of time during which earned value was , and was s not , measured against a validated baseline .

not , measured against a validated baseline .

according to program officials , the delays in validating performance baselines were due to instability in the work to be performed , and the need to temporarily stop block 1 design and deployment work between september 2008 and january 2009 because of dhs's decision to redirect funds from sbinet to the physical infrastructure .

without validated baselines , dhs was not positioned to identify potential cost and schedule problems early and to take timely corrective actions to mitigate those problems .

an integrated baseline review ( ibr ) is used to validate the performance measurement baseline .

this review is intended to verify that the baseline is realistic and ensure that the contractor and the government mutually understand scope , schedule , and risks for a given task order before a substantial amount of work is performed .

according to relevant guidance , establishing a complete and realistic performance measurement baseline includes ( 1 ) assigning responsibility for managing , tracking , and reporting earned value data for work performed ; ( 2 ) estimating needed resources ( i.e. , budgets and staff ) , including management reserve , for performing assigned tasks ; ( 3 ) defining a product - oriented description of all work to be performed ; ( 4 ) scheduling all work in a time - phased sequence that reflects the duration of the program's activities ; and ( 5 ) establishing objective performance measures for each task .

in validating the performance measurement baselines for the four task orders that we reviewed , the program office implemented two of the above elements , but it did not implement the other three .

specifically , for each of the six baselines associated with the task orders , the program office ( 1 ) assigned responsibility for managing , tracking , and reporting earned value data associated with each work breakdown structure element and ( 2 ) estimated a time - phased budget , including the anticipated staff needed , for each work breakdown structure element , and established a management reserve .

however , as discussed in the following section , the program office did not ( 1 ) define a product - oriented description of all work to be performed , ( 2 ) reliably estimate schedule baselines , and ( 3 ) adequately measure earned value performance .

program officials attribute these limitations in establishing comprehensive baselines to instability in the nature of the work to be performed and the prime contractor's method for determining subcontractor performance .

nevertheless , without complete and realistic baselines , the spo has been hampered in its ability to conduct meaningful measurement and oversight of the prime contractor's status and progress , as well as holding the contractor accountable for results .

more importantly , the lack of meaningful measurement and oversight has contributed to program cost overruns and schedule delays .

according to relevant guidance , a work breakdown structure deconstructs a program's end product into successively smaller levels until the work is subdivided to a level suitable for management control .

further , a work breakdown structure should be product oriented and include all work to be performed .

the work breakdown structure that was used to define each of the task order baselines was not product oriented .

instead , it was defined in terms of functions that span multiple system products , such as systems engineering , system test and evaluation , and program management .

additionally , the work breakdown structure did not reflect all work to be performed .

specifically , for four of the six performance measurement baselines , the work breakdown structure did not include all work described in the corresponding task order's statement of work .

for example , the work breakdown structure used to define the may 2008 sto baseline did not include the work associated with identifying and selecting components that meet system requirements and program security .

similarly , dcma reported in june 2008 that the work breakdown structure included in this baseline did not account for all work identified in the system task order .

a reliable schedule provides a road map for systematic execution of a program and the means by which to gauge progress , identify and address potential problems , and promote accountability .

our research has identified nine best practices associated with developing and maintaining a reliable schedule: ( 1 ) capturing all activities , ( 2 ) sequencing all activities , ( 3 ) assigning resources to all activities , ( 4 ) establishing the duration of all activities , ( 5 ) integrating activities horizontally and vertically , ( 6 ) establishing the critical path for all activities , ( 7 ) identifying reasonable “float” between activities , ( 8 ) conducting a schedule risk analysis , and ( 9 ) updating the schedule using logic and durations .

the six task order baselines were not reliable because they substantially complied with only two of the eight key schedule estimating practices , and they did not comply with , or only partially or minimally complied with , the remaining six practices .

 ( see figure 5 for a summary of the extent to which each of the baseline schedules met each of the eight practices. ) .

capturing all activities .

the six schedules did not capture all activities defined in the task order baseline .

specifically , five of the six schedules did not reflect the work to be performed across the four task orders ( i.e. , integrated master schedule ) .

further , as previously mentioned , four of six work breakdown structures were missing elements defined in the respective task order statements of work .

moreover , two of the six schedules did not reflect all work that was defined in the work breakdown structure .

for example , the december 2009 dto schedule omitted efforts associated with design work for tus - 1 and ajo - 1 .

sequencing all activities .

the six schedules substantially met this practice .

each of the schedules identified almost all of the predecessor and successor activities .

however , each contained improper predecessor and successor relationships .

for example , the may 2008 sto baseline included 52 of 538 activities ( about 10 percent ) with improper predecessor and successor relationships .

additionally , many activities in four of the schedules were constrained by “start no earlier than” dates .

for example , as previously reported , the september 2009 baseline schedule contained 403 of 1,512 activities ( about 27 percent ) with “start no earlier than” constraints , which means that these activities are not allowed to start earlier than their assigned dates , even if their respective predecessor activities have been completed .

assigning resources to all activities .

two of the six schedules partially met this practice .

specifically , two schedules included resources ; however , those resources were allocated to less than 15 percent of the activities identified in each schedule .

moreover , the remaining four schedules did not include estimated resources .

instead , resources for all six schedules were maintained separately as part of the contractor's earned value system and only available to dhs upon request .

establishing the duration of all activities .

each of the six baseline schedules substantially met this practice .

specifically , each schedule established the duration of key activities and included baseline start and end dates for most of the activities .

further , reasonable durations were established for the majority of the activities in the schedules , meaning that the durations established were less than 44 days .

nevertheless , each of the schedules included activities that were not of short duration , that is , more than 44 days .

for example , the april 2009 adto baseline included 29 of 1,009 activities with durations ranging from 45 days to 352 days .

integrating activities horizontally and vertically .

each of the schedules partially met this practice .

as mentioned previously , the six schedules did not capture all activities defined in the task order baseline .

further , four of six work breakdown structures were missing elements defined in respective task order statements of work .

additionally , five of six schedules did not reflect the work performed across the four task orders ( i.e. , integrated master schedule ) , and each had improper predecessor and successor relationships .

establishing the critical path for all activities .

each of the six schedules partially met this practice .

specifically , four of six work breakdown structures were missing elements defined in the respective task order statements of work .

additionally , four of the six schedules were missing predecessor and successor activities , and each of the schedules included improper predecessor and successor relationships .

further , five of the six schedules did not reflect the work to be performed across the four task orders ( i.e. , integrated master schedule ) .

unless all activities are included and properly linked , it is not possible to generate a true critical path .

identifying reasonable float between activities .

each of the schedules identified float ; however , the amount of float was excessive .

for example , the february 2008 c3i / cop task order baseline included 259 of 294 activities ( about 88 percent ) with float greater than 100 days and 189 of the 259 ( about 73 percent ) with float in excess of 200 days .

conducting a schedule risk analysis .

dhs did not conduct a risk analysis of any of the schedules .

according to the ansi standard for evm systems , only work for which measurement is impractical may be classified as “level - of - effort.” our research shows that if more than 15 percent of a program's budget is measured using level - of - effort , then that amount should be scrutinized because it does not allow schedule performance to be measured ( i.e. , performance equals planned work ) .

however , the six baselines had between 34 and 85 percent of the baseline dollar value categorized as level - of - effort , including four with more than 50 percent ( see table 2 ) .

moreover , for five of the six baselines , program documentation showed that the program office did not identify any action items during the respective ibrs related to the high use of level - of - effort .

according to program officials , the sto , which categorized between 70 and 85 percent of the baseline dollar value as level - of - effort , includes many program management activities ( eg , cost , schedule , and subcontractor management ) .

nevertheless , they recognized that the level - of - effort for this task order was high , and in november 2009 , they directed the contractor to minimize the use of level - of - effort for sto .

according to program officials , the high level - of - effort was due , in part , to the prime contractor's use of this measurement for subcontractor work .

in november 2009 , dcma stated that the spo's use of level - of - effort activities was high , noting that this could be masking true contractor performance .

if performed properly , evm can provide an objective means for measuring program status and forecasting potential program cost overruns and schedule slippages so that timely action can be taken to minimize their impact .

to do so , however , the underlying evm data must be reliable , meaning that they are complete and accurate and all data anomalies are explained .

in the case of sbinet , the evm data provided by the prime contractor for the 21-month period ending in february 2010 have not been reliable , as evidenced by numerous and unexplained anomalies in monthly evm reports .

reasons for the anomalies include the contractor's use of estimated , rather than actual , costs for subcontractor work , which are subsequently adjusted when actual costs are received .

without reliable performance data , the true status of the sbinet program is unclear , thus limiting the spo's ability to identify potential cost and schedule shortfalls .

evm is a proven program measurement approach that , if implemented appropriately , can create a meaningful and coherent understanding of a program's true health and status .

as a result , the use of evm can alert decision makers to potential program problems sooner than is possible by using actual versus planned expenditure alone , and thereby reduce the chance and magnitude of program cost overruns and schedule slippages .

simply stated , evm measures the value of completed work in a given period ( i.e. , earned value ) against ( 1 ) the actual cost of work completed for that period ( i.e. , actual cost ) and ( 2 ) the value of the work that is expected to be completed for that period ( i.e. , planned value ) .

differences in these values are referred to as cost and schedule variances , respectively .

cost variances compare the value of the work completed with the actual cost of the work performed .

for example , if a contractor completed $5 million worth of work and the work actually cost $6.7 million , there would be a negative $1.7 million cost variance .

schedule variances are also measured in dollars , but they compare the value of the work completed with the value of the work that was expected to be completed .

for example , if a contractor completed $5 million worth of work at the end of the month but was expected to complete $10 million worth of work , there would be a negative $5 million schedule variance .

positive variances indicate that activities are costing less or are being completed ahead of schedule .

negative variances indicate activities are costing more or are falling behind schedule .

to determine both cost and schedule variances , all three values are necessary .

according to relevant guidance , evm data should be valid and free from unexplained anomalies ( eg , missing or negative values ) because they can limit program management's ability to identify potential cost and schedule shortfalls .

therefore , anomalies should be minimized for each of the three values — earned value , planned value , and actual cost .

moreover , all anomalies should be identified , and the reason for each should be fully explained in the monthly evm reports .

to do less limits the completeness and accuracy of these values , and thus makes the resulting variance determinations unreliable .

while an industry standard for what constitutes an acceptable volume of anomalies does not exist , evm experts in the public and private sectors that we interviewed stated that the occurrence of evm data anomalies should be rare .

of these experts , some agreed that an anomaly should occur in no more than 5 percent of the work breakdown structure elements for a given contract or task order , while some of these advocated an occurrence percentage of no more than 1-2 percent .

however , the evm data that the prime contractor delivered to the spo from june 2008 through february 2010 ( 21 months ) contained numerous , unexplained anomalies .

specifically , the monthly evm reports for all four task orders that we reviewed showed one or more anomalies ( eg , missing or negative values for earned value , planned value , and actual cost ) in each of the months that had a validated performance measurement baseline .

more specifically , the average number of work breakdown structure elements across the four task orders that had data anomalies during this 21-month period ranged from 11 percent to 41 percent .

for the c3i / cop task order in particular , the monthly percentage of work breakdown structure elements with anomalies ranged between 25 and 67 percent over the 21 months .

 ( see figure 6 for the percentage of work breakdown structure elements with anomalies by month for each of the four task orders. ) .

the october 2009 sto monthly evm report illustrates how the anomalies can distort the contractor's performance .

according to this report , about $13,000 worth of work was planned to be completed on integration and management , approximately $13,000 worth of work was performed , and actual costs were about negative $550,000 .

thus , the report erroneously suggests that the contractor performed $13,000 worth of work , and actually saved about $550,000 in doing so .

similarly , the september 2009 adto monthly report showed that about $200,000 worth of work was planned to be completed on tower sites and infrastructure , and $25,000 worth of work was performed , but that no costs were incurred , suggesting that the work was performed for free .

exacerbating the large percentage of monthly data anomalies across the four task orders is the fact that in most cases the reasons for the anomalies were not explained in the monthly evm variance analysis reports .

specifically , about 79 percent of all anomalies across all four task orders during the 21-month period were not explained .

in particular , 82 of 119 ( or about 69 percent ) of all data anomalies for the sto task order were not explained in the monthly reports , and none of the anomalies were explained for dto .

 ( see figure 7 for the total number of data anomalies and the number that were explained in the monthly reports across the four task orders. ) .

program officials acknowledged problems with the evm data and stated that they meet with the prime contractor each month to discuss the evm reports , including the reliability of the data .

according to program officials , limitations in the evm data are due , in part , to the contractor's use of estimated , rather than actual , costs for subcontractor work , which are subsequently adjusted when actual costs are received .

officials further stated that they have been working with the contractor to reduce the volume of unexplained anomalies , and they believe that the reliability of the data has improved since february 2010 .

however , program officials did not provide any documentation to support this statement .

without reliable evm data , the program office is unable to identify actual cost and schedule shortfalls , which along with the other contractor tracking and oversight weaknesses discussed in this report , has limited its ability to effectively minimize program cost increases and schedule delays .

effective management and oversight of a program's prime contractor is essential to successfully acquiring and deploying a system like sbinet .

integral to accomplishing this is defining and implementing a range of contractor management and oversight controls ( eg , processes and practices ) that reflect relevant federal guidance and best practices .

to do less increases the chances that contractor - delivered products and services will not satisfy stated requirements and will not meet customer expectations .

the result is incurring the additional time and expense to redo or rework contractor deliverables , accepting products and services that do not perform as intended and do not meet mission needs , or both .

overall , dhs has not done an effective job of managing and overseeing its prime contractor , including monitoring the contractor's performance .

dhs has largely defined key management and oversight processes and practices that it should have followed , and it implemented a number of these processes and practices .

however , several key management and oversight controls were not adequately defined , and essential controls were not implemented .

most significantly , dhs did not adequately document deliverable reviews and communicate the basis for rejecting certain deliverables in writing to the contractor , which contributed to deliverables that did not live up to expectations and necessitated rework and caused later problems .

further , technical reviews were not grounded in explicit criteria for determining when reviews should begin and conclude , which also contributed to contract deliverables requiring costly and time - consuming rework .

in addition , the cost and schedule baselines for measuring the contractor's performance were frequently validated too late and without sufficient accuracy and completeness to provide a meaningful basis for understanding performance , which precluded dhs from taking timely action to correct unfavorable results and trends .

compounding these serious baseline limitations was contractor - provided data about actual performance that were replete with unexplained anomalies , thus rendering the data unfit for effective contractor management and oversight .

notwithstanding of a number of contractor management and oversight definition and implementation efforts that dhs executed well , such as defining key processes and practices and training key staff , these above - cited weaknesses collectively mean that dhs's management and oversight of its prime contractor has been a major contributor to the sbinet program's well - chronicled history of not delivering promised system capabilities on time and on budget .

these limitations can be attributed to a number of factors , including gaps in how certain processes and practices were defined , as well as not enforcing other processes and practices that were defined and applicable and not taking sufficient time to review deliverables that were submitted late .

the limitations can be further attributed to the fact that sbinet has from its outset lacked clear definition and stability , and thus experienced continuous change in scope and direction — an issue that we have previously reported and made recommendations to address .

collectively , these factors have helped to create a contractor management and oversight environment , which , when combined with the many other acquisition management weaknesses that we have previously reported about and made recommendations to address , have produced a program that to date has not been successful , and if not corrected , can become worse .

to improve dhs management and oversight of the sbinet prime contractor , we recommend that the secretary of homeland security direct the commissioner of the u.s. customs and border protection to have the sbi executive director , in collaboration with the sbinet program director , take the following four actions: revise and implement , as applicable , contractor deliverable review processes and practices to ensure that ( 1 ) contractor deliverables are thoroughly reviewed and are not constrained by late contractor deliverables and imposed milestones , ( 2 ) the reviews are sufficiently documented , and ( 3 ) the acceptance or the rejection of each contractor deliverable is communicated in writing to the contractor , to include explicit explanations of the basis for any rejections .

ensure that applicable entry and exit criteria for each technical review are used and satisfied before initiating and concluding , respectively , a given review .

establish and validate timely , complete , and accurate performance measurement baselines for each new task order or major modification of an existing task order , as appropriate , to include , but not be limited to , ensuring that ( 1 ) the work breakdown structure includes all work to be performed , ( 2 ) baseline schedules reflect the key schedule estimating practices discussed in this report , and ( 3 ) level - of - effort performance measurement in excess of 15 percent is scrutinized , justified , and minimized .

ensure that all anomalies in contractor - delivered monthly earned value management reports are identified and explained , and report periodically to dhs acquisition leadership on relevant trends in the number of unexplained anomalies .

because we have already made recommendations in prior reports to address the other management and oversight weaknesses discussed in this report , such as those related to requirements management , risk management , and systems engineering plan implementation , we are not making any additional recommendations at this time .

in written comments on a draft of this report , signed by the director , departmental gao / oig liaison office and reprinted in appendix ii , dhs agreed with our four recommendations and described actions under way or planned , which we summarize below , to address them .

with respect to our recommendation to revise and implement the contractor deliverable review process , dhs stated that it is updating the process to require written documentation of each review and the communication to the contractor of review results .

with respect to our recommendation to ensure that entry and exit criteria are used to initiate and conclude each technical review , dhs stated that it has established an sbi systems engineering directorate to focus on technical oversight of the acquisition process , adding that the directorate is developing a technical review guide that describes in detail the review process and the relevant entry and exit criteria for each technical review .

with respect to our recommendation to establish and validate timely , complete , and accurate performance measurement baselines , dhs stated that it is mindful of the need to establish and maintain current performance baselines , and to plan and implement baseline updates as completely and promptly as practicable , which it indicated is done through ibrs .

dhs also noted that while scheduling practices remain a challenge , it continues to make improvements to its process , including implementing scheduling tools and templates .

with respect to our recommendation to identify and explain all anomalies in monthly evm reports , and to report periodically relevant trends to dhs acquisition leadership , dhs acknowledged the need to correctly document anomalies in the monthly evm reports , and stated that it is working with dcma to improve contractor quality control issues and the content of the monthly evm reports .

it also stated that it is augmenting the reports with routine conversations between contractor and project management staff .

the department also committed to advising the appropriate acquisition leaders through established reporting and oversight opportunities as issues arise with contractor performance or reporting .

notwithstanding its agreement with our recommendations , the department also commented that it took exception to selected findings and conclusions regarding the program's implementation of evm .

a summary of dhs's comments and our responses are provided below .

the department stated that it took exception to our finding that it did not ensure performance measurement baselines were validated in a timely manner , and said that it was not accurate to conclude that the lack of validated baselines precluded the program office from identifying cost and schedule problems and taking corrective action .

in support of these positions , the department made the following three points , which our response addresses .

first , the department stated that the sbinet program office delayed formal ibrs until it had finalized negotiated modifications to the task orders , and in doing so , was able to complete an ibr within 90 days of each major task order modification .

we do not question whether the program office held ibrs within 90 days of final negotiation of major task order modifications .

our point is that dhs did not validate task order performance measurement baselines ( i.e. , hold ibrs ) within 90 days of task order award , which is what dhs guidance states should occur .

as our report states , the program office only met this 90-day threshold for two of the six baselines that we reviewed .

further , the length of time to validate the performance baselines for the four task orders far exceeded 90 days ( 5 to 10 months ) , during which time dhs reports show that significant work was performed and millions of dollars were expended .

in fact , the dhs reports show that most of the planned work for some of these task orders had already been performed by the time the ibr was held and the baseline was validated .

as we state in our report , and dhs acknowledged in its comments , the purpose of an ibr is to verify that the performance baseline is realistic and that the scope , schedule , and risks are mutually understood by the contractor and the government before a substantial amount of work is performed .

second , dhs commented that the program office maintained what it referred to as “interim” performance measurement baselines during the period of major program scope , schedule , and budget changes .

we acknowledge that in some cases the program office had these “interim” baselines .

however , these baselines are the contractor - provided baselines , meaning that the program office and the contractor had not mutually agreed to the scope , schedule , and risks associated with the work to be performed .

moreover , for two of the task orders , the program office did not have an “interim” baseline , even though the contractor performed significant work under these task orders .

third , the department stated that program leadership reviewed the contractor's technical and financial performance information relative to performance measurement baselines and implemented management actions as needed .

we do not question whether program leadership reviewed contractor - provided performance information or whether actions to address problems may have been taken .

however , our report does conclude , as is discussed later in this section , that the combination of the evm weaknesses that our report cites , to include unreliable performance baselines and contractor - provided performance data , did not allow the program office to identify performance problems early and to take timely actions to avoid the well - documented schedule delays and cost increases that the program has experienced .

the department expressed two concerns with how it said our report characterized and quantified evm anomalies .

first , the department stated that our report failed to distinguish between factual errors and legitimate monthly accounting adjustments .

we agree that our report does not distinguish between the two types of anomalies , and would add that this was intentional because making the distinction was not relevant to our finding .

specifically , our finding is that the reasons for the numerous anomalies were not explained in the monthly evm variance analysis reports , therefore making the true status of the program unclear .

second , dhs stated that we incorrectly concluded that both errors and adjustments are problematic , distort cost performance , and limit management insight .

in response , we did not conclude that all errors and adjustments have these impacts , but rather that the lack of explanation associated with such a large volume of anomalies made the true status of the program unclear , thus limiting the program office's ability to identify actual cost and schedule shortfalls , which is certainly problematic .

further , our report cites examples of cost performance data that provide a distorted picture of actual performance vis - à - vis expectations .

accordingly , the correct characterization of the report's conclusion concerning the reliability of evm data is that the lack of explanation of the numerous anomalies in monthly reports is problematic , provides a distorted picture of cost performance , and limits management insight .

to this very point , dhs acknowledged in its comments the importance of explaining the reason for anomalies in the monthly variance reports , regardless of whether they are due to factual errors or accounting adjustments .

the department stated that it took exception to our conclusion that the program office's lack of validated baselines in particular , and evm shortcomings in general , contributed to cost and schedule growth and made it unable to identify cost and schedule problems early and take corrective actions to avoid them .

in response , we did not conclude that the lack of validated baselines alone had either of these impacts .

however , we did conclude that the collection of evm weaknesses discussed in our report , to include untimely validated baselines , incomplete and unreliable baselines , and unreliable performance data , together precluded the program office from identifying problems early and taking corrective action needed to avoid the program's well - chronicled history of schedule delays and cost increases .

in support of this conclusion , we state in the report , for example , that the performance measurement baselines that we reviewed understated the cost and time necessary to complete the work because they did not capture all work in the task orders' statements of work and because they were not grounded in a range of scheduling best practices .

given that cost and schedule growth is a function of the baseline against which actual cost and schedule performance is measured , it follows logically that an understated baseline would produce actual cost overruns and schedule delays .

in addition , we would note that beyond these evm shortcomings , our report also recognizes other contract tracking and oversight , test management , and requirements management weaknesses that have collectively contributed to the program's cost , schedule , and performance shortfalls .

in addition to the above points , dhs provided technical comments , which we have incorporated in the report as appropriate .

we are sending copies of this report to the chairmen and ranking members of the senate and house appropriations committees and other senate and house committees and subcommittees that have authorization and oversight responsibilities for homeland security .

we will also send copies to the secretary of homeland security , the commissioner of u.s. customs and border protection , and the director of the office of management and budget .

in addition , the report will be available at no cost on the gao web site at http: / / www.gao.gov .

should you or your offices have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iii .

our objectives were to determine the extent to which the department of homeland security ( dhs ) ( 1 ) has defined and implemented effective controls for managing and overseeing the secure border initiative network ( sbinet ) prime contractor and ( 2 ) is effectively monitoring the prime contractor's progress in meeting cost and schedule expectations .

to accomplish our objectives , we focused on four key task orders — the arizona deployment task order , the design task order , the system task order , and the command , control , communications , and intelligence / common operating picture task order — that are integral to the design , development , and deployment of the first increment of sbinet , known as block 1 .

we also focused on the sbinet system program office's ( spo ) contracting and oversight activities that occurred from june 2008 through february 2010 .

to determine the extent to which dhs has defined and implemented effective controls for managing and overseeing the sbinet prime contractor , we focused on training , verifying and accepting contract deliverables , conducting technical reviews , and conducting management reviews .

we chose these four because each is important to tracking and overseeing contractor performance for the four task orders .

training .

we compared relevant dhs contractor management and oversight training requirements to the program managers' , contracting officers' , and contracting officer's technical representatives' training certifications .

verifying and accepting contract deliverables .

we compared u.s. customs and border protection's ( cbp ) process for verifying and accepting contract deliverables to leading industry practices to identify any variances .

we then assessed a nonprobability , random sample of 28 contract deliverables that the spo identified as being delivered between june 1 , 2008 , and august 31 , 2009 .

we also judgmentally selected one additional review that was conducted between september 1 , 2009 , and february 28 , 2010 .

for each of the 29 deliverables , we reviewed relevant documentation , such as the contract deliverables , review comment forms , and documented communications with the prime contractor indicating acceptance or rejection of the deliverable , and compared them to the cbp process and leading industry practices to determine what , if any , deviations existed .

technical reviews .

we compared relevant dhs and cbp guidance and entry and exit criteria in the task order data item descriptions to leading industry practices to identify any variances .

we assessed a nonprobability , random sample of technical reviews .

specifically , we assessed a technical review from each of the eight unique combinations of task orders and review types held between june 1 , 2008 , and august 31 , 2009 .

we also judgmentally selected one additional review that was conducted between september 1 , 2009 , and february 28 , 2010 .

for each of the nine reviews , we compared the package of documentation prepared for and used during these reviews to the criteria defined in the relevant task orders to determine the extent to which the reviews satisfied the criteria .

management reviews .

we compared relevant cbp guidance to leading industry practices to identify any variances .

we then compared relevant documentation prepared for and used during monthly joint program management reviews to determine the extent to which the reviews addressed cost , schedule , and program risks .

we also assessed a nonprobability sample of 11 action items that were identified during the reviews held from october 2008 through october 2009 , and assessed relevant documentation to determine the extent to which they were tracked to closure .

to determine the extent to which dhs is effectively monitoring the prime contractor's progress in meeting cost and schedule expectations , we focused on the program's implementation of earned value management ( evm ) because it was the tool used to monitor the contractor's cost and schedule performance .

specifically , we analyzed the six performance measurement baselines , and the associated integrated baseline review documentation , such as briefings , the work breakdown structure ( wbs ) governing all task orders , task order statements of work , schedules , monthly contract performance reports , control account plans , and responsibility assignment matrixes .

in doing so , we compared this documentation to evm and scheduling best practices as identified in our cost estimating and assessment guide .

specifically , for each of the six baselines: we reviewed control account plans and responsibility assignment matrixes to determine the period of performance and scope of work for each baseline , compared the work described in the respective task order statements of work to the work described in the responsibility assignment matrix , and reviewed the control account plans to determine the extent to which the level - of - effort measurement method was to measure contractor performance .

we analyzed the schedule presented at each baseline review against eight key schedule estimating practices in our cost estimating and assessment guide .

in doing so , we used commercially available software tools to determine whether each schedule , for example , included all critical activities , a logical sequence of activities , and reasonable activity durations .

further , we characterized the extent to which the schedule met each of the practices as either not met , minimally met , partially met , substantially met , or met .

we analyzed the contract performance reports for each of the four task orders for each month that there was a validated baseline .

specifically , we identified instances of the following: ( 1 ) negative planned value , earned value , or actual cost ; ( 2 ) planned value and earned value without actual cost ; ( 3 ) earned value and actual cost without planned value ; ( 4 ) actual cost without planned value or earned value ; ( 5 ) earned value without planned value and actual cost ; ( 6 ) inconsistencies between the estimated cost at completion and the planned cost at completion ; ( 7 ) actual cost exceeding estimated cost at completion ; and ( 8 ) planned or earned values exceeding planned cost at completion .

to determine the number of anomalies , we identified each wbs element that had one or more of the above anomalies .

then , we identified the number of wbs elements at the beginning and the end of the baseline period of performance , and calculated the average number of wbs elements .

we used this to determine the percentage of wbs elements with anomalies for each task order and for each month for which there was a validated baseline .

to support our work across this objective , we interviewed officials from the department of defense's defense contract management agency ( dcma ) , which provides contractor oversight services to the spo , including oversight of evm implementation , and prime contractor officials .

we also reviewed dcma monthly status reports and corrective action reports .

for both objectives , we interviewed program officials to obtain clarification on the practices , and to determine the reasons for any deviations .

to assess the reliability of the data that we used to support the findings in this report , we reviewed relevant program documentation to substantiate evidence obtained through interviews with knowledgeable agency officials , where available .

we determined that the data used in this report are sufficiently reliable .

we have also made appropriate attribution indicating the sources of the data .

we performed our work at the cbp headquarters and prime contractor facilities in the washington , d.c. , metropolitan area and with dcma officials from huntsville , alabama .

we conducted this performance audit from june 2009 to october 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , deborah a. davis ( assistant director ) , tisha derricotte , neil doherty , kaelin kuhn , lee mccracken , jamelyn payan , karen richey , matt snyder , sushmita srikanth , stacey steele , and matthew strain made key contributions to this report .

