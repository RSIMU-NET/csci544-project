this report responds to your request that we review selected aspects of the u.s. department of justice's ( justice ) office of justice programs' ( ojp ) program evaluations of discretionary grants awarded by ojp's bureau of justice assistance's ( bja ) byrne program ( byrne ) and the violence against women office ( vawo ) .

between fiscal years 1997 and 2000 , byrne and vawo discretionary grant awards grew , in constant fiscal year 2000 dollars , about 85 percent — from about $105 million to approximately $194 million .

these funds were awarded directly to various organizations , such as state and local governments , either on a competitive basis or pursuant to legislation allocating funds through congressional earmarks or direction .

discretionary grants awarded under the byrne program were designed to help state and local governments make communities safe and improve criminal justice .

discretionary grants awarded under vawo programs are aimed at improving criminal justice system responses to domestic violence , sexual assault , and stalking .

questions have been raised , however , regarding what these and other justice grant programs have accomplished .

to address your request , we are reporting on ( 1 ) the number , type , status of completion , and award amount of byrne and vawo discretionary grant program evaluations during fiscal years 1995 through 2001 and ( 2 ) the methodological rigor of the impact evaluation studies of byrne and vawo discretionary grant programs during fiscal years 1995 through 2001 .

in addition , information that you requested on ojp's approaches to disseminating evaluation results about byrne and vawo discretionary grant programs is presented in appendix ii .

our review covered discretionary grant program evaluations managed by justice's national institute of justice ( nij ) .

program evaluations are systematic studies that are conducted periodically or ad hoc to assess how well a program is working .

these studies can include impact evaluations — designed to assess the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of the program — and process evaluations — designed to assess the extent to which a program is operating as intended .

nij is ojp's principal research and development agency and is responsible for evaluating byrne and vawo programs .

for byrne program evaluations , nij awarded funding to evaluators using its own funds and funds made available through bja .

for vawo program evaluations , nij awarded funding to evaluators using funds made available exclusively through vawo .

the justice assistance act of 1984 ( p.l .

98-473 ) created ojp to provide federal leadership in developing the nation's capacity to prevent and control crime , administer justice , and assist crime victims .

ojp carries out its responsibilities by providing grants to various organizations , including state and local governments , indian tribal governments , nonprofit organizations , universities , and private foundations .

ojp comprises five bureaus , including bja , and seven program offices , including vawo .

in fulfilling its mission , bja provides grants for programs and for training and technical assistance to combat violent and drug - related crime and help improve the criminal justice system .

vawo administers grants to help prevent and stop violence against women , including domestic violence , sexual assault , and stalking .

during fiscal years 1995 through 2001 , bja and vawo awarded about $943 million to fund 700 byrne and 1,264 vawo discretionary grants .

one of bja's major grant programs is the byrne program .

bja administers the byrne program , just as its counterpart , vawo , administers its programs .

under the byrne discretionary grants program , bja provides federal financial assistance to grantees for educational and training programs for criminal justice personnel ; for technical assistance to state and local units of government ; and for projects that are replicable in more than one jurisdiction nationwide .

during fiscal years 1995 through 2001 , byrne discretionary grant programs received appropriations of about $385 million .

vawo was created in 1995 to carry out certain programs created under the violence against women act of 1994 .

the victims of trafficking and violence prevention act of 2000 reauthorized most of the existing vawo programs and added new programs .

vawo programs seek to improve criminal justice system responses to domestic violence , sexual assault , and stalking by providing support for law enforcement , prosecution , courts , and victim advocacy programs across the country .

during fiscal years 1995 through 2001 , vawo's five discretionary grant programs that were subject to program evaluation were ( 1 ) stop ( services , training , officers , and prosecutors ) violence against indian women discretionary grants , ( 2 ) grants to encourage arrest policies , ( 3 ) rural domestic violence and child victimization enforcement grants , ( 4 ) domestic violence victims' civil legal assistance grants , and ( 5 ) grants to combat violent crimes against women on campuses .

during fiscal years 1995 through 2001 , about $505 million was appropriated to these discretionary grant programs .

as already mentioned , nij is the principal research and development agency within ojp , and its duties include developing , conducting , directing , and supervising byrne and vawo discretionary grant program evaluations .

under 42 u.s.c .

3766 , nij is required to “conduct a reasonable number of comprehensive evaluations” of the byrne discretionary grant program .

in selecting programs for review under section 3766 , nij is to consider new and innovative approaches , program costs , potential for replication in other areas , and the extent of public awareness and community involvement .

according to nij officials , the implementation of various types of evaluations , including process and impact evaluations , fulfills this legislative requirement .

although legislation creating vawo does not require evaluations of the vawo discretionary grant programs , justice's annual appropriations for vawo during fiscal years 1998 through 2002 included monies for nij research and evaluations of violence against women .

in addition , justice has promulgated regulations requiring that nij conduct national evaluations of two of vawo's discretionary grant programs .

as with the byrne discretionary programs , nij is not required by statute or justice regulation to conduct specific types of program evaluations , such as impact or process evaluations .

the director of nij is responsible for making the final decision on which byrne and vawo discretionary grant programs to evaluate ; this decision is based on the work of nij staff in coordination with byrne or vawo program officials .

once the decision has been made to evaluate a particular program , nij issues a solicitation for proposals for grant funding from potential evaluators .

when applications or proposals are received , an external peer review panel comprising members of the research and relevant practitioner communities is convened .

peer review panels identify the strengths , weaknesses , and potential methodologies to be derived from competing proposals .

when developing their consensus reviews , peer review panels are to consider the quality and technical merit of the proposal ; the likelihood that grant objectives will be met ; the capabilities , demonstrated productivity , and experience of the evaluators ; and budget constraints .

each written consensus review is reviewed and discussed with partnership agency representatives ( eg , staff from bja or vawo ) .

these internal staff reviews and discussions are led by nij's director of the office of research and evaluation who then presents the peer review consensus reviews , along with agency and partner agency input , to the nij director for consideration and final grant award decisions .

the nij director makes the final decision regarding which application to fund .

to meet our objectives , we conducted our work at ojp , bja , vawo , and nij headquarters in washington , d.c. we reviewed applicable laws and regulations , guidelines , reports , and testimony associated with byrne and vawo discretionary grant programs and evaluation activities .

in addition , we interviewed responsible ojp , nij , bja , and vawo officials regarding program evaluations of discretionary grants .

as agreed with your offices , we focused on program evaluation activities associated with the byrne and vawo discretionary grant programs .

in particular , we focused on the program evaluations of discretionary grants that were funded during fiscal years 1995 through 2001 .

to address our first objective , regarding the number , type , status of completion , and award amount of byrne and vawo discretionary grant program evaluations , we interviewed nij , bja , and vawo officials and obtained information on byrne and vawo discretionary grant programs and program evaluations .

because nij is responsible for carrying out program evaluations of byrne and vawo discretionary grant programs , we also obtained and analyzed nij data about specific byrne and vawo discretionary grant program evaluations , including information on the number of evaluations as well as the type , cost , source of funding , and stages of implementation of each evaluation for fiscal years 1995 through 2001 .

we did not independently verify the accuracy or completeness of the data that nij provided .

to address the second objective , regarding the methodological rigor of the impact evaluation studies of byrne and vawo discretionary grant programs during fiscal years 1995 through 2001 , we initially identified the impact evaluations from the universe of program evaluations specified by nij .

we excluded from our analysis any impact evaluations that were in the formative stage of development — that is , the application had been awarded but the methodological design was not yet fully developed .

as a result , we reviewed four program evaluations .

for the four impact evaluations that we reviewed , we asked nij to provide any documentation relevant to the design and implementation of the impact evaluation methodologies , such as the application solicitation , the grantee's initial and supplemental applications , progress notes , interim reports , requested methodological changes , and any final reports that may have become available during the data collection period .

we also provided nij with a list of methodological issues to be considered in our review and requested them to submit any additional documentation that addressed these issues .

we used a data collection instrument to obtain information systematically about each program being evaluated and about the features of the evaluation methodology .

we based our data collection and assessments on generally accepted social science standards .

we examined such factors as whether evaluation data were collected before and after program implementation ; how program effects were isolated ( i.e. , the use of nonprogram participant comparison groups or statistical controls ) ; and the appropriateness of sampling and outcome measures .

two of our senior social scientists with training and experience in evaluation research and methodology separately reviewed the evaluation documents and developed their own assessments before meeting jointly to discuss the findings and implications .

this was done to promote a grant evaluation review process that was both independent and objective .

to obtain information on the approaches that bja , vawo , and nij used to disseminate program evaluation results , we requested and reviewed , if available , relevant handbooks and guidelines on information dissemination , including , for example , nij's guidelines .

we also reviewed bja , vawo , and nij's available print and electronic products as related to their proven programs and evaluations , including two nij publications about byrne discretionary programs and their evaluation methodologies and results .

we conducted our work between february 2001 and december 2001 in accordance with generally accepted government auditing standards .

we requested comments from justice on a draft of this report in january 2002 .

the comments are discussed near the end of this letter and are reprinted as appendix iii .

during fiscal years 1995 through 2001 , nij awarded about $6 million to carry out five byrne and five vawo discretionary grant program evaluations .

nij awarded evaluation grants using mostly funds transferred from bja and vawo .

specifically , of the approximately $1.9 million awarded for one impact and four process evaluations of the byrne discretionary program , nij contributed about $299,000 ( 16 percent ) and bja contributed about $1.6 million ( 84 percent ) .

vawo provided all of the funding ( about $4 million ) to nij for all program evaluations of five vawo discretionary grant programs .

according to nij , the five vawo program evaluations included both impact and process evaluations .

our review of information provided by nij showed that 6 of the 10 program evaluations — all 5 vawo evaluations and 1 byrne evaluation — included impact evaluations .

the remaining four byrne evaluations were exclusively process evaluations that measured the extent to which the programs were working as intended .

as of december 2001 , only one of these evaluations , the impact evaluation of the byrne car program , had been completed .

the remaining evaluations were in various stages of implementation .

table 1 lists each of the five byrne program evaluations and shows whether it was a process or an impact evaluation , its stage of implementation , the amount awarded during fiscal years 1995 through 2001 , and the total amount awarded since the evaluation was funded .

table 2 lists each of the five vawo program evaluations and shows that it was both a process and an impact evaluation , its stage of implementation , and the amount awarded during fiscal years 1995 through 2001 , which is the total amount awarded .

our review showed that methodological problems have adversely affected three of the four impact evaluations that have progressed beyond the formative stage .

all three vawo evaluations that we reviewed demonstrated a variety of methodological limitations , raising concerns as to whether the evaluations will produce definitive results .

the one byrne evaluation was well designed and used appropriate data collection and analytic methods .

we recognize that impact evaluations , such as the type that nij is managing , can encounter difficult design and implementation issues .

in the three vawo evaluations that we reviewed , program variation across sites has added to the complexity of designing the evaluations .

sites could not be shown to be representative of the programs or of particular elements of these programs , thereby limiting the ability to generalize results ; the lack of comparison groups hinders the ability to minimize the effects of factors external to the program .

furthermore , data collection and analytical problems compromise the ability of evaluators to draw appropriate conclusions from the results .

in addition , peer review committees found methodological problems in two of the three vawo evaluations that we considered .

the four program evaluations are multiyear , multisite impact evaluations .

some program evaluations used a sample of grants , while others used the entire universe of grants .

for example , the grants to encourage arrests policies program used 6 of the original 130 grantee sites .

in contrast , in the byrne children at risk impact evaluation , all five sites participated .

as of december 2001 , nij had already received the impact findings from the byrne children at risk program evaluation but had not received impact findings from the vawo discretionary grant program evaluations .

an impact evaluation is an inherently difficult task , since the objective is to isolate the effects of a particular program or factor from all other potential contributing programs or factors that could also effect change .

given that the byrne and vawo programs are operating in an ever changing , complex environment , measuring the impact of these specific byrne and vawo programs can be arduous .

for example , in the evaluation of vawo's rural domestic violence program , the evaluator's responsibility is to demonstrate how the program affected the lives of domestic violence victims and the criminal justice system .

several other programs or factors besides the rural domestic violence program may be accounting for all or part of the observed changes in victims' lives and the criminal justice system ( eg , a co - occurring program with similar objectives , new legislation , a local economic downturn , an alcohol abuse treatment program ) .

distinguishing the effects of the rural domestic violence program requires use of a rigorous methodological design .

all three vawo programs permitted their grantees broad flexibility in the development of their projects to match the needs of their local communities .

according to the assistant attorney general , this variation in projects is consistent with the intent of the programs' authorizing legislation .

we recognize that the authorizing legislation provides vawo the flexibility in designing these programs .

although this flexibility may make sense from a program perspective , the resulting project variation makes it more difficult to design and implement a definitive impact evaluation of the program .

instead of assessing a single , homogeneous program with multiple grantees , the evaluation must assess multiple configurations of a program , thereby making it difficult to generalize about the entire program .

although all of the grantees' projects under each program being evaluated are intended to achieve the same or similar goals , an aggregate analysis could mask the differences in effectiveness among individual projects and thus not result in information about which configurations of projects work and which do not .

the three vawo programs exemplify this situation .

the arrest policies program provided grantees with the flexibility to develop their respective projects within six purpose areas: implementing mandatory arrest or proarrest programs and policies in police departments , tracking domestic violence cases , centralizing and coordinating police domestic violence operations , coordinating computer tracking systems , strengthening legal advocacy services , and educating judges and others about how to handle domestic violence cases .

likewise , the stop grants program encouraged tribal governments to develop and implement culture - specific strategies for responding to violent crimes against indian women and provide appropriate services for those who are victims of domestic abuse , sexual assault , and stalking .

finally , the rural domestic violence program was designed to provide sites with the flexibility to develop projects , based on need , with respect to the early identification of , intervention in , and prevention of woman battering and child victimization ; with respect to increases in victim safety and access to services ; with respect to enhancement of the investigation and prosecution of crimes of domestic violence ; and with respect to the development of innovative , comprehensive strategies for fostering community awareness and prevention of domestic abuse .

because participating grant sites emphasized different project configurations , the resulting evaluation may not provide information that could be generalized to a broader implementation of the program .

the sites participating in the three vawo evaluations were not shown to be representative of their programs .

various techniques are available to help evaluators choose representative sites and representative participants within those sites .

random sampling of site and participant selection are ideal , but when this is not feasible , other purposeful sampling methods can be used to help approximate the selection of an appropriate sample ( eg , choosing the sample in such proportions that it reflects the larger population - - stratification ) .

at a minimum , purposeful selection can ensure the inclusion of a range of relevant sites .

as discussed earlier , in the case of the arrest policies program , six purpose areas were identified in the grant solicitation .

the six grantees chosen for participation in the evaluation were not however , selected on the basis of their representativeness of the six purpose areas or the program as a whole .

rather , they were selected on the basis of factors related solely to program “stability ; ” that is ; they were considered likely to receive local funding after the conclusion of federal grant funding , and key personnel would continue to participate in the coordinated program effort .

similarly , the 10 rural domestic violence impact evaluation grantees were not selected for participation on the basis of program representativeness or the specific purpose areas discussed earlier .

rather , sites were selected by the grant evaluator on the basis of “feasibility” ; specifically , whether the site would be among those participants equipped to conduct an evaluation .

similarly , the stop violence against indian women program evaluation used 3 of the original 14 project sites for a longitudinal study ; these were not shown to be representative of the sites in the overall program .

for another phase of the evaluation , the principal investigator indicated that grantee sites were selected to be geographically representative of american indian communities .

while this methodology provides for inclusion of a diversity of indian tribes in the sample from across the country , geography as a sole criterion does not guarantee representativeness in relation to many other factors .

each of the three vawo evaluations was designed without comparison groups — a factor that hinders the evaluator's ability to isolate and minimize external factors that could influence the results of the study .

use of comparison groups is a standard practice employed by evaluators to help determine whether differences between baseline and follow - up results are due to the program under consideration or to some other programs or external factors .

for example , as we reported in 1997 , to determine whether a drug court program has been effective in reducing criminal recidivism and drug relapse , it is not sufficient to merely determine whether those participating in the drug court program show changes in recidivism and relapse rates .

changes in recidivism and relapse variables between baseline and program completion could be due to other external factors , irrespective of the drug court program ( eg , the state may have developed harsher sentencing procedures for those failing to meet drug court objectives ) .

if , however , the drug court participant group is matched at baseline against another set of individuals , “the comparison group” who are experiencing similar life circumstances but who do not qualify for drug court participation ( eg , because of area of residence ) , then the comparison group can help in isolating the effects of the drug court program .

the contrasting of the two groups in relation to recidivism and relapse can provide an approximate measure of the program's impact .

all three vawo program impact evaluations lacked comparison groups .

one issue addressed in the arrest policies program evaluation , for example , was the impact of the program on the safety and protection of the domestic violence victim .

the absence of a comparison group , however , makes it difficult to firmly conclude that change in the safety and protection of participating domestic abuse victims is due to the arrest policies program and not to some other external factors operating in the environment ( eg , economic changes , nonfederal programs such as safe houses for domestically abused women , and church - run support programs ) .

instead of using comparison groups , the arrest policies program evaluation sought to eliminate potential competing external factors by collecting and analyzing extensive historical and interview data from subjects and by conducting cross - site comparisons ; the latter method proved unfeasible .

the stop violence against indian women discretionary grant program has sought in part , to reduce violent crimes against indian women by changing professional staff attitudes and behaviors .

to do this , some grantees created and developed domestic violence training services for professional staff participating in site activities .

without comparison groups , however , assessing the effect of the stop training programs is difficult .

attitudes and behaviors may change for myriad reasons unrelated to professional training development initiatives .

if a treatment group of professional staff receiving the stop training had been matched with a comparison group of professional staff that was similar in all ways except receipt of training , there would be greater confidence that positive change could be attributed to the stop program .

similarly , the lack of comparison groups in the rural domestic violence evaluation makes it difficult to conclude that a reduction in violence against women and children in rural areas can be attributed entirely , or in part , to the rural domestic program .

other external factors may be operating .

all three vawo impact evaluations involved data collection and analytical problems that may affect the validity of the findings and conclusions .

for example , we received documentation from nij on the stop grant program for reducing violence against indian women showing that only 43 percent of 127 grantees returned a mail survey .

in addition , only 25 percent of 127 tribes provided victim outcome homicide and hospitalization rates — far less than the percentage needed to draw broad - based conclusions about the intended goal of assessing victim well being .

in the arrest policies evaluation , nij reported that the evaluators experienced difficulty in collecting pre - grant baseline data from multiple sites and the quality of the data was oftentimes inadequate , which hindered their ability to statistically analyze change over time .

in addition , evaluators were hindered in several work areas by lack of automated data systems ; data were missing , lost , or unavailable ; and the ability to conduct detailed analyses of the outcome data was sometimes limited .

for the rural domestic violence evaluation , evaluators proposed using some variables ( eg , number and type of awareness messages disseminated to the community each month , identification of barriers to meeting the needs of women and children , and number of police officers who complete a training program on domestic violence ) that are normally considered to relate more to a process evaluation than an impact evaluation .

nij noted that outcome measurement indicators varied by site , complicating the ability to draw generalizations .

nij further indicated that the evaluation team did not collect baseline data prior to the start of the program , making it difficult to identify change resulting from the program .

nij does not require applicants to use particular evaluation methodologies .

nij employs peer review committees in deciding which evaluation proposals to fund .

the peer review committees expressed concerns about two of the three vawo program evaluation proposals ( i.e. , those for the arrest policies and rural domestic violence programs ) that were subsequently funded by nij .

whereas nij funded the arrest policies evaluation as a grant , nij funded the rural domestic violence evaluation as a cooperative agreement so that nij could provide substantial involvement in conducting the evaluation .

a peer - review panel and nij raised several concerns about the arrest policies program evaluation proposal .

these concerns included issues related to site selection , victim interviewee selection and retention in the sample , and the need for additional impact measures and control variables .

the grant applicant's responses to these issues did not remove concerns about the methodological rigor of the application , thus calling into question the ability of the grantee to assess the impact of the arrest policies program .

for example , the grantee stated that victim interviewee selection was to be conducted through a quota process and that the sampling would vary by site .

this would not allow the evaluators to generalize program results .

also , the evaluators said that they would study communities at different levels of “coordination” when comparison groups were not feasible , but they did not adequately explain ( 1 ) how the various levels of coordination would be measured , ( 2 ) the procedures used to select the communities compared , and ( 3 ) the benefits of using this method as a replacement for comparison groups .

nij subsequently funded this evaluation , and it is still in progress .

a peer review committee for the rural domestic violence and child victimization enforcement grant program evaluation also expressed concerns about whether the design of the evaluation application , as proposed , would demonstrate whether the program was working .

in its consensus review notes , the peer review committee indicated that the “ability to make generalizations about what works and does not work will be limited.” the peer review committee also warned of outside factors ( eg , unavailability of data , inaccessibility of domestic violence victims ) that could imperil the evaluation efforts of the applicant .

based on the peer review committee's input , nij issued the following statement to the applicant: “as a national evaluation of a major programmatic effort we hope to have a research design and products on what is working , what is not working , and why .

we are not sure that the proposed design will get us to that point.” we reviewed the grant applicant's response to nij's concern in its application addendum and found that the overall methodological design was still not discussed in sufficient detail or depth to determine whether the program was working .

although the deputy director of nij's office of research and evaluation asserted that this initial application was only for process evaluation funding , our review of available documents showed that the applicant had provided substantial information about both the process and impact evaluation methodologies in the application and addendum .

we believe that the methodological rigor of the addendum was not substantially improved over that of the original application .

the deputy director told us that , given the “daunting challenge faced by the evaluator,” nij decided to award the grant as a cooperative agreement .

under this arrangement , nij was to have substantial involvement in helping the grantee conduct the program evaluation .

the results of that evaluation have not yet been submitted .

the evaluator's draft final report is expected no earlier than april 2002 .

in contrast to the three vawo impact evaluations , the byrne impact evaluation employed methodological design and implementation procedures that met a high standard of methodological rigor , fulfilling each of the criteria indicated above .

in part , this may reflect the fact that byrne's car demonstration program , unlike the vawo programs , was according to the assistant attorney general , intended to test a research hypothesis , and the evaluation was designed accordingly .

car provided participants with the opportunity to use a limited number of program services ( eg , family services , education services , after - school activities ) that were theoretically related to the impact variables and the prevention and reduction of drug use and delinquency .

as a result , the evaluation was not complicated by project heterogeneity .

all five grantees participated in the evaluation .

high - risk youths within those projects were randomly selected from targeted neighborhood schools , providing student representation .

additionally , car evaluators chose a matched comparison group of youths with similar life circumstances ( eg , living in distressed neighborhoods and exposed to similar school and family risk factors ) and without access to the car program .

finally , no significant data collection implementation problems were associated with the car program .

the data were collected at multiple points in time from youths ( at baseline , at completion of program , and at one year follow - up ) and their caregivers ( at baseline and at completion of program ) .

self - reported findings from youths were supplemented by the collection of more objective data from school , police , and court records on an annual basis , and rigorous test procedures were used to determine whether changes over time were statistically significant .

additionally , car's impact evaluation used control groups , a methodologically rigorous technique not used in the three vawo evaluations .

to further eliminate the effects of external factors , youths in the targeted neighborhood schools were randomly assigned either to the group receiving the car program or to a control group that did not participate in the program .

since the car program group made significant gains over the same - school group and the matched comparison group not participating in the program , there was good reason to conclude that the car program was having a beneficial effect on the targeted audience .

appendix i provides summaries of the four evaluations .

despite great interest in assessing results of ojp's discretionary grant programs , it can be extremely difficult to design and execute evaluations that will provide definitive information .

our in - depth review of one byrne and three vawo impact evaluations that have received funding since fiscal year 1995 has shown that , in some cases , the flexibility that can be beneficial to grantees in tailoring programs to meet their communities' needs has added to the complexities of designing impact evaluations that will result in valid findings .

furthermore , the lack of site representativeness , appropriate comparison groups , and problems in data collection and analysis may compromise the reliability and validity of some of these evaluations .

we recognize that not all evaluation issues that can compromise results are easily resolvable , including issues involving comparison groups and data collection .

to the extent that methodological design and implementation issues can be overcome , however , the validity of the evaluation results will be enhanced .

nij spends millions of dollars annually to evaluate ojp grant programs .

more up - front attention to the methodological rigor of these evaluations will increase the likelihood that they will produce meaningful results for policymakers .

unfortunately , the problematic evaluation grants that we reviewed are too far along to be radically changed .

however , two of the vawo evaluation grants are still in the formative stage ; more nij attention to their methodologies now can better ensure useable results .

we recommend that the attorney general instruct the director of nij to assess the two vawo impact evaluations that are in the formative stage to address any potential methodological design and implementation problems and , on the basis of that assessment , initiate any needed interventions to help ensure that the evaluations produce definitive results .

we further recommend that the attorney general instruct the director of nij to assess its evaluation process with the purpose of developing approaches to ensure that future impact evaluation studies are effectively designed and implemented so as to produce definitive results .

we provided a copy of a draft of this report to the attorney general for review and comment .

in a february 13 , 2002 , letter , the assistant attorney general commented on the draft .

her comments are summarized below and presented in their entirety in appendix iii .

the assistant attorney general agreed with the substance of our recommendations and said that nij has begun , or plans to take steps , to address them .

although it is still too early to tell whether nij's actions will be effective in preventing or resolving the problems we identified , they appear to be steps in the right direction .

with regard to our first recommendation — that nij assess the two vawo impact evaluations in the formative stage to address any potential design and implementation problems and initiate any needed intervention to help ensure definitive results — the assistant attorney general noted that nij has begun work to ensure that these projects will provide the most useful information possible .

she said that for the crimes against women on campus program evaluation , nij is considering whether it will be possible to conduct an impact evaluation and , if so , how it can enhance its methodological rigor with the resources available .

for the civil legal assistance program evaluation , the assistant attorney general said that nij is working with the grantee to review site selection procedures for the second phase of the study to enhance the representativeness of sites .

the assistant attorney general was silent about any additional steps that nij would take during the later stages of the civil legal assistance program process evaluation to ensure the methodological rigor of the impact phase of the study .

however , it seems likely that as the process evaluation phase of the study continues , nij may be able to take advantage of additional opportunities to address any potential design and implementation problems .

with regard to our second recommendation — that nij assess its evaluation process to develop approaches to ensure that future evaluation studies are effectively designed and implemented to produce definitive results — the assistant attorney general stated that ojp has made program evaluation , including impact evaluations of federally funded programs , a high priority .

the assistant attorney general said that nij has already launched an examination of nij's evaluation process .

she also noted that , as part of its reorganization , ojp plans to measurably strengthen nij's capacity to manage impact evaluations with the goal of making them more useful for congress and others .

she noted as an example that ojp and nij are building measurement requirements into grants at the outset , requiring potential grantees to collect baseline data and track the follow - up data through the life of the grant .

we have not examined ojp's plans for reorganizing , nor do we have a basis for determining whether ojp's plans regarding nij would strengthen nij's capacity to manage evaluations .

however , we believe that nij and its key stakeholders , such as congress and the research community , would be well served if nij were to assess what additional actions it could take to strengthen its management of impact evaluations regardless of any reorganization plans .

in her letter , the assistant attorney general pointed out that the report accurately describes many of the challenges facing evaluators when conducting research in the complex environment of criminal justice programs and interventions .

however , she stated that the report could have gone further in acknowledging these challenges .

the assistant attorney general also stated that the report contrasts the byrne evaluation with the three vawo evaluations and obscures important programmatic differences that affect an evaluator's ability to achieve “gao's conditions for methodological rigor.” she pointed out that the byrne car program was intended to test a research hypothesis and that the evaluation was designed accordingly , i.e. , the availability of baseline data were ensured ; randomization of effects were stipulated as a precondition of participation ; and outcome measures were determined in advance on the basis of the theories to be tested .

she further stated that , in contrast , all of the vawo programs were ( 1 ) highly flexible funding streams , in keeping with the intention of congress , that resulted in substantial heterogeneity at the local level and ( 2 ) well into implementation before the evaluation started .

the assistant attorney general went on to say that it is ojp's belief that evaluations under less than optimal conditions can provide valuable information about the likely impact of a program , even though the conditions for methodological strategies and overall rigor of the car evaluation were not available .

we recognize that there are substantive differences in the intent , structure , and design of the various discretionary grant programs managed by ojp and its bureaus and offices .

and , as stated numerous times in our report , we acknowledge not only that impact evaluation can be an inherently difficult and challenging task but also that measuring the impact of these specific byrne and vawo programs can be arduous , given that they are operating in an ever changing , complex environment .

we agree that not all evaluation issues that can compromise results are easily resolvable , but we firmly believe that , with more up - front attention to design and implementation issues , there is a greater likelihood that nij evaluations will provide meaningful results for policymakers .

absent this up - front attention , questions arise as to whether nij is ( 1 ) positioned to provide the definitive results expected from an impact evaluation and ( 2 ) making sound investments given the millions of dollars spent on these evaluations .

the assistant attorney general also commented that although our report discussed “generally accepted social science standards,” it did not specify the document that articulates these standards or describe our elements of rigor .

as a result , the assistant attorney general said , ojp had to infer that six elements had to be met to achieve what “gao believes” is necessary to “have a rigorous impact evaluation.” specifically , she said that she would infer that , for an impact evaluation to be rigorous would require ( 1 ) selection of homogenous programs , ( 2 ) random or stratified site sampling procedures ( or selection of all sites ) , ( 3 ) use of comparison groups , ( 4 ) high response rates , ( 5 ) available and relevant automated data systems that will furnish complete and accurate data to evaluators in a timely manner , and ( 6 ) funding sufficient to accomplish all of the above .

furthermore , the attorney general said that it is rare to encounter all of these conditions or be in a position to engineer all of these conditions simultaneously ; and when all of these conditions are present , the evaluation would be rigorous .

she also stated that it is possible to glean useful , if not conclusive , evidence of the impact of a program from an evaluation that does not rise to the standard recommended by gao because of the unavoidable absence of “one or more elements.” we agree that our report did not specify particular documents that articulate generally accepted social science standards .

however , the standards that we applied are well defined in scientific literature .

all assessments of the impact evaluations we reviewed were completed by social scientists with extensive experience in evaluation research .

throughout our report , we explain our rationale and the criteria we used in measuring the methodological rigor of nij's impact evaluations .

furthermore , our report does not suggest that a particular standard or set of standards is necessary to achieve rigor , nor does it suggest that other types of evaluations , such as comprehensive process evaluations , are any less useful in providing information on how a program is operating .

in this context , it is important to point out that the scope of our work covered impact evaluations of byrne and vawo discretionary grant programs — those designed to assess the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of the program .

we differ with the assistant attorney general with respect to the six elements cited as necessary elements for conducting an impact evaluation .

contrary to the assistant attorney general's assertion , our report did not state that a single homogeneous program is a necessary element for conducting a rigorous impact evaluation .

rather , we pointed out that heterogeneity or program variation is a challenge that adds to the complexity of designing an evaluation .

in addition , contrary to her assertion , the report did not assert that random sampling or stratification was a necessary element for conducting a rigorous evaluation ; instead it stated that when random sampling is not feasible , other purposeful sampling methods can be used .

with regard to comparison groups , the assistant attorney general's letter asserted that gao standards required using groups that do not receive program benefits as a basis of comparison with those that do receive such benefits .

in fact , we believe that the validity of evaluation results can be enhanced through establishing and tracking comparison groups .

if other ways exist to effectively isolate the impacts of a program , comparison groups may not be needed .

however , we saw no evidence that other methods were effectively used in the vawo impact evaluations we assessed .

the assistant attorney general also suggested that we used a 75 percent or greater response rate for evaluation surveys as a standard of rigor .

in fact , we did not — we simply pointed out that nij documents showed a 43 percent response rate on one of the stop grant program evaluation surveys .

this is below omb's threshold response rate level — the level below which omb particularly believes nonresponse bias and statistical problems could affect surveys .

given omb guidance , serious questions could be raised about program conclusions drawn from the results of a survey with a 43 percent response rate .

in addition , the assistant attorney general suggested that , by gao standards , she would have to require state , local , or tribal government officials to furnish complete and accurate data in a timely manner .

in fact , our report only points out that nij reported that evaluators were hindered in carrying out evaluations because of the lack of automated data systems or because data were missing , lost , or unavailable — again , challenges to achieving methodologically rigorous evaluations that could produce meaningful and definitive results .

finally , the assistant attorney general's letter commented that one of the elements needed to meet “all of gao's conditions” of methodological rigor is sufficient funding .

she stated that more rigorous impact evaluations cost more than those that provide less scientific findings , and she said that ojp is examining the issue of how to finance effective impact evaluations .

we did not assess whether funding is sufficient to conduct impact evaluations , but we recognize that designing effective and rigorous impact evaluations can be expensive — a condition that could affect the number of impact evaluations conducted .

however , we continue to believe that with more up - front attention to the rigor of ongoing and future evaluations , nij can increase the likelihood of conducting impact evaluations that produce meaningful and definitive results .

in addition to the above comments , the assistant attorney general made a number of suggestions related to topics in this report .

we have included the assistant attorney general's suggestions in the report , where appropriate .

also , the assistant attorney general provided other comments in response to which we did not make changes .

see appendix iii for a more detailed discussion of the assistant attorney general's comments .

we are sending copies of this report to the chairman and the ranking minority member of the senate judiciary committee ; to the chairman and ranking minority member of the house judiciary committee ; to the chairman and ranking minority member of the subcommittee on crime , house committee on the judiciary ; to the chairman and the ranking minority member of the house committee on education and the workforce ; to the attorney general ; to the ojp assistant attorney general ; to the nij director ; to the bja director ; to the vawo director ; and to the director , office of management and budget .

we will also make copies available to others on request .

if you or your staff have any questions about this report , please contact john f. mortin or me at ( 202 ) 512-8777 .

key contributors to this report are acknowledged in appendix iv .

evaluation findings assessment of evaluation	 this evaluation has several limitations .

 ( 1 ) the choice of the 10 impact sites is skewed toward the national evaluation of the rural domestic violence and child victimization grant program cosmos corporation the violence against women office's ( vawo ) rural domestic violence program , begun in fiscal year 1996 , has funded 92 grantees through september 2001 .

the primary purpose of the program is to enhance the safety of victims of domestic abuse , dating violence , and child abuse .

the program supports projects that implement , expand , and establish cooperative efforts between law enforcement officers , prosecutors , victim advocacy groups , and others in investigating and prosecuting incidents of domestic violence , dating violence , and child abuse ; provide treatment , counseling , and assistance to victims ; and work with the community to develop educational and prevention strategies directed toward these issues .

the impact evaluation began in july 2000 , with a final report expected no earlier than april 2002 .

initially , 10 grantees were selected to participate in the impact evaluation ; 9 remain in the evaluation .

two criteria were used in the selection of grant participants: the “feasibility” of earlier site - visited grantees to conduct an outcome evaluation and vawo recommendations based on knowledge of grantee program activities .

logic models were developed , as part of the case study approach , to show the logical or plausible links between a grantee's activities and the desired outcomes .

the specified outcome data were to be collected from multiple sources , using a variety of methodologies during 2- to - 3-day site visits ( eg , multiyear criminal justice , medical , and shelter statistics were to be collected from archival records ; community stakeholders were to be interviewed ; and grantee and victim service agency staff were to participate in focus groups ) .

at the time of our review , this evaluation was funded at $719,949 .

the national institute of justice ( nij ) could not separate the cost of the impact evaluation from the cost of the process evaluation .

too early to assess .

technically developed evaluation sites and is not representative of either all rural domestic violence program grantees , particular types of projects , or delivery styles .

 ( 2 ) the lack of comparison groups will make it difficult to exclude the effect of external factors , such as victim safety and improved access to services , on perceived change .

 ( 3 ) several so - called short - term outcome variables are in fact process variables ( eg , number of police officers who complete a training program on domestic violence , identification of barriers to meeting the needs of women and children ) .

 ( 4 ) it is not clear how interview and focus group participants are to be selected , ( 5 ) statistical procedures to be used in the analyses have not been sufficiently identified .

the nij peer review committee had concerns about whether the evaluation could demonstrate that the program was working .

nij funded the application as a cooperative agreement because a substantial amount of agency involvement was deemed necessary to meet the objectives of the evaluation .

evaluation findings assessment of evaluation	 this evaluation has several limitations: the absence of a representative sampling frame for site national evaluation of the arrest policies program institute for law and justice ( ilj ) the purpose of vawo's arrest policies program is to encourage states , local governments , and indian tribal governments to treat domestic violence as a serious violation of criminal law .

the program received a 3-year authorization ( fiscal years 1996 through 1998 ) at approximately $120 million to fund grantees under six purpose areas: implementing mandatory arrest or proarrest programs and policies in police departments , tracking domestic violence cases , centralizing and coordinating police domestic violence operations , coordinating computer tracking systems , strengthening legal advocacy services , and educating judges and others about how to handle domestic violence cases .

grantees have flexibility to work in several of these areas .

at the time the nij evaluation grant was awarded , 130 program grantees had been funded ; the program has since expanded to 190 program grantees .

the impact evaluation began in august 1998 , with a draft final report due in march 2002 .

six grantees were chosen to participate in the impact evaluation .

each of the six sites was selected on the basis of program “stability,” not program representativeness .

within sites , both quantitative and qualitative data were to be collected and analyzed to enable better understanding of the impact of the arrest program on offender accountability and victim well being .

this process entailed reviewing data on the criminal justice system's response to domestic violence ; tracking a random sample of 100 offender cases , except in rural areas , to determine changes in offender accountability ; conducting content analyses of police incident reports to assess change in police practices and documentation ; and interviewing victims or survivors at each site to obtain their perceptions of the criminal justice system's response to domestic violence and its impact on their well - being .

ilj had planned cross - site comparisons and the collection of extensive historical and interview data to test whether competing factors could be responsible for changes in arrest statistics .

at the time of our review , this evaluation was funded at $1,130,574 .

nij could not separate the cost of the impact evaluation from the cost of the process evaluation .

too early to assess .

selection , the lack of comparison groups , the inability to conduct cross - site comparisons , and the lack of a sufficient number of victims in some sites to provide a perspective on the changes taking place in domestic violence criminal justice response patterns and victim well - being .

in addition , there was difficulty collecting pre - grant baseline data , and the quality of the data was oftentimes inadequate , limiting the ability to measure change over time .

further , automated data systems were not available in all work areas , and data were missing , lost , or unavailable .

an nij peer review committee also expressed some concerns about the grantee's methodological design .

evaluation findings assessment of evaluation	 methodological design and implementation issues may cause difficulties in attributing program impact .

a impact evaluation of stop grant programs for reducing violence against indian women the university of arizona vawo's stop ( services , training , officers , and prosecutors ) grant programs for reducing violence against indian women discretionary grant program was established under title iv of the violent crime control and law enforcement act of 1994 .

the program's principal purpose is to reduce violent crimes against indian women .

the program , which began in fiscal year 1995 with 14 grantees , encourages tribal governments to develop and implement culture - specific strategies for responding to violent crimes against indian women and providing appropriate services for those who are victims of domestic abuse , sexual assault , and stalking .

in this effort , the program provided funding for the services and training , and required the joint coordination , of nongovernmental service providers , law enforcement officers , and prosecutors hence the name , the stop grant programs for reducing violence against indian women .

the university of arizona evaluation began in october 1996 with an expected final report due in march 2002 .

the basic analytical framework of this impact evaluation involves the comparison of quantitative and qualitative pre - grant case study histories of participating tribal programs with changes taking place during the grant period .

various data collection methodologies have been adopted ( at least in part , to be sensitive to the diverse indian cultures ) : 30-minute telephone interviews , mail surveys , and face - to - face 2- to 3 day site visits .

at the time of our review , this evaluation was funded at $468,552 .

nij could not separate the cost of the impact evaluation from the cost of the process evaluation .

too early to assess .

number of methodological aspects of the study remain unclear: the site selection process for “in - depth case study evaluations ; ” the methodological procedures for conducting the longitudinal evaluation ; the measurement , validity , and reliability of the outcome variables ; the procedures for assessing impact ; and the statistical tests to be used for determining significant change .

comparison groups are not included in the methodological design .

in addition , only 43 percent of the grantees returned the mail survey , only 25 percent could provide the required homicide and hospitalization rates ; and only 26 victims of domestic violence and assault could be interviewed ( generally too few to measure statistical change ) .

generalization of evaluation results to the entire stop grant programs for reducing violence against indian women will be difficult , given these problems .

longitudinal impact evaluation of the strategic intervention for high risk youth ( a.k.a .

the children at risk program ) the urban institute the children at risk ( car ) program , a comprehensive drug and delinquency prevention initiative funded by the bureau of justice assistance ( bja ) , the office of juvenile justice and delinquency prevention ( ojjdp ) , the center on addiction and substance abuse , and four private foundations , was established to serve as an experimental demonstration program from 1992 to 1996 in five grantee cities .

low - income youths ( 11 to 13 years old ) and their families , who lived in severely distressed neighborhoods at high - risk for drugs and crime , were targeted for intervention .

eight core service components were identified: case management , family services , education services , mentoring , after - school and summer activities , monetary and nonmonetary incentives , community policing , and criminal justice and juvenile intervention ( through supervision and community service opportunities ) .

the goals of the program were to reduce drug use among targeted families and improve the safety and overall quality of life in the community .

the evaluation actually began in 1992 , and the final report was submitted in may 1998 .

the study used both experimental and quasi - experimental evaluation designs .

a total of 671 youths in target neighborhood schools were randomly assigned to either a treatment group ( which received car services and the benefit of a safer neighborhood ) or to a control group ( which received only a safer neighborhood ) .

comparison groups ( n=203 youths ) were selected from similar high - risk neighborhoods by means of census tract data ; comparison groups did not have access to the car program .

interviews were conducted with youth participants at program entry ( baseline ) , program completion ( 2 years later ) , and 1-year after program completion .

a parent or caregiver was interviewed at program entry and completion .

records from schools , police , and courts were collected annually for each youth in the sample as a means of obtaining more objective data .

the total evaluation funding was $1,034,732 .

youths participating in car were significantly less likely than youths in the control group to have used gateway and serious drugs , to have sold drugs , or to have committed violent crimes in the year after the program ended .

car youths were more likely than youths in the control and comparison groups to report attending drug and alcohol abuse programs .

car youths received more positive peer support than controls , associated less frequently with delinquent peers , and were pressured less often by peers to behave in antisocial ways .

car households used more services than control group households , but the majority of car households did not indicate using most of the core services available .

assessment of evaluation	 car is a methodologically rigorous evaluation in both its design and implementation .

the evaluation findings demonstrate the value of the program as a crime and drug prevention initiative .

nij has the primary role of disseminating byrne and vawo discretionary grant program evaluation results of evaluations managed by nij , according to nij , bja , and vawo officials , because nij is responsible for conducting these types of evaluations .

nij is authorized to share the results of its research with federal , state , and local governments .

nij also disseminates information on methodology designs .

nij's practices for disseminating program evaluation results are specified in its guidelines .

according to the guidelines , once nij receives a final evaluation report from the evaluators and the results of peer reviews have been incorporated , nij grant managers are to carefully review the final product and , with their supervisor , recommend to the nij director which program results to disseminate and the methods for dissemination .

before making a recommendation , grant managers and their supervisors are to consider various criteria , including policy implications , the nature of the findings and research methodology , the target audience and their needs , and the cost of various forms of dissemination .

upon receiving the recommendation , the director of nij is to make final decisions about which program evaluation results to disseminate .

nij's director of planning and management said that nij disseminates program evaluation results that are peer reviewed , are deemed successful , and add value to the field .

once the decision has been made to disseminate program evaluation results and methodologies with researchers and practitioners , nij can choose from a variety of publications , including its research in brief ; nij journal – at a glance: recent research findings ; research review ; nij journal – feature article ; and research report .

in addition , nij provides research results on its internet site and at conferences .

for example , using its research in brief publication , nij disseminated impact evaluation results on the byrne children at risk ( car ) program to 7,995 practitioners and researchers , including state and local government and law enforcement officials ; social welfare and juvenile justice professionals ; and criminal justice researchers .

in addition , using the same format , nij stated that it distributed the results of its process evaluation of the byrne comprehensive communities program ( ccp ) to 41,374 various constituents , including local and state criminal and juvenile justice agency administrators , mayors and city managers , leaders of crime prevention organizations , and criminal justice researchers .

nij and other ojp offices and bureaus also disseminated evaluation results during nij's annual conference on criminal justice research and evaluation .

the july 2001 conference was attended by 847 public and nonpublic officials , including criminal justice researchers and evaluation specialists from academic institutions , associations , private organizations , and government agencies ; federal , state , and local law enforcement , court , and corrections officials ; and officials representing various social service , public housing , school , and community organizations .

in addition to nij's own dissemination activities , nij's director of planning and management said that it allows and encourages its evaluation grantees to publish their results of nij - funded research via nongovernmental channels , such as in journals and through presentations at professional conferences .

although nij requires its grantees to provide advance notice if they are publishing their evaluation results , it does not have control over its grantees' ability to publish these results .

nij does , however , require a justice disclaimer that the “findings and conclusions reported are those of the authors and do not necessarily reflect the official position or policies of the u.s. department of justice.” for example , although nij has not yet disseminated the program evaluation results of the three ongoing vawo impact evaluations that we reviewed , one of the evaluation grantees has already issued , on its own internet site , 9 of 20 process evaluation reports on the arrests policies evaluation grant .

the process evaluations were a component of the nij grantee's impact evaluation of the arrest policies program .

because the evaluations were not completed , nij required that the grantee's publication of the process evaluations be identified as a draft report pending final nij review .

as discussed earlier , nij publishes the results of its evaluations in several different publications .

for example , nij used the research in brief format to disseminate evaluation results for two of the five byrne discretionary grant programs comprehensive communities program ( ccp ) and children at risk program ( car ) that were evaluated during fiscal years 1995 through 2001 .

both publications summarize information including each program's evaluation results , methodologies used to conduct the evaluations , information about the implementation of the programs themselves , and services that the programs provided .

ccp's evaluation results were based on a process evaluation .

although a process evaluation does not assess the results of the program being evaluated , it can provide useful information that explains the extent to which a program is operating as intended .

the nij research in brief on the byrne car discretionary grant program provides a summary of issues and findings regarding the impact evaluation .

that summary included findings reported one year after the end of the program , in addition to a summary of the methodology used to conduct the evaluation , the outcomes , the lessons learned , and a major finding from the evaluation .

following are gao's comments on the department of justice's february 13 , 2002 , letter .

1 .

we have amended the text to further clarify that bja administers the byrne program , just as its counterpart , vawo , administers its programs ( see page 4 ) .

however it is important to point out that regardless of the issues raised by ojp , the focus of our work was on the methodological rigor of the evaluations we reviewed , not the purpose and structure of the programs being evaluated .

as discussed in our scope and methodology section , our work focused on program evaluation activities associated with byrne and vawo discretionary grant programs generally and the methodological rigor of impact evaluation studies associated with those programs in particular .

to make our assessment , we relied on nij officials to identify which of the program evaluations of byrne and vawo grant programs were , in fact , impact evaluation studies .

we recognize that there are substantial differences among myriad ojp programs that can make the design and implementation of impact evaluations arduous .

but , that does not change the fact that impact evaluations , regardless of differences in programs , can benefit from stronger up - front attention to better ensure that they provide meaningful and definitive results .

2 .

we disagree with ojp's assessment of our report's treatment of program variation .

as discussed earlier , the scope of our review assessed impact evaluation activities associated with byrne and vawo discretionary grant programs , not the programs themselves .

we examined whether the evaluations that nij staff designated as impact evaluations were designed and implemented with methodological rigor .

in our report we observe that variations in projects funded through vawo programs complicate the design and implementation of impact evaluations .

according to the assistant attorney general , this variation in projects is consistent with the intent of the programs' authorizing legislation .

we recognize that the authorizing legislation provides vawo the flexibility in designing these programs .

in fact , we point out that although such flexibility may make sense from a program perspective , project variation makes it much more difficult to design and implement a definitive impact evaluation .

this poses sizable methodological problems because an aggregate analysis , such as one that might be constructed for an impact evaluation , could mask the differences in effectiveness among individual projects and therefore not result in information about which configurations of projects work and which do not .

3 .

we have amended the results in brief to clarify that peer reviews evaluated proposals .

however , it is important to note that while the peer review committees may have found the two vawo grant applications to be the most superior , this does not necessarily imply that the impact evaluations resulting from these applications were well designed and implemented .

as discussed in our report , the peer review panel for each of the evaluations expressed concerns about the proposals that were submitted , including issues related to site selection and the need for additional impact measures and control variables .

our review of the documents nij made available to us , including evaluators' responses to peer review comments , led to questions about whether the evaluators' proposed methodological designs were sufficient to allow the evaluation results to be generalized and to determine whether the program was working .

4 .

we have amended the background section of the report to add this information ( see page 6 ) .

5 .

as discussed in ojp's comments , we discussed external factors that could account for changes that the rural program evaluation observed in victims' lives and the criminal justice system .

we did so not to critique or endorse activities that the program was or was not funding , but to demonstrate that external factors may influence evaluation findings .

to the extent that such factors are external , the rural program evaluation methodology should account for their existence and attempt to establish controls to minimize their affect on results ( see page 14 ) .

we were not intending to imply that alcohol is a cause for domestic violence , as suggested by the assistant attorney general , but we agree that it could be an exacerbating factor that contributes to violence against women .

6 .

as discussed earlier , we recognize that there are substantive differences in the intent , structure , and design of the various discretionary grant programs managed by ojp and its bureaus and offices .

also , as stated numerous times in our report , we acknowledge not only that impact evaluation can be an inherently difficult and challenging task but also that measuring the impact of these specific byrne and vawo programs can be arduous given that they are operating in an ever changing , complex environment .

we agree that not all evaluation issues that can compromise results are easily resolvable , but we firmly believe that with more up - front attention to design and implementation issues , there is a greater likelihood that nij impact evaluations will provide meaningful results for policymakers .

regarding the representativeness of sites , nij documents that were provided during our review indicated that sites selected during the rural program evaluation were selected on the basis of feasibility , as discussed in our report — specifically , whether the site would be among those participants equipped to conduct an evaluation .

in its comments , ojp stated that the 6 sites selected for the impact evaluation were chosen to maximize geographical and purpose area diversity while focusing on sites with high program priority .

ojp did not provide any additional information that would further indicate that the sites were selected on a representative basis .

ojp did , however , point out that the report does not address how immensely expensive the arrest evaluation would have become if it had included all 130 sites .

we did not address specific evaluation site costs because we do not believe that there is a requisite number of sites needed for any impact evaluation to be considered methodologically rigorous .

regarding ojp's comment about the flexibility given to grantees in implementing vawo grants , our report points out that project variation complicates evaluation design and implementation .

although flexibility may make sense from a program perspective , it makes it difficult to generalize about the impact of the entire program .

7 .

we used the drug court example to illustrate , based on our past work , how comparison groups can be used in evaluation to isolate and minimize external factors that could influence the study results .

we did not , nor would we , suggest that any particular unit of analysis is appropriate for vawo evaluations since the appropriate unit of analysis is dependent upon the specific circumstances of the evaluation .

we were only indicating that since comparison groups were not utilized in the studies , the evaluators were not positioned to demonstrate that change took place as a result of the program .

8 .

we do not dispute that vawo grant programs may provide valuable outputs over the short term .

however , as we have stated previously , the focus of our review was on the methodological rigor of impact evaluations - - those evaluations that are designed to assess the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of the program .

given the methodological issues we found , it is unclear whether nij will be able to discern long - term effects due to the program .

9 .

as stated in our report , we acknowledge not only that impact evaluation can be an inherently difficult and challenging task , but that measuring the impact of byrne and vawo programs can be arduous given the fact that they are operating in an ever changing , complex environment .

we agree that not all evaluation issues that can compromise results are easily resolvable , but we firmly believe that , with more up - front attention to design and implementation issues , there is a greater likelihood that nij evaluations will provide meaningful results for policymakers .

as we said before , absent this up - front attention , questions arise as to whether nij is ( 1 ) positioned to provide the definitive results expected from an impact evaluation and ( 2 ) making sound investments given the millions of dollars spent on these evaluations .

if nij believes that the circumstances of a program are such that it cannot be evaluated successfully ( in relation to impact ) they should not proceed with an impact evaluation .

10 .

we have amended the footnote to state that from fiscal year 1995 through fiscal year 1999 , this program was administered by vawo .

as of fiscal year 2000 , responsibility for the program was shifted to ojp's corrections program office ( see page 5 ) .

11 .

in regard to the number of grants , we have amended the text to reflect that the information nij provided during our review is the number of grantees , not the number of grants ( see pages 25 and 26 ) .

we have also amended our report to reflect some of the information provided in vawo's description of the rural domestic violence program to further capture the essence of the program ( see page 25 ) .

12 .

we disagree .

we believe that separating the cost of the impact and process evaluations is more than a matter of bookkeeping .

even though the work done during the process phase of an evaluation may have implications for the impact evaluation phase of an evaluation , it would seem that , given the complexity of impact evaluations , ojp and nij would want to have in place appropriate controls to provide reasonable assurance that the evaluations are being effectively and efficiently carried out at each phase of the evaluation .

tracking the cost of these evaluation components would also help reduce the risk that ojp's , nij's , and , ultimately , the taxpayer's investment in these impact evaluations is not wasted .

13 .

as discussed earlier , we recognize that there are substantive differences in the intent , structure , and design of the various discretionary grant programs managed by ojp and its bureaus and offices , including those managed by vawo .

our report focuses on the rigor of impact evaluations of grant programs administered by vawo and not on the program's implementing legislations .

although flexibility may make sense from a program perspective , it makes it difficult to develop a well designed and methodologically rigorous evaluation that produces generalizeable results about the impact of the entire program .

14 .

our report does not suggest that other types of evaluations , such as comprehensive process evaluations , are any less useful in providing information about how well a program is operating .

the scope of our review covered impact evaluations of byrne and vawo discretionary grant programs — those designed to assess the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of the program .

in addition to the above , wendy c. simkalo , jared a. hermalin , chan my j. battcher , judy k. pagano , grace a. coleman , and ann h. finley made key contributions to this report .

