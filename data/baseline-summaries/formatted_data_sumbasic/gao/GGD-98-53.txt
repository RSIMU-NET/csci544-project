congressional and agency decisionmakers need evaluative information about whether federal programs are working well or poorly , both to manage programs effectively and to help decide how to allocate limited federal resources .

increased interest in learning the results of federal programs and activities is reflected in government reforms , such as the government performance and results act of 1993 ( gpra or the results act ) , which institutes a governmentwide requirement for agencies to , among other things , report on their results in achieving their agency and program goals .

however , other recent reforms , such as reducing the size and authority of the federal government while maintaining a level of services , has the potential to hinder agencies' ability to obtain this information .

because data on program results are typically more difficult and resource intensive to obtain than data on program activities , limited budget dollars mean that investing in obtaining information on results may compete with spending on program activities .

the proper balance between the two spending priorities is essential , since information on program results can contribute to deciding how to allocate resources to activities to maximize program benefits .

federal agencies are the primary source of evaluation information about their programs .

in past surveys of federal agencies , we found limited ( and diminishing ) resources spent on formal studies of program results , that is , program evaluation .

because evaluation can be vitally important in improving program results , we asked how in a context of limited federal resources and responsibility , can agencies support additional requests for program results information ? .

this report , which we prepared under our basic legislative responsibilities , responds to that question by discussing the current status of and future needs for program evaluation in federal agencies .

because of your interest in improving the quality of information on federal programs , we are addressing this report to you .

our objectives were to identify ( 1 ) the current resources and roles for program evaluation in federal agencies , ( 2 ) the anticipated effects of governmentwide reforms and other initiatives on evaluation of federal programs , and ( 3 ) potential strategies for agencies to respond to the anticipated effects and provide information on program results .

program and policy decisions require a wide array of information that answers various questions .

for example , descriptive information tells how a program operates — what activities are performed , who performs them , and who is reached .

in contrast , evaluative information speaks to how well a program is working — such as whether activities are managed efficiently and effectively , whether they are carried out as intended , and to what extent the program is achieving its intended objectives or results .

there are a variety of methods for obtaining information on program results , such as performance measurement and program evaluation , which reflect differences in how readily one can observe program results .

performance measurement , as defined by the results act , is the ongoing monitoring and reporting of program accomplishments , particularly progress towards preestablished goals .

it tends to focus on regularly collected data on the type and level of program activities ( process ) , the direct products and services delivered by the program ( outputs ) , and the results of those activities ( outcomes ) .

while performance may be defined more broadly as program process , inputs , outputs , or outcomes , results usually refer only to the outcomes of program activities .

for programs that have readily observable results , performance measurement may provide sufficient information to demonstrate program results .

in some programs , however , results are not so readily defined nor measured .

in such cases , program evaluations may be needed , in addition to performance measurement , to examine the extent to which a program is achieving its objectives .

program evaluations are systematic studies conducted periodically to assess how well a program is working .

while they may vary in their focus , these evaluations typically examine a broader range of information on program performance and its context than is feasible in ongoing performance measurement .

where programs aim to produce changes , as a result of program activities , outcome ( or effectiveness ) evaluations assess the extent to which those outcomes or results were achieved , such as whether students increased their understanding of or skill in the material of instruction .

in cases where the program's outcomes are influenced by complex systems or events outside the program's control , impact evaluations use scientific research methods to establish the causal connection between outcomes and program activities , estimate what would have happened in the absence of the program , and thus isolate the program's contribution to those changes .

for example , although outcome measures might show a decline in a welfare program's caseload after the introduction of job placement activities , a systematic impact evaluation would be needed to assess how much of the observed change was due to an improved economy rather than the new program .

in addition , a program evaluation that also systematically examines how a program was implemented can provide important information about why a program did or did not succeed and suggest ways to improve it .

for the purposes of this report , we used the definition of program evaluation that is used in the results act , “an assessment , through objective measurement and systematic analysis , of the manner and extent to which federal programs achieve intended objectives.” we asked about assessments of program results , which could include both the analysis of outcome - oriented program performance measures as well as specially conducted outcome or impact evaluations .

two government initiatives could influence the demand for and the availability and use of program evaluation information .

the results act seeks to promote a focus on program results , by requiring agencies to set program and agency performance goals and to report annually on their progress in achieving them ( beginning with fiscal year 1999 ) .

in addition to encouraging the development of information on program results for activities across the government , the results act recognizes the complementary nature of program evaluation and performance measurement .

it requires agencies to include a schedule for future program evaluations in their strategic plans , the first of which was to be submitted to congress by september 30 , 1997 .

the results act also requires agencies to review their success in achieving their annual performance goals ( which are set forth in their annual performance plans ) and to summarize the findings of program evaluations in their annual program performance reports ( the first of which is due by march 31 , 2000 ) .

the national performance review ( npr ) led by the vice president's office has asked agencies to reexamine their policies , programs , and operations to find and implement ways to improve performance and service to their customers .

both of these initiatives — because of their focus on program results — could be expected to increase the demand for and the availability and use of program evaluation information .

other recent governmentwide initiatives could have potentially conflicting effects .

in several program areas , devolution of program responsibility from the federal level and consolidation of individual federal programs into more comprehensive , multipurpose grant programs has shifted both program management and accountability responsibilities toward the states .

these initiatives may thus make it more difficult for federal agencies to evaluate the results of those programs .

in addition , efforts to reduce the growth of the federal budget have resulted in reductions in both federal staff and program resources in many agencies .

the combination of these initiatives raises a question: in an environment of limited federal resources and responsibility , how can agencies meet the additional needs for program results information ? .

to identify the roles and resources available for federal program evaluation , in 1996 , we conducted a mail survey of offices identified by federal agency officials that were conducting studies of program results or effectiveness in 13 cabinet - level departments and 10 independent executive agencies .

detailed information on program evaluation studies refers to those conducted during fiscal year 1995 ( regardless of when they began or ended ) .

to identify how recent reforms were expected to affect federal evaluation activities and what strategies were available for responding to those changes , we interviewed external evaluation experts and evaluation and other officials at selected federal and state agencies .

in this report , we use the term “agency” to include both cabinet - level departments and independent agencies .

we distributed surveys in 1996 regarding federal evaluation activities within 13 cabinet level departments and 10 independent executive agencies in the federal government .

we excluded the department of defense from our survey of evaluation offices because of the prohibitively large number of offices it identified as conducting assessments of effectiveness .

although we asked agency officials to be inclusive in their initial nominations of offices that conducted evaluations of program results , some offices that conducted evaluations may have been overlooked and excluded from our survey .

however , many offices that were initially identified as having conducted evaluations later reported that they had not done so .

in our survey , we asked each office about the range of its analytic and evaluation activities and about the length , cost , purpose , and other characteristics of the program evaluation studies they conducted during fiscal year 1995 .

 ( see appendix i for more details on the scope and methodology of the survey. ) .

between 1996 and 1997 , we conducted interviews of program evaluation practitioners selected to represent divergent perspectives .

we asked what had been or were expected to be the effects of various government changes and reforms on federally supported and related program evaluation activities and strategies for responding to those effects .

we identified individuals with evaluation knowledge and expertise from a review of the survey responses , the evaluation literature , and our prior work ; they were from an array of federal and state agencies and the academic and consulting communities .

we then judgmentally selected 18 people to interview to reflect ( 1 ) a mix of different program types and diverse amounts of experience with program evaluation and ( 2 ) experience with some of the reforms at the state or federal level .

those selected included nine evaluation officials ( six from offices in federal agencies and three from state legislative audit agencies ) and seven external evaluation experts ( four from private research organizations and three from universities ) .

in addition , we interviewed an omb official and one official from a state executive branch agency , and we also asked the officials from the state legislative audit agencies about their experiences with state performance reporting requirements .

we conducted our review between may 1996 and july 1997 in accordance with generally accepted government auditing standards .

however , we did not independently verify the types of studies conducted , other information reported by our respondents , nor information gained from interviewees .

the resources allocated to conducting systematic assessments of program results ( or evaluation studies ) were small and unevenly distributed across the 23 agencies ( departments and independent agencies ) we surveyed .

we found 81 offices that reported expending resources — funds and staff time — on conducting program effectiveness studies in fiscal year 1995 .

over half of those offices had 18 or fewer full - time equivalent staff ftes , while only a few offices had as many as 300 to 400 ftes .

 ( see figure 1. ) .

moreover , about one - third of the offices reported spending 50 percent or more of their time on evaluation activities ( including development of performance measures and assessments of program effectiveness , compliance , or efficiency ) , since program evaluation was only one of these offices' many responsibilities .

 ( see survey question 9 in appendix i. ) .

two of the 3 largest offices ( over 300 ftes ) spent about 10 percent of their staff time on program evaluation activities .

thus , the estimated staff and budget resources that the 81 offices actually devoted to evaluation activities totaled 669 ftes and at least $194 million across the 23 agencies surveyed .

in addition , most ( 61 of 81 ) offices reported also conducting management analysis activities ; the most frequent activities were conducting management studies , developing strategic plans , and describing program implementation .

of those offices that could estimate their staff time , about half reported spending less than 25 percent of their time on management analysis .

similarly , many offices reported conducting policy planning and analysis , but most of them reported spending less than 25 percent of their time on it .

thus , a majority of the offices ( 45 of the 81 identified ) conducted few evaluation studies ( 5 or less in fiscal year 1995 ) , while 16 offices — representing 7 agencies — accounted for two - thirds of the 928 studies conducted .

 ( see table 1. ) .

finally , 6 of the 23 agencies we surveyed did not report any offices conducting evaluation studies in fiscal year 1995 .

a few of these agencies indicated that they analyzed program accomplishments or outputs or conducted management reviews to assess their programs' performance but did not conduct an evaluation study per se .

some of the 6 agencies also reported conducting other forms of program reviews that focused on assessing program compliance or efficiency rather than program results .

offices conducting program evaluations were located at various levels of federal agencies .

a few of the 81 offices were located in the central policy or administrative office at the highest level of the organization ( 5 percent ) or with the inspector generals ( 5 percent ) ; many more were located in administrative offices at a major subdivision level ( 43 percent ) or in program offices or technical or analytic offices supporting program offices ( 30 and 16 percent , respectively ) .

 ( see table 2. ) .

four of the 23 agencies surveyed had offices at all 3 levels ( agency , division , and program ) , and over half the agencies ( 14 of 23 ) had conducted evaluations at the program level .

the 16 offices conducting 20 or more studies were more likely to be centralized at the agency or division level than at the program level .

a diverse array of evaluation studies were described in the surveys .

just over half of the studies for which we have such information were conducted in - house ( 51 percent ) , and 27 percent lasted under 6 months .

but the studies that were contracted out tended to be larger investments — almost two - thirds of them took over a year to complete , and over half cost between $100,000 and $500,000 .

moreover , almost a third of all the studies lasted more than 2 years , reflecting some long - term evaluations .

 ( see table 3. ) .

for example , a study of the impact of a medical treatment program , which used an experimental design with a complex set of medicare program and clinical data from thousands of patients on numerous outcomes ( for both patients and program costs ) , took over 2 years and cost over $1 million to complete .

many of the 1995 studies reportedly used relatively simple designs or research methods , and many relied on existing program data .

the two most commonly reported study designs were judgmental assessments ( 18 percent ) as well as experimental designs employing random assignment ( 14 percent ) .

 ( see table 4 for a list of designs ranging from the most to least amount of control over the study conditions. ) .

many of the studies ( over 70 of the 129 ) that used experimental designs were evaluations of state demonstration programs , which were required by law to use such methods , and were conducted out of one office .

experimental designs and designs using statistical controls are used to identify a program's net impact on its objectives where external factors are also known to affect its outcome .

however , without knowing the circumstances of many of the programs being evaluated , it is impossible for us to determine the adequacy of the designs used to assess program effectiveness .

at least 40 percent of the studies employed existing program records in their evaluations , while about one - quarter employed special surveys or other ad hoc data - collection methods specially designed for the studies .

just under half ( 40 percent ) of the studies used data from program administrative records that were produced and reported at the federal level ; almost a third ( 28 percent ) used data from routinely produced , but not typically reported , program records ; 5 percent of the studies used data from administrative records of other federal agencies ; and 14 percent used administrative records from state programs .

some studies may have used many types of data sources , which would suggest a heavy reliance on administrative and other program - related data .

 ( see table 5. ) .

the primary reported purpose of the studies was to evaluate ongoing programs , either on an office's own initiative or at the request of top agency officials .

in the survey , most officials conducting evaluations reported having a formal and an ad hoc planning process for deciding what evaluation work they would do .

many criteria were indicated as being used to select which studies to do ( such as a program office request , congressional interest , or continuation or follow - up of past work ) , but the criterion most often cited was the interest of high - level agency officials in the program or subject area .

moreover , about one - fourth of the studies were requested by top agency officials .

about one - fourth of the studies were indicated to be self - initiated .

most offices were not conducting studies for the congress or as the result of legislative mandates ; only 17 percent of the studies were reported to have been requested in those ways .

 ( see table 6. ) .

for those offices reporting that they conducted studies , about half of the 570 studies for which we have information evaluated ongoing programs.ongoing programs of all sizes were evaluated , ranging in funding from less than $10 million to over $1 billion .

about one - third of these studies evaluated demonstration programs and many of them cost less than $10 million .

in contrast , few reported evaluations of new programs and many of these new programs reportedly were small ( with funding under $10 million ) .

program evaluation was reported to be used more often for general program improvement than for direct congressional oversight .

their primary uses most often were said to be to improve program performance ( 88 percent ) , assess program effectiveness ( 86 percent ) , increase general knowledge about the program ( 62 percent ) , and guide resource allocation decisions within the program ( 56 percent ) .

 ( see table 7. ) .

accordingly , these offices overwhelmingly ( over three - fourths of respondents ) reported program managers and higher - level agency officials as the primary audience of their studies .

 ( see table 8. ) .

about one - third of the offices reported support for budget requests as a primary use and one - third reported congressional audiences were primary users for their studies .

fewer respondents ( 20 percent ) reported program reauthorization as a primary use of the study results .

 ( see tables 7 and 8. ) .

program evaluation was not the primary responsibility for most of these offices and the offices often reported “seldom , if ever” performing the program evaluation roles we asked about .

the role most likely to be characterized as ‘most often performed' was conducting studies of programs administered elsewhere in their agency .

 ( see table 9. ) .

about one - half of those who responded reported “sometimes” providing technical or design assistance to others or conducting joint studies , while a few offices saw their role as training others in research or evaluation methods .

one office dealing with an evaluation mandate conducted work sessions with state and local program managers and evaluators as well as provided training to enhance state evaluation capabilities .

two - thirds of the offices seldom , if ever , designed evaluations conducted by other offices or agencies , trained others in research or evaluation methods , or approved plans for studies by others .

some of our interviewees thought that recent governmentwide reforms would increase interest in learning the results of federal programs and policies but would also complicate the task of obtaining that information .

devolution of federal program responsibility in the welfare and health care systems has increased interest in evaluation because the reforms are major and largely untested .

however , in part because programs devolved to the states are expected to operate quite diversely across the country , some evaluation officials noted that evaluating the effects of these reforms was expected to be more difficult .

in addition , federal budget reductions over the past few years were said by some not only to have reduced the level of federal evaluation activity but also to have diminished agency technical capacity through the loss of some of their most experienced staff .

because implementation of the results act's performance reporting requirements is not very far along ( the first annual reports on program performance are not due until march 2000 ) , several of our interviewees thought it was too early to estimate the effect of the results act .

some hoped the act would increase the demand for results information and expand the role of data and analysis in decisionmaking .

one interviewee thought it would improve the focus of the evaluations they now conduct .

a few evaluation officials were concerned that a large investment would be required to produce valid and reliable outcome ( rather than process ) data .

a few also noted that resources for obtaining data on a greatly expanded number of program areas would compete for funds used for more in - depth evaluations of program impact .

other evaluators noted that changes in the unit of analysis for performance reporting from the program level to budget account or organization might make classic program evaluation models obsolete .

as we previously reported , the federal program officials who have already begun implementing performance measurement appeared to have an unusual degree of program evaluation support and found it quite helpful in addressing the analytic challenges of identifying program goals , developing measures , and collecting data .

many of these program officials said they could have used more of such assistance ; but , when asked why they were not able to get the help they needed , the most common response was that it was hard to know in advance that evaluation expertise would be needed .

in addition to using program evaluation techniques to clarify program goals and develop reliable measures , several of these program officials saw the need for impact evaluations to supplement their performance data .

their programs typically consisted of efforts to influence highly complex systems or events outside government control , where it is difficult to attribute a causal connection between their program and its desired outcomes .

thus , without an impact evaluation or similar effort to separate the effects of their programs from those of other external events or factors , program officials from the previous study recognized that simple examination of outcome measures may not accurately reflect their programs' performance .

some states' experiences with performance measurement suggested that performance measurement will take time to implement , and the federal experience suggests that it will not supplant the need for effectiveness evaluations .

two state officials described a multiyear process to develop valid and reliable measures of program performance across the state government .

while performance measures were seen as useful for program management , some state agency and legislative staff also saw a continuing need for evaluations to assess policy impact or address problems of special interest or “big - picture” concerns , such as whether a government program should be continued or privatized .

npr was seen by several of those we interviewed as not having much of an effect on efforts to evaluate the results of their programs beyond increasing the use of customer surveys .

this may have been because it was seen as primarily concerned with internal government operations , or because , as one agency official reported , its effect was most noticeable in only a few areas: regulatory programs and other intergovernmental partnerships .

however , one agency official said that npr had a big impact on reorienting their work toward facilitating program improvement , while two others felt that it reaffirmed changes they had already begun .

given constraints on federal budgets , some officials we interviewed in general did not expect federal evaluation resources to rise to meet demand , so they described efforts to leverage and prioritize available resources .

while an evaluation official reported supplementing his evaluation teams with consultants , concern was also expressed that staff reductions in their unit had left the technical expertise too weakened to competently oversee consultants' work .

another evaluation official explained that they responded to the increasing demand for information by narrowing the focus and scope of their studies to include only issues with major budget implications or direct implications for agency action .

both a state official and two external evaluation experts felt that states grappling with new program responsibilities would have difficulty evaluating them as well , so that continued federal investment would be needed .

a federal official , however , noted that private foundations could fund the complex rigorous studies needed to answer causal questions about program results .

some of the evaluators we interviewed expected that fewer impact studies would be done .

some expected that the range of their work may broaden to rely on less rigorous methods and include alternatives such as monitoring program performance and customer satisfaction .

from our interviews , we learned that a few agencies have devolved responsibility for evaluations to the program offices , which may have more interest in program improvement .

another agency reported that it had built evaluation into its routine program review system , which provides continuous information on the success of the program and its outcomes , noting that it thereby reduced the need for special evaluation studies .

one evaluation official reported that by having redefined evaluation as part of program management , program evaluation became more acceptable in his agency because it no longer appeared to be overhead .

a few agencies reported that they were adapting the elements of their existing program information systems to yield information on program results .

but in other agencies , evaluation officials and external experts thought that their systems were primarily focused on program process , rather than results .

the evaluation official said that structural changes to , and a major investment in , their data systems will be required to provide valid and meaningful data on results .

as program responsibility shifts to state and local entities , evaluation officials and others we interviewed described the need for study designs that can handle greater contextual complexity , new ways to measure outcomes , and the need to build partnerships with the programs' stakeholders .

one of the officials saw classical experimental research designs as no longer feasible in programs , which , due to increased state flexibility in how to deliver services , no longer represented a discrete national program or were unlikely to employ rigorous evaluation techniques that entailed random assignment of particular program services to individuals .

others noted the need to develop evaluation designs that could reflect the multiple levels on which programs operate and the organizational partnerships involved .

to address some of these complexities , federal offices with related program interests have formed task groups to attempt to integrate their research agendas on the effects of major changes in the health and welfare systems .

similarly , a few federal evaluation officials reported an interest in consulting with their colleagues in other federal offices to share approaches for tackling the common analytic problems they faced .

in other strategies , federal evaluation officials described existing or planned efforts to change the roles they and other program stakeholders played in conducting evaluations .

one agency has arranged for the national academy of sciences to work with state program officials and the professional communities involved to help build a prototype performance measurement system for federal assistance to state programs .

one evaluation office expects to shift its role toward providing more technical assistance to local evaluators and synthesizing their studies' results .

another federal office has delegated some evaluation responsibility to the field while it synthesizes the results to answer higher level policy questions , such as which types of approaches work best .

the results act recognizes and encourages the complementary nature of program evaluations and performance measures by asking agencies to provide a summary of program evaluation findings along with performance measurement results in their annual performance reports .

one federal evaluation official said his agency had efforts under way to “align” program evaluation and performance measurement through , for example , planning evaluations so that they will provide the performance data needed .

but , the official also expressed concern about how to integrate the two types of information .

officials from states that had already begun performance measurement and monitoring said they would like to see the federal government provide more leadership by ( 1 ) providing a catalog of performance measures available for use in various program areas and ( 2 ) funding and designing impact evaluations to supplement their performance information .

seeking to improve government performance and public confidence in government , the results act has instituted new requirements for federal agencies to report on their results at the same time that other management reforms may complicate the task of obtaining such information .

comparison of current federal program evaluation resources with the anticipated challenges leads us to several conclusions .

first , federal agencies' evaluation resources have important roles to play in responding to increased demand for information on program results , but — as currently configured and deployed — they are likely to be challenged to meet these future roles .

it is implausible to expect that , by simply conducting more program evaluation studies themselves , these offices can produce data on results across all activities of the federal government .

moreover , some agencies reported that they had reduced their evaluation resources to the point that the remaining staff feel unable to meet their current responsibilities .

lastly , the devolution of some program responsibilities to state and local governments has increased the complexity of the programs they are being asked to evaluate , creating new challenges .

second , in the future , carefully targeting and reshaping the use of federal evaluation resources and leveraging federal and nonfederal resources show promise for addressing the most important questions about program results .

in particular , federal evaluators could assist program managers to develop valid and reliable performance reporting by sharing their expertise through consultation and training .

early agency efforts to meet the results act's requirements found program evaluation expertise helpful in managing the numerous analytical challenges involved , such as clarifying program goals and objectives , developing measures of program outcomes , and collecting and analyzing data .

in addition , because performance measures will likely leave some gaps in needed information , strategic planning for future evaluations might strive to fill those gaps by focusing on those questions judged to have the most policy importance .

in many programs , performance measures alone are not sufficient to establish program impact or the reasons for observed performance .

program evaluations can also serve as valuable supplements to program performance reporting by addressing policy questions that extend beyond or across program borders , such as the comparative advantage of one policy alternative to another .

finally , without coordination , it is unlikely that the increasingly diverse activities involved in evaluating an agency's programs will efficiently supplement each other to meet both program improvement and policymaking information needs .

as some agencies devolve some of the evaluations they conducted in the past to program staff or state and local evaluators , they run the risk that , due to differences in evaluation resources and questions , data from several studies conducted independently may not likely be readily aggregated .

thus , in order for such devolution of evaluation responsibility to better provide an overall picture of a national program , those evaluations would have to be coordinated in advance .

similarly , as federal agencies increasingly face common analytic problems , they could probably benefit from cross - agency discussion and collaboration on approaches to those problems .

the director of omb commented on a draft of this report and generally agreed with our conclusions .

omb noted that other countries are experiencing public sector reforms that include a focus on results and increasing interest in program evaluation .

omb also provided technical comments that we have incorporated as appropriate throughout the text .

omb's comments are reprinted in appendix ii .

we are sending copies of this report to the chair and ranking minority member of the house committee on government reform and oversight , the director of omb , and other interested parties .

we will also make copies available to others on request .

please contact me or stephanie shipman , assistant director at ( 202 ) 512-7997 if you or your staff have any questions .

major contributors to this report are listed in appendix iii .

the 23 federal executive agencies ( 13 cabinet - level departments and 10 independent agencies ) that we surveyed are listed as follows .

these represent 23 of the 24 executive agencies ( we excluded the department of defense ) covered by the chief financial officer's act .

the 24 represent about 97 percent of the executive branch's full - time staff and cover over 99 percent of the federal government's outlay for fiscal year 1996 .

to identify the roles and resources expended on federal program evaluation , we surveyed all offices ( or units ) in the 23 executive branch departments and independent agencies that we identified as conducting evaluation in fiscal year 1995 .

we defined evaluation as systematic analysis using objective measures to assess the results or the effects of federal programs , policies , or activities .

to identify these evaluation offices , we ( 1 ) began with the list of evaluation offices that we surveyed in 1984 ( 2 ) added offices based on a review of office titles implying analytical responsibilities and discussions with experts knowledgeable about evaluation studies , and ( 3 ) talked with our liaison staff and other officials in the federal departments and agencies to ensure broad yet appropriate survey coverage .

in some instances , the survey was distributed to offices throughout an agency by agency officials , while in other instances we sent the survey directly to named evaluation officials .

we attempted to survey as many evaluation offices as possible ; however , in some cases , we may not have been told about or directed to all such offices .

therefore , we cannot assume that we have identified all offices that conducted program evaluation studies in fiscal year 1995 .

overall , we received about 160 responses , of which 81 were from offices that conducted such studies .

the survey was directed toward results - oriented evaluation studies , such as formal impact studies , assessments of program results , and syntheses or reviews of evaluation studies .

we sought to exclude studies that focused solely on assessing client needs , describing program operations or implementation , or assessing fraud , compliance , or efficiency .

however , we allowed the individual offices to ( 1 ) define “program” since a federal program could be tied to a single budget account , represent a combination of several programs , or involve several state programs and ( 2 ) determine whether or not they did this type of study and , if not , they could exempt themselves from completing the survey .

we did not verify the accuracy of the responses provided by evaluation units .

we also had some information on fiscal year 1996 activities but did not report those results since they were comparable to the fiscal year 1995 results .

some respondents were unable to complete different parts of the survey .

about one - third of the respondents did not report either the office's budget , its number of full - time equivalent staff ( fte ) , cost information about studies , or the sources of data used in the studies .

for some questions , respondents were asked to answer in terms of the number of studies conducted , and we used the total number of studies indicated by all respondents to the question as the denominator when computing percents .

however , when the level of nonresponse to individual survey questions was above 20 percent or was unclear due to incomplete information on how many studies had been reported on , we used the full complement of 928 studies to provide a conservative estimate .

the questions for which we reported results are reproduced on the following pages .

committee on governmental affairs , united states senate .

“government performance and results act of 1993.” report no .

103-58 , june 16 , 1993 .

evaluation practice .

“past , present , future assessments of the field of evaluation.” entire issue .

m.f .

smith , ed. , vol .

15 , #3 , oct. 1994 .

martin , margaret e. , and miron l. straf ( eds. ) .

principles and practices for a federal statistical agency .

washington , d.c.: national academy press , 1992 .

national performance review .

“mission - driven , results - oriented budgeting.” accompanying report of the national performance review of the office of the vice president , sept. 1993 .

new directions for program evaluation .

“evaluation in the federal government: changes , trends , and opportunities.” entire issue .

c.g .

wye and r. sonnichsen , eds .

#55 , fall 1992 .

new directions for program evaluation .

“progress and future directions in evaluation: perspectives on theory , practice , and methods.” entire issue .

debra rog and deborah fournier , eds .

#76 , winter 1997 .

office of evaluation and inspections .

practical evaluation for public managers: getting the information you need .

washington , d.c.: office of inspector general , department of health and human services , 1994 .

public law 103-62 , aug. 3 , 1993 , “government performance and results act of 1993.” wargo , michael j .

“the impact of federal government reinvention on federal evaluation activity.” evaluation practice , 16 ( 3 ) ( 1995 ) , pp .

227-237 .

the results act: an evaluator's guide to assessing agency annual performance plans ( gao / ggd - 10.1.19 , mar .

1998 ) .

balancing flexibility and accountability: grant program design in education and other areas ( gao / t - ggd / hehs - 98-94 , feb. 11 , 1998 ) .

the government performance and results act: 1997 governmentwide implementation will be uneven ( gao / ggd - 97-109 , june 2 , 1997 ) .

managing for results: analytic challenges in measuring performance ( gao / hehs / ggd - 97-138 , may 30 , 1997 ) .

block grants: issues in designing accountability provisions ( gao / aimd - 95-226 , sept. 1995 ) .

program evaluation: improving the flow of information to the congress ( gao / pemd - 95-1 , jan. 30 , 1995 ) .

management reform: implementation of the national performance review's recommendations ( gao / ogc - 95-1 , dec. 5 , 1994 ) .

public health service: evaluation set - aside has not realized its potential to inform the congress ( gao / pemd - 93-13 , apr .

1993 ) .

program evaluation issues ( gao / ocg - 93-6tr , dec. 1992 ) .

“improving program evaluation in the executive branch.” a discussion paper by the program evaluation and methodology division ( gao / pemd - 90-19 , may 1990 ) .

program evaluation issues ( gao / ocg - 89-8tr , nov. 1988 ) .

federal evaluation: fewer units , reduced resources , different studies from 1980 ( gao / pemd - 87-9 , jan. 23 , 1987 ) .

 ( 966704 / 973810 ) the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 37050 washington , dc 20013 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 202 ) 512-6061 , or tdd ( 202 ) 512-2537 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

