each year , federal agencies hire thousands of contractors to help accomplish their missions .

in fiscal year 2007 alone , federal agencies worked with over 160,000 contractors , obligating over $456 billion .

contractors are involved in a broad array of activities , from basic functions , such as landscaping and janitorial services , to more complex functions , like acquisition support and security services .

these contractors often employ subcontractors to help them meet contract requirements .

this reliance on contractors makes it critical that federal agencies have the information necessary to properly evaluate a contractor's prior history of performance and better inform agencies' contract award decisions .

to facilitate the sharing of such information , the office of federal procurement policy ( ofpp ) created the past performance information retrieval system ( ppirs ) — a system intended to be a repository of performance information on federal contractors .

however , more than 5 years after the implementation of ppirs in july 2002 , questions have been raised about how well federal agencies are documenting and sharing information on contractor past performance .

specifically , you have noted that agencies were renewing or awarding contracts to contractors with questionable performance records .

consequently , you asked us to review several issues related to the use of past performance information .

in response to your request , we ( 1 ) assessed agencies' use of information on contractors' past performances in awarding contracts ; ( 2 ) identified challenges that hinder systematic , governmentwide sharing of past performance information ; and ( 3 ) described efforts under way or planned to improve the sharing of information on contractor performance .

to conduct our work , we reviewed and analyzed the federal acquisition regulation ( far ) and ofpp guidance on the use of past performance information .

we also reviewed guidance from the department of defense ( dod ) , department of energy ( doe ) , department of homeland security ( dhs ) , national aeronautics and space administration ( nasa ) , and the general services administration ( gsa ) .

we discussed this guidance and a broad range of issues related to how agencies use past performance information with 121 contracting officials at 11 buying offices that represented a range of acquisition activities .

further , we selected and analyzed 62 contract files from fiscal years 2007 and 2008 — focusing on how buying offices considered past performance information relative to other evaluation factors .

we selected contracts that represent a range of products and services , types of contracts , contract dollar values , and contractors across the government , but our findings cannot be generalized to all federal contracts .

we also analyzed data in ppirs , past performance information that agencies feed into ppirs , the evaluation factors and rating scales used in the system , and met with agency officials who administer the system .

we conducted this performance audit from february 2008 to february 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

see appendix i for a more detailed discussion of our scope and methodology .

the federal government relies heavily on contractors to provide a range of goods and services .

in fiscal year 2007 , about 160,000 contractors provided support to federal agencies .

a large portion of these contractors was concentrated in five agencies: dod , dhs , doe , nasa , and gsa .

among these five agencies , dod accounts for 72 percent of all contract obligations across about 77,000 contractors in fiscal year 2007 ( see table 1 ) .

these five agencies often rely on the same contractors .

table 2 shows the number and percentage of contractors dhs , nasa , doe , and gsa had in common with dod in fiscal year 2007 .

the far requires agencies to consider past performance information as an evaluation factor in certain negotiated competitive procurements — along with other evaluation factors such as price , management capability , and technical excellence .

contractor past performance information may include the contractor's record of conforming to contract requirements and to standards of good workmanship ; record of forecasting and controlling costs ; adherence to contract schedules ; and history of reasonable and cooperative behavior and commitment to customer satisfaction .

although the far requires officials in selecting contractors to consider past performance as an evaluation factor in certain negotiated procurements , agencies have broad discretion in deciding its importance relative to other factors in the evaluation scheme .

agencies determine which of the contractor's past contracts are similar to the contract to be awarded in terms of size , scope , complexity , or contract type and the relative importance of past performance .

for procurements with clearly defined requirements and minimal risk of unsuccessful contract performance , cost or price may play a more important role than past performance in selecting contractors .

for procurements with less clearly defined requirements and a higher risk of unsuccessful contract performance , it may be in the government's best interest to consider past performance , technical capability , and other factors as more important than cost or price .

the far requires that solicitations disclose the evaluation factors that will be used in selecting a contractor and their relative importance .

in evaluating past performance information , agencies must consider , among other things , the 1 ) currency and relevancy , 2 ) source and context , and 3 ) general trends in the contractor's performance .

the solicitation must also describe how offerors with no performance history will be evaluated .

once a contract is awarded , the government should monitor a contractor's performance throughout the performance period .

surveillance includes oversight of a contractor's work to provide assurance that the contractor is providing timely and quality goods or services and to help mitigate any contractor performance problems .

an agency's monitoring of a contractor's performance may serve as a basis for past performance evaluations .

the far requires agencies to prepare an evaluation of contractor performance for each contract that exceeds the simplified acquisition threshold at the time the work is completed and gives agencies discretion to include interim evaluations for contracts with a performance period exceeding one year .

the dod has generally higher thresholds based on business sectors .

a number of systems across the government are used to capture contractor performance information , which is eventually passed on to ppirs .

dod maintains three systems for its military departments and agencies — architect - engineer contract administration support system ( acass ) , construction contractor appraisal support system ( ccass ) , and contractor performance assessment reporting system ( cpars ) .

nasa has its own system , the past performance database ( ppdb ) .

dhs and doe are transitioning to using dod's cpars .

other civilian departments use the contractor performance system ( cps ) managed by the national institutes of health .

effective july 1 , 2002 , all federal contractor past performance information currently captured through these disparate systems was to be centrally available for use by all federal agency contracting officials through ppirs — a web - enabled , governmentwide application for consolidating federal contractor performance information .

since its implementation , concerns have been raised about the completeness of the information in ppirs .

in february 2008 , a dod inspector general report noted that the information in cpars , which feeds information into ppirs , was incomplete and questioned whether or not acquisition officials had access to all the information they needed to make business decisions .

specifically , in reviewing performance assessment reports in cpars , the inspector general reported that for dod contracts valued at more than $5 million , 82 percent did not contain detailed narratives sufficient to establish that ratings were credible and justifiable ; 68 percent had performance reports that were overdue ; and 39 percent were registered more than a year late .

in addition , the report identified material internal control weaknesses in the air force , army , and navy procedures for documenting and reporting contractor performance information .

agencies considered past performance information in evaluating contractors for the contract solicitations we reviewed , but many of the officials we spoke with noted that past performance rarely , if ever , was the deciding factor in their contract award decisions .

their reluctance to base award decisions on past performance was due , in part , to their skepticism about the comprehensiveness and reliability of past performance information and difficulty assessing its relevance to specific acquisitions .

for the 62 contract solicitations we reviewed , the ranking of past performance as an evaluation factor relative to other non - cost factors varied .

the company's technical approach was the non - cost factor considered most important for most solicitations .

past performance as an evaluation factor was ranked first in order of importance in about 38 percent of solicitations ( appendix i provides more details on the methodology for selecting and reviewing contract solicitations ) .

contracting officials who viewed past performance as an important evaluation factor noted that basing contract award decisions , in part , on past performance encourages companies to achieve better acquisition outcomes over the long term .

for example , according to officials at one air force location , an incumbent contractor was not awarded a follow - on contract worth over $1 billion primarily because of poor performance on the prior contract .

as a result , the contractor implemented several management and procedural changes to improve its performance on future contracts .

despite the fact that past performance was an evaluation factor in all the solicitations we reviewed , over 60 percent of the contracting officers we talked with stated that past performance is rarely or never a deciding factor in selecting a contractor .

many contracting officers stated they preferred to rely on other more objective factors such as technical approach or price .

officials cited several reasons for their reluctance to rely more on past performance in making award decisions including difficulty obtaining objective and candid past performance information .

for example , over half of the contracting managers we met with noted that officials who are assessing a contractor's performance have difficulty separating problems caused by the contractor from those caused by the government , such as changing or poorly defined government requirements .

fear of damaging contractor relations may also influence assessments of contractor performance , particularly in areas where there are a limited number of contractors that can provide a particular good or service .

some contracting officials told us there may also be a tendency to “water down” assessments if they perceive a contractor may contest a negative rating .

contracting officials also cited other challenges for not relying more on past performance information including 1 ) difficulty assessing relevance to the specific acquisition or offerors with no relevant past performance information , 2 ) lack of documented examples of past performance , and 3 ) lack of adequate time to identify , obtain , and analyze past performance information .

contracting officials often rely on multiple sources of past performance information .

most officials told us they found information from the prospective contractor's prior government or industry customer references — gathered through interviews or questionnaires — as the most useful source of past performance information .

moreover , several contracting officials noted that they use questionnaires to obtain past performance information on major subcontractors .

officials noted , however , that questionnaires are time - consuming and the performance information collected through them is not shared governmentwide .

other sources of past performance information include informal contacts such as from other contracting officers who have dealt with the contractor in the past .

most contracting officials we spoke with also used ppirs , but cited the absence of information in ppirs as one reason for typically relying on other sources along with challenges in ascertaining information that was relevant to the specific acquisitions .

several contracting officials stated a governmentwide system like ppirs , if populated , could reduce the time and effort to collect past performance information for use in selecting contractors .

regardless of the source used , contracting officials agreed that for past performance information to be meaningful in contract award decisions , it must be documented , relevant , and reliable .

our review of ppirs data for fiscal years 2006 and 2007 found relatively little past performance information available for sharing and potential use in contract award decisions .

one reason is that agencies are not documenting contractor performance information that feeds into ppirs to include , in some cases , contract actions involving task or delivery orders placed against gsa's mas .

other information that could provide key insights into a contractor's performance , such as information on contract terminations for default and a prime contractor's management of subcontractors , was also not systematically documented .

contracting managers also lack tools and metrics to monitor the completeness of past performance data in the systems agencies use to record past performance information .

further , the lack of standardized evaluation factors and rating scales in the systems that collect past performance information has limited the system's usefulness in providing an aggregate level picture of how contractors are performing .

finally , lack of central oversight of ppirs has undermined efforts to capture adequate past performance information .

the far requires agencies to prepare an evaluation of contractor performance for each contract that exceeds the simplified acquisition threshold ( $100,000 in most cases ) when the contract work is completed .

while the far definition of a contract can be read to include orders placed against gsa's multiple award schedule ( mas ) , the far does not specifically state whether this requirement applies to contracts or task or delivery order contracts awarded by another agency .

while dod and many agencies we reviewed have issued supplemental guidance reiterating the far requirement to evaluate and document contractor performance — information that ultimately should be fed into ppirs — the agencies generally did not comply with the requirement .

we estimated that the number of contracts that required a performance assessment in fiscal year 2007 for agencies we reviewed would have totaled about 23,000 .

for the same period , we found about 7,000 assessments in ppirs — about 31 percent of those contracts requiring an assessment ( see table 3 ) .

about 75 percent of all past performance reports in ppirs were from dod , with the air force accounting for the highest percent of completed assessments ; however , there were relatively few for some military services — a finding consistent with the dod ig's february 2008 report .

for the civilian agencies we reviewed , there were relatively few performance reports in ppirs compared to the number we estimated .

for example , for fiscal year 2007 , an estimated 13 percent of dhs contracts that would potentially require a performance assessment were documented in ppirs .

for specific types of contract actions , such as task and delivery orders placed against gsa's mas , we found little contractor performance information in ppirs .

between fiscal years 1998 and 2008 , purchases made against mas have grown from over $7 billion to $37 billion .

similarly , the number of mas contracts has increased from 5,200 in the mid - 1990s to 18,000 in fiscal year 2008 .

despite this significant growth , the number of performance reports in ppirs for orders placed against mas contracts is minimal .

for example , about 5 percent of the dhs orders and none of nasa's were assessed in fiscal year 2007 .

contracting officials we spoke with confirmed that these assessments were generally not being done ; some told us that they believed gsa was collecting this information .

according to gsa officials , however , agencies are responsible for documenting and reporting mas contractor performance , and gsa does not generally request feedback on performance for mas contractors .

without this information , gsa is in no position to know how a contractor is performing when deciding whether or not to continue doing business with that contractor .

currently , there is no governmentwide requirement for agencies to document in ppirs when a contract has been terminated because the contractor defaulted on the terms of the contract .

consequently , contracting officers may not have access to all information on a contractor's past performance that could factor into a contract award decision .

the recent awarding of contracts to defaulted contractors highlights the need for information on contract terminations when making contracting decisions .

for example , a $280-million army munitions contract was awarded to a contractor that had previously been terminated for default on several different contracts .

the contracting officer told us that this information , if available , would have factored into the contract award decision .

subsequently , this same contractor defaulted under that contract .

similarly , an october 2008 report issued by the office of the special inspector general for iraq reconstruction documented that at least eight contractors that had one or more of their projects terminated for default received new contracts and purchase orders .

as part of this audit , the office examined whether the agencies had evaluated the contractors' prior performance before awarding contracts and whether they had considered suspending or debarring the poor performing contractors .

although the report found that the awards to defaulted contractors were within the authority provided by the far , it raised questions about the degree to which the contractors' prior performance was considered .

in june 2008 , the far council opened a case to address termination for default reporting .

in addition , dod issued policy in july 2008 on the need for departmentwide centralized knowledge of all contracts that have been terminated regardless of dollar amount .

at the subcontractor level , apart from evaluating a prime contractor's management of its subcontractors , historically , the federal government has had limited visibility into subcontractor performance despite the increased use in subcontractors .

in january 2008 , we reported that total subcontract awards from dod contracts had increased by 27 percent over a 4-year period — from $86.5 billion in fiscal year 2002 to $109.5 billion in fiscal year 2006 .

as we reported , federal contractors must manage contract performance , including planning and administering subcontracts as necessary , to ensure the lowest overall cost and minimize technical risk to the government .

the far provides that the agency's past performance evaluation should take into account past performance information regarding a prospective contractor's subcontractors that will perform major or critical aspects of a requirement when such information is relevant to an acquisition .

agency contracting officials informed us that they do not assess the performance of these subcontractors .

rather , if they collect any information , it is in their assessments of the prime contractor's subcontract management .

however , not all collection systems used by agencies allow for systematic capturing of subcontract management information , if it was applicable in a procurement .

dod's cpars system has a separate rating factor for subcontract management for systems contracts whereas systems used by nasa and other civilian agencies do not have a separate factor .

dod guidance states assessments must not be done on subcontractors , but cpars allows the assessing official to address the prime contractor's ability to manage and coordinate subcontractor efforts .

beyond this information on subcontractors , no additional information is routinely collected on subcontractors .

in addition , the far was recently revised to explain that information on contractor ethics can be considered past performance information .

the far now states that a contractor's history of reasonable and cooperative behavior and commitment to customer satisfaction may be considered part of a contractor's past performance .

this type of data is not currently being systematically captured and documented for use in contract award decisions .

several contracting officials acknowledged that documenting contractor performance was generally not a priority , and less than half of the contracting managers we talked with tracked performance assessment completeness .

some agency officials we spoke with said that a lack of readily accessible system tools and metrics on completeness has made it difficult to manage the assessment process .

cpars and cps — assessment reporting systems used by dod and dhs — do not have readily accessible system tools and metrics on completeness for managers to track compliance .

according to officials who manage cpars , a team is developing requirements for system tools and metrics but has been challenged to develop useful measures because of a lack of complete and reliable contract information from fpds .

ofpp officials similarly acknowledged there was a lack of tools and metrics for agency contracting officials to monitor and manage the process of documenting contractor performance .

for example , managers currently do not have the ability to readily identify contracts that require an assessment , how many are due and past due , and who is responsible for completing assessments .

according to these officials , holding managers accountable for outcomes without adequate tools to manage the assessment process would be difficult .

however , a few contracting managers we spoke with placed a high priority on documenting contractor performance , noting that doing so tended to improve communication with contractors and encourage good performance .

one air force commander issued guidance reiterating that cpars is a key component in selecting contractors ; that commander personally oversees the performance reporting system , requiring a meeting with responsible officials when a cpars report is overdue .

dhs officials recognized that more emphasis is needed on documenting performance assessments and told us they have included a past performance review as part of their chief procurement officer oversight program for fiscal year 2009 .

other indicators that some management officials placed a high priority on documenting performance include the following: assigning past performance focal points — some activities assigned focal points , individuals with specific responsibilities that included providing training and oversight .

at two air force locations , focal points also reviewed performance narratives for quality .

designating assessing officials — some activities designated managers as the official assessor of contractor performance rather than contracting officers or program office officials .

who to assign accountability to is another challenge .

ofpp generally views the completion of contractor performance assessments as a contracting officer function .

however , many contracting officials we talked with stated they often do not have the required information to complete an assessment and have to rely on program officials to provide the information .

some contracting offices delegated responsibility for completing assessments to the program office but acknowledged program office officials have little incentive to complete assessments because they often did not see the value in them .

we previously reported in 2005 that conducting contactor surveillance at dod , which includes documenting contractor performance , was not a high priority and that accountability for performing contractor surveillance was lacking .

differing number and type of rating factors and rating scales agencies use to document contractor performance limit the usefulness of the information in ppirs .

nasa's ppdb system has four rating factors , and the cps database , which is used by other civilian agencies , has five rating factors .

in contrast , dod's cpars system has a total of 16 rating factors .

each system also uses a different rating scale .

table 4 highlights these differences .

officials from gsa's integrated acquisition environment , which has oversight of governmentwide acquisition systems , acknowledged the utility of ppirs is currently limited by the differences in rating factors and scales .

because the ratings are brought into ppirs as - is , aggregate ratings for contractors cannot be developed — the data are too disparate .

as a result , contracting officials making contract award decisions may have to open and read through many ratings to piece together an overall picture of a contractor's performance .

ultimately , the lack of this information hinders the federal government's ability to readily assess a contractor's performance at an aggregate level or how overall performance is trending over time .

no one agency oversees , monitors , manages , or funds ppirs to ensure agency data fed into the system is adequate , complete , and useful for sharing governmentwide .

while gsa is responsible for overseeing , and consolidating governmentwide acquisition related systems , which include ppirs , ofpp is responsible for overall policy concerning past performance , and dod funds and manages the technical support of the system .

in may 2000 , ofpp published discretionary guidance entitled “best practices for collecting and using current and past performance information.” consistent with the far , this guidance stated that agencies are required to assess contractor performance and emphasized the need for an automated means to document and share this information .

subsequently , ofpp issued a draft contractor performance guide in 2006 designed to help agencies know their role in addressing and using contractor performance information .

however , the guide was not intended to , nor does it , establish governmentwide roles and responsibilities for managing and overseeing ppirs data .

since 2005 , several efforts have been initiated to improve ppirs and provide pertinent and timely performance information , but little progress has been made .

several broad goals for system improvement , established in 2005 by an ofpp interagency group , have yet to be met .

likewise , a short - term goal of revising the far to mandate the use of ppirs by all government agencies has yet to be achieved .

ofpp acknowledges that ppirs falls short of its goal to provide useful information to contracting officials making contracting decisions .

when ppirs was established in 2002 , ofpp officials envisioned it would simplify the task of collecting past performance information by eliminating redundancies among the various systems .

in 2005 , the chief acquisition officers council , through an ofpp interagency work group , established several broad goals for documenting , sharing , and using past performance information , including the following: standardize different contracting ratings used by various agencies .

provide more meaningful past performance information , including terminations for default .

develop a centralized questionnaire system for sharing governmentwide .

possibly eliminate multiple systems that feed performance information in ppirs .

however , little progress has been made in addressing these goals .

according to ofpp officials , funding needs to be dedicated to address these goals and realize long - term improvements to the current past performance system .

gsa officials who oversee acquisition related systems , to include ppirs , told us that as of february 27 , 2009 , efforts remain unfunded and no further action had been taken to make needed improvements .

the first step in securing funding , according to ofpp and gsa officials , is mandating the use of ppirs .

however , proposed changes to the far that would clarify past performance documentation requirements and require the use of ppirs have been stalled .

the proposed rule provides clearer instruction to contracting officers by delineating the requirement to document contractor performance for orders that exceed the simplified acquisition threshold , including those placed against gsa mas contracts , or for orders against contracts awarded by another agency .

in proposing far changes , ofpp focused , in part , on accountability by requiring agencies to identify individuals responsible for preparing contractor performance assessments .

while the comment period for the proposed changes closed in june 2008 , the changes have not been finalized .

an ofpp policy official stated that the final rule is expected to be published by june 2009 .

with the federal government relying on many of the same contractors to provide goods and services across agencies , the need to share information on contractors' past performance in making contract award decisions is critical .

while the need for a centralized repository of reliable performance information on federal contractors was identified in 2002 when ofpp implemented ppirs , we identified several underlying problems that limit the usefulness of information in ppirs for governmentwide sharing .

these problems include the lack of accountability or incentive at agencies to document assessments in the system , lack of standard evaluation factors and rating scales across agencies , and a lack of central oversight to ensure the adequacy of information fed into the system .

any efforts to improve sharing and use of contractor performance information must , at a minimum , address these deficiencies .

until then , ppirs will likely remain an inadequate information source for contracting officers .

more importantly , the government cannot be assured that it has adequate performance information needed to make sound contract award decisions and investments .

to facilitate governmentwide sharing and use of past performance information , we recommend that the administrator of ofpp , in conjunction with agency chief acquisition officers , take the following actions: standardize evaluation factors and rating scales governmentwide for documenting contractor performance .

establish policy for documenting performance - related information that is currently not captured systematically across agencies , such as contract terminations for default and a prime contractor's management of its subcontractors .

specify that agencies are to establish procedures and management controls , to include accountability , for documenting past performance in ppirs .

define governmentwide roles and responsibilities for managing and overseeing ppirs data .

develop system tools and metrics for agencies to use in monitoring and managing the documenting of contractor performance , such as contracts requiring an evaluation and information on delinquent reports .

take appropriate action to finalize proposed changes to the far that clarify responsibilities and performance documentation requirements for contract actions that involve orders placed against gsa's multiple award schedule .

to improve management and accountability for timely documenting of contractor past performance information at the agency level , we recommend that the departments of defense , energy , homeland security , and nasa establish management controls and appropriate management review of past performance evaluations as required and in line with any ofpp policy changes .

we provided a draft of this report to ofpp and the departments of defense , energy , homeland security , gsa , and nasa .

we received e - mail comments from ofpp , in which ofpp concurred with the recommendations .

we received written comments from the other five agencies , which are included as appendixes iii through vii .

in their written comments , the agencies agreed with the recommendation on improving management controls and most agencies outlined specific actions planned or taken to address the recommendation .

in written comments to the draft of this report , dhs did not agree with the figures contained in table 3 of the report regarding estimated contracts requiring an assessment and number of assessments in ppirs for selected agencies .

dhs stated that our numbers significantly understate the percentage of dhs contracts for which assessments were performed and are possibly inaccurate or misleading in how dhs compared to other agencies .

dhs presented its own data and requested that we revise ours .

we applied the same methodology across all civilian agencies , including dhs , and found no basis for using the numbers or methodology provided by dhs .

for example , while dhs indicates we should not include delivery orders , as we state in the note under table 3 , our estimates did not include individual orders issued by agencies that exceed the threshold .

therefore , we stand by our methodology and data , which as we stated in the report , presents a conservative estimate of the contracts that required an assessment .

also , we assessed the reliability of data we used and found it to be sufficiently reliable for the purposes of our analyses .

as a result , we are not revising the figures in table 3 .

as noted in our report , improvements are needed across agencies for the management and accountability of timely documenting contractor past performance information .

in its response , dhs agreed that significant strides need to be made in this area .

in written comments to the draft of this report , gsa stated that our recommendation should be changed to show that the far council in lieu of agency chief acquisition officers would be involved in developing and disseminating governmentwide acquisition policy through the far .

according to an ofpp policy official , while the far council would be involved in evaluating policy and making changes to the far , ofpp is responsible for overall policy concerning past performance and can make policy changes without involving the far council .

in line with our recommendations , this would include standards for evaluating past performance and policies for collecting and maintaining the information .

as we state in the report , the chief acquisition officers council , through an ofpp interagency work group , has already established several broad goals for documenting , sharing , and using past performance information .

our recommendations to ofpp , in coordination with this council , are in part aimed at actions necessary to address these goals .

these recommendations could be implemented through an ofpp policy memorandum and could result in changes to the far , which we recognize would need to be coordinated through the far council as appropriate .

as a result , we are not making changes to the recommendation .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution of it until 30 days from the date of this report .

we will then send copies of this report to interested congressional committees ; the director of the office of management and budget , the secretary of defense ; the secretaries of the army , navy , and air force ; the secretary of the department of homeland security ; the secretary of the department of energy ; the secretary of the national aeronautics and space administration ; and the administrator of the general services administration .

in addition , we will also make copies available at no charge on the gao web site at http: / / www.gao.gov .

if you have questions about this report or need additional information , please contact me at ( 202 ) 512-4146 or lasowskia@gao.gov .

contact points for our office of congressional relations and public affairs may be found on the last page of this report .

see appendix viii for a list of key contributors to this report .

to assess agencies' use of information on contractors' past performance in awarding contracts , we reviewed and analyzed the federal acquisition regulation ( far ) and office of federal procurement policy ( ofpp ) guidance on use of past performance .

we also reviewed source selection guidance for the department of defense ( dod ) , department of energy ( doe ) , department of homeland security ( dhs ) , national aeronautics and space administration ( nasa ) , and the general services administration ( gsa ) — agencies accounting for a large percentage of federal contractors .

to obtain agency contracting officials' views on using past performance , we used fpds - ng data to select 11 buying offices across the agencies to provide a cross - section of buying activities .

at these locations , we interviewed 121 contracting officials including supervisory contract personnel to include division / branch contracting managers , contracting officers , and contract specialists to discuss 1 ) how past performance factored into the contract award decision , 2 ) sources upon which they rely for the information , 3 ) completing contractor performance assessments , and 4 ) challenge in using and sharing past performance information .

to identify the importance of past performance relative to other non - cost factors in specific solicitations , we used fpds - ng data from fiscal year 2007 and the first eight months of fiscal year 2008 , to identify 62 competitively awarded contracts — 49 definitive contracts and 13 orders placed against indefinite delivery vehicle contracts .

we selected these contracts to represent a range of contracts across different buying activities and — though not generalized to all contract actions within these agencies — represented a range of products and services , types of contracts , and dollar values as shown in appendix ii .

we obtained contract documents to verify the fields used in fpds - ng to select the contracts , including type of contract and product service code , and found the data reliable enough for the purpose of selecting the contracts .

for these contracts , we obtained source selection documents including sections m of the request for proposals , which described the evaluation factors for award , and the source selection decision document that described how past performance was evaluated for each offeror .

we reviewed the evaluation factors for each solicitation to identify how past performance ranked in order of importance relative to other non - cost factors in the evaluation scheme and summarized the results .

to assess the extent to which selected agencies in our review complied with requirements for documenting contractor performance , we analyzed fpds - ng and ppirs data and used information provided by the dod cpars program office .

in estimating the number of contracts requiring an assessment for fiscal years 2006 and 2007 for civilian agencies in our review , we aggregated contract actions in fpds - ng for each year to identify the number of contracts that exceeded the reporting thresholds of $550,000 for construction contracts ( far § 36.201 ) , $30,000 for architect and engineering ( far § 36.604 ) , and generally $100,000 for most other contracts ( far § 2.101 ) .

we excluded contracts that are exempt from performance assessments under far subpart 8.7 — acquisitions from non profit agencies employing people who are blind or severely disabled .

for indefinite delivery contracts , including gsa's multiple award schedule , orders were accumulated against the base contract for each agency and counted as one contract if the cumulative orders exceeded the reporting thresholds .

this analysis provides a conservative estimate of the number of contracts that require an assessment because it does not include individual orders that may exceed the threshold or contract actions that span fiscal years .

for this analysis , we used contract number and dollar obligation fields from fpds - ng and found them reliable enough for the purpose of this analysis .

because dod uses different reporting thresholds based on business sectors — information that is not available in fpds - ng — we obtained compliance reports from the cpars program office for fiscal years 2006 and 2007 , which included estimates of the number of performance assessments that would have been required for dod components and the number of those contracts with completed assessments .

to determine the number of fiscal year 2006 and 2007 contracts with performance assessments for civilian agencies , we obtained and analyzed data from the ppirs program office on contracts with assessments , including the number of assessments against gsa mas contracts , as of february 26 , 2009 .

to assess the reliability of data provided , we accessed the ppirs system and compared the number of contracts with assessments with those provided by the cpars and ppirs program offices , and found the data sufficiently reliable for the purpose of our analysis .

to assess the usefulness of ppirs for governmentwide sharing of past performance information , we compared information in each of the three systems used to document contractor performance information including rating factors and rating scales .

in addition , we met with agency officials who have responsibilities for managing the various systems — including the naval sea logistics center detachment , portsmouth , which administers cpars and ppirs , and officials at nasa who administer the past performance database .

to identify challenges that may hinder the systematic governmentwide sharing of past performance information , we interviewed contracting officials from 11 buying offices regarding a number of issues to include 1 ) roles in the assessment process , 2 ) challenges in completing assessments , 3 ) performance information not currently captured that might be useful for selecting contractors , 4 ) and use of metrics for managing and monitoring compliance with reporting requirements .

finally , we met with ofpp , gsa , and dod to discuss the extent of oversight of ppirs data and roles and responsibilities as applicable .

to assess efforts under way or planned to improve the sharing of information on contractor performance , we obtained and reviewed memorandums , plans , and other documents produced by ofpp including proposed far changes and any proposed past performance guidelines .

we met with officials from these offices to discuss challenges already identified in sharing and using past performance information , goals they may have established for improving the system , and status of efforts to address them .

our work was conducted at the following locations: ofpp , washington d.c. ; gsa , arlington , va ; the air force space and missile systems center , el segundo , ca ; hill air force base , ogden , utah ; the army communications and electronics command , fort monmouth , n.j. ; the army sustainment command , rock island , ill. ; the army contracting command , fort belvoir , va. ; the naval air systems command , patuxent river , m.d .

 ; the naval sea systems command , washington , d.c. ; the defense contract management agency located in arlington , va. ; dhs including the customs and border protection , washington , d.c. , and the transportation security administration , arlington , va. ; nasa including the goddard space flight center , greenbelt , m.d .

and the johnson space center , houston , tex .

 ; doe including the national nuclear security administration service center located in albuquerque , n.m. we conducted this performance audit from february 2008 to february 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the individual named above , ann calvaresi barr , director ; james fuquay , assistant director ; usman ahmad ; jeffrey barron ; barry deweese ; julia kennon ; flavio martinez ; susan neill ; karen sloan ; sylvia schatz ; and bradley terry made key contributions to this report .

