although there have been numerous initiatives undertaken to better manage the more than $80 billion that is annually invested in information technology ( it ) , federal it investments have too frequently failed or incurred cost overruns and schedule slippages while contributing little to mission - related outcomes .

as such , we have recently placed improving the management of it acquisitions and operations on our high risk list .

this high - risk area highlights several critical it initiatives in need of additional progress , including the it dashboard , and also identified actions needed to make progress .

recognizing the severity of issues related to government - wide management of it , in december 2014 , the carl levin and howard p. “buck” mckeon national defense authorization act for fiscal year 2015 enacted provisions commonly referred to as the federal information technology acquisition reform act ( fitara ) .

among other things , fitara states that the office of management and budget ( omb ) shall make available to the public a list of each major it investment including data on cost , schedule , and performance .

accordingly , it is vital that omb provide timely and accurate data on the federal it dashboard — its public website that reports performance and supporting data for major it investments .

launched in june 2009 , the dashboard is intended to provide transparency for these investments in order to facilitate public monitoring of government operations and accountability for investment performance by the federal chief information officers ( cio ) who oversee them .

among other things , agencies are to submit ratings from their cios , which , according to omb's instructions , should reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

in december 2014 , fitara codified the requirement for cios to categorize their major it investment risks in accordance with omb guidance .

this report responds to your request to review the cio ratings on the dashboard .

specifically , our objectives were to ( 1 ) describe agencies' processes for determining the cio risk ratings for major it investments and ( 2 ) assess the risk of federal it investments and analyze any differences with the investments' cio risk ratings .

to select the agencies and investments , we reviewed data reported to omb as part of the federal budget process to identify major investments which planned to spend at least 80 percent of their fiscal year 2015 funding on development , modernization , and enhancement activities .

this produced a list of 17 agencies and 107 selected investments .

to address our first objective , we met with the selected agencies to discuss their cio rating processes .

we collected process documentation , which we used to compare agencies' processes to omb's guidance and determine how the specifics of agencies' processes varied .

to address our second objective , we reviewed the 107 investments , but excluded 12 that were inactive , not in development , lacked a key risk document , or were managed as part of a larger development program .

this resulted in 95 investments at 15 agencies .

we made the decision to review the ratings from april 2015 , the month that our audit work began , in order to minimize any influence that our ongoing work could have on the agencies' processes and resulting ratings .

we then interviewed appropriate agency officials and collected march 2015 risk documentation ( the data we would expect to be reflected in the april ratings ) , as well as associated performance data , review board briefings , and relevant reports ( eg , gao and inspector general reports ) .

in cases where agencies were unable to provide march documentation , we used documents from the closest available date .

we did not consider risks that were introduced after march in these documents .

we combined and scored this information based upon industry and government best practices to create our assessments of investments' risk .

we then compared these assessments to agencies' april 2015 cio risk ratings .

details of our objectives , scope , and methodology are contained in appendix i .

we conducted this performance audit from april 2015 through june 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

omb plays a key role in overseeing how federal agencies manage their it investments by working with them to better plan , justify , and determine how to manage them .

to provide visibility into the performance of such investments , omb deployed the it dashboard in 2009 , which displays federal agencies' cost , schedule , and performance data for over 770 major federal it investments at 26 federal agencies , accounting for $42 billion of those agencies' planned $82 billion in it spending for fiscal year 2017 .

according to omb , these data are intended to provide a near - real - time perspective on the performance of these investments , as well as a historical perspective .

the dashboard's data span the period from its june 2009 inception to the present , and are based , in part , on agency assessments of individual investment performance and each agency's budget request to omb .

further , the public display of these data is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold government agencies accountable for progress and results .

the dashboard visually presents performance ratings for agencies and for individual investments using metrics that omb has defined — cost , schedule , and cio evaluation .

cost and schedule ratings .

the dashboard calculates these ratings by determining cost and schedule variances based on agency - submitted data , such as planned versus actual costs or planned versus actual completion dates .

the dashboard then assigns rating colors ( red , yellow , green ) based on the magnitude of the variances .

specifically , a variance greater than 30 percent is red , a variance between 10 percent and 30 percent is yellow , and a variance less than 10 percent is green .

cio ratings .

unlike the cost and schedule ratings , the dashboard's “investment evaluation by agency cio” ( also called the cio rating ) is determined by the agency cio .

according to omb's instructions , each agency cio is to assess his or her it investments against a set of pre - established evaluation factors and then assign a rating of 1 ( high risk ) to 5 ( low risk ) based on the cio's best judgment of the level of risk facing the investment .

omb suggests six evaluation factors , as shown in table 1 .

omb recommends that cios consult with appropriate stakeholders in making their evaluations , including chief acquisition officers , program managers , and other interested parties .

according to an omb staff member , agency cios are responsible for determining appropriate thresholds for the risk levels and for applying them to investments when assigning cio ratings .

omb requires agencies to update these ratings as soon as new information becomes available which will affect an investment's assessment and , since june 2015 , has required that this be at least once each calendar month .

after agencies assign a level of risk to each investment , the dashboard assigns colors to cio ratings according to a five - point scale: high risk and moderately high risk are red , medium risk is yellow , and moderately low risk and low risk are green .

recognizing the importance of government - wide management of it , in december 2014 , congress enacted it acquisition reform legislation , fitara .

the law was designed to improve agencies' acquisition of it and enable congress to monitor agencies' progress and hold them accountable for reducing duplication and achieving cost savings .

fitara contains specific requirements related to seven areas , including one titled “enhanced transparency and improved risk management in information technology investments.” among other things , that area requires omb and agencies to make publicly available detailed information on federal it investments , and agency cios to categorize their it investments by risk .

this requirement is addressed by omb's it dashboard .

over the past 5 years , we have issued a series of reports about the it dashboard that noted both significant steps omb has taken to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , as well as issues with the accuracy and reliability of data .

further , we have reported on the dashboard's cio ratings: in october 2012 , we reported that cios at six agencies rated a majority of investments listed on the it dashboard as low or moderately low risk from june 2009 — when the dashboard was implemented — through march 2012 and two agencies , the department of defense ( defense ) and the national science foundation , rated no investments as high or moderately high risk during this time period ( categorized as “red” by the dashboard ) .

additionally , agencies generally followed omb's instructions for assigning cio ratings , although defense's ratings were unique in reflecting additional considerations , such as the likelihood of omb review .

most of the selected agencies reported various benefits associated with producing and reporting cio ratings , such as increased quality of their performance data and greater transparency and visibility of investments .

we recommended that omb analyze and report on agencies' cio ratings over time , and that defense ensure that its cio ratings reflect available investment performance assessments and its risk management guidance .

both agencies concurred with our recommendations .

subsequently , omb reported on cio rating trends .

further , defense now identifies red investments on the dashboard .

more recently in december 2013 , we reported that , as of august 2013 , the cios at eight selected agencies had rated 198 of their 244 major it investments listed on the dashboard as low risk or moderately low risk , 41 as medium risk , and 5 as high risk or moderately high risk .

however , the total number of investments reported by these agencies had varied over time , which impacted the number of investments receiving cio ratings .

for example , the department of energy ( energy ) reclassified several of its supercomputer investments from it to facilities , and the department of commerce ( commerce ) decided to reclassify its satellite ground system investments .

both decisions resulted in the removal of the investments from the dashboard , even though the investments were clearly it .

we recommended that these agencies appropriately categorize all investments , but they disagreed with our recommendation .

in that same report , we reviewed 80 investments and found that 53 of the cio ratings were consistent with the investment risk , 20 were partially consistent , and 7 were inconsistent .

while two agencies' cio ratings were entirely consistent , other agencies' ratings were inconsistent for a variety of reasons , including delays in updating the dashboard and how investment performance was tracked .

for example , the social security administration ( ssa ) resets investment cost and schedule performance baselines annually , an approach that increases the risk of undetected cost or schedule variances that will impact investment success .

as such , we recommended that ssa revise its investment management approach .

the agency agreed with our recommendation and discussed planned actions to address it .

additionally , we reported that omb does not update the public version of the dashboard as the president's annual budget request is being created .

consequently , the public version of the dashboard was not updated for 15 of the past 24 months .

we recommended that omb make dashboard information available independent of the budget process .

omb recently updated the dashboard with a number of changes , and intends for the dashboard to be able to show updates throughout the year .

agencies determine investments' cio ratings using a variety of processes , which include omb's suggested factors .

however , their interpretation of these factors varies significantly .

in addition , the majority of agencies base their ratings on qualitative assessments , but several base theirs on formulas .

further , 13 agencies' process guidance calls for at least monthly updates to cio ratings , 1 agency ( the department of homeland security ( dhs ) ) schedules its reviews based on risk , and 3 agencies require less frequent updates .

as described earlier , omb requires that each agency cio rate the risk of his or her it investments .

omb gives the cios the flexibility to use their judgment and suggests six evaluation factors .

as noted above , we reviewed data reported to omb as part of the federal budget process to identify major investments which planned to spend at least 80 percent of their fiscal year 2015 funding on development , modernization , and enhancement activities .

this selection produced a list of 17 agencies and 107 investments .

each of the 17 agencies has incorporated at least 2 of omb's suggested factors into their cio's risk rating processes and 9 use all of the factors .

omb requires that agencies provide cio evaluations for all major it investments which reflect the cio's best judgment of the current level of risk for the investment in terms of its ability to accomplish its goals .

according to omb's guidance , the evaluation can be informed by the following factors , including , but not limited to: risk management , requirements management , contractor oversight , historical performance , human capital , and other factors that the cio deems important to forecasting future success .

table 2 summarizes the extent to which the 17 selected agencies incorporate omb's suggested evaluation factors into their cio's risk rating processes .

appendix ii provides more information on the selected agencies' cio rating processes .

while the factors suggested by omb were considered in the agencies' cio rating processes , their interpretation of these factors varied .

in particular , most agencies considered active risks when rating investments , but others only evaluated compliance with risk processes .

for example: defense , the department of the treasury ( treasury ) , and the general services administration ( gsa ) consider individual active risks , rather than investments' compliance with risk guidance .

for example , defense considered the risk caused by budget cuts and their potential impact through the following fiscal year when rating its base information transport infrastructure wired investment .

in addition , gsa , when rating its integrated award environment investment , considered the risk involved with transitioning from an existing contract to one that could provide better expertise and greater oversight .

the departments of agriculture ( agriculture ) , education ( education ) , energy , the interior ( interior ) , state ( state ) , and veterans affairs ( va ) , and the office of personnel management ( opm ) review compliance with risk management practices , but do not assess active risks .

for example , compliance may include whether mitigation plans exist , risk logs are current , and risks are clearly prioritized .

the rest of the agencies that include the risk factor — commerce , dhs , the department of labor ( labor ) , the department of transportation ( transportation ) , as well as the environmental protection agency ( epa ) and ssa — considered both process compliance and reviews of active risks .

for example , commerce reviews at least the top three active risks for investments , verifies that these risks are specific to the investment , appropriately managed and mitigated , and that the risk register is updated regularly .

dhs also considers active investment risks , ensures that they are current , and that risk mitigation plans are in place .

furthermore , the selected agencies considered different types of historical data when rating their it investments .

while all of the agencies considered performance measures and cost and schedule variances , five considered changes to the investment's baseline ; eight considered the accomplishment of milestones ; and three considered relevant news , gao , or inspector general reports .

while the details of these approaches vary , they align with omb's suggested factors .

of the 17 selected agencies , 6 used formulas to create cio ratings .

specifically , agriculture , the department of health and human services ( hhs ) , dhs , education , treasury , and va determined their ratings by quantifying and combining inputs such as cost and schedule variances , risk exposure values , and compliance with agency processes .

metrics for compliance with agency processes included those related to program and project management , project execution , the quality of investment documentation , and whether the investment is regularly updating risk management plans and logs .

the remaining 11 agencies based their cio ratings on qualitative assessments of performance metrics , risks , investment documentation , and informal investment knowledge .

in particular , they assign ratings based on metrics such as investment performance , discussions with management staff , and the quality of investment documentation .

thirteen agencies' process guidance calls for at least monthly updates to cio ratings , one agency ( dhs ) schedules its reviews based on risk , and three agencies require less frequent updates .

although a monthly review and update process was not previously required by omb , the fiscal year 2017 capital planning guidance issued in june 2015 requires agencies to update their cio ratings at least once per month .

table 3 summarizes the frequency of cio rating updates called for by the selected agencies' processes .

the three selected agencies that do not comply with omb's current requirement for monthly rating updates are defense , education , and ssa .

in particular: defense updates cio ratings semi - annually .

education updates cio ratings based on bi - monthly reviews of investments .

ssa conducts monthly investment reviews , but updates cio ratings on a quarterly basis .

additionally , dhs staggers its updates based on investment risk , with high risk ( red ) investments reviewed monthly , moderate ( yellow ) investments reviewed quarterly , and low ( green ) investments reviewed semi - annually .

these four agencies' practices are inconsistent with omb's guidance and can limit the transparency and oversight of the government's it investments .

however , staff from omb told us that the capital planning guidance for fiscal year 2018 would not contain the monthly reporting requirement and would instead encourage agencies to keep their cio ratings accurate and current , rather than mandate reporting frequency .

moving forward , it will be important for any such revised guidance to encourage the frequent and appropriate updating of agencies' cio ratings while also remaining compliant with relevant provisions of fitara .

these provisions require that agencies report at least semi - annually to omb on each major it investment and include data on cost , schedule , and performance .

these provisions also require joint omb and agency reviews of any investment that has been evaluated as high risk for four consecutive quarters .

as discussed earlier , to assess the risk of individual assessments , we reviewed the 107 investments that we originally selected , but excluded 12 that were inactive , not in development , lacked a key risk document , or were managed as part of a larger development program .

this resulted in 95 investments at 15 agencies .

our assessments of these investments generally showed more risk than the associated cio ratings .

in particular , of the 95 investments we reviewed , our assessments matched the cio ratings 22 times , showed more risk 60 times , and showed less risk 13 times .

we identified three factors which contributed to these differences: ( 1 ) 40 of the 95 cio ratings were not updated in april 2015 , ( 2 ) three agencies' rating processes span longer than 1 month , and ( 3 ) seven agencies' rating processes did not focus on active risks ( as previously discussed ) .

according to omb's guidance , cio ratings “should reflect the cio's assessment of the risk and the investment's ability to accomplish its goals.” such assessments of risk inherently involve a great deal of human judgment .

consequently , risk assessments should be expected to vary both across and within organizations .

for example , defense's cio ratings process documentation states that , since its major investments are “inherently high risk,” its ratings are “assessments of relative risk implemented within this risk baseline.” that is , when measuring risk , defense is more tolerant and uses a different scale than other agencies .

similarly , risk assessments can vary within agencies .

for example , officials at several agencies expressed concerns that the assignment of risk scores ( probability and impact ) were not consistent across investments .

officials at dhs also noted that program managers may score risks higher to flag an issue for management attention .

further , in many cases , agency cios could have more information than we examined in our assessments .

we attempted to minimize the subjectivity in our risk assessments by using the agencies' own lists of risks , known as risk registers , as the basis of our assessments ( see appendix i for additional details on our methodology ) .

we also augmented our ratings with agencies' cost and schedule data , briefings to review boards , and relevant reports .

our calculations are only intended to provide a standardized view of risk across all the departments and investments we reviewed and this methodology is not intended to serve as a prescriptive approach to the agencies' evaluation of investment risk .

while the variety of methodologies and inputs meant that some differences were inevitable , almost two thirds of our assessments showed more risk than the associated cio ratings for our 95 selected investments .

figure 1 summarizes how our assessments compared to the select investments' cio ratings .

of the 95 investments we reviewed , our assessments showed less risk 13 times , matched the cio ratings 22 times , and showed more risk 60 times .

additionally , our assessments showed more risk for at least 1 investment at 13 of the 15 agencies we assessed .

table 4 summarizes these comparisons by agency , and appendix iii lists the april 2015 cio ratings and our assessments for each of the selected investments .

overall , our assessments reflected more risk than 63 percent of the associated cio ratings , and 13 of the 15 agencies reported less risk for at least 1 investment .

of the 13 , 11 reported less risk for at least half of the selected investments and the remaining 2 reported less risk for just under half of those agencies' selected investments .

for example , we identified more risk at 3 of dhs's 7 investments and 4 of ssa's 9 investments .

our assessments showed less risk than the cio ratings for 13 of the 95 selected investments ( 14 percent ) .

specifically , we assessed 5 green that the agencies rated yellow and 8 yellow that the agencies rated red ; there were no instances where we assessed an investment green that the agencies rated red .

these investments belonged to 8 of the 15 selected agencies: hhs ( 3 investments ) , defense ( 2 investments ) , dhs ( 2 investments ) , education ( 2 investments ) , commerce ( 1 investment ) , energy ( 1 investment ) , gsa ( 1 investment ) , and ssa ( 1 investment ) .

table 5 lists those investments , the april 2015 cio rating , and our associated assessment .

the reasons why our assessments showed less risk varied among these investments .

for instance: commerce rated its integrated dissemination program as yellow , but we assessed it as green .

a department official explained that the rating was because ( 1 ) the investment had not been transparent in its activities making it difficult to determine whether a milestone was achieved or services were provided and ( 2 ) the investment had only identified generic , non - specific risks .

further , the official noted that the department had even debated rating this investment red .

we assessed this investment as green because of the low overall level of risk and low cost and schedule variances .

dhs rated its continuous diagnostics and mitigation investment as yellow , but we assessed it as green .

dhs officials stated that the investment was rated yellow because ( 1 ) it was considered complex and higher risk due to the involvement of several civilian government agencies , ( 2 ) cost and schedule variances exceeded omb's thresholds , and ( 3 ) concerns about the schedule and availability of resources .

in contrast , we assessed it as green because it did not have any risks with both high impact and high probability scores and more than half of the risks scored low in overall risk exposure .

hhs rated its medicaid and children's health insurance program business information and solutions investment as red , but we assessed it as yellow .

hhs officials rated this investment red because the investment team did not submit required data and a rebaseline caused a large cost variance .

even though the investment's overall risk score was low , we assessed the investment as yellow based upon documented cost and schedule variances and program issues identified in review board briefings .

gsa rated its integrated award environment as yellow , but we assessed it as green .

gsa officials stated that the investment was rated yellow as a precaution: the investment was undergoing a contract transition and the cio knew that problems could develop .

gsa officials stated that another contributing factor was the investment's late contract award , which had residual impact on investment performance .

we assessed this investment green because of its overall low risk score .

as noted earlier , cio ratings are intended to reflect the cio's assessment of the risk and may be based on additional programmatic information not included in our assessment methodology , which focused primarily on investments' risk registers .

as such , the inherently judgmental nature of the cios' assessments may reflect broader considerations that , in their organization's view , better represent the overall risk of an investment .

for 22 of the 95 selected investments ( 23 percent ) , our assessments matched the cio rating .

specifically , we matched 10 green ratings , 8 yellow ratings , and 4 red ratings .

table 6 lists those investments , the april 2015 cio rating , and our associated assessment .

in particular , there were 4 investments at defense , 4 at ssa , 4 at transportation , 3 at commerce , 2 at gsa , 2 at dhs , 1 at hhs , 1 at interior , and 1 at state that had cio ratings that matched our assessments .

these investments were a mix of red , yellow , and green ratings .

however , the reasoning behind the cio ratings and our individual assessments differed .

for example , interior rated its integrated reporting of wildland - fire information investment as yellow because the investment's required documentation did not meet agency standards .

specifically , the investment's most recent artifact review before the april 2015 cio rating period showed that the investment lacked required documentation , including a risk management plan .

however , we assessed the investment as yellow because the it dashboard showed significant cost and schedule variances at the time of our review .

for 60 of the 95 selected investments ( 63 percent ) , our assessments reflected more risk than agencies' cio ratings .

specifically , we assessed 9 red that the agencies rated yellow , 28 yellow that the agencies rated green , and 23 red that the agencies rated green .

further , these investments were at 13 of the 15 agencies we assessed .

table 7 lists those investments , the april 2015 cio rating , and our associated assessment .

the agencies' explanations as to why our assessments showed more risk varied in these 60 cases .

for example , agriculture rated its optimized computing environment investment as green , but we assessed it as red .

while agriculture officials noted that this investment's funding from partner agencies was uncertain , the investment received a green cio rating because the funding uncertainties were a recurring concern that had been previously managed without issue .

conversely , we assessed the investment as red because 21 of the 44 risks in the investment's risk register had high overall risk scores .

dhs rated its next generation networks priority services investment as green , but we assessed it as red .

dhs officials stated that they rated this investment green because the investment was progressing well and because it had successfully mitigated its high impact / high probability risks .

however , our assessment was partly based on two risks with both high probability and high impact scores that the investment team categorized as potentially causing investment failure .

these risk scores and descriptions indicated that the program believed that it was likely that these risks would be realized and cause critical , perhaps investment - threatening problems .

dhs officials questioned the probability and impact scores and explained that investment teams may inflate such scores to flag potential issues for management .

ssa rated its supplemental security income modernization investment as green , but we assessed it as red .

ssa officials stated that their review did not see significant reason to lower the rating , even though part of the investment was working through significant technical challenges .

we assessed the investment red because half of its risks had high risk scores .

included in the most critical risks were those pertaining to requirements changes , system complexity , and staffing losses .

when asked about these risks , ssa officials explained that , generally , they were common risks faced by every investment .

consequently , they expressed doubt that these risks necessitated such high risk scores .

treasury rated its post payment system investment as green , but we assessed it as red .

treasury officials stated that this investment was rated green because it was well - run and had previously kept its risks from becoming realized issues .

conversely , we rated this investment as red because 20 out of its 26 risks had high overall risk scores , including 2 that indicated a high probability of schedule delays .

treasury officials stated that they were monitoring the risks that we identified .

as noted earlier , the judgmental nature of a cio's assessment may reflect a broader organizational view of investment risk beyond the contents of the investment's risk register .

however , unlike the cio ratings that reflected more risk than our assessments , many of these cio ratings minimized the potential severity and impact of high risk scores .

our past work has shown that such an approach to risk management can often lead to cost and schedule overruns or failed projects .

in addition to the previously discussed issue of rating subjectivity , we identified three factors which contributed to differences between our assessments and cio ratings at 10 of the 15 selected agencies .

in particular , ratings were not updated in april 2015 , rating processes spanned longer than 1 month , and rating processes did not focus on active risks .

rather than pertaining to the cios' personal evaluations of risk , these additional issues relate to the 10 agencies' update practices or rating processes .

because these issues are with underlying practices and processes , they have the potential to impact all investment ratings — whether or not we reviewed them as part of our assessment .

specifically , we found that 40 of the 95 cio ratings were not updated in april 2015 , 3 agencies' rating processes span longer than 1 month , and 7 agencies' rating processes did not focus on active risks ( see table 8 ) .

following the table is a further description of these issues .

of the 95 investments we selected , we found that agencies had not updated cio ratings for 40 in april 2015 ( see table 9 ) .

further , we found that our assessments were more likely to match those investments updated in april because these recently updated ratings reflected the then - current investment information that we also used in our assessments .

in particular , 17 of the 22 investments where our assessment matched the cios' ratings were updated in april 2015 , whereas we matched only 5 of the 40 without april 2015 updates .

of the 40 ratings that were not updated , 15 were at 6 agencies , which provided the following explanations for the lack of april updates: agriculture officials stated that they were in the midst of switching systems used to update ratings to the dashboard , so they were unable to update ratings that month .

commerce officials stated that they did not receive quality data from the investment until the following month and thus did not have a valid basis for changing the rating or cio comments .

dhs officials stated that they did not update 2 of the selected investments' ratings because they were undergoing techstat reviews .

additionally , 2 investments' cio ratings were not updated in april even though they initially received april reviews .

according to officials , 1 investment's review occurred too late in the month and the other investment's update was delayed .

they also stated that they were revising their process so that rating updates would occur during such reviews .

additionally , dhs's risk - based review cycle meant that two investments were not due to be rated in april .

in particular , those investments were rated yellow less than 3 months prior and dhs reviews yellow - rated investments on a quarterly basis .

an education official explained that the agency only updates its ratings if an investment's status has changed enough to warrant an update .

epa officials told us that its selected investment was not updated in april 2015 because the investment was in a state of flux during that time and a review would not have been useful .

hhs officials stated that a combination of system and human errors kept 2 investment updates from posting .

the remaining 25 investments which did not have an april 2015 update were at defense , which updates its ratings on a semi - annual basis ( see earlier discussion of cio rating update frequency ) .

if we had used defense's subsequent update from june 2015 , the number of instances in which the cio ratings matched our assessments would have increased from 4 to 10 .

the preceding examples underscore the importance of omb's june 2015 policy change , requiring that agencies provide monthly rating updates .

such updates will help to ensure that the information on the dashboard is timely and accurately reflects recent changes .

without such updates , the cio ratings on the dashboard may not reflect the current level of investment risk .

the duration of agencies' cio rating processes also impacted the comparison between the cio ratings and our assessment .

since we used march 2015 risk registers as the basis for our assessment , process times longer than a month mean that the data we used for our assessment would not have been included in agencies' april 2015 cio ratings .

fourteen of the 17 selected agencies indicated that it takes 1 month or less to process investment data and update the it dashboard .

however , processes at 3 agencies — education , state , and treasury — can be longer than 1 month .

for example , treasury officials stated that late invoices from contractors can delay their processes past the 1 month time frame .

state officials explained that their conversion of certain cost and schedule data into the format required by the dashboard can take between 1 and 2 months to complete .

longer processes mean that cio ratings are based upon older data and may not reflect the current level of investment risk .

further , these longer processes may prevent the agencies from meeting omb's requirement to update ratings monthly .

when developing cio ratings , seven agencies did not consider active risks , as discussed earlier .

six of those agencies ( agriculture , education , energy , interior , state , and va ) instead chose to focus on investments' risk management processes , such as whether a process was in place or whether a risk log was current .

such approaches did not consider individual risks , such as funding cuts or staffing changes , which detail the probability and impact of pending threats to success .

instead , va's cio rating process considers several specific risk management criteria: whether an investment ( 1 ) has a risk management strategy , ( 2 ) keeps the risk register current and complete , ( 3 ) clearly prioritizes risks , and ( 4 ) puts mitigation plans in place to address risks .

considering process compliance , rather than active risks , contributed to our assessments only matching 2 of the cio ratings for the 14 selected investments at these six agencies .

the remaining agency , hhs , did not factor risk into its cio ratings ( as discussed earlier ) .

this contributed to our assessments matching only 1 of the 9 selected hhs investments .

in all cases , cio ratings that do not incorporate active risks increase the chance that ratings do not reflect the true likelihood of investment success .

since its inception in 2009 , the it dashboard has increased the transparency of the government's multi - billion dollar spending on major it investments .

the dashboard's cio ratings , in particular , have improved visibility into the risks facing these critically important efforts .

to that end , agency cios have developed a variety of processes to assess and report the risk of their investments .

although the effectiveness of the dashboard depends on the quality of the cios' ratings , selected agencies' rating methods do not provide an accurate assessment of investment risk and thus reduce the value of this important tool for transparency and oversight .

further , multiple agencies' infrequent submissions raise concerns that those updates are not reflecting timely and accurate risk information , contrary to omb's current policy requiring monthly updates .

such practices limit the transparency and oversight of the government's billions of dollars in it investments .

beyond the transparency they provide , cio ratings present an opportunity to improve cios' understanding of their it portfolio and identify those investments in need of additional oversight .

while these ratings are by definition inherently judgmental , our assessments of the selected investments generally showed more risk than almost 65 percent of the associated cio ratings .

consequently , the associated risk rating processes used by the agencies generally are understating the level of risk , raising the likelihood that critical federal investments in it are not receiving the appropriate levels of oversight .

finally , agencies that do not factor active risks into their cio ratings trigger additional questions about the degree to which information reported on the dashboard provides full and accurate information about an investment's risk .

while agencies' consideration of active risk is not explicitly called for by omb's guidance , this represents a gap in the agencies' processes that is understating the amount of risk reflected in the dashboard's cio ratings .

to better ensure that the dashboard ratings more accurately reflect risk , we recommend that: the secretaries of the departments of agriculture , education , energy , health and human services , the interior , state , and veterans affairs ; and the director of the office of personnel management direct their cios to factor active risks into their it dashboard cio ratings ; the secretaries of the departments of defense , education , and homeland security ; and the commissioner of the social security administration direct their cios to update their cio ratings at least as frequently as required in omb's guidance ; and the secretaries of the departments of agriculture , commerce , defense , education , energy , health and human services , homeland security , state , transportation , the treasury , veterans affairs ; the administrator of the environmental protection agency ; and the commissioner of the social security administration direct their cios to ensure that their cio ratings reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

we received comments on a draft of this report from omb , the 15 agencies to which we made recommendations , and the remaining 2 to which we did not make recommendations .

specifically , 9 agencies agreed with our recommendations , 2 ( education and ssa ) agreed with one or more while partially agreeing with another , 1 ( dhs ) agreed with one and disagreed with another , 1 ( defense ) partially agreed with one and disagreed with another , 1 ( epa ) disagreed , 1 ( omb ) did not agree or disagree , and 1 ( treasury ) did not comment on our recommendation .

the 2 agencies without recommendations ( labor and gsa ) stated that they had no comments .

multiple agencies also provided technical comments , which we have incorporated as appropriate .

the following is a detailed discussion of each agency's comments .

in comments provided via e - mail on april 25 , 2016 , an omb official from the office of general counsel did not agree or disagree with our recommendations .

omb also provided technical comments , which we have incorporated as appropriate .

our draft report to omb for comment included a recommendation that defense , education , dhs , and ssa update their agencies' cio ratings on a monthly basis , as required by omb's fiscal year 2017 it budget capital planning guidance .

subsequently , omb informed us that the fiscal year 2018 capital planning guidance will be revised to only require that agencies update the dashboard as soon as new information becomes available or when cio reviews are performed .

taking into account omb's planned course of action , we have modified our recommendation to those four agencies to reflect that they should , at a minimum , comply with omb's required reporting frequency .

in comments provided via e - mail on may 16 , 2016 , a senior advisor for oversight and compliance from agriculture's office of the cio stated that the department concurred with our recommendation .

agriculture also provided technical comments , which we incorporated as appropriate .

in written comments , commerce concurred with our recommendation and committed to ensuring that the department's cio ratings will reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

commerce's written comments are provided in appendix iv .

in written comments , defense partially concurred with our recommendation to reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

while defense agreed with the need of cio ratings to reflect risk , the department stated that its current cio rating process already incorporates this factor .

while we also agree that risk plays a role in defense's documented cio ratings process , our findings indicate that the cio ratings for the selected investments may have underreported investment risk .

specifically , our assessments for 19 of the selected 25 defense investments ( or 76 percent ) show more risk than the cio ratings on the dashboard in april 2015 .

we therefore believe that our recommendation is appropriate .

in addition , the department did not concur with our recommendation to update its cio ratings on a monthly basis .

specifically , the department states that its semi - annual reporting is consistent with fitara requirements and is documented in the department's omb - approved fitara implementation plan .

as noted earlier , we recognize omb's plans to remove the monthly reporting requirement for cio ratings and have modified this recommendation to reflect that planned change .

we acknowledge that when this new policy is finalized , defense's semi - annual reporting may be in compliance with the new requirement .

until such time , agencies are still required by existing policy to report monthly and consequently , we believe that our recommendation is appropriate .

defense's written comments are provided in appendix v. in written comments , education concurred with our recommendations to factor active risks into its cio ratings and to have cio ratings reflect the level of risk facing an investment relative to that investments' ability to accomplish its goals , and described plans to implement those recommendations .

specifically , the department will include consideration of active risks when formulating its cio ratings and its investment review board chair will provide specific guidance that the cio should ensure the ratings reflect the level of risk facing an investment .

the department partially concurred with our recommendation to update cio ratings monthly , stating that omb's fiscal year 2017 it budget guidance addresses the required frequency of updates in several places , and the section specific to cio evaluations only requires agencies to update their ratings as soon as new information becomes available .

while we agree that omb's fiscal year 2017 guidance does address dashboard reporting frequency in several places , the requirement for monthly updates is nonetheless explicitly stated and was confirmed by omb staff .

however , as noted earlier , we recognize omb's plans to remove the monthly reporting requirement for cio ratings and have modified this recommendation to reflect that planned change .

we acknowledge that when this new policy is finalized , education's reporting may be in compliance with the new requirement .

until such time , agencies are still required by existing policy to report monthly and consequently , we believe that our recommendation is appropriate .

education's written comments are provided in appendix vi .

in written comments , energy concurred with our recommendations and noted that the office of the cio would work collaboratively with it executives to address the recommendations .

energy also provided technical comments , which we have incorporated as appropriate .

energy's written comments are provided in appendix vii .

in written comments , hhs concurred with our recommendations , but noted that the recommendations were based on a now - outdated department methodology and that a new methodology , which went into effect in january 2016 , addresses our recommendations .

hhs also provided technical comments , which we have incorporated as appropriate .

hhs's written comments are provided in appendix viii .

in written comments , dhs concurred with our recommendation to reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

the department did not concur with our recommendation to update its cio ratings on a monthly basis , specifically noting that its risk - based process complies with omb's fiscal year 2017 capital planning guidance with regards to the frequency of its updates to the it dashboard , and that investments receive health assessments on a risk - based basis , either monthly , quarterly , or semi - annually .

however , we disagree with the assertion that this update frequency aligns with current omb guidance , which explicitly requires that the dashboard be updated at least monthly .

however , as noted earlier , we recognize omb's plans to remove the monthly reporting requirement for cio ratings and have modified this recommendation to reflect that planned change .

we acknowledge that when this new policy is finalized , dhs's reporting may be in compliance with the new requirement .

until such time , agencies are still required by existing policy to report monthly and consequently , we believe that our recommendation is appropriate .

dhs also provided technical comments , which we have incorporated as appropriate .

dhs's written comments are provided in appendix ix .

in written comments , interior concurred with our recommendation and noted it is currently enhancing the department's cio ratings process to maximize standardization across investment cio ratings and to strengthen the assessment of active risks .

interior's written comments are provided in appendix x .

in written comments , state agreed with our recommendations , noting that the department currently analyzes active risks and reviews those risks on a monthly basis .

however , our review found that while state's cio ratings process included compliance with risk management issues ( eg , ensuring that the risk register is being updated and that mitigation strategies are properly planned ) , the department did not review active risks by evaluating the probability and impact of individual investment risks and applying that knowledge to the cio ratings .

by considering active risks , state can increase the chance that the ratings will better reflect the true likelihood of investment success .

state's written comments are presented in appendix xi .

in comments provided via e - mail on april 27 , 2016 , transportation's director of audit relations and program improvement in the office of the secretary stated that the department concurred with our recommendation .

in comments provided via e - mail on may 10 , 2016 , a gao liaison from treasury's office of the cio did not comment on our recommendation .

treasury also provided technical comments , which we have incorporated as appropriate .

in written comments , va concurred with our recommendations and described actions the department is taking to address both of them .

for our recommendation to factor active risks into its it dashboard cio ratings , va indicated that it would amend its current monthly process to include a requirement for investment managers to review at least the top three active operational risks .

additionally , for our recommendation to ensure that cio ratings reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals , va plans , among other things , to amend its current monthly process to include a requirement for investment managers to assess operational risks that detail the probability and impact of pending threats to success .

va's written comments are presented in appendix xii .

in written comments , epa did not agree with our recommendation .

while agreeing that cio ratings should reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals , the agency asserted that its current process already allows for this through the criteria used to determine an investment's cio rating .

epa specifically cited an epa investment that was rated yellow , but we assessed as red .

further , epa indicated that this disagreement does not mean that epa's process does not consider risks .

we agree that the difference between epa's cio rating and our assessment does not necessarily mean that epa does not factor risk or the investment's ability to accomplish its goals .

however , it does indicate that the level of risk may be underreported .

consequently , we believe that our recommendation is appropriate .

epa also provided technical comments , which we have incorporated as appropriate .

epa's written comments are provided in appendix xiii .

in written comments , opm concurred with our recommendation and stated the agency will begin factoring active risks into cio ratings .

opm's written comments are provided in appendix xiv .

in written comments , ssa agreed with our recommendation to update its cio ratings on a monthly basis .

however , as noted earlier , we recognize omb's plans to remove the monthly reporting requirement for cio ratings and have modified this recommendation to reflect that planned change .

ssa partially agreed with our recommendation to reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

noting that the agency head and the cio should work together to appropriately consider investment risk , the agency disagreed with the implication that these individuals were not doing so .

additionally , ssa stated that it was too early in the implementation of fitara to conclude that following omb's related guidance would result in agencies misestimating risk and that our report should not imply that ssa's risk assessments do not fulfill the legislative intent of fitara .

while we recognize the collaborative efforts of ssa's executives , our assessments nevertheless showed more risk for four out of nine selected ssa investments .

further , three of the four were rated green by ssa but assessed as red by us , indicating the possibility that these cio ratings did not fully reflect the risk being faced by these investments .

additionally , our report does not conclude that omb's guidance leads to agencies misestimating risk , but rather that agencies' processes are understating risk .

as such , we believe that our recommendation is warranted .

ssa's written comments are provided in appendix xv .

comments from the agencies to which we did not make recommendations are discussed in more detail here .

in comments provided via e - mail on april 18 , 2016 , a representative from labor's office of the assistant secretary for administration and management stated that the department had no comments on the report .

in comments provided via e - mail on april 15 , 2016 , a representative from gsa's gao / ig audit response division stated that the agency had no comments on the report .

we are sending copies of this report to interested congressional committees ; the secretaries of agriculture , commerce , defense , homeland security , education , energy , health and human services , the interior , labor , state , transportation , the treasury , and veterans affairs ; the administrator of the environmental protection agency ; the administrator of the general services administration ; the director of the office of personnel management ; the commissioner of the social security administration ; the director of the office of management and budget ; and other interested parties .

this report will also be available at no charge on our website at http: / / www.gao.gov .

if you or your staff have any questions on matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix xvi .

our objectives were to ( 1 ) describe agencies' processes for determining the chief information officer ( cio ) risk ratings for major information technology ( it ) investments and ( 2 ) assess the risk of federal it investments and analyze any differences with the investments' cio risk ratings .

to select the agencies and investments , we reviewed data reported to omb as part of the federal budget process to identify major investments that planned to spend at least 80 percent of their fiscal year 2015 funding on development , modernization , and enhancement activities .

this produced a list 17 agencies and 107 selected investments .

these agencies were: the departments of agriculture ( agriculture ) , commerce ( commerce ) , defense ( defense ) , homeland security ( dhs ) , education ( education ) , energy ( energy ) , health and human services ( hhs ) , the interior ( interior ) , labor ( labor ) , state ( state ) , transportation ( transportation ) , the treasury ( treasury ) , and veterans affairs ( va ) , as well as the environmental protection agency ( epa ) , the general services administration ( gsa ) , the social security administration ( ssa ) , and the office of personnel management ( opm ) .

appendix iii contains a complete listing of the selected agencies and investments .

to address our first objective , we met with the 17 selected agencies to discuss their cio rating processes and collected relevant documentation , such as capital planning and investment control guides , program health assessment guidance , and rating processes .

we then compared the agencies' processes to the office of management and budget's ( omb ) suggested evaluation factors for creating cio ratings .

these evaluation factors are: ( 1 ) risk management , ( 2 ) requirements management , ( 3 ) contractor oversight , ( 4 ) historical performance , ( 5 ) human capital , and ( 6 ) other ( i.e. , the other factors that the cio deems important to forecasting future success ) .

we also analyzed the agencies' documents and interviewed officials to determine how the agencies' use of omb's factors varied .

to address our second objective , we reviewed the 107 investments , but excluded 12 after agencies told us that: 2 were inactive in our period of review , 5 were not primarily in development , 4 were too new to have a risk register ( a key document for our assessments ) , and 1 was managed as part of a larger development program and did not have its own risk register .

these exclusions eliminated the 2 selected investments from labor and the 1 from opm .

this resulted in 95 investments at 15 agencies .

we made the decision to review the ratings from april 2015 , the month that our audit work began , in order to minimize any influence that our ongoing work could have on the agencies' processes and resulting ratings .

we first downloaded the april ratings , interviewed appropriate officials at the 15 selected agencies , and collected march 2015 risk documentation ( the data we would expect to be reflected in the april ratings ) , performance data , review board briefings , and relevant reports ( eg , gao and inspector general reports ) .

in cases where agencies were unable to provide march documentation , we used documents from the closest available date .

we did not consider risks that were introduced after march in these documents .

we used this information to assess each selected investment's overall investment risk and compared the result to the april 2015 cio ratings .

according to omb's guidance , cio ratings “should reflect the cio's assessment of the risk and the investment's ability to accomplish its goals.” to create our assessments of risk , we combined each investment's detailed risk lists , known as risk registers , with several additional metrics .

specifically , we combined the probability and impact of every active risk in the risk registers of each of the selected investments to determine what is known as the exposure of each risk .

these exposure scores ranged from “very low” to “very high” and were based upon industry and government best practices .

table 10 shows how probability and impact values derived from these sources were combined to determine risk exposure .

we then weighted each risk exposure , placing significantly increased emphasis on higher risks so that they were not canceled out by lower risks .

table 11 lists the weights we assigned to the exposures .

we then averaged these weights and translated the result into green , yellow , and red grades according to the following scale .

for example , we would assess the following risk register as yellow .

we then lowered the assessments based on consideration of the following elements: ( 1 ) cost and schedule data from the it dashboard , ( 2 ) relevant review board briefings , and ( 3 ) relevant reports ( eg , gao and inspector general reports ) .

specifically , we first reviewed each investment's project - level cost and schedule data from the dashboard as of april 2015 and if more than half of its projects' variances were colored red by the dashboard , we lowered our assessment one level ( i.e. , from green to yellow or from yellow to red ) .

as a result of this review , we identified a total of 19 investments ; however , 11 of those could not be lowered further as we had already assessed them as red .

the grades for the remaining 8 investments were lowered one level .

in particular , 6 grades were lowered from yellow to red and 2 were lowered from green to yellow .

then , we examined review board briefings covering march 2015 , as well as relevant gao and inspector general reports , and lowered our assessment if we deemed the identified issues represented serious risks to the investment and remained relevant in april 2015 .

using this approach , we considered lowering 12 of our assessments due to such information ; however , 11 of the 12 assessments were already either red or were already lowered due to cost and schedule issues .

consequently , the only investment for which we lowered our assessment was defense's warfighter information network - tactical increment 2 , which was reduced from yellow to red .

a briefing for that investment showed that the program's cost variance triggered what is known as a nunn - mccurdy breach , that the program was being restructured , that its scope was being reduced , and we had recently reported that the program struggled to demonstrate required performance and reliability during operational testing .

we then compared our assessment to the cio ratings on the dashboard , discussed our findings with agency officials , and corroborated the dashboard's data with agency officials .

our calculations are only intended to provide a standardized view of risk across all the departments and investments we reviewed , and this methodology is not intended to serve as a prescriptive approach to the agencies' evaluation of investment risk .

we conducted this performance audit from april 2015 to june 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

table 14 describes the processes that the selected agencies reported as using to create the chief information officer ( cio ) ratings for their investments .

table 15 lists the selected agencies and investments , including those which we exempted ( as discussed in appendix i and shaded in gray ) , as well as the associated chief information officer ( cio ) ratings and our assessments .

in addition to the contact named above , individuals making contributions to this report included dave hinchman ( assistant director ) , karl seifert ( assistant director ) , kevin walsh ( assistant director ) , andrew banister , rebecca eyler , sandra kerr , and meredith raymond .

