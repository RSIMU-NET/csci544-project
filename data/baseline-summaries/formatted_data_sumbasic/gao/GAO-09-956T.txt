we are pleased to be here today to testify on our past work on the small business innovation research ( sbir ) program .

as you know , to be competitive in the global economy , the united states relies heavily on innovation through research and development ( r&d ) .

recognizing the potential of small businesses to be a source of significant innovation , the congress passed the small business innovation development act of 1982 .

the act established the sbir program to stimulate technological innovation , use small businesses to meet federal r&d needs , foster and encourage participation by minority and disadvantaged persons in technological innovation , and increase private sector commercialization of innovations derived from federal r&d .

the act provided for a three - phased program: phase i to determine the feasibility and scientific and technical merit of a proposed research idea ; phase ii to further develop the idea ; and phase iii to commercialize the resulting product or process with no further sbir funding .

federal agencies that have budgets of $100 million for research conducted by others , called extramural research , are required to use 2.5 percent of these budgets to establish and operate an sbir program .

currently , 11 federal agencies participate in the sbir program .

each agency manages its own program , including targeting research areas , reviewing proposed projects , and making research awards through grants , contracts , or cooperative agreements .

the small business administration ( sba ) plays a central administrative role by , for example , issuing policy directives to the participating federal agencies , collecting data from participating agencies on awards and recipients , and reporting program results annually to the congress .

in 2005 awards from three agencies — the department of defense ( dod ) , national institutes of health ( nih ) , and national aeronautics and space agency ( nasa ) — accounted for the majority of sbir funds .

from its inception in fiscal year 1983 through fiscal year 2004 , federal agencies had awarded over $17 billion for more than 82,000 projects .

since it was established in 1982 , the sbir program has been reauthorized and modified by the congress at various times .

for example , the small business research and development enhancement act of 1992 directed sba and participating agencies to , among other things , emphasize the goal of increasing commercialization of research results and to improve the government's dissemination of program - related data .

as a result , agencies were required to include commercialization potential as a criterion for selecting award recipients .

during this same period , sba began to develop a publicly available database , known as tech - net , that contained information on all awards made through the sbir program .

the tech - net database is intended to be , among other things , an electronic gateway of technology information and resources for researchers , scientists , and government officials about federally funded , leading edge technology research .

the small business innovation research program reauthorization act of 2000 formalized this database by requiring sba to develop , maintain , and make available to the public a searchable , up - to - date , electronic database that contained sbir award information .

the 2000 reauthorization act also required sba to develop and maintain another restricted government database that would contain additional information on commercialization not contained in the public tech - net database , thereby allowing better evaluations of the sbir program on an ongoing basis .

this database was to be established by mid - 2001 and made available only to government agencies and certain other authorized users .

sba has established , through a policy directive , a series of data elements for all the agencies to submit for its public tech - net database .

the sbir program is currently being considered by the congress for reauthorization , and both the house and senate have recently passed bills to reauthorize the program .

in this context , you asked us to summarize the successes and challenges that our past work has identified about the sbir program , summarize the concerns we have previously identified on sba's efforts to establish an interagency database that includes information on sbir applicants and awards , and describe the process that agencies use to determine the eligibility of sbir applicants for the program .

this statement is based largely on our prior reviews of the sbir program and contacts with sba officials .

our work on the prior reviews was conducted in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audits to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence we obtained for those reviews provided a reasonable basis for our findings and conclusions based on our audit objectives .

our reviews of the sbir program between 1985 and 1999 found numerous examples of program successes such as the following: funding high - quality research .

throughout the life of the program , awards have been based on technical merit and are generally of good quality .

encouraging widespread competition .

the sbir program successfully attracts many qualified companies , has had a high level of competition , consistently has had a high number of first - time participants , and attracts hundreds of new companies annually .

providing effective outreach .

sbir agencies consistently reach out to foster participation by women - owned or socially and economically disadvantaged small businesses by participating in regional small business conferences and workshops targeting these types of small businesses .

increasing successful commercialization .

at various points in the life of the program we have reported that sbir has succeeded in increasing private sector commercialization of innovations .

helping to serve mission needs .

sbir has helped serve agencies' missions and r&d needs , although we found that agencies differ in the emphasis they place on funding research to support their mission versus more generalized research .

our reviews of the sbir program during that time have also identified a number of areas of weakness that , over time , have been either fully or partially addressed by the congress in reauthorizing the program or by the agencies themselves .

for example , duplicate funding .

in 1995 , we identified duplicate funding for similar , or even identical , research projects by more than one agency .

a few companies received funding for the same proposals two , three , and even five times before agencies became aware of the duplication .

contributing factors included the fraudulent evasion of disclosure by companies applying for awards , the lack of a consistent definition for key terms such as “similar research,” and the lack of interagency sharing of data on awards .

to address these concerns , we recommended that sba take three actions: ( 1 ) determine if the certification form needed to be improved and make any necessary revisions , ( 2 ) develop definitions and guidelines for what constitutes “duplicative” research , and ( 3 ) provide interagency access to current information regarding sbir awards in response to our recommendations , sba strengthened the language agencies use in their application packages to clearly warn applicants about the illegality of entering into multiple agreements for essentially the same effort .

in addition , sba planned to develop internet capabilities to provide sbir data access for all of the agencies .

inconsistent interpretations of extramural research budgets .

in 1998 , we found that while agency officials adhered to sbir's program and statutory funding requirements , they used differing interpretations of how to calculate their “extramural research budgets.” as a result , some agencies were inappropriately including or excluding some types of expenses .

we recommended that sba provide additional guidance on how participating agencies were to calculate their extramural research budgets .

the congress addressed this program weakness in 2000 , when it required that the agencies report annually to sba on the methods used to calculate their extramural research budgets .

geographical concentration of awards .

in 1999 , in response to congressional concerns about the geographical concentration of sbir awards , we reported that companies in a small number of states , especially california and massachusetts , had submitted the most proposals and won the majority of awards .

the distribution of awards generally followed the pattern of distribution of non - sbir expenditures for r&d , venture capital investments , and academic research funds .

we reported that some agencies had undertaken efforts to broaden the geographic distribution of awards .

in the 2000 reauthorization of the program , the congress directed the sba administrator to establish the federal and state technology ( fast ) partnership program to help strengthen the technological competitiveness of small businesses , especially in those states that receive fewer sbir grants .

the fast program was not reauthorized when it expired in 2005 .

in 2006 when we looked at the geographical concentration of awards made by dod and nih , we found that while a firm in every state received at least one sbir award from both agencies , sbir awards continued to be concentrated in a handful of states and about one third of awards had been made to firms in california and massachusetts .

clarification on commercialization and other sbir goals .

finally , in 2000 , the congress directed the sba administrator to require companies applying for a phase ii award to include a commercialization plan with their sbir proposals .

this addressed our continuing concern that clarification was needed on the relative emphasis that agencies should give to a company's commercialization record and sbir's other goals when evaluating proposals .

in addition , in 2001 , sba initiated efforts to develop standard criteria for measuring commercial and other outcomes of the sbir program and incorporate these criteria into its tech - net database .

in fiscal year 2002 , sba further enhanced the reporting system to include commercialization results that would help establish an initial baseline rate of commercialization .

in addition , small business firms participating in the sbir program are required to provide information annually on sales and investments associated with their sbir projects .

many of the solutions cited above to improve and strengthen the sbir program relied to some extent on the collection of data or the establishment of a government - use database , so that sba and participating agencies could share information and enhance their efforts to monitor and evaluate the program .

however , in 2006 , we reported that sba was 5 years behind schedule in complying with the congressional mandate to develop a government database that could facilitate agencies' monitoring and evaluation of the program .

we also reported that the information sba was collecting for the database was incomplete and inconsistent , thereby limiting its usefulness for program evaluations .

specifically , we identified the following concerns with sba's data - gathering efforts: sba had not met its obligation to implement a restricted government - use database that would allow sbir program evaluation as directed by the 2000 sbir reauthorization act .

as outlined in the legislation , sba , in consultation with federal agencies participating in the sbir program , was to develop a secure database by june 2001 and maintain it for program evaluation purposes by the federal government and certain other entities .

sba planned to meet this requirement by expanding the existing tech - net database to include a restricted government - use section that would be accessible only to government agencies and other authorized users .

in constructing the government - use section of the database , sba planned to supplement data already gathered for the public - use section of the tech - net database with information from sbir recipients and from participating agencies on commercialization outcomes for phase ii sbir awards .

however , according to sba officials , the agency was unable to meet the statutory requirement , primarily because of increased security and other information technology project requirements , agency management changes , and budgetary constraints .

when we reported on this lack of compliance with the database mandate , sba told us that it anticipated having the government - use section of the tech - net database operational early in fiscal year 2007 .

however , according to an sba official , the database became operational in october 2008 , and agencies have begun to provide data on their sbir programs using the internet .

while federal agencies participating in the sbir program submitted a wide range of descriptive award information to sba annually , these agencies did not consistently provide all of the required data elements .

as outlined in sba's policy directive , each year , sbir participating agencies are required to collect and maintain information from recipients and provide it to sba so that it can be included in the tech - net database .

specifically , the policy directive established over 40 data elements for participating agencies to report for each sbir award they make ; a number of these elements are required .

these data include award - specific information , such as the date and amount of the award , an abstract of the project funded by the award , and a unique tracking number for each award .

participating agencies are also required to provide data about the award recipient , such as gender and socio - economic status , and information about the type of firms that received the awards , such as the number of employees and geographic location .

much of the data participating agencies collected are provided by the sbir applicants when they apply for an award .

agencies provide additional information , such as the grant / contract number and the dollar amount of the award , after the award is made .

for the most part , all of the agencies we reviewed in 2006 provided the majority of the data elements outlined in the policy directive .

however , some of the agencies were not providing the full range of required data elements .

as a result , sba did not have complete information on the characteristics of all sbir awards made by the agencies .

sba officials told us that agencies did not routinely provide all of the data elements outlined in the policy directive because either they did not capture the information in their agency databases or they were not requesting the information from the sbir applicants .

officials at the participating agencies cited additional reasons for the incomplete data they provided to sba .

for example , some officials noted that sba's tech - net annual reporting requirements often change and others said that if the company or contact information changes and the sbir recipient fails to provide updated information to the agency , the agency cannot provide this information to sba .

participating agencies were providing some data that are inconsistent with sba's formatting guidance , and while some of these inconsistencies were corrected by sba's quality assurance processes , others were not .

in 2006 , we determined that almost a quarter of the data provided by five of the eight agencies we reviewed was incorrectly formatted for one or more fields in the tech - net database .

as a result , we concluded that these inconsistent or inaccurate data elements compromised the value of the database for program evaluation purposes .

sba's quality assurance efforts focus on obtaining complete and accurate data for those fields essential to tracking specific awards , such as the tracking number and award amount , rather than on those fields that contain demographic information about the award recipient .

we found that sba electronically checked the data submitted by the participating agencies to locate and reformat inconsistencies , but it did not take steps to ensure that all agency - provided data were accurate and complete .

we also determined that inconsistencies or inaccuracies could arise in certain data fields because sba interpreted the absence of certain data elements as a negative entry without confirming the accuracy of such an interpretation with the agency .

as we reported in 2006 , such inaccuracies and inconsistencies were a concern because information in the tech - net database would be used to populate the government - use section of the database that sba was developing ( as discussed above ) to support sbir program evaluations .

however , at the time of our review , sba had no plans to correct any of the errors or inconsistencies in the database that related to the historical data already collected .

as a result , we concluded that the errors in the existing database would migrate to the government - use section of the database and would compromise the usefulness of the government - use database for program evaluation and monitoring purposes .

to address the concerns that we identified with regard to the quality of the data that sba was collecting for the tech - net database , we recommended in our 2006 report that sba work with the participating agencies to strengthen the completeness , accuracy , and consistency of its data collection efforts .

according to an sba official , the database is currently operational and agencies have entered data for fiscal years 2007 and 2008 over the internet .

moreover , according to this official , the system is set up in such a way that it does not accept incorrectly formatted data .

in 2006 , we also found that sba and some participating agencies focused on a few select criteria for determining applicants' eligibility for sbir awards .

specifically , we reviewed dod's , nih's , and sba's processes to determine eligibility of applicants for the sbir program and found that they focused largely on three sbir criteria in their eligibility reviews — ownership , size in terms of the number of employees , and for - profit status of sbir applicants .

although agency officials also told us that they consider information on the full range of criteria , such as whether the principal investigator is employed primarily by the applying firm , and the extent to which work on the project will be performed by others .

moreover , we found that both nih and dod largely relied on applicants to self - certify that they met all of the sbir eligibility criteria as part of their sbir applications .

for example , at nih , applicants certified that they met the eligibility criteria by completing a verification statement when nih notified them that their application had been selected for funding but before nih made the award .

the verification statement directs applicants to respond to a series of questions relating to for - profit status , ownership , number of employees , where the work would be performed , and the primary employment of the principal investigator , among others .

similarly , dod's cover sheet for each sbir application directs applicants to certify that they met the program's eligibility criteria .

nih and dod would not fund applications if the questions on their agency's verification statement or cover sheet were not answered .

both nih and dod also warned applicants of the civil and criminal penalties for making false , fictitious , or fraudulent statements .

in some cases the agencies made additional efforts to ensure the accuracy of the information applicants provided when they observed certain discrepancies in the applications .

in 2006 , we reported that when officials at the agencies had unresolved concerns about the accuracy of an applicant's eligibility information , they referred the matter to sba to make an eligibility determination .

we found that when sba received a letter from the agency detailing its concerns , sba officials contacted the applicants and asked them to re - certify their eligibility status and might request additional documentation on the criteria of concern .

upon making a determination of eligibility , sba then notified the official at the inquiring agency , and the applicant , of its decision .

although , sba made the information about firms it found ineligible publicly available on its web site so that all participating agencies and the public could access the information , we found that it did not consistently include information on the web site identifying whether or not the determination was for the sbir program .

an sba official told us the agency planned to include such information on its web site more systematically before the end of fiscal year 2006 .

once the agencies received information about applicants' eligibility they also had different approaches for retaining and sharing this information .

for example , while both nih and dod noted the determination of ineligibility in the applicant's file , nih also centrally tracked ineligible firms and made this information available to all of its institutes and centers that make sbir awards .

in contrast , dod did not have a centralized process to share the information across its awarding components , although dod officials told us it was common practice for awarding components to share such information electronically .

in conclusion , mr. chairman , while the sbir program is generally recognized as a successful program that has encouraged innovation and helped federal agencies achieve their r&d goals , it has continued to suffer from some long - standing evaluation and monitoring issues that are made more difficult because of a lack of accurate , reliable , and comprehensive information on sbir applicants and awards .

the congress recognized the need for a comprehensive database in 2000 when it mandated that sba develop a government - use database .

although sba did not meet its statutorily mandated deadline of june 2001 , the database has been operational since october 2008 , and contains limited new information but may also contain inaccurate historical data .

mr. chairman , this concludes my prepared statement .

i would be happy to respond to any questions that you or other members of the committee may have .

for further information about this statement , please contact me at ( 202 ) 512-3841 or at daltonp@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this statement .

vondalee hunt , anu mittal , and cheryl williams also made key contributions to this statement .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

