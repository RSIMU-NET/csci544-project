in march 2008 , we designated the 2010 decennial census as a high - risk area , citing a number of long - standing and emerging challenges .

these include weaknesses in managing information technology , operational planning , and cost estimating , as well as uncertainty over dress rehearsal plans and the ultimate cost of the census .

because the census is fundamental for many government decisions , threats to a timely and reliable census can affect the public's confidence in government .

from may to june 2007 , the u.s. census bureau ( bureau ) conducted the address canvassing operation of the 2008 dress rehearsal .

this operation was the bureau's final opportunity to test , under census - like conditions , handheld computers ( hhc ) developed by the contractor that will be deployed during the 2010 census address canvassing operation — scheduled to take place in the spring of 2009 .

in previous decennial censuses , the bureau relied on a paper - based operation .

according to the bureau , the hhcs were to be a keystone to the reengineered census because they were to be used in developing an accurate address list for the bureau and in obtaining information from households that fail to return census forms .

the bureau believed that the hhcs would reduce the amount of paper used , process data in real time , and improve the quality of the data .

however , at a march 2008 hearing , the department of commerce ( commerce ) and the bureau stated that the field data collection automation ( fdca ) program , under which the hhcs are being developed , was likely to incur significant cost overruns and announced a redesigning effort to get the 2010 decennial census back on track .

the secretary of commerce outlined several alternatives for redesigning this central technology investment , and on april 3 , 2008 , he decided to continue with the hhcs for address canvassing .

during redesign deliberations , bureau officials pointed out that it was too late in the decennial cycle to consider dropping the use of the hhcs for address canvassing in 2009 .

they considered that with hard deadlines fast approaching , there was not enough time to revert to a paper - based address canvassing operation .

the decision to use the hhcs in the 2010 address canvassing operation makes it critical that any problems identified with the hhcs in the dress rehearsal are resolved quickly and that the bureau understand the implications of proceeding with this technology .

continued oversight of 2010 census preparation is critical as the bureau is redesigning operations late in the decennial cycle and relying on new technology to modernize its address listing and mapping activities .

to respond to your interest in performance of the hhcs during 2008 dress rehearsal address canvassing , we examined whether the hhcs worked in collecting and transmitting address and mapping data .

as part of this subcommittee's ongoing oversight of the 2010 census , we testified in april 2008 on our preliminary observation of weaknesses with hhc performance and the potential implications for the 2010 census .

we also raised the importance of performance measures and planning , recommending that the bureau establish specific quantifiable measures in such areas as productivity and performance .

at the subcommittee's request , we ( 1 ) analyzed bureau and contractor data showing how hhcs operated and its implications on operations , and ( 2 ) examined implications the redesign may have on plans for address canvassing in the 2010 census .

in responding to these objectives , we reviewed bureau planning documents , data on hhc performance and staff productivity , evaluation reports , and staff observations of address canvassing operations .

we reviewed contract documents , help desk logs , contractor data on transmissions , and contractor evaluations of hhc performance .

we also interviewed bureau and contractor officials to determine the functionality of the hhcs during dress rehearsal address canvassing .

finally , we visited the two dress rehearsal sites in california and north carolina to attend address canvassing lister training and to observe and document the use of the hhcs in the field during the dress rehearsal in the summer of 2007 .

appendix i provides more detail on our scope and methodology .

we conducted this performance audit from april 2007 to july 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in preparation for the 2010 census , the address canvassing operation was tested as part of the 2008 dress rehearsal .

from may 7 to june 25 , 2007 , the bureau conducted its address canvassing operation for its 2008 dress rehearsal in selected localities in california ( see fig .

1 ) and north carolina ( see fig .

2 ) .

the 2008 census dress rehearsal took place in san joaquin county , california , and nine counties in the fayetteville , north carolina , area .

according to the bureau , the dress rehearsal sites provided a comprehensive environment for demonstrating and refining planned 2010 census operations and activities , such as the use of hhcs equipped with global positioning system ( gps ) .

prior to census day , bureau listers perform the address canvassing operation , during which they verify the addresses of all housing units .

address canvassing is a field operation to help build a complete and accurate address list .

the bureau's master address file ( maf ) is intended to be a complete and current list of all addresses and locations where people live or potentially live .

the topographically integrated geographic encoding and referencing ( tiger® ) database is a mapping system that identifies all visible geographic features , such as type and location of streets , housing units , rivers , and railroads .

consequently , maf / tiger® provides a complete and accurate address list ( the cornerstone of a successful census ) because it identifies all living quarters that are to receive a census questionnaire and serves as the control mechanism for following up with households that do not respond .

if the address list is inaccurate , people can be missed , counted more than once , or included in the wrong location ( s ) .

generally , during address canvassing , census listers go door to door verifying and correcting addresses for all households and street features contained on decennial maps .

the address listers add to the 2010 census address list any additional addresses they find and make other needed corrections to the 2010 census address list and maps using gps - equipped hhcs .

listers are instructed to compare what they discover on the ground to what is displayed on their hhc .

as part of the 2004 and 2006 census tests , the bureau produced a prototype of the hhc that would allow the bureau to automate operations , and eliminate the need to print millions of paper questionnaires , address registers , and maps used by temporary listers to conduct address canvassing and non - response follow - up as well as to allow listers to electronically submit their time and expense information .

the hhcs for these tests were off - the - shelf computers purchased and programmed by the bureau .

while the bureau was largely testing the feasibility of using hhcs for collecting data , it encountered a number of technical problems .

the following are some of the problems we observed during the 2004and 2006 tests: slowness and frequent lock - up , problems with slow or unsuccessful transmissions , and difficulty in linking a mapspot to addresses for multi - unit structures .

for the 2008 dress rehearsal and the 2010 census , the bureau awarded the development of the hardware and software for a hhc to a contractor .

in march 2006 , the bureau awarded a 5-year contract of $595,667,000 to support the fdca project .

the fdca project includes the development of hhcs , and bureau officials stated that the hhcs would ultimately increase the efficiency and reduce costs for the 2010 census .

according to the director of the census bureau , the fdca program was designed to supply the information technology infrastructure , support services , hardware , and software to support a network for almost 500 local offices and for hhcs that will be used across the country .

he also indicated that fdca can be thought of as being made up of three fundamental components: ( 1 ) automated data collection using handheld devices to conduct address canvassing , and to collect data during the non - response follow - up of those households that do not return the census form ; ( 2 ) the operations control system ( ocs ) that tracks and manages decennial census workflow in the field ; and ( 3 ) census operations infrastructure , which provides office automation and support for regional and local census offices .

the 2008 dress rehearsal address canvassing operation marked the first time the contractor - built hhcs and the operations control system were used in the field .

in 2006 , we reported that not using the contractor - built hhcs until 2008 dress rehearsal address canvassing would leave little time to develop , test , and incorporate refinements to the hhcs in preparation for the 2010 census .

we also reported that because the bureau - developed hhc had performance problems , the introduction of a new hhc added another level of risk to the success of the 2010 census .

for the 2008 dress rehearsal , the fdca contractor developed the hardware and software used in census offices and on the hhcs .

see figure 3 for more details .

the hhc included several applications that varied depending on the role of the user: software enabling listers to complete their time and expense electronically ; text messaging software enabling listers to communicate via text message ; software enabling staff to review all work assigned to them and enabling crew leaders to make assignments ; software enabling staff to perform address canvassing ; and an instrument enabling quality control listers to perform quality assurance tasks .

the dress rehearsal address canvassing started may 7 , 2007 , and ended june 25 , 2007 , as planned .

the bureau reported in its 2008 census dress rehearsal address canvassing assessment report being able to use the hhc to collect address information for 98.7 percent of housing units visited and map information for 97.4 percent of the housing units visited .

there were 630,334 records extracted from the bureau's address and mapping database and sent to the bureau's address canvassing operation and 574,606 valid records following the operation .

mapspots ( mapping coordinates ) were collected for each structure that the bureau defined as a housing unit , other living quarters , or uninhabitable .

each single - family structure received its own mapspot , while multi - unit structures shared a single mapspot for all the living quarters within that structure .

according to the bureau's 2008 dress rehearsal address canvassing assessment report , the address canvassing operation successfully collected gps mapspot coordinates in the appropriate block for approximately 92 percent of valid structures ; most of the remaining 8 percent of cases had a manual coordinate that was used as the mapspot .

it is not clear whether this represents acceptable performance because the bureau did not set thresholds as to what it expected during the address canvassing dress rehearsal .

listers experienced multiple problems using the hhcs .

for example , we observed and the listers told us that they experienced slow and inconsistent data transmissions from the hhcs to the central data processing center .

the listers reported the device was slow to process addresses that were a part of a large assignment area .

bureau staff reported similar problems with the hhcs in observation reports , help desk calls , and debriefing reports .

in addition , our analysis of bureau documentation revealed problems with the hhcs consistent with those we observed in the field: bureau observation reports revealed that listers most frequently had problems with slow processing of addresses , large assignment areas , and transmission .

the help desk call log revealed that listers most frequently reported issues with transmission , the device freezing , mapspotting , and large assignment areas .

the bureau's debriefing reports illustrated the impact of the hhcs problems on address canvassing .

for example , one participant commented that the listers struggled to find solutions to problems and wasted time in replacing the devices .

collectively , the observation reports , help desk calls , debriefing reports , and motion and time study raised serious questions about the performance of the hhcs during the address canvassing operation .

the bureau's 2008 dress rehearsal address canvassing assessment report cited several problems with hhcs .

for example , the bureau observed the following problems: substantial software delays for assignment areas with over 700 housing substantial software delays when linking mapspots at multi - unit unacceptable help desk response times and insufficient answers , which “severely” affected productivity in the field , and inconsistencies with the operations control system that made management of the operation less efficient and effective .

the assessment reported 5,429 address records with completed field work were overwritten during the course of the dress rehearsal address canvassing operation , eliminating the information that had been entered in the field .

the bureau reported that this occurred due to an administrative error that assigned several hhcs the same identification number .

upon discovering the hhc mistake , the fdca contractor took steps during the dress rehearsal address canvassing operation to ensure that all of the hhc devices deployed for the operation had unique identification numbers .

left uncorrected , this error could have more greatly affected the accuracy of the bureau's master address list during dress rehearsal .

the hhcs are used in a mobile computing environment where they upload and download data from the data processing centers using a commercial mobile broadband network .

the data processing centers housed telecommunications equipment and the central databases , which were used to communicate with the hhcs and manage the address canvassing operation .

the hhcs download data , such as address files , from the data processing centers , and upload data , such as completed work and time and expense forms , to the data processing centers .

the communications protocols used by the hhcs were similar to those used on cellular phones to browse web pages on the internet or to access electronic mail .

for hhcs that were out of the coverage area of the commercial mobile broadband network or otherwise unable to connect to the network , a dial - up capability was available to transfer data to the data processing centers .

fdca contract officials attributed hhc transmission performance problems to this mobile computing environment , specifically: telecommunication and database problems that prevented the hhc from communicating with the data center , extraneous data being transmitted ( such as column and row headings ) , an unnecessary step in the data transmission process .

when problems with the hhc were identified during address canvassing , the contractor downloaded corrected software in five different instances over the 7-week period of the dress rehearsal address canvassing operation .

after address canvassing , the bureau established a review board and worked with its contractor to create task teams to address fdca performance issues such as ( 1 ) transmission problems relating to the mobile computing environment , ( 2 ) the amount of data transmitted for large assignment areas , and ( 3 ) options for improving hhc performance .

one factor that may have contributed to these performance problems was a compressed schedule that did not allow for thorough testing before the dress rehearsal .

given the tighter time frames going forward , testing and quickly remedying issues identified in these tests becomes even more important .

productivity results were mixed when census listers used the hhc for address canvassing activities .

a comparison of planned versus reported productivity reveals lister productivity exceeded the bureau's target by almost two housing units per hour in rural areas , but missed the target by almost two housing units per hour in urban / suburban areas .

further , the reported productivity for urban / suburban areas was more than 10 percent lower than the target , and this difference will have cost implications for the address canvassing operation .

table 1 shows planned and reported productivity data for urban / suburban and rural areas .

while productivity results were mixed , the lower than expected productivity in urban / suburban areas represents a larger problem as urban / suburban areas contain more housing units — and therefore a larger workload .

according to the bureau's dress rehearsal address canvassing assessment report , hhc problems appear to have negatively affected listers' productivity .

the bureau's assessment report concluded that “productivity of listers decreased because of the software problems.” however , the extent of the impact is difficult to measure , as are other factors that may have affected productivity .

the effect of decreases in productivity can mean greater costs .

the bureau , in earlier cost estimates , assumed a productivity rate of 25.6 housing units per hour , exceeding both the expected and reported rates for the dress rehearsal .

we previously reported that substituting the actual address canvassing productivity for the previously assumed 25.6 units per hour resulted in a $270 million increase in the existing life - cycle cost estimate .

the bureau has made some adjustments to its cost estimates to reflect its experience with the address canvassing dress rehearsal , but could do more to update its cost assumptions .

we recommended the bureau do so in our prior report .

the bureau took some steps to collect data , but did not fully evaluate the performance of the hhcs .

for instance , the contractor provided the bureau with data such as average transmission times collected from transmission logs on the hhc , as required in the contract .

but the bureau has not used these data to analyze the full range of transmission times , nor how this may have changed throughout the entire operation .

without this information , the magnitude of the handheld computers' performance issues throughout dress rehearsal was not clear .

also , the bureau had few benchmarks ( the level of performance it is expected to attain ) to help evaluate the performance of hhcs throughout the operation .

for example , the bureau has not developed an acceptable level of performance for total number of failed transmissions or average connection speed .

additionally , the contractor and the bureau did not use the dashboard specified in the contract for dress rehearsal activities .

since the dress rehearsal , the bureau has specified certain performance requirements that should be reported on a daily , weekly , monthly , and on an exception basis .

in assessing an “in - house built” model of the hhc , we recommended in 2005 that the bureau establish specific quantifiable measures in such areas as productivity that would allow it to determine whether the hhcs were operating at a level sufficient to help the bureau achieve cost savings and productivity increases .

further , our work in the area of managing for results has found that federal agencies can use performance information , such as that described above , to make various types of management decisions to improve programs and results .

for example , performance information can be used to identify problems in existing programs , identify the causes of problems , develop corrective actions , plan , identify priorities , and make resource allocation decisions .

managers can also use performance information to identify more effective approaches to program implementation .

the bureau had planned to collect certain information on operational aspects of hhc use , but did not specify how it would measure hhc performance .

specifically , sections of the fdca contract require the hhcs to have a transmission log with what was transmitted , the date , time , user , destination , content / data type , and outcome status .

in the weeks leading up to the january 16 , 2008 , requirements delivery , bureau officials drafted a document titled “fdca performance reporting requirements,” which included an array of indicators such as average hhc transmission duration , total number of successful hhc transmissions , total number of failed hhc transmissions , and average hhc connection speed .

such measures may be helpful to the bureau in evaluating its address canvassing operations .

while these measures provide certain useful information , they only cover a few dimensions of performance .

for example , to better understand transmission time performance , it is important to include analyses that provide information on the range of transmission times .

the original fdca contract also requires that the contractor provide near real - time reporting and monitoring of performance metrics on a “control panel / dashboard” application to visually report those metrics from any internet - enabled pc .

such real - time reporting would help the bureau and contractor identify problems during the operation , giving them the opportunity to quickly make corrections .

however , the “control panel / dashboard” application was not used during the dress rehearsal .

the bureau explained that it needed to use the dress rehearsal to identify what data or analysis would be most useful to include on the dashboard it expects to use for address canvassing in 2009 .

in january and february 2008 , the bureau began to make progress in identifying the metrics that will be used in the dashboard .

according to bureau officials , the dashboard will include a subset of measures from the “fdca performance reporting requirements” such as average hhc transmission time and total number of successful and failed hhc transmissions , which would be reported on a daily basis .

between april 28 , 2008 , and may 1 , 2008 , the bureau and its contractor outlined the proposed reporting requirements for the dashboard .

the bureau indicated that the dashboard will be tested during the systems testing phase , which is currently scheduled for november and december 2008 .

they did not specify if the dashboard will be used in the operational field test of address canvassing , which is the last chance for the bureau to exercise the software applications under census - like conditions .

the dress rehearsal address canvassing study assessment plan outlines the data the bureau planned to use in evaluating the use of the hhc , but these data do not allow the bureau to completely evaluate the magnitude of performance problems .

the plan calls for using data such as the number of hhcs shipped to local census offices , the number of defective hhcs , the number of hhcs broken during the dress rehearsal address canvassing operation , the number checked in at the end of the operation , whether deployment affected the ability of staff to complete assignments , software / hardware problems reported through the help desk , the amount of time listers lost due to hardware or software malfunctions , and problems with transmissions .

the plan also called for the collection of functional performance data on the hhcs , such as the ability to collect mapspots .

despite reporting on the data outlined in the study plan , the bureau's evaluation does not appear to cover all relevant circumstances associated with the use of the hhc .

for example , the bureau does not measure when listers attempt transmissions but the mobile computing environment does not recognize the attempt .

additionally , the bureau's evaluation does not provide conclusive information about the total amount of downtime listers experienced when using the hhc .

for example , in the bureau's final 2008 census dress rehearsal address canvassing assessment report , the bureau cites its motion and time study as reporting observed lister time lost due to hardware or software malfunctions as 2.5 percent in the fayetteville and 1.8 percent in the san joaquin county dress rehearsal locations .

the report also notes that the basis for these figures does not include either the downtime between the onset of an hhc error and the last / successful resolution attempt , nor does it include the amount of time a lister spent unable to work due to an hhc error .

these times were excluded because they were not within the scope of the motion and time study of address canvassing tasks .

however , evaluating the full effect of hhc problems should entail accounting for the amount of time listers spend resolving hhc errors or are not engaged in address canvassing tasks due to hhc errors .

because of the performance problems observed with hhcs during the 2008 dress rehearsal , and the bureau's subsequent redesign decision to use the hhcs for the actual address canvassing operation , hhc use will have significant implications for the 2010 address canvassing operation .

in his april 9 , 2008 , congressional testimony , the bureau's director outlined next steps that included developing an integrated schedule for address canvassing and testing .

on may 22 , 2008 , the bureau issued this integrated schedule , which identifies activities that need to be accomplished for the decennial and milestones for completing tasks .

however , the milestones for preparing for address canvassing are very tight and in one case overlap the onset of address canvassing .

specifically , the schedule indicates that the testing and integrating of hhcs will begin in december 2008 and be completed in late march 2009 ; however , the deployment of the hhcs for address canvassing will actually start in february 2009 , before the completion of testing and integration .

it is uncertain whether the testing and integration milestones will permit modification to technology or operations prior to the onset of operations .

separately , the bureau on june 6 , 2008 , produced a testing plan for the address canvassing operation .

this testing plan includes a limited operational field test of address canvassing ; however , the plan does not specify that the dashboard described earlier will be used in this test .

the address canvassing testing plan is a high - level plan that describes a partial redo of the dress rehearsal to validate certain functionality and represents a reasonable approach .

however , it does not specify the basis for readiness of the fdca solution for address canvassing and when and how this determination will occur — when the bureau would say that the contractor's solution meets its operational needs .

field staff reported problems with hhcs when working in large assignment areas during address canvassing .

according to bureau officials , the devices could not accommodate more than 720 addresses — 3 percent of dress rehearsal assignment areas were larger than that .

the amount of data transmitted and used slowed down the hhcs significantly .

in a june 2008 , congressional briefing , bureau officials indicated once other hhc technology issues are resolved the number of addresses the hhcs can accommodate may increase or decrease from the current 720 .

identification of these problems caused the contractor to create a task team to examine the issues , and this team recommended improving the end - to - end performance of the mobile solution by controlling the size of assignment area data delivered to the hhc for address canvassing .

one specific recommendation was limiting the size of assignment areas to 200 total addresses .

however , the redesign effort took another approach and decided that the bureau will use laptops and software used in other demographic surveys to collect information in large blocks ( assignment areas comprise one or more blocks ) .

specifically , the collection of information in large blocks ( those with over 700 housing units ) will be accomplished using existing systems and software known as the demographic area address listing ( daal ) and the automated listing and mapping instrument ( almi ) .

prior to the start of the address canvassing operation , blocks known to have more than 700 housing units would be removed from the scope of the fdca solution .

these blocks will be flagged in the data delivered to the contractor and will not be included for the address canvassing operation .

because this plan creates dual - track operations , bureau officials stated that differences exist in the content of the extracts and that they are currently working to identify the differences and determine how to handle those differences .

additionally , they said that plans for the testing of the large block solution are expected to occur throughout various phases of the testing for address canvassing and will include performance testing , interface testing , and field testing .

the costs for a help desk that can support listers during address canvassing were underestimated during planning and have increased greatly .

originally , the costs for the help desk were estimated to be approximately $36 million , but current estimates have the cost of the help desk rising as high as $217 million .

the increased costs are meant to increase the efficiency and responsiveness of the help desk so that listers do not experience the kind of delays in getting help that they did during the address canvassing dress rehearsal .

for example , the bureau's final assessment of dress rehearsal address canvassing indicated that unacceptable help desk response times and insufficient answers severely affected productivity in the field .

field staff told us that help desk resources were unavailable on the weekends and that they had difficulty getting help .

the increased costs cited above are due in part to improvements to the help desk , such as expanded availability and increased staffing .

lower than expected productivity has cost implications .

in fact , the bureau is beginning to recognize part of this expected cost increase .

specifically , the bureau expects to update assumptions for the number of hours listers may work in a given week .

the model assumes 27.5 hours per week , but the bureau now expects this to be 18 .

this will make it necessary to hire more listers and , therefore , procure more hhcs .

the bureau adjusted its assumptions based on its experience in the dress rehearsal .

our related report recommends updating assumptions and cost estimates .

the dress rehearsal represents a critical stage in preparing for the 2010 census .

this is the time when congress and others should have the information they need to know how well the design for 2010 is likely to work , what risks remain , and how those risks will be mitigated .

we have highlighted some of the risks facing the bureau in preparing for its first major field operation of the 2010 census — address canvassing .

going forward , it will be important for the bureau to specify how it will ensure that this operation will be successfully carried out .

if the solutions do not work in resolving hhc technology issues the bureau will not achieve productivity targets , and decennial costs will continue to rise .

without specifying the basis and time frame for determination of readiness of the fdca address canvassing solution , the bureau will not have the needed assurance that the hhcs will meet its operational needs .

such testing is especially critical for changes to operations that were not part of the address canvassing dress rehearsal .

for example , because data collection in large blocks will be conducted in parallel with the address canvassing operation , and the bureau is currently working to identify the differences in the content of the resulting extracts , it is important that this dual - track be tested to ensure it will function as planned .

furthermore , without benchmarks defining successful performance of the technology , the bureau and stakeholders will be less able to reliably assess how well the technology worked during address canvassing .

although the bureau field tested the hhcs in its dress rehearsal last year , it did not then have in place a dashboard for monitoring field operations .

the bureau's proposal for a limited field operations test this fall provides the last opportunity to use such a dashboard in census - like conditions .

to be most effective , test results , assessments , and new plans need to be completed in a timely fashion , and they must be shared with those with oversight authority as soon as they are completed .

to ensure that the bureau addresses key challenges facing its implementation of the address canvassing operation for the 2010 census , we recommend that the secretary of commerce direct the bureau to take the following four actions: specify the basis for determining the readiness of the fdca solution for address canvassing and when and how this determination will occur — when the bureau would say that the contractor's solution meets its operational needs .

specify how data collection in large blocks will be conducted in parallel with the address canvassing operation , and how this dual - track will be tested in order to ensure it will function as planned .

specify the benchmarks for measures used to evaluate the hhc performance during address canvassing .

use the dashboard to monitor performance of the hhcs in the operational field test of address canvassing .

the secretary of commerce provided written comments on a draft of this report on july 25 , 2008 .

the comments are reprinted in appendix ii .

commerce had no substantive disagreements with our conclusions and recommendations and cited actions it is taking to address challenges gao identified .

commerce offered revised language for one recommendation , which we have accepted .

commerce also provided technical corrections , which we incorporated .

specifically , we revised our recommendation that the bureau “specify the basis for acceptance of the fdca solution for address canvassing and when that acceptance will occur — when the bureau would say it meets its operational needs and accepts it from the contractor” to “specify the basis for determining the readiness of the fdca solution for address canvassing and when and how this determination will occur — when the bureau would say that the contractor's solution meets its operational needs.” also , after further discussion with bureau officials , we provided more specific measures of address and map information successfully collected .

we revised our discussion of the 2004 and 2006 census tests to make clear that the hhc prototype was only used for non - response follow - up in the 2004 test .

finally , we revised our language on their decision to contract the development of hhc hardware and software to address the bureau's concerns about how we characterized the timing of its decision .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to other interested congressional committees , the secretary of commerce , and the director of the u.s. census bureau .

copies will be made available to others upon request .

this report will also be available at no charge on gao's web site at http: / / www.gao.gov .

if you have any questions on matters discussed in this report , please contact mathew j. scirè at ( 202 ) 512-6806 or sciremj@gao.gov , or david a. powner at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

our objectives for this report were to analyze u.s. census bureau ( bureau ) and contractor data showing how handheld computers ( hhc ) operated and its implications on operations , and examine implications the redesign may have on plans for address canvassing in the 2010 census .

to determine how well the hhc worked in collecting and transmitting address and mapping data , and what data the bureau and contractor used in assessing hhc performance during address canvassing , we examined bureau documents , observed hhcs in use , and interviewed bureau and contractor officials .

for example , we reviewed census bureau memos that outline the data on hhc performance the bureau planned to collect .

we reviewed the field data collection automation ( fdca ) contract , focusing specifically on what performance specifications and requirements were included in the contract .

we observed hhc use during dress rehearsal address canvassing , and interviewed bureau officials and contractor officials about hhc use and performance during the dress rehearsal of address canvassing .

specifically , we observed five different listers over the course of 2 days in the fayetteville , north carolina , dress rehearsal site and six different listers over 3 days in the san joaquin county , california , dress rehearsal site .

we also analyzed data on hhc use including data on hhc functionality / usability , hhc log data , the bureau's motion and time study , the bureau's 2008 dress rehearsal assessments , observational and debriefing reports , a log of help desk tickets , and lessons - learned documents .

additionally , we interviewed knowledgeable bureau and contractor officials .

we did not independently verify the accuracy and completeness of the data either input into or produced by the operation of the hhcs .

to better understand how hhc performance affected worker productivity , we attended the dress rehearsal address canvassing training for listers , interviewed bureau officials about hhc performance , and examined data provided in the bureau's motion and time study and other sources related to predicted and reported productivity .

in addition , we identified and analyzed the factors that contribute to hhc performance on aspects of address canvassing productivity .

we examined the bureau's motion and time study results , conducted checks for internal consistency within the reported results , and met with bureau officials to obtain additional information about the methodology used .

the results reported in the study are estimates based on a non - random sample of field staff observed over the course of the address canvassing operation .

within the context of developing estimates for the time it takes address listers to perform address canvassing tasks and successfully resolve certain hhc problems , we determined that these data were sufficiently reliable for the purposes of our analysis .

however , the study's methodology did not encompass a full accounting of the time field staff spent on the job , nor did the report explain how some results attributed to the motion and time study were derived .

we also compared the bureau's expected productivity rates to productivity rates reported to us by the bureau in response to our request for actual productivity data from the 2008 dress rehearsal addressing canvassing operation .

after analyzing the bureau's productivity data , we requested information about how the productivity data figures were calculated in order to assess their reliability .

in reviewing documentation on the methodology and data , we identified issues that raise concerns .

the bureau acknowledged that data for all address field staff were not included in its analysis .

even though the productivity figures reported to us and presented in this report are generally in line with the range of productivity figures shown in the bureau's motion and time study , the missing data , along with the bureau's lack of response to some of our questions about calculations of productivity figures , limit the reliability of these data .

we determined that they are adequate for purposes of this report in that they provide a rough estimate of field worker productivity , but are not sufficiently reliable to be characterized as definitive representation of the actual productivity experienced in the 2008 dress rehearsal address canvassing operation .

to ascertain the implications the redesign may have on plans for address canvassing in the 2010 census , we observed meetings with officials of the bureau , commerce , office of management and budget , and the contractor who were working on the fdca redesign at bureau headquarters .

we also met with the director of the census bureau and analyzed key department of commerce , bureau , and contractor documents including the 2010 census risk reduction task force report and a program update provided by the contractor ( as well as new and clarified requirements ) .

the bureau is in the process of revising some of its plans for conducting address canvassing and had not finalized those plans prior to the completion of this audit .

we conducted this performance audit from april 2007 to july 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact names above , assistant director signora may , stephen ander , thomas beall , jeffrey demarco , richard hung , barbara lancaster , andrea levine , amanda miller , niti tandon , lisa pearson , cynthia scott , timothy wexler , and katherine wulff made key contributions to this report .

