to assess the ability of u.s. forces to execute the wartime missions for which they were designed , as well as other assigned missions , and to make decisions about deploying forces , the department of defense ( dod ) relies heavily on readiness information derived from multiple information systems .

over the years , we and others have identified shortcomings in dod's readiness assessment and reporting , such as limitations in the completeness and precision of readiness data and a tendency to focus on examining the status of personnel , equipment , and other resources rather than broader capabilities .

congress addressed dod's readiness reporting in the strom thurmond national defense authorization act for fiscal year 1999 by adding section 117 to title 10 of the u.s. code , directing the secretary of defense to establish a comprehensive readiness reporting system to measure , in an “objective , accurate , and timely manner,” the capability of the armed forces to carry out the national security strategy prescribed by the president , the defense planning guidance provided by the secretary of defense , and the national military strategy prescribed by the chairman of the joint chiefs of staff .

in june 2002 , the deputy secretary of defense directed the under secretary of defense for personnel and readiness ( usd p&r ) to develop , field , maintain , and fund the enhanced status of resources and training system ( esorts ) , which is the automated readiness reporting system within the defense readiness reporting system ( drrs ) .

he also directed that drrs build upon existing processes and readiness assessment tools to establish a capabilities - based , adaptive , near - real - time readiness reporting system .

in addition , in june 2004 , the secretary of defense directed usd ( p&r ) to develop drrs in a manner that would support the data requirements of various users of readiness information , such as the chairman of the joint chiefs of staff , the combatant commanders , the secretaries of the military departments , and the chief of the national guard bureau , including their requirements for data on the availability , readiness , deployment , and redeployment of forces .

usd ( p&r ) established a drrs implementation office ( dio ) to manage the system's acquisition , including managing system development and engaging the user community .

the dio has used support contractors to develop the system and reported obligating about $96.5 million for drrs from fiscal year 2001 through fiscal year 2008 .

some fielded system capabilities are currently being used to varying degrees by the user community .

the dio originally estimated that drrs would achieve full operational capability in fiscal year 2007 , but currently expects drrs to reach full capability in 2014 .

in september 2008 , the dio projected that it would spend about $135 million through fiscal year 2014 .

recognizing that drrs was not yet fully deployed or operational and in light of our prior work on readiness - related issues , you asked us to review dod's efforts to develop and implement drrs , including the program's status , and the extent that drrs addresses the challenges that led congress to require a new system , such as the availability of information on capabilities , and the precision , timeliness , reliability , and objectivity of readiness metrics .

in addressing these issues , we assessed the extent to which ( 1 ) dod has effectively managed and overseen drrs acquisition and deployment , and ( 2 ) features of drrs have been implemented and are consistent with legislative requirements and dod guidance .

to determine the extent that dod has effectively managed and overseen drrs acquisition and deployment , we analyzed a range of program documentation and interviewed cognizant program and contractor officials relative to the following acquisition management disciplines: requirements development and management , test management , schedule reliability , and human capital .

for each discipline , we compared key program documentation , such as a requirements management plan , test and evaluation master plans , and test reports to relevant dod , federal , and related guidance , and we analyzed a statistical sample of individual requirements and test cases to determine consistency among them .

in addition , we attended meetings of organizations established to monitor or govern drrs development and reviewed information from meetings that we did not attend and interviewed officials associated with these meetings .

to determine the extent to which the features of drrs have been implemented and are consistent with legislative requirements and dod guidance , we reviewed criteria such as the legislation that mandated a comprehensive dod readiness reporting system , the dod directive that established drrs , program documentation and usd ( p&r ) guidance memorandums , dio briefings to the readiness community , other departmental instructions , and directives and memorandums related to drrs requirements and implementation .

from these documents , we identified desired features of drrs and compared them to documentary and testimonial evidence concerning system performance during meetings with a full range of officials responsible for developing and using the system .

to obtain the developer's perspective , on numerous occasions throughout our review we met with officials from usd ( p&r ) , the dio , and the three current drrs contractors .

to obtain user perspectives , we met with and surveyed by questionnaire officials from the joint staff , the geographic and functional combatant commands , and the services , and also met with officials from usd ( p&r ) .

we also attended meetings of organizations established to monitor or govern drrs development and analyzed information from meetings that we did not attend .

we also directly observed the system's capabilities through our own limited use of the system and by observing others using the system .

we did not evaluate the department's overall ability to assess the readiness of its forces or the extent that data contained in any of its readiness reporting systems , including drrs and gsorts , reflect capabilities , deficiencies , vulnerabilities , or performance issues .

our review focused on acquisition and program management issues , such as requirements management , schedule and human capital requirements , the current usage of drrs , and the extent to which drrs' features address legislative requirements and dod guidance .

we conducted this performance audit from april 2008 through august 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

additional details on our scope and methodology are in appendix i .

historically , dod has used its readiness assessment system to assess the ability of units and joint forces to fight and meet the demands of the national security strategy .

dod's readiness assessment and reporting system is designed to assess and report on military readiness at three levels — ( 1 ) the unit level ; ( 2 ) the joint force level ; and ( 3 ) the aggregate , or strategic , level .

using information from its readiness assessment system , dod prepares and sends legislatively mandated quarterly readiness reports to congress .

drrs is dod's new readiness reporting system that is intended to capture information from the previous system , as well as information about organizational capabilities to perform a wider variety of missions and mission essential tasks .

drrs is also intended to capture readiness information from defense agencies and installations , which were not required to report under the previous system .

some drrs features are currently fielded and being used to varying degrees by the user community .

laws , directives , and guidance , including a dod directive , chairman of the joint chiefs of staff instruction ( cjcsi ) , secretary of defense and usd ( p&r ) memorandums , and service regulations and messages , show that readiness information and data are needed to support a wide range of decision makers .

these users of readiness data include congress , the secretary of defense , the chairman of the joint chiefs of staff , the combatant commanders , the secretaries of the military departments , and the chief of the national guard bureau .

the directives and guidance also list roles and responsibilities for collecting and reporting various types of readiness data .

for example , cjcsi 3401.02a assigns the service chiefs responsibility for ensuring required global status of resources and training system ( gsorts ) reports are submitted .

gsorts is dod's legacy , resource - based readiness reporting system that provides a broad assessment of unit statuses based on units' abilities to execute the missions for which they were organized or designed as well as the current missions for which they may be employed .

the information in the required gsorts reports includes units' abilities to execute the missions for which they were organized or designed , as well as the status of their training , personnel , and equipment .

in addition , dod directive 7730.65 , which established drrs as dod's new readiness reporting system , assigns the secretaries of the military departments and the commanders of the combatant commands responsibilities for developing mission essential tasks for all of their assigned missions .

prior to 1999 , we identified challenges with dod's existing readiness reporting system , gsorts , and in 1999 , congress directed the secretary of defense to establish a comprehensive readiness reporting system .

the legislation requires the system to measure in an objective , accurate , and timely manner the capability of the armed forces to carry out ( 1 ) the national security strategy prescribed by the president , ( 2 ) the defense planning guidance provided by the secretary of defense , and ( 3 ) the national military strategy prescribed by the chairman of the joint chiefs of staff .

to address the requirements established by congress , the office of the deputy under secretary of defense ( readiness ) began in 2001 to build consensus among dod's senior readiness leaders for an improved readiness assessment system .

for example , the deputy's office distributed a list of key characteristics of the improved readiness assessment system to the leaders in advance of scheduled meetings .

the system's key desired characteristics included allowing near - real - time access to readiness data and trends , enabling rapid , low - cost development using classified internet technology , and reducing the reporting burdens on people .

since then various directives and memorandums have been issued regarding drrs responsibilities , requirements , and related issues .

for example: on june 3 , 2002 , the deputy secretary of defense established dod's new readiness reporting system , as directed by congress , by signing dod directive 7730.65 .

according to this directive , drrs is intended to build upon dod's existing processes and readiness assessment tools to establish a capabilities - based , near - real - time readiness reporting system .

the drrs directive assigned usd ( p&r ) responsibilities for developing , fielding , maintaining , and funding esorts ( the tool to collect capability , resource , and training information ) and overseeing drrs to ensure accuracy , completeness , and timeliness of its information and data , its responsiveness , and its effective and efficient use of modern practices and technologies .

in addition , the usd p&r is responsible for ensuring that esorts information , where appropriate , is integrated into dod's planning systems and processes .

the directive also states that until esorts becomes fully operational , the chairman of the joint chiefs of staff shall maintain the gsorts database .

on june 25 , 2004 , the secretary of defense issued a memorandum , which directed usd ( p&r ) to develop drrs to support data requirements identified by the chairman of the joint chiefs of staff , the combatant commanders , the secretaries of the military departments , and the chief , national guard bureau to include availability , readiness , deployment , and redeployment data .

on november 2 , 2004 , usd ( p&r ) issued a drrs interim implementation guidance memorandum .

in this memorandum , the undersecretary noted that he had established a dio to provide reporting assistance for units .

the memorandum also stated that combatant commanders would begin reporting readiness by mission essential tasks by november 30 , 2004 .

the memorandum also directed the services to develop detailed implementing guidance for reporting and assessing mission essential task readiness in esorts within their respective services , and set a goal for the services to implement the mission essential task reporting process by september 30 , 2005 .

to meet these mission essential task reporting requirements , usd ( p&r ) directed commanders to rate their organizational capabilities as ( 1 ) yes or “y” , ( 2 ) qualified yes or “q” , or ( 3 ) no or “n.” a “y” indicates that an organization can accomplish the rated tasks or missions to prescribed standards and conditions in a specified environment .

it should reflect demonstrated performance in training or operations .

a “q” indicates that performance has not been demonstrated , and , although data may not readily support a “y,” the commander believes the organization can accomplish the rated task or mission to standard under most conditions .

an “n” indicates that an organization cannot accomplish the rated task or mission to prescribed standards in the specified environment at the time of the assessment .

the november 2004 memorandum also stated that the expected transition from gsorts to esorts was scheduled to begin in fiscal year 2005 .

according to the 2004 memorandum , the esorts module of drrs would provide , among other things , visibility of the latest gsorts information reported by units , and detailed resource information from authoritative data sources with the capability to aggregate or separate the data .

this memorandum signaled a change in program direction .

although the 2002 dod directive stated that drrs is intended to build upon dod's existing processes and readiness assessment tools , the 2004 memorandum indicated that drrs was to replace gsorts , as the esorts module of drrs captured both capabilities and resource data .

since its establishment , the dio has operated within the office of usd ( p&r ) and has relied on multiple contractors .

to provide governance of drrs , and enhance communication between the development community , represented by the dio and contractors , and the user community , which includes the joint staff , military services , and combatant commands , usd ( p&r ) established various bodies with representatives from the user community , including military services , combatant commands , and the defense agencies .

representatives from the office of usd ( p&r ) and the joint staff currently serve as cochairs of the various bodies .

drrs battle staffs comprise colonels , navy captains , and similar - graded civilians .

they track drrs development and identify issues with the system .

at the one - star level , the drrs general and flag officer steering committee discusses issues raised by the battle staff .

in december 2007 , usd ( p&r ) created a committee at the three - star level , referred to as the drrs executive committee .

its charter , finalized about a year later in january 2009 , calls for the committee to review and approve proposals and plans to establish policy , processes , and system requirements for drrs , including approving software development milestones required to reach objectives .

to ensure that leadership is provided for the direction , oversight , and execution of dod's business transformation efforts , including business systems modernization efforts such as drrs , dod relies on several entities .

these entities include the defense business systems management committee , which is chaired by the deputy secretary of defense and serves as the department's highest - ranking governance body and the approval authority for business systems modernization activities ; the investment review boards , which are chartered by the principal staff assistants — senior leaders from various offices within dod — and serve as the review and certification bodies for business system investments in their respective areas of responsibility ; and the business transformation agency , which is responsible for supporting the investment review boards and for leading and coordinating business transformation efforts across the department .

among other things , the business transformation agency supports the office of the under secretary of defense , acquisition , technology and logistics in conducting system acquisition risk assessments .

our research and evaluations of information technology programs , including business systems modernization efforts within dod , have shown that delivering promised system capabilities and benefits on time and within budget largely depends on the extent to which key program management disciplines are employed by an adequately staffed program management office .

among other things , these disciplines include a number of practices associated with effectively developing and managing system requirements , adequately testing system capabilities , and reliably scheduling the work to be performed .

they also include proactively managing the program office's human capital needs , and promoting program office accountability through executive - level program oversight .

dod acquisition policies and guidance , along with other relevant guidance , recognize the importance of these management and oversight disciplines .

as we have previously reported , not employing these and other program management disciplines increases the risk that system acquisitions will not perform as intended and require expensive and time - consuming rework .

in 2003 , we reported that , according to usd ( p&r ) officials , drrs was a large endeavor , and that development would be challenging and require buy - in from many users .

we also reported that the program was only a concept without detailed plans to guide its development and implementation .

based on the status of the program at that time and dod's past record on readiness reporting initiatives , we recommended that the secretary of defense direct the office of usd ( p&r ) to develop an implementation plan that identified performance goals that are objective , quantifiable , and measurable ; performance indicators to measure outcomes ; an evaluation plan to compare program results with established goals ; milestones to guide drrs development to the planned 2007 full capability date .

dod did not agree with our recommendation , stating that it had established milestones , cost estimates , functional responsibilities , expected outcomes , and detailed plans for specific information technology requirements and progress indicators .

in evaluating the dod comments , we noted that dod had established only two milestones — initial capability in 2004 and full capability in 2007 — and did not have a road map explaining the steps needed to achieve full capability by 2007 .

dod has not effectively managed and overseen the acquisition and deployment of drrs in accordance with a number of key program management disciplines that are recognized in dod acquisition policies and guidance , along with other relevant guidance , and are fundamental to delivering a system that performs as intended on time and within budget .

in particular , drrs requirements have not been effectively developed and managed , and drrs testing has not been adequately performed and managed .

further , drrs has not been guided by a reliable schedule of the work needed to be performed and the key activities and events that need to occur .

these program management weaknesses can be attributed in part to long - standing limitations in program office staffing and oversight .

as a result , the program has not lived up to the requirements set for it by congress , and the department has not received value from the program that is commensurate with the time and money invested — about 7 years and $96.5 million .

each of these weaknesses are summarized below and discussed in detail in appendix ii .

according to dod and other relevant guidance , effective requirements development and management includes , among other things , ( 1 ) effectively eliciting user needs early and continuously in the system life - cycle process , ( 2 ) establishing a stable baseline set of requirements and placing the baseline under configuration management , ( 3 ) ensuring that system requirements are traceable backward to higher level business or operational requirements ( eg , concept of operations ) and forward to system design documents ( eg , software requirements specification ) and test plans , and ( 4 ) controlling changes to baseline requirements .

however , none of these conditions have been met on drrs .

specifically , key users have only recently become fully engaged in developing requirements , and requirements have been experiencing considerable change and are not yet stable .

further , different levels of requirements and related test cases have not been aligned with one another , and changes to requirements have not been effectively controlled .

as a result , efforts to develop and deliver initial drrs capabilities have taken longer than envisioned and these capabilities have not lived up to user expectations .

these failures increase the risk of future drrs capabilities not meeting expectations and increase the likelihood that expensive and time - consuming system rework will be necessary .

until recently , key users were not fully or effectively engaged in drrs requirements development and management .

one of the leading practices associated with effective requirements development is engaging system users early and continuously in the process of defining requirements .

however , dio officials and representatives from the military services and the joint staff agree that until recently , key users were not effectively engaged in drrs requirements development and management , although they disagree at to why user involvement has suffered .

regardless , drrs executive committee direction has improved the situation .

specifically , in january 2008 , the committee directed the joint staff to conduct an analysis of drrs capabilities , referred to as the “capabilities gap analysis,” which involved the entire readiness community and resulted in 530 additional user requirements .

in our view , this analysis is a positive step in addressing long - standing limited involvement by key drrs users in defining requirements that has contributed to significant delays in the program , as discussed later in the report .

as of april 2009 , drrs requirements continued to be in a state of flux .

establishing an authoritative set of baseline requirements prior to system design and development provides a stable basis for designing , developing , and delivering a system that meets its users' operational needs .

however , the fact that these 530 user requirements have recently been identified means that the suite of requirements documentation associated with the system , such as the detailed system requirements , will need to change and thus is not stable .

to illustrate , these 530 requirements have not been fully evaluated by the dio and the drrs governance boards and according to program officials , have not yet been approved , and thus their impact on the program is not clear .

compounding this instability in the drrs requirements is the fact that additional changes are envisioned .

according to program officials , the changes resulting from the gap analysis and reflected in the latest version of the drrs concept of operations , which was approved by the drrs executive committee in january 2009 , have yet to be reflected in other requirements documents , such as the detailed system requirements .

although defining and developing requirements is inherently an iterative process , having a baseline set of requirements that are stable is a prerequisite to effective and efficient system design and development .

without them , the dio has not been able to deliver a system that meets user needs on time , and it is unlikely that future development and deployment efforts will produce better results .

during our review , dio officials could not demonstrate that requirements and related system design and testing artifacts are properly aligned .

one of the leading practices associated with developing and managing requirements is maintaining bidirectional traceability from high - level operational requirements through detailed lower - level requirements and design documents to test cases .

we attempted on three separate occasions to verify the traceability of system requirements backwards to higher - level requirements and forward to lower - level software specifications and test cases , and each time we found that traceability did not exist .

dio and contractor officials attributed the absence of adequate requirements traceability to the ongoing instability in requirements and efforts to update program documentation .

without traceable requirements , the dio does not have a sufficient basis for knowing that the scope of the design , development , and testing efforts will produce a system solution on time and on budget and that will meet users' operational needs and perform as intended .

as a result , the risk is significant that expensive and time - consuming system rework will be required .

since the inception of the program in 2002 , drrs has been developed and managed without a formally documented and approved process for managing changes to system requirements .

adopting a disciplined process for reviewing and accepting changes to an approved baseline set of requirements in light of the estimated costs , benefits , and risk of each proposed change is a recognized best practice .

however , requirements management and change - control plans developed in 2006 by the drrs software development contractor , according to dio officials , were not adequate .

to address this , the joint staff developed what it referred to as a conceptual requirements change - control process in february 2008 , as a basis for the dio to develop more detailed plans that could be implemented .

in january 2009 , the dio drafted more detailed requirements management and configuration management plans , the latter of which the dio updated in march 2009 .

however , the plans have yet to be approved and implemented .

until the dio effectively controls requirements changes , it increases the risk of needed drrs capabilities taking longer and costing more to deliver than necessary .

according to dod and other relevant guidance , system testing should be progressive , meaning that it should consist of a series of test events that first focus on the performance of individual system components , then on the performance of integrated system components , followed by system - level tests that focus on whether the system ( or major system increments ) are acceptable , interoperable with related systems , and operationally suitable to users .

for this series of related test events to be conducted effectively , each test event needs to be executed in accordance with well - defined test plans , the results of each test event need to be captured and used to ensure that problems discovered are disclosed and corrected , and all test events need to be governed by a well - defined test management structure .

however , the dio cannot demonstrate that it has adequately tested any of the drrs increments , referred to as system releases and subreleases , even though it has already acquired and partially deployed a subset of these increments .

moreover , the dio has yet to establish the test management structures and controls needed to effectively execute drrs testing going forward .

more specifically , the test events for already acquired , as well as currently deployed and operating , drrs releases and subreleases were not based on well - defined plans .

for example , the test plan did not include a schedule of activities to be performed or defined roles and responsibilities for performing them .

also , the test plan did not consistently include test entrance and exit criteria , a test defect management process , and metrics for measuring progress .

further , test events have not been fully executed in accordance with plans , or executed in a verifiable manner , or both .

for example , although increments of drrs functionality have been put into production , the dio has not performed system integration testing , system acceptance testing , or operational testing on any drrs release or subrelease .

moreover , the results of all executed test events have not been captured and used to ensure that problems discovered were disclosed to decision makers , and ultimately corrected .

for example , the dio has not captured the test results for at least 20 out of 63 drrs subreleases .

test results that were captured did not include key elements , such as entrance / exit criteria status and unresolved defects and applicable resolution plans .

the dio has also not established an effective test management structure to include , for example , a clear assignment of test management roles and responsibilities , or a reliable schedule of planned test events .

compounding this absence of test management structures and controls is the fact that the dio has yet to define how the development and testing to date of a series of system increments ( system releases and subreleases ) relate to the planned development and testing of the 10 system modules established in january 2009 .

 ( see table 1 for a list and description of these modules. ) .

collectively , this means that it is unlikely , that already developed and deployed drrs increments can perform as intended and meet user operational needs .

equally doubtful are the chances that the dio can adequately ensure that yet - to - be developed drrs increments will meet expectations .

the success of any program depends in part on having a reliable schedule that defines , among other things , when work activities will occur , how long they will take , and how they are related to one another .

from its inception in 2002 until november 2008 , the dio did not have an integrated master schedule , and thus has long been allowed to proceed without a basis for executing the program and measuring its progress .

in fact , the only milestone that we could identify for the program prior to november 2008 was the date that drrs was to achieve full operational capability , which was originally estimated to occur in fiscal year 2007 , but later slipped to fiscal year 2008 and then fiscal year 2011 , and is now fiscal year 2014- - a 7-year delay .

moreover , the drrs integrated master schedule that was first developed in november 2008 , and was updated in january 2009 and again in april 2009 to address limitations that we identified and shared with the program office , is still not reliable .

specifically , our research has identified nine practices associated with developing and maintaining a reliable schedule .

these practices are ( 1 ) capturing all key activities , ( 2 ) sequencing all key activities , ( 3 ) assigning resources to all key activities , ( 4 ) integrating all key activities horizontally and vertically , ( 5 ) establishing the duration of all key activities , ( 6 ) establishing the critical path for all key activities , ( 7 ) identifying float between key activities , ( 8 ) conducting a schedule risk analysis , and ( 9 ) updating the schedule using logic and durations to determine the dates for all key activities .

the program's latest integrated master schedule does not address three of the practices and only partially addresses the remaining six .

for example , the schedule does not establish a critical path for all key activities , nor does it include a schedule risk analysis , and it is not being updated using logic and durations to determine the dates for all key activities .

in addition , the schedule introduces considerable concurrency across key activities and events for several modules , which introduces increased risk .

these limitations in the program's latest integrated master schedule , coupled with the program's 7- year slippage to date and continued requirements instability , make it likely that drrs will incur further delays .

the dio does not currently have adequate staff to fulfill its system acquisition and deployment responsibilities , and it has not managed its staffing needs in an effective manner .

effective human capital management should include an assessment of the core competencies and essential knowledge , skills , and abilities needed to perform key program management functions , an inventory of the program's existing workforce capabilities , an analysis of the gap between the assessed needs and the existing capabilities , and plans for filling identified gaps .

dio performs a number of fundamental drrs program management functions , such as acquisition planning , performance management , requirements development and management , test management , contractor tracking and oversight , quality management , and configuration management .

to effectively perform such functions , program offices , such as the dio , need to have not only well - defined policies and procedures and support tools for each of these functions , but also sufficient human capital to implement the processes and use the tools throughout the program's life cycle .

however , the dio is staffed with only a single full - time government employee — the dio director .

all other key program office functions are staffed by either contractor staff or staff temporarily detailed , on an as - needed basis , from other dod organizations .

in addition , key positions , such as performance manager and test manager , have either not been established or are vacant .

according to dio and contractor officials , they recognize that additional program management staffing is needed but stated that requests for additional staff had not been approved by usd ( p&r ) due to competing demands for staffing .

further , they stated that the requests were not based on an assessment of the program's human capital needs and the gap between these needs and its onboard workforce capabilities .

until dio adopts a strategic , proactive approach to managing its human capital needs , it is unlikely that it will have an adequate basis for obtaining the people it needs to effectively and efficiently manage drrs .

a key principle for acquiring and deploying system investments is to establish a senior - level governance body to oversee the investment and hold program management accountable for meeting cost , schedule , and performance commitments .

moreover , for investments that are organization wide in scope and introduce new ways of doing business , like drrs , the membership of this oversight body should represent all stakeholders and have sufficient organizational seniority to commit their respective organizations to any decisions reached .

for significant system investments , the department's acquisition process provides for such executive governance bodies .

for example , major automated information systems , which are investments over certain dollar thresholds or that are designated as special interest because of , among other things , their mission importance , are reviewed at major milestones by a designated milestone decision authority .

these authorities are supported by a senior advisory group , known as the information technology acquisition board , which comprises senior officials from the joint staff , the military departments , and staff offices within the office of the secretary of defense .

in addition , all business system investments in dod that involve more than $1 million in obligations are subject to review and approval by a hierarchy of dod investment review boards that comprise senior dod leaders , including the defense business systems management committee , which is chaired by the deputy secretary of defense .

through these executive oversight bodies and their associated processes , programs are to be , among other things , governed according to corporate needs and priorities , and program offices are to be held accountable for meeting cost , schedule , and performance expectations .

until april 2009 , drrs was not subject to any of dod's established mechanisms and processes for overseeing information technology systems .

as previously discussed , usd ( p&r ) established the drrs battle staff , which is a group of midlevel military officers and civilians from drrs stakeholder organizations , and it established a higher - ranked general and flag officer steering committee , consisting of stakeholder representatives .

however , neither of these entities had specific oversight responsibilities or decision - making authority for drrs .

moreover , neither was responsible for holding the program office accountable for results .

according to meeting minutes and knowledgeable officials , these entities met on an irregular basis over the last several years , with as much as a 1- year gap in meeting time for one of them , to discuss drrs status and related issues .

in december 2007 , usd ( p&r ) recognized the need for a more senior - level and formal governance body , and established the drrs executive committee .

since january 2008 , this committee , which consists of top - level representatives from stakeholder organizations , has met at least seven times .

in january 2009 , the drrs executive committee's charter was approved by the deputy under secretary of defense ( readiness ) and the three - star director of the joint staff .

according to the charter , the committee is to review and approve proposals and plans to establish policy , processes , and system requirements for drrs , including approving software development milestones required to reach objectives .

consistent with its charter , the committee has thus far made various program - related decisions , including approving a drrs concept of operations to better inform requirements development , and directing the joint staff to conduct an analysis to identify any gaps between drrs requirements and user needs .

however , the committee has not addressed the full range of acquisition management weaknesses previously discussed in this report , and it has not taken steps to ensure that the program office is accountable for well - defined program baseline requirements .

more recently , the dod human resources management investment review board and the defense business systems management committee reviewed drrs and certified and approved , respectively , the program to invest $24.625 million in fiscal years 2009 and 2010 .

these entities comprise senior leadership from across the department , including the deputy secretary of defense as the defense business systems management committee chair , military service secretaries , the defense agency heads , principal staff assistants , and representatives from the joint staff and combatant commands .

however , neither the investment review board's certification nor the defense business systems management committee's approval was based on complete and accurate information from usd ( p&r ) .

specifically , the certification package submitted to both oversight bodies by the usd ( p&r ) precertification authority ( office of readiness programming and assessment ) stated that drrs was on track for meeting its cost , schedule , and performance measures and highlighted no program risks despite the weaknesses discussed in this report .

according to the chairwoman of the investment review board , the board does not have a process or the resources to validate the information received from the programs that it reviews .

moreover , the chairwoman stated that program officials did not make the board aware of the results of our review that we shared with the dio prior to either the investment review board or defense business systems management committee reviews .

since we briefed the chairwoman , the investment review board has requested that the dio provide it with additional information documenting drrs compliance with applicable dod regulations and statutes .

according to usd ( p&r ) and dio officials , drrs was not subject to department executive - level oversight for almost 6 years because , among other things , they did not consider drrs to be a more complex information technology system .

furthermore , because of the nature of the authority provided to the usd ( p&r ) in the drrs charter , they did not believe it was necessary to apply the same type of oversight to drrs as other information systems within dod .

this absence of effective oversight has contributed to a void in program accountability and limited prospects for program success .

dod has implemented drrs features that allow users to report certain mission capabilities that were not reported under the legacy system , but these features are not fully consistent with legislative requirements and dod guidance ; and dod has not yet implemented other envisioned features of the system .

while some users are consistently reporting enhanced capability information , reporting from other users has been inconsistent .

in addition , drrs has not fully addressed the challenges with metrics that were identified prior to 1999 when congress required dod to establish a new readiness reporting system .

users have also noted that drrs lacks some of the current and historical data and connectivity with dod's planning systems necessary to manage and deploy forces .

the geographic combatant commands are capturing enhanced capability data in drrs , and dod's quarterly readiness reports to congress currently contain this information , as well as information that is drawn from dod's legacy readiness reporting system , gsorts .

however , the military services have not consistently used the enhanced capability reporting features of drrs .

because drrs does not yet fully interface with legacy systems to allow single reporting of readiness data , the army and navy developed additional system interfaces and are reporting in drrs .

until may 2009 , the marine corps directed its units to report only in the legacy system to avoid the burden of dual reporting .

the air force chose not to develop an interface and instructed its units to report in both drrs and the legacy system .

drrs and gsorts both contain capabilities information and resource ( numbers of personnel , equipment availability , and equipment condition ) and training data .

however , drrs currently provides more capabilities data than gsorts .

when congress directed dod to establish a new readiness reporting system , gsorts was already providing capability information concerning unit capabilities to perform the missions for which they were organized or designed .

more recently , some of the military services began reporting limited capability information on unit capabilities to perform missions other than those that they were organized or designed to perform into gsorts .

however , drrs is designed to capture capabilities on a wider variety of missions and mission essential tasks .

for example , organizations can report their capabilities to conduct missions associated with major war plans and operations such as operation iraqi freedom into drrs , as well as their capabilities to perform the missions for which they were organized or designed .

drrs also captures capability information from a wider range of organizations than gsorts .

although the primary ( monthly ) focus is on operational units and commands , drrs collects and displays readiness information from defense agencies and installations .

geographic combatant commands — such as u.s. central command , u.s. pacific command , and u.s. northern command — are currently reporting their commands' capabilities to execute most of their operations and major war plans in drrs .

dod reports this enhanced capability information from the geographic combatant commands in its quarterly readiness report to congress .

the geographic combatant commands are also using drrs to report their capabilities to perform headquarters - level , joint mission essential tasks , and some of these commands utilize drrs as their primary readiness reporting tool .

for example , u.s. northern command uses drrs to assess risk and analyze capability gaps , and u.s. pacific command identifies critical shortfalls by evaluating mission essential task assessments that are captured in drrs .

while drrs currently has the necessary software to collect and display these enhanced capability data from organizations at all levels throughout dod , a variety of technical and other factors have hindered service reporting of capability data .

as a result , the services have either developed their own systems to report required readiness data or have delayed issuing implementing guidance that would require their units to report standardized mission essential task data in drrs .

by 2005 , drrs was able to collect and display mission essential task information from any organizations that had access to a secure internet protocol router network ( siprnet ) workstation .

in august 2005 , usd ( p&r ) issued a memorandum that directed the services to ensure that all of their gsorts - reporting units were reporting mission essential task capabilities in drrs by september 30 , 2005 .

the memorandum stated that , for tactical units , mission essential tasks were to be drawn from the service universal task list and standardized across like - type entities , such as tank battalions , destroyers , or f - 16 squadrons .

however , two factors that have hindered compliance with the memorandum's direction to report mission essential task capabilities in drrs are discussed below .

while drrs has been able to collect and display mission essential task data since 2005 , some army and navy users did not have the means to directly access drrs and update mission essential task assessments .

for example , some ships lacked hardware necessary to be able to transmit their mission essential task data directly into drrs while at sea .

in addition , many national guard units lacked , and still lack , direct access to the siprnet workstations that are necessary to directly input mission essential task data directly into drrs .

however , the army and the navy have developed systems , respectively designated drrs - a and drrs - n that interface with drrs and thus allow all of their units to report mission essential task data .

after army and navy units report mission essential task data in their respective drrs - a and drrs - n service systems , the services transmit these data to drrs .

as a result , army and navy officials told us that they are currently complying with the requirement to ensure that all their gsorts - reporting units report mission essential task data in drrs .

unlike the army and the navy , the marine corps and the air force have not developed their own systems to allow their units to use a single tool to enter readiness data to meet office of the secretary of defense , chairman of the joint chiefs of staff , and service readiness reporting requirements .

while the dio has developed the software for users to enter mission essential task data into drrs , the dio has been unsuccessful in attempts to develop a tool that would allow air force and marine corps users to enter readiness data to meet all of their readiness reporting requirements through drrs .

as a result , rather than reducing the burden on reporting units , drrs has actually increased the burden on air force and marine corps units because they are now required to report readiness information in both drrs and gsorts .

on september 29 , 2005 , usd ( p&r ) issued a memorandum stating that drrs is the single readiness reporting system for the entire department of defense and that legacy systems , such as gsorts and associated service readiness systems , should be phased out .

since that time , officials have discussed whether to phase out gsorts and tentative dates for this action have slipped several times .

in 2001 , the office of the deputy undersecretary of defense ( readiness ) listed reducing reporting burdens as a key characteristic of its envisioned improved readiness assessment system .

in an effort to eliminate this burden of dual reporting , the dio began to develop a “current unit status” tool as a means for users to manage unit - specific readiness data and submit required reports in support of all current reporting guidelines .

the tool was to minimize the burden associated with dual reporting by collecting , displaying , and integrating resource data from service authoritative data sources with gsorts and drrs .

however , in december 2007 , the dio reported that it was unable to deliver the intended functionality of the “current unit status” tool .

instead , the dio decided to develop an interim reporting tool , known as the sortsrep tool , which would not provide the type of new capabilities envisioned for the “current unit status” tool , but would simply replicate the functionality of the input tool that the air force and marines already used to input data into gsorts .

after delays , and 10 months of effort , the dio delivered the sortsrep tool to the marine corps for review .

based on this review , in december , 2008 , the marine corps provided the developers and the dio with 163 pages of detailed descriptions and graphics to explain the sortsrep tool's deficiencies .

it then informed the dio that it would no longer expend energy and resources to review future versions of the sortsrep tool and would instead look at leveraging the army's or navy's drrs - a or drrs - n systems .

the air force also informed the dio that it was no longer interested in the sorstsrep tool , and said efforts should be focused on the “current unit status” tool instead .

as a result , the air force and marine corps are currently faced with dual reporting requirements , as illustrated in figure 1 .

on march 3 , 2009 , the air force deputy chief of staff ( operations , plans and requirements ) issued a memorandum that updated the air force's previous implementing guidance and directed all gsorts - reporting units to begin assessing readiness in drrs based on standardized core task lists within 90 days .

as a result , air force units will report readiness in both drrs and gsorts until the dio is able to deliver the intended functionality of the “current unit status” tool .

while some marine corps units are reporting their capabilities in drrs , the marine corps had not yet directed its units to report in the system as of may 2009 .

the commandant of the marine corps had stated that he supported the development and implementation of drrs , but that he would not direct units to assess mission essential tasks in drrs until the system met its stated requirements and was accepted as the single readiness reporting system of record .

marine corps officials said that they did not want to place a burden on operational units , which were fighting or preparing to fight a war , by requiring that they report readiness in two different systems .

after we completed our audit work , on may 12 , 2009 , the marine corps issued an administrative message that required that units assess their mission essential tasks and missions in drrs .

the message stated that doing so would improve familiarity with drrs , which will lead to an easier transition when the marine corps fields drrs - marine corps ( drrs - mc ) .

without a viable tool for inputting data , drrs is not fully integrated with gsorts or with the service readiness reporting systems and it is not capable of replacing those systems since it does not capture the required data that are contained in those systems .

while drrs is being used to provide congress with enhanced capability information , the quality of drrs metrics still faces the same challenges , including limitations in timeliness , precision , and objectivity that existed prior to 1999 when congress directed dod to establish a new readiness reporting system .

section 117 of title 10 of the u.s. code directed the secretary of defense to establish a comprehensive readiness reporting system to measure the capability of the armed forces in an “objective , accurate , and timely manner.” however , the enhanced capability data that are captured in drrs and reported to congress are no more timely than the readiness data that were being provided to congress in 1999 using gsorts .

furthermore , the metrics that are being used to capture the enhanced capability information are less objective and precise than the metrics that were used to report readiness in 1999 .

the statute directing the development of a new readiness reporting system requires that the reporting system measure in a timely manner the capability of the armed forces to carry out the national security strategy , the secretary of defense's defense planning guidance , and the national military strategy .

the legislation also lists a number of specific requirements related to frequency of measurements and updates .

for example , the law requires that the capability of units to conduct their assigned wartime missions be measured monthly , and that units report any changes in their overall readiness status within 24 hours of an event that necessitated the change in readiness status .

in its drrs directive , dod assigned usd ( p&r ) responsibility for ensuring the timeliness of drrs information and data , and it specified that drrs was to be a near - real - time readiness reporting system .

while dod is reporting readiness information to congress on a quarterly basis as required , and units are measuring readiness on a monthly basis , drrs is not a near - real - time reporting system .

specifically , in drrs , as in gsorts , operational commanders assess the readiness of their organizations on a monthly basis or when an event occurs that changes the units' overall reported readiness .

thus , drrs has not improved the timeliness of the key readiness data that are reported to congress .

according to usd ( p&r ) officials , drrs data will be more timely than gsorts data because drrs will update underlying data from authoritative data sources between the monthly updates .

however , drrs is not yet capturing all the data from the authoritative data sources , and according to service officials , the service systems that support gsorts also draw information from their service authoritative data sources between the monthly updates .

furthermore , the source and currency of some of the authoritative data that are currently in drrs are not clearly identified .

as a result , some users told us that they are reluctant to use drrs data to support their decisions .

we previously reported that the readiness information that dod provided to congress lacked precision , noting that gsorts readiness measures that differed by 10 percentage points or more could result in identical ratings , with dod often not reporting the detailed information behind the ratings outside of the department .

for example , units that were at 90 and 100 percent of their authorized personnel strengths both were reported as p - 1 in dod's reports to congress .

in 2003 , usd ( p&r ) recognized the imprecision of the reported metrics from gsorts and noted that its efforts to develop drrs would allow enhancements to reported readiness data .

as previously noted , the drrs capability information that dod is reporting to congress covers a broader range of missions than the gsorts information that was provided in the past .

however , when comparing the drrs and gsorts assessments of units' capabilities to perform the missions for which the units were organized or designed , drrs assessments are actually less precise than the gsorts assessments .

specifically , within gsorts , overall capability assessments are grouped into four categories based on four percentage ranges for the underlying data .

for example , commanders compare on - hand and required levels of personnel and equipment .

within drrs , mission essential task assessments are reported on a scale that includes only three ratings — ”yes” , “no” , and “qualified yes,” which can include any assessments that fall between the two extremes .

the law directing dod to establish a new readiness reporting system also requires that the system measure readiness in an objective manner .

gsorts assessments of units' capabilities to execute the missions for which they were organized or designed are based on objective personnel and equipment data and training information that may include both objective and subjective measures .

furthermore , the overall capability assessment in gsorts is based on an objective rule that calls for the overall assessment to be the same level as the lowest underlying resource or training data level .

for example , if a unit reported the highest personnel level ( p - 1 ) and the lowest training level ( t - 4 ) , the rules in the gsorts guidance instruct the commander to rate the unit's overall capability at the c - 4 level .

because gsorts contains these objective measures and rules , it is easy to evaluate reported readiness to see if it aligns with established reporting criteria .

within drrs , organizations rate their capabilities based on mission essential tasks .

these mission essentials tasks have conditions and standards associated with them .

the conditions specify the types of environments that units are likely to face as they execute the tasks , such as weather conditions and political or cultural factors .

standards describe what it means for the unit to successfully execute the task under specified conditions .

for example , a unit may have to achieve a 90 percent success rate for measures associated with the task being assessed .

in spite of these conditions and standards , drrs mission assessments are often subjective rather than objective .

in drrs program guidance , dod has defined mission essential tasks as tasks that are approved by the commander and that , based on mission analysis , are “absolutely necessary , indispensable , or critical to mission success.” in prior briefings and reports to congress , we have noted examples that highlight the subjective nature of drrs mission assessments .

for example , we noted that one commander used his professional judgment to decide that his command was “qualified” to execute a mission even though the preponderance of the “indispensable” tasks that supported that mission were rated as “no.” in contrast , other commanders used their professional judgments to rate missions as “qualified” based on one or more “qualified” tasks among many “yes” tasks .

drrs does not have all of the resource , training , readiness data , and connectivity with the department's operations planning and execution system that the services , joint staff , and certain combatant commands need to manage and deploy forces .

as a result , drrs is not yet able to operate as the department's single readiness reporting system , as intended .

the secretary of defense's and the under secretary of defense's guidance documents recognize that drrs needs to support the data requirements of multiple users .

for example , the secretary of defense's june 25 , 2004 , memorandum directed usd ( p&r ) to develop drrs to support the data requirements identified by the chairman of the joint chiefs of staff , the combatant commanders , the secretaries of the military departments , and the chief of the national guard bureau .

furthermore , the 2002 drrs directive noted that drrs was to build upon dod's existing processes and readiness assessment tools and that esorts information ( capability , resource , and training ) , where appropriate , is integrated into dod's planning systems and processes .

it also directed the chairman of the joint chiefs of staff to maintain the gsorts database until key capabilities of drrs become fully operational .

officials with u.s. joint forces command and u.s. special operations command reported that historical data are needed to manage forces and provide users the ability to analyze readiness trends .

similarly , service officials stated a need for historical data so they can manage their forces and take action to address readiness issues .

in 2005 , usd ( p&r ) reported that unit resource data , including detailed inventory and authorization data on personnel , equipment , supply , and ordnance were available in drrs .

however , in response to a survey we conducted in december 2008 , the services and certain combatant commands stated that necessary current and historical resource and training data were not available in drrs .

for example , officials from all four services responded that drrs , at that time , contained less than half of their gsorts resources and training data .

in addition , officials from u.s. joint forces command , u.s. special operations command , the u.s. strategic command , and the u.s. transportation command all responded that historical resource data were not available in drrs .

we confirmed that this information was still not available when we concluded our review , and in april , 2009 , the dio said it was still working on this data availability issue .

furthermore , user organizations have reported inaccuracies in the data that are available in drrs .

marine corps and u.s. special operations command officials stated that inconsistencies between drrs data and the data in other readiness systems have caused them to adjudicate the inconsistencies by contacting their subordinate units directly .

army officials noted that searches of drrs data can produce different results than searches in the army's data systems .

for example , they noted that a drrs search for available personnel with a particular occupational specialty produced erroneously high results because drrs did not employ the appropriate business rules when conducting the search .

specifically , drrs did not apply a business rule to account for the fact that an individual person can possess multiple occupational specialty codes but can generally fill only one position at a time .

dio officials informed us that they intend to correct issues with the accuracy of data drawn from external databases .

however , the current version of the drrs integrated master schedule indicates that the ability of drrs to provide the capability to correctly search , manipulate , and display current and historical gsorts and mission essential task data will not be complete until june 2010 .

as a result , the reliability of the drrs data is likely to remain questionable and a number of dod organizations will likely continue to rely on gsorts and other sources of readiness data to support their decision making .

one important drrs function is integration with dod's planning systems .

specifically , the 2002 drrs directive requires usd ( p&r ) to ensure that , where appropriate , esorts information ( capability , resource , and training ) is compatible and integrated into dod's planning systems and processes .

global force management is one of the dod planning processes that is to be integrated with drrs .

global force management is a process to manage , assess , and display the worldwide disposition of u.s. forces , providing dod with a global view of requirements and availability of forces to meet those requirements .

the integration of drrs with global force management planning processes is supposed to allow dod to link force structure , resources , and capabilities data to support analyses , and thus help global force managers fill requests for forces or capabilities .

officials from the four organizations with primary responsibilities for providing forces ( u.s. joint forces command , u.s. special operations command , u.s. strategic command , and u.s. transportation command ) all stated that they are unable to effectively use drrs to search for units that will meet requested capabilities .

these commands also reported that drrs does not currently contain the information and tools necessary to support global force management .

for example , officials from u.s. northern command told us that when they used drrs to search for available helicopters of a certain type , they found thousands , but when u.s. joint forces command did the same drrs search they found hundreds .

the current version of the drrs integrated master schedule indicates that drrs will not be able to fully support global force management until march 2011 .

as a result , these commands continue to rely on gsorts rather than drrs to support their planning and sourcing decisions .

drrs is not currently and consistently providing timely , objective , and accurate information , and it is not exactly clear where the department stands in its efforts to meet this expectation because system requirements remain in a state of flux , and the program office lacks disciplined program management and results information due to a long - standing lack of rigor in its approach to acquiring and deploying system capabilities .

this situation can be attributed , in part , to long - standing limitations in the program office's focus on acquiring human capital skills needed to manage such a complex initiative .

it can also be linked to many of years of limited program office oversight and accountability .

although program oversight has recently increased , oversight bodies have not had sufficient visibility into the program's many management weaknesses .

drrs is providing congress and readiness users with additional mission and mission essential task capability data that were not available in gsorts .

however , after investing about 7 years and about $96.5 million in developing and implementing drrs , the system's schedule has been extended , requirements are not stable , and the system still does not meet congressional and dod requirements for a comprehensive readiness reporting system to assess readiness and help decision makers manage forces needed to conduct combat and contingency operations around the world .

given drrs performance and management weaknesses , it is critical that immediate action be taken to put the program on track and position it for success .

without this action , it is likely that drrs will cost more to develop and deploy than necessary and that dod will not have a comprehensive reporting system that meets the needs of all the decision makers who rely on accurate , timely , and complete readiness information .

to address the risks facing dod in its acquisition and deployment of drrs , and to increase the chances of drrs meeting the needs of the dod readiness community and congress , we recommend that the secretary of defense direct the deputy secretary of defense , as the chair of the defense business systems management committee , to reconsider the committee's recent approval of drrs planned investment for fiscal years 2009 and 2010 , and convene the defense business systems management committee to review the program's past performance and the dio's capability to manage and deliver drrs going forward .

to fully inform this defense business systems management committee review , we also recommend that the secretary direct the deputy secretary to have the director of the business transformation agency , using the appropriate team of functional and technical experts and the established risk assessment methodology , conduct a program risk assessment of drrs , and to use the findings in our report and the risk assessment to decide how to redirect the program's structure , approach , funding , management , and oversight .

in this regard , we recommend that the secretary direct the deputy secretary to solicit the advice and recommendations of the drrs executive committee .

we also recommend that the secretary , through the appropriate chain of command , take steps to ensure that the following occur: 1 .

drrs requirements are effectively developed and managed with appropriate input from the services , joint staff , and combatant commanders , including ( 1 ) establishing an authoritative set of baseline requirements prior to further system design and development ; ( 2 ) ensuring that the different levels of requirements and their associated design specifications and test cases are aligned with one another ; and ( 3 ) developing and instituting a disciplined process for reviewing and accepting changes to the baseline requirements in light of estimated costs , benefits , and risk .

2 .

drrs testing is effectively managed , including ( 1 ) developing test plans and procedures for each system increment test event that include a schedule of planned test activities , defined roles and responsibilities , test entrance and exit criteria , test defect management processes , and metrics for measuring test progress ; ( 2 ) ensuring that all key test events are conducted on all drrs increments ; ( 3 ) capturing , analyzing , reporting , and resolving all test results and test defects of all developed and tested drrs increments ; and ( 4 ) establishing an effective test management structure that includes assigned test management roles and responsibilities , a designated test management lead and a supporting working group , and a reliable schedule of test events .

3 .

drrs integrated master schedule is reliable , including ensuring that the schedule ( 1 ) captures all activities from the work breakdown structure , including the work to be performed and the resources to be used ; ( 2 ) identifies the logical sequencing of all activities , including defining predecessor and successor activities ; ( 3 ) reflects whether all required resources will be available when needed and their cost ; ( 4 ) ensures that all activities and their duration are not summarized at a level that could mask critical elements ; ( 5 ) achieves horizontal integration in the schedule by ensuring that all external interfaces ( hand - offs ) are established and interdependencies among activities are defined ; ( 6 ) identifies float between activities by ensuring that the linkages among all activities are defined ; ( 7 ) defines a critical path that runs continuously to the program's finish date ; ( 8 ) incorporates the results of a schedule risk analysis to determine the level of confidence in meeting the program's activities and completion date ; and ( 9 ) includes the actual start and completion dates of work activities performed so that the impact of deviations on downstream work can be proactively addressed .

4 .

the drrs program office is staffed on the basis of a human capital strategy that is grounded in an assessment of the core competencies and essential knowledge , skills , and abilities needed to perform key drrs program management functions , an inventory of the program office's existing workforce capabilities , and an analysis of the gap between the assessed needs and the existing capabilities .

5 .

drrs is developed and implemented in a manner that does not increase the reporting burden on units and addresses the timeliness , precision , and objectivity of metrics that are reported to congress .

to ensure that these and other drrs program management improvements and activities are effectively implemented and that any additional funds for drrs implementation are used effectively and efficiently , we further recommend that the secretary direct the deputy secretary to ensure that both the human resources management investment review board and the drrs executive committee conduct frequent oversight activities of the drrs program , and report any significant issues to the deputy secretary .

in written comments on a draft of this report , signed by the deputy under secretary of defense ( military personnel policy ) performing the duties of the under secretary of defense ( personnel and readiness ) , dod stated that the report is flawed in its assessment of drrs , noting that drrs is a net - centric application that provides broad and detailed visibility on readiness issues , and that achieving data sharing across the dod enterprise was groundbreaking work fraught with barriers and obstacles , many of which have now been overcome .

in addition , dod stated that it was disappointed that the report did not address cultural impediments that it considers to be the root cause of many of the issues cited in the report and of many previous congressional concerns on readiness reporting .

dod further stated that the report instead focuses on past acquisition process and software development problems that it believes have now been remedied according to the department , this focus , coupled with inaccurate and misleading factual information included in the report , led us to develop an incomplete picture of the program .

notwithstanding these comments , dod agreed with two of our recommendations and partially agreed with a third .

however , it disagreed with the remaining five recommendations , and provided comments relative to each recommendation .

dod's comments are reprinted in their entirety in appendix iii .

in summary , we do not agree with dod's overall characterization of our report or the positions it has taken in disagreeing with five of our recommendations , finding them to be inconsistent with existing guidance and recognized best practices on system acquisition management , unsupported by verifiable evidence , and in conflict with the facts detailed in our report .

further , we recognize that developing drrs is a significant and challenging undertaking that involves cultural impediments .

as a result , our report explicitly focuses on the kind of program management rigor and disciplines needed to address such impediments and successfully acquire complex systems , including effective requirements development and management and executive oversight .

we also disagree that our report focuses on past issues and problems .

rather , it provides evidence that demonstrates a long - standing and current pattern of system acquisition and program oversight weaknesses that existed when we concluded our audit work and that dod has not provided any evidence to demonstrate has been corrected .

in addition , we would emphasize that we defined our objectives , scope , and methodology , and executed our audit work in accordance with generally accepted government auditing standards , which require us to subject our approach as well as the results of our audit work to proven quality assurance checks and evidence standards that require us to seek documentation rather than relying solely on testimonial evidence .

while we support any departmental efforts , whether completed or ongoing , that would address the significant problems cited in our report , we note that dod , in its comments , did not specifically cite what these efforts are or provide documentation to support that they have either been completed or are ongoing .

therefore , we stand by our findings and recommendations .

moreover , we are concerned that in light of the program's significant and long - standing management weaknesses , the department's decision not to pursue recommendations aimed at corrective actions for five of our eight recommendations will further increase risk to achieving program success , and is not in the best interests of the military readiness community or the u.s. taxpayer .

accordingly , we encourage the department to reconsider its position when it submits its written statement of the actions taken on our recommendations to the senate committee on homeland security and governmental affairs and the house committee on oversight and government reform , as well has the house and senate committees on appropriations , as required under 31 u.s.c .

720 .

dod's specific comments on each recommendation , along with our responses to its comments follow .

the department did not agree with our recommendation for the deputy secretary of defense , as the chair of the defense business systems management committee , to reconsider the committee's recent approval of drrs planned investment for fiscal years 2009 and 2010 , and to convene the defense business systems management committee to review the program's past performance and the dio's capability to manage and deliver drrs going forward in deciding how best to proceed .

in this regard , dod stated that the investment review board certification and defense business systems management committee approval were granted in compliance with the established processes .

it also added that oversight of the specific issues identified in this report are the responsibility of the drrs executive committee , which it stated has and will continue to provide appropriate governance for this effort .

it also stated that usd ( p&r ) will ensure that the program is compliant with all acquisition requirements prior to submission for further certifications .

we do not question whether the investment review board certification and defense business systems management committee approval were provided in accordance with established processes , as this is not relevant to our recommendation .

rather , our point is that the investment review board and defense business systems management committee were provided , and thus based their respective decisions , on erroneous and incomplete information about drrs progress , management weaknesses , and risks .

moreover , neither the investment review board nor the defense business systems management committee were informed about the findings in our report , even though we shared each of them with the drrs program director and other dio officials prior to both the investment review board and the defense business systems management committee deliberations .

therefore , while the investment review board certification and the defense business systems management committee approval were granted in accordance with established processes , they were not based on a full disclosure of facts .

moreover , while we support dod's comment that it will ensure that the program is in compliance with all acquisition requirements prior to further certifications , nothing precludes the board or the committee from reconsidering their respective decisions in light of our report .

with respect to dod's comment that the drrs executive committee has and will continue to provide appropriate governance for this effort , we do not disagree that the drrs executive committee has an oversight role .

however , the drrs executive committee should not be solely responsible for oversight of the specific issues in our report .

both the investment review board and the defense business systems management committee provide additional layers of oversight pursuant to law and dod policy .

accordingly , we stand by our recommendation as it appropriately seeks to have the investment review board and defense business systems management committee , in collaboration with the drrs executive committee , act in a manner that is consistent with their respective roles as defined in law .

in doing so , our intent is to promote accountability for drrs progress and performance , and prompt action to address the many risks facing the program .

the department agreed with our recommendation for the deputy secretary of defense , as the chair of the defense business systems management committee , to have the business transformation agency conduct a risk assessment of drrs , and with the advice and recommendation of the drrs executive committee , to use the results of this assessment and the findings in our report to decide how to redirect the program .

in this regard , the department stated that this assessment will be complete by the middle of fiscal year 2010 .

the department did not agree with our recommendation for ensuring that drrs requirements are effectively developed and managed .

in this regard , it stated that the program has an authoritative set of baseline requirements established with an effective governance process for overseeing the requirements management process , to include biweekly reviews as part of the drrs configuration control process .

we do not agree .

at the time we concluded our work , drrs requirements were not stable , as evidenced by the fact that an additional 530 requirements had been identified that the dio was still in the process of reviewing and had yet to reach a position on their inclusion , or process them through the drrs change control governance process .

moreover , when we concluded our work , this change control process had yet to be approved by the drrs governance structure .

as we state in our report , the introduction of such a large number of requirements provided a compelling basis for concluding that requirements had yet to progress to the point that they could be considered sufficiently complete and correct to provide a stable baseline .

our recommendation also noted that the secretary should take steps to ensure that the different levels of requirements be aligned with one another .

dod's comments did not address this aspect of our recommendation .

the department did not agree with our recommendation for ensuring that drrs testing is effectively managed .

in this regard , it stated that drrs testing is already in place and performing effectively , and stated , among other things , that ( 1 ) the dio goes through a rigorous testing regimen that includes documenting test plans with user test cases for each incremental release to include utilizing system integration , acceptance , interoperability , and operational testing ; ( 2 ) user test cases and functionality are validated by designated testers independent of the developers prior to a deployment ; and ( 3 ) for interoperability testing the dio has a designated test director and the joint interoperability test command ( jitc ) is the designated interoperability and operational test activity .

we do not agree .

as our report concludes , drrs testing has not been effectively managed because it has not followed a rigorous testing regimen that includes documented test plans , cases , and procedures .

to support this conclusion , our report cites numerous examples of test planning and execution weaknesses , as well as the dio's repeated inability to demonstrate through requisite documentation that the testing performed on drrs has been adequate .

our report shows that test events for already acquired , as well as currently deployed and operating , drrs releases and subreleases were not based on well - defined plans and dod had not filled its testing director vacancy .

further , our report shows that test events were not fully executed in accordance with plans that did exist , or executed in a verifiable manner , or both .

for example , although increments of drrs functionality had been put into production , the program had no documentation ( eg , test procedures , test cases , test results ) to show that the program office had performed system integration testing , system acceptance testing , or operational testing on any drrs release or subrelease , even though the dio's test strategy stated that such tests were to be performed before system capabilities became operational .

moreover , evidence showed that the results of all executed test events had not been captured and used to ensure that problems discovered were disclosed to decision makers , and ultimately corrected .

with respect to dod's comments that jitc is the designated lead for interoperability and operational testing , our report recognizes that jitc is to conduct both interoperability and operational testing before the system is deployed and put into production ( i.e. , used operationally ) .

however , during the course of our audit , the dio could not produce any evidence to show that interoperability and operational testing of all operating system increments had been conducted .

the department did not agree with our recommendation for ensuring that the drrs integrated master schedule is reliable .

in this regard , it stated that a process is already in place to ensure that the schedule is current , reliable , and meets all the criteria outlined in the recommendation .

we do not agree .

as our report states , an integrated master schedule for drrs did not exist until november 2008 , which was 2 months after we first requested one .

moreover , following our feedback to the dio on limitations in this initial version , a revised integrated master schedule was developed in january 2009 , which was also not reliable .

subsequently , a revised integrated master schedule was developed in april 2009 .

however , as we detail in our report , that version still contained significant weaknesses .

for example , it did not establish a critical path for all key activities or include a schedule risk analysis , and was not being updated using logic and durations to determine the dates for all key activities .

these practices are fundamental to producing a sufficiently reliable schedule baseline that can be used to measure progress and forecast slippages .

in addition , the schedule introduced considerable concurrency across key activities and events for several modules , which introduces increased risk .

therefore , we stand by our recommendation .

the department partially agreed with our recommendation for ensuring that it has an effective human capital strategy .

in this regard , it stated that actions are underway to add more full - time civilian support to the dio , and that plans exist to convert some contractor to civilian billets during the 2010 / 2011 time frame .

we support the department's actions and plans described in its comments to address the dio human capital management limitations discussed in our report , but would note that they do not go far enough to systematically ensure that the program has the right people with the right skills to manage the program in both the near term and the long term .

to accomplish this , the department needs to adopt the kind of strategic and proactive approach to drrs workforce management that our report describes and our recommendation embodies .

as our evaluations and research show , failure to do so increases the risk that the program office will not have the people it needs to effectively and efficiently manage drrs .

therefore , we believe that the department needs to fully implement our recommendation .

the department did not agree with our recommendation to take steps to ensure that drrs is developed and implemented in a manner that does not increase the reporting burden on units and addresses the timeliness , precision , and objectivity of metrics that are reported to congress .

in this regard , it stated that one of the primary tenets of drrs has been to reduce reporting requirements on the war fighter .

it also stated that drrs is already using state - of - the - art technology to ensure that near - real - time data are available for the war fighters .

finally it stated that the drrs governance structure that is currently in place ensures that drrs development does not deviate from these core principles .

while we recognize that a goal of drrs is to reduce a reporting burden on the war fighter , we disagree with the department's position because the system has not yet achieved this goal .

as our report states , while the dio has developed the software for users to enter mission essential task data into drrs , the dio has been unsuccessful in attempts to develop a tool that would allow air force and marine corps users to enter readiness data to meet all of their readiness reporting requirements through drrs .

as a result , rather than reducing the burden on reporting units , drrs actually increased the burden on air force and marine corps units because they were required to report readiness information in both drrs and gsorts .

without a viable tool for inputting data , drrs is not fully integrated with gsorts or with the service readiness reporting systems and it is not capable of replacing those systems since it does not capture the required data that are contained in those systems .

in addition , the drrs readiness data that are currently reported to congress are not near - real - time data .

specifically , the periodicity for drrs capability assessments is the same as the legacy gsorts system's readiness reports — monthly or when an event occurs that changes a unit's overall readiness .

furthermore , our report shows that drrs mission assessments are often subjective and imprecise because they are reported on a scale that includes only three ratings — ”yes,” “no,” and “qualified yes,” which can include any assessments that fall between the two extremes .

therefore , because additional actions are still needed to reduce reporting burdens and improve the timeliness , precision , and objectivity of the drrs data that are reported to congress , we stand by our recommendation .

the department agreed with our recommendation for ensuring that both the human resources management investment review board and the drrs executive committee conduct frequent oversight activities of the drrs program and report any significant issues to the deputy secretary of defense .

in this regard , the department stated that the usd ( p&r ) component acquisition executive is working with the program to ensure that it becomes fully compliant with all acquisition requirements .

in addition , it stated that the acquisition executive will certify to the human resources investment review board and the deputy chief management officer of compliance prior to submission of future certification requests .

further , it stated that the current drrs governance process will provide sustained functional oversight of the program and that issues that arise in any of these areas will be elevated for review , as appropriate .

we believe these are positive steps .

we are sending copies of this report to the appropriate congressional committees ; the secretary of defense ; and the director , office of management and budget .

the report will also be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staffs have questions about this report , please contact us at pickups@gao.gov or hiter@gao.gov or at our respective phone numbers , ( 202 ) 512-9619 and ( 202 ) 512- 3439 .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iv .

our objectives were to assess the extent to which ( 1 ) the department of defense ( dod ) has effectively managed and overseen drrs acquisition and deployment , and ( 2 ) features of the defense readiness reporting system ( drrs ) have been implemented and are consistent with legislative requirements and dod guidance .

we did not evaluate the department's overall ability to assess the readiness of its forces or the extent that data contained in any of its readiness reporting systems , including drrs and the global status of resources and training system ( gsorts ) , reflect capabilities , deficiencies , vulnerabilities , or performance issues .

our review focused on acquisition and program management issues , such as requirements management , schedule , and human capital requirements ; the current usage of drrs ; and the extent to which drrs' features address legislative requirements and dod guidance .

to determine the extent to which the drrs acquisition and deployment has been effectively managed and overseen , we focused on the following acquisition management areas: ( 1 ) requirements development and management , ( 2 ) test planning and execution , ( 3 ) drrs schedule reliability , and ( 4 ) human capital planning .

in doing so , we analyzed a range of program documentation , such as high - level and detailed - level requirements documentation , test plans and reports , the current drrs schedule , and program management documentation and interviewed cognizant program and contractor officials .

to determine the extent to which the program had effectively implemented requirements development and management , we reviewed relevant program documentation , such as the concept of operations document , capability requirements document , software requirements document , requirements traceability matrix , configuration management plan , and the program management plan , as well as minutes of change control board meetings , and evaluated them against relevant guidance .

moreover , we reviewed briefing slides from meetings of drrs oversight bodies in order to identify concerns about drrs expressed by representatives from the drrs community of users , as well as the efforts by the joint staff ( at the direction of drrs executive committee ) to identify and address any gaps identified by users in the development of drrs requirements .

to determine the extent to which the program has maintained traceability backward to high - level business operation requirements and system requirements , and forward to system design specifications and test plans , we randomly selected 60 program requirements and traced them both backward and forward .

this sample was designed with a 5 percent tolerable error rate at the 95 percent level of confidence so that , if we found zero problems in our sample , we could conclude statistically that the error rate was less than 5 percent .

in addition , we interviewed program and development contractor officials to discuss the requirements development and management process .

to determine if the drrs implementation office ( dio ) is effectively managing drrs testing , we reviewed relevant documentation , such as the drrs test and evaluation master plans and test reports and compared them to dod and other relevant guidance .

further , we reviewed developmental test plans and procedures for each release / subrelease that to date has either been developed or fielded and compared them with best practices to determine whether test activities had been adequately documented .

we also examined test results and reports for the already acquired , as well as currently deployed and operating , drrs releases and subreleases and compared them against plans to determine whether they had been executed in accordance with plans .

moreover , we reviewed key test documentation , such as the software version descriptions , and compared them against relevant guidance to determine whether defect data were being captured , analyzed , prioritized , and reported .

we also interviewed program and contractor officials to gain clarity beyond what was included in the program documentation , including the defense information systems agency's joint interoperability test center in order to determine the results of their efforts to independently test drrs interoperability .

in addition , to determine the extent to which the program had effectively tested its system requirements , we observed the dio's efforts to demonstrate the traceability of 60 program requirements to test cases and results .

this sample was designed with a 5 percent tolerable error rate at the 95 percent level of confidence so that , if we found zero problems in our sample , we could conclude statistically that the error rate was less than 5 percent .

to determine the extent to which the program's schedule reflects key estimating practices that are fundamental to having a reliable schedule , we reviewed the drrs integrated master schedules and schedule estimates and compared them with relevant guidance .

we also used schedule analysis software tools to determine whether the latest schedule included key information , such as the activities critical to on - time completion of drrs , a logical sequence of activities , and evidence that the schedule was periodically updated .

we also reviewed the schedule to determine the time frames for completing key program activities and to determine any changes to key milestones .

in addition , we shared the results of our findings with program and contractor officials and asked for clarifications .

we then reviewed the revised schedule , prepared in response to the weaknesses we found , and compared it with relevant guidance .

to evaluate whether dod is adequately providing for the drrs program's human capital needs , we compared the program's efforts against relevant criteria and guidance , including our own framework for strategic human capital management .

in doing so , we reviewed key program documentation , such as the program management plan and the dio organizational structure to determine whether it reflected key acquisition functions and identified whether these functions were being performed by government or contractor officials .

we interviewed key officials to discuss workforce analysis and human capital planning efforts .

to determine the level of oversight and governance available to the drrs community of users , we attended relevant meetings , met with officials responsible for program certification , and reviewed relevant guidance and program documentation .

specifically , we attended battle staff meetings and analyzed briefing slides and meeting minutes from the drrs executive committee , general and flag officer's steering committee , and battle staff meetings — the main drrs governance bodies .

in addition , we reviewed key drrs certification and approval documentation provided by the human resources management investment review board , such as economic viability analyses and the business system certification dashboard and met with investment review board officials to determine the basis for certifying and approving drrs .

to determine the extent to which the features of drrs have been implemented and are consistent with legislative requirements and dod guidance , we first examined the language of section 117 of title 10 of the united states code , which directs the secretary of defense to establish a comprehensive readiness reporting system .

we identified criteria for this system in dod's directive formally establishing the system .

we evaluated the system by conducting interviews — see table 2 below for a list of these organizations — and receiving system demonstrations from members of the readiness community to determine how they used drrs and how their usage compared with the criteria established for the system .

we also conducted content and data analysis of system documents and briefing packages provided by the dio and joint staff .

in order to capture the broadest amount of data about the system we conducted a survey of readiness offices at all of the service headquarters , combatant commands , and the national guard bureau regarding how drrs was currently being used and the types and amount of data available in the system .

in addition , to track the development of drrs capabilities , we attended battle staff meetings and analyzed documentation from meetings of all the drrs governance bodies .

we also searched for and extracted information from drrs in order to support other gao ongoing readiness reviews .

while our usage of the system was not intended as a formal test of the system , our general observations concerning system functionality and the range of available data were consistent with the observations of most other users , which were noted in our survey .

we conducted our work from april 2008 through august 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

our research and evaluations of information technology programs have shown that the ability to deliver promised system capabilities and benefits on time and within budget largely depends on the extent to which key program management disciplines are employed by an adequately staffed program management office .

among other things , these disciplines include a number of practices associated with effectively developing and managing system requirements , adequately testing system capabilities , and reliably scheduling the work to be performed .

they also include proactively managing the program office's human capital needs , and promoting program office accountability through effective program oversight .

department of defense ( dod ) acquisition policies and guidance , along with other relevant guidance , recognize the importance of these management and oversight disciplines .

as we have previously reported , not employing these and other program management disciplines increases the risk that system acquisitions will not perform as intended and require expensive and time - consuming rework .

defense readiness reporting system ( drrs ) acquisition and deployment has for years not been effectively managed in accordance with these key program management disciplines that are recognized in dod and other relevant guidance , and are fundamental to delivering a system that performs as intended on time and within budget .

well - defined and well - managed requirements are a cornerstone of effective system development and acquisition .

according to recognized guidance , documenting and implementing a disciplined process for developing and managing requirements can help to reduce the risks of producing a system that is not adequately tested , does not meet user needs , and does not perform as intended .

effective requirements development and management includes , among other things , ( 1 ) effectively eliciting user needs early and continuously in the system life - cycle process , ( 2 ) establishing a stable baseline set of requirements and placing this baseline under configuration management , ( 3 ) ensuring that system requirements are traceable backward to higher level business or operational requirements ( eg , concept of operations ) and forward to system design documents ( eg , software requirements specification ) and test plans , and ( 4 ) controlling changes to baseline requirements .

drrs requirements have not been effectively developed and managed .

specifically , ( 1 ) key users have only recently become engaged in developing requirements , ( 2 ) requirements have been experiencing considerable change and are not yet stable , ( 3 ) different levels of requirements and related test cases have not been aligned with one another , and ( 4 ) changes to requirements have not been effectively controlled .

as a result , efforts to develop and deliver initial drrs capabilities have taken longer than envisioned and these capabilities have not lived up to the readiness communities' expectations .

these failures increase the risk of future drrs capabilities not meeting expectations and ensure that expensive and time - consuming system rework will be necessary .

one of the leading practices associated with effective requirements development is engaging system users early and continuously in the process of defining requirements .

as we have previously reported , assessing user needs early in the process increases the probability of success in defining , designing , and delivering a system that meets user needs and performs as intended .

to the drrs implementation office's ( dio ) credit , the october 2008 drrs risk management plan recognizes this by stating that the success of drrs depends on participation and support from the broad readiness community , which includes combatant commands , joint staff , and the military services .

however , until recently , key users were not effectively engaged in drrs requirements development and management , although reasons vary why they have not .

specifically , dio officials told us that beginning in 2002 , they reached out to all user groups — combatant commands , joint staff , and the military services — in defining requirements .

for example , they cited a july 2002 memorandum issued by the office of the under secretary of defense for personnel and readiness ( usd p&r ) that encouraged the director of the joint chiefs of staff , deputy commanders of the combat commands , service operations deputies , and directors of defense agencies to actively support the drrs effort by ensuring that their organizations are represented at battle staff meetings .

however , these officials told us that the military services and joint staff chose not to participate .

in contrast , officials from these user groups told us their involvement had been limited by what they characterized as difficulties in submitting requirements through the drrs governance boards that were in place at that time .

for example , an official from the joint forces command said that the forces battle staff governance board did not meet for about a year between 2005 and 2006 .

further , the official said that the meetings that were held did not offer users the opportunity to discuss their concerns or influence the requirements process .

similarly , an official from the marine corps cited a lack of clear and transparent communication from the dio as a significant impediment .

notwithstanding this lack of stakeholder involvement in setting requirements , the office of usd ( p&r ) developed and issued a drrs concept of operations in 2004 , which dio officials told us was based on input from the combatant commands , relevant departmental directives , and drrs governance boards ( eg , battle staff ) .

in our view , this document provided a high - level overview of proposed drrs capabilities from which more detailed requirements could be derived .

however , the concept of operations was not approved by all key players in the readiness community .

specifically , dio officials stated that the document had not been approved by the military services and the joint staff .

according to these officials , the reason for not seeking all stakeholders' approval , and the decision to begin developing more detailed requirements in the absence of an approved concept of operations , was that the 2002 drrs dod directive provided a sufficient basis to begin developing and deploying what they anticipated being the initial versions of drrs .

in 2008 , after 6 years of effort to define drrs requirements and develop and deploy system capabilities , the joint staff , at the direction of the drrs executive committee , conducted an analysis of drrs capabilities — referred to as the “capabilities gap analysis.” to the joint staff's credit , this analysis has appropriately focused on soliciting comments from the entire readiness community and on identifying any gaps between the drrs requirements and the needs of this community .

as will be discussed in the next section , this analysis resulted in 530 additional user requirements .

the extended period of limited involvement by key drrs users in defining a concept of operations and related capabilities and requirements has impeded efforts to develop a clear understanding of drrs expectations , constraints , and limitations , which , in turn , has contributed to significant delays in providing the readiness community with needed system support .

while the recent joint staff action to engage the entire drrs user community is a positive step towards overcoming this long - standing problem , it remains to be seen whether this engagement will produce agreement and commitment across the entire readiness user community around drrs requirements .

as previously noted , establishing an authoritative set of baseline requirements prior to system design and development is necessary to design , develop , and deliver a system that performs as intended and meets users' operational needs .

in general , a baselined set of requirements are those that are defined to the point that extensive changes are not expected , placed under configuration management , and formally controlled .

drrs requirements are currently in a state of flux .

specifically , the fact that 530 new user requirements have recently been identified means that the suite of requirements documentation associated with the system will need to be changed and thus are not stable .

to illustrate , program officials told us that , as of late february 2009 , these 530 new requirements had not been fully evaluated by the dio and drrs governance boards and thus not yet approved .

as a result , their impact on the program is not clear .

compounding this instability in the drrs requirements is the fact that additional changes are envisioned .

according to program officials , the changes resulting from the gap analysis and reflected in the latest version of the drrs concept of operations , which was approved by the drrs executive committee in january 2009 , have yet to be reflected in other requirements documents , such as the detailed system requirements .

although defining and developing requirements is inherently an iterative process , having a baseline set of requirements that are stable is a prerequisite to effective and efficient development of an operationally capable and suitable system .

without them , the dio will not be able to deliver a system that meets user needs on time , and it is unlikely that future development and deployment efforts will produce better results .

one of the leading practices associated with developing and managing requirements is maintaining bidirectional traceability from high - level operational requirements ( eg , concept of operations and functional requirements ) through detailed lower - level requirements and design documents ( eg , software requirements specification ) to test cases .

such traceability is often accomplished through the use of a requirements traceability matrix , which serves as a crosswalk between different levels of related requirements , design , and testing documentation .

the drrs program management plan recognizes the importance of traceability , stating that requirements are to be documented and linked to acceptance tests , scripts , and criteria .

despite the importance of traceability , dio officials could not demonstrate that requirements and related system design and testing artifacts are properly aligned .

specifically , we attempted on three separate occasions to verify the traceability of system requirements backward to higher - level requirements and forward to lower - level software specifications and test cases , and each time we found that traceability did not exist .

each attempt is discussed here: in november 2008 , our analysis of the requirements traceability matrix and the software requirements specification showed significant inconsistencies .

for example , the traceability matrix did not include 29 requirements that were included in the software requirements specification .

as a result , we did not have an authoritative set of requirements to use to generate a random sample of requirements to trace .

program officials attributed the inconsistencies to delays in updating all the documents to reflect the aforementioned capability gap analysis .

they also stated that these documents would be updated by december 2008 .

in december 2008 , we used an updated requirements traceability matrix to generate a randomized sample of 60 software requirements specifications and observed a dio demonstration of the traceability of this sample .

however , dio officials were unable to demonstrate for us that these specifications could be traced backward to higher - level requirements and forward to test cases .

specifically , attempts to trace the first 21 requirements forward to test cases failed , and dio officials stated that they could not trace the 60 requirements backward because the associated requirements documents were still being updated .

according to the officials , 11 of the 21 could not be traced forward because these were implemented prior to 2006 and the related test information was not maintained by the program office but rather was at the development contractor's site .

they added that the remaining 10 either lacked test case information or test results .

in february 2009 , we used an updated dio - provided requirements traceability matrix , a capabilities requirement document , and software requirements specification to generate another randomized sample of 60 detailed specifications .

we then observed the development contractor's demonstration of traceability using the contractor's requirements management tool .

because of time constraints , this demonstration focused on 46 of the 60 requirements , and it showed that adequate traceability still did not exist .

specifically , 38 of the 46 could not be traced backward to higher - level requirements or forward to test cases .

this means that about 83 percent of the drrs specifications ( 95 percent degree of confidence of being between 72 and 91 percent ) were not traceable .

of the 38 , 14 did not trace because of incomplete traceability documentation ; 5 due to inconsistent traceability documentation ; 3 due to requirements not being resident in the tracking tool ; and 16 due to no actual development work being started .

in addition , none of the 46 requirements were traceable to the january 2009 concept of operations .

according to contractor officials , this is because the newly developed capability requirements document is considered to be a superset of the concept of operations , and thus traceability to this new document is their focus .

however , they were unable to demonstrate traceability to the requirements in either the capability requirements document or the concept of operations .

further , we also found numerous inconsistencies among the capabilities requirements document , software requirements specification , and the requirements traceability matrix .

for example , 15 capabilities requirements listed on the traceability matrix were not listed in the capabilities requirements document , but were listed in the updated software requirements specification , dated february 2009 .

further , one requirement listed in the traceability matrix was not listed in either of these documents .

one possible reason for these inconsistencies is that the traceability matrix was prepared manually , rather than being automatically generated from the tool , which would increase the probability of these and other discrepancies caused by human error .

another reason cited by program officials is that test results that occurred prior to october 2006 had yet to be fully recorded in the contractor's tracking tool .

dio and contractor officials attributed the absence of adequate requirements traceability to the ongoing instability in requirements and magnitude of the effort to update the chain of preexisting and new requirements documentation .

they added that they expect traceability to improve as requirements become more stable and the documentation is updated .

regardless , the dio has and continues to invest in the development of drrs in the absence of requirements traceability .

without traceable requirements , the dio does not have a sufficient basis for knowing that the scope of the design , development , and testing efforts will produce a system solution on time and on budget and that will meet users' operational needs and perform as intended .

as a result , the risk is significant that expensive and time - consuming system rework will be required .

adopting a disciplined process for reviewing and accepting changes to an approved and authoritative baseline set of requirements in light of the estimated costs , benefits , and risk of each proposed change is a recognized best practice .

elements of a disciplined process include: ( 1 ) formally documenting a requirements change process ; ( 2 ) adopting objective criteria for considering proposed changes , such as estimated cost or schedule impact ; and ( 3 ) rigorously adhering to the documented change control process .

since the inception of the program in 2002 , drrs has been developed and managed without a formally documented and approved process for managing changes to system requirements .

further , while requirements management and change control plans were developed in 2006 by the drrs software development contractor , according to dio officials , the plans were not adequate .

for example , the plans did not detail how drrs user requirements were collected or how objective factors , such as cost , impacted development decisions .

to address these problems , the joint staff developed what it referred to as a conceptual requirements change control process in february 2008 , which was to serve as a basis for the dio to develop more detailed plans that could be implemented .

eleven months later , in january 2009 , the dio drafted more detailed plans — a drrs requirements management plan and a drrs requirements configuration management plan , the latter of which the dio updated in march 2009 .

specifically , the draft plans call for new drrs requirements to be collected using an online tool and reviewed by the dio to determine whether the requirement constitutes a major change to drrs .

once approved , the dio and the contractor are to provide the battle staff with a formatted report specifying the anticipated benefit of each new requirement and an initial analysis of the cost and performance impact .

the battle staff then is to prioritize the requirement based on the dio's impact analysis .

if the issue cannot be resolved by the battle staff , it is to be elevated to the senior oversight bodies ( i.e. , the general officer's steering committee and the drrs executive committee ) .

after a requirement has been approved , the software developer may prepare a more detailed “customer acceptance document” that analyzes the potential cost , schedule , and quality impact to drrs objectives , which is then to be reviewed by the dio at subsequent change control board meetings .

however , according to the user community and the dio director , the revised plans have not been submitted for review and approval to the drrs community .

specifically , they stated that only a proposed process flow diagram was briefed at the battle staff , and according to them , the change control process was still being evaluated .

moreover , the dio has yet to implement key aspects of its draft plans .

for example , the drrs chief engineer stated that until recently , the dio had continued to accept changes to drrs requirements that were submitted outside of the designated online tool .

in addition , the reports that the battle staff are to use in making their requirement change determination do not include the anticipated benefit and estimated cost or schedule impact of new requirements .

rather , these reports only include the estimated number of hours necessary to complete work on a proposed requirement .

moreover , contractor officials and users from the special operations command told us that cost or schedule impacts have rarely been discussed at the battle staff or change control board meetings .

our analysis of minutes from change control meetings confirmed this .

furthermore , the drrs chief engineer stated that the customer acceptance documents have only recently been used .

until the dio effectively controls requirements changes , it increases the risk of needed drrs capabilities taking longer and costing more to deliver than necessary .

effective system testing is essential to successfully developing and deploying systems like drrs .

according to dod and other relevant guidance , system testing should be progressive , meaning that it should consist of a series of test events that first focus on the performance of individual system components , then on the performance of integrated system components , followed by system - level tests that focus on whether the system ( or major system increments ) are acceptable , interoperable with related systems , and operationally suitable to users .

for this series of related test events to be conducted effectively , ( 1 ) each test event needs to be executed in accordance with well - defined test plans , ( 2 ) the results of each test event need to be captured and used to ensure that problems discovered are disclosed and corrected , and ( 3 ) all test events need to be governed by a well - defined test management structure .

despite acquiring and partially deploying a subset of drrs increments , the dio cannot demonstrate that it has adequately tested any of these system increments , referred to as system releases and subreleases .

specifically , ( 1 ) the test events for already acquired , as well as currently deployed and operating , drrs releases and subreleases were not based on well - defined plans , and test events have not been fully executed in accordance with plans or executed in a verifiable manner , or both ; ( 2 ) the results of executed test events have not been captured and used to ensure that problems discovered were disclosed to decision makers and ultimately corrected ; and ( 3 ) the dio has not established an effective test management structure to include , for example , a clear assignment of test management roles and responsibilities , or a reliable schedule of planned test events .

compounding this absence of test management structures and controls is the fact that the dio has yet to define how the series of system releases and subreleases relate to its recent restructuring of drrs increments into a series of 10 modules .

collectively , this means that it is unlikely that already developed and deployed drrs capabilities can perform as intended and meet user operational needs .

equally doubtful are the chances that the dio can adequately ensure that yet - to - be developed drrs capabilities will meet expectations .

key tests required for already developed and partially fielded drrs increments either did not have well - defined test plans , or these tests have yet to be conducted .

according to program documentation , system releases and subreleases have been subjected to what are described as 30- day test cycles , during which: ( 1 ) a software test plan is updated if applicable , ( 2 ) test procedures are developed and incorporated in the software test description , ( 3 ) a series of developmental tests on each release / subrelease is performed , ( 4 ) weekly meetings are held to review software defects identified during testing , ( 5 ) final test results are summarized within the software test report and software version description , and ( 6 ) the release / subrelease is made available to users .

however , the program office has yet to provide us with the developmental test plans and procedures for each release / subrelease that to - date has either been developed or fielded .

instead , it provided us with a software test plan and two software test descriptions that it said applied to two subreleases within release 4.0 .

however , available information indicates that drrs subreleases total at least 63 , which means that we have yet to receive the test plans and procedures for 61 .

further , the test plan that we were provided is generic in nature , meaning that it was not customized to apply specifically to the two subreleases within release 4.0 .

moreover , the plan and procedures lack important elements specified in industry guidance .

for example , the test plan does not include a schedule of activities to be performed or defined roles and responsibilities for performing them .

also , the test plan does not consistently include test entrance and exit criteria , a test defect management process , and metrics for measuring progress .

moreover , the dio has yet to demonstrate that it has performed other key developmental and operational test events that are required before the software is fielded for operational use .

according to dio officials , developmental testing concludes only after system integration testing and system acceptance testing , respectively , are performed .

further , following developmental testing , the joint interoperability test command ( jitc ) , which is a dod independent test organization , is to conduct both interoperability and operational testing before the system is deployed and put into production ( i.e. , used operationally ) .

although increments of drrs functionality have been put into production , the dio has not performed system integration testing , system acceptance testing , or operational testing on any drrs release or subrelease .

further , jitc documentation shows that while an interoperability test of an increment of drrs functionality known as esorts was conducted , this test did not result in an interoperability certification .

according to jitc and joint staff officials , this was because the dio did not address jitc's identified limitations to the program's information support plan , which identifies essential information - exchange sharing strategies between interdependent systems that are needed for interoperability certification .

without interoperability certification , the ability of the drrs to exchange accurate and timely readiness data with other critical systems , such as the joint operation planning and execution system , cannot be ensured .

similarly , while dio officials stated that acceptance testing has occurred for one increment of drrs functionality known as sortsrep , the dio does not have either a finalized acceptance test plan or documented test results .

furthermore , the integrated master schedule ( last updated in april 2009 ) shows that acceptance testing is not to occur until the july / august 2009 time frame , which is about 15 months later than originally envisioned .

moreover , this delay in acceptance testing has in turn delayed interoperability and operational testing by 16 months ( may / june 2008 to september / november 2009 ) , according to the latest schedule .

program officials attributed the delays to marine corps and air force concerns about the quality of sortsrep .

until the dio has effectively planned and executed the series of tests needed to demonstrate the readiness of drrs increments to operate in a production environment , the risk of fielded system increments not performing as intended and requiring expensive rework to correct will be increased , and dod will continue to experience delays in delivering mission - critical system capabilities to its readiness community .

available results of tests performed on already developed and at least partially deployed drrs releases / subreleases show that the test results have not been effectively captured and analyzed , and have not been fully reported .

moreover , test results for other releases / subreleases do not exist , thus minimizing the value of any testing that has been performed .

according to relevant guidance , effective system testing includes recording the results of executing each test procedure and test case as well as capturing , analyzing , correcting , and disclosing to decision makers problems found during testing ( test defects ) .

it also includes ensuring that test entry and exit criteria are met before beginning and ending , respectively , a given test event .

the dio does not have test results of all developed and tested drrs releases and subreleases .

specifically , program officials provided us with the software test reports and software version descriptions that , based on program documentation , represent the full set of test results for three subreleases and a partial set of test results for 40 subreleases within releases 1.0 , 3.0 , and 4.0 .

however , as noted earlier , drrs subreleases total at least 63 , which means that test reports and results for at least 20 subreleases do not exist .

moreover , the test reports and version descriptions that we received do not consistently include key elements provided for in industry guidance , such as a documented assessment of system capabilities and limitations , entrance / exit criteria status , an assessment as to whether the applicable requirements / thresholds were met , and unresolved defects and applicable resolution plans .

this information is important because it assists in determining and disclosing to decision makers current system performance and efforts needed to resolve known problems , and provides program officials with a needed basis for ensuring that a system increment is ready to move forward and be used .

without this information , the quality and readiness of a system is not clear .

furthermore , the dio does not have detailed test defect documentation associated with all executed drrs test events .

according to relevant guidance , defect documentation should , among other things , identify each issue discovered , assign each a priority / criticality level , and provide for each a strategy for resolution or mitigation .

in lieu of detailed test defect documentation , program officials referred us to the above - mentioned software version descriptions , and stated that additional information is available in an automated tool , known as the isi bugtracker , that it uses to capture , among other things , defect data .

however , these documents do not include the above - cited defect information , and defect data for each test event do not exist in the isi bugtracker .

compounding the absence and limitations of test results are weaknesses in the program office's process for collecting such results during test execution .

according to relevant guidance , test results are to be collected and stored according to defined procedures and placed under appropriate levels of control .

furthermore , these test results are to be reviewed against the source data to ensure that they are complete , accurate , and current .

for drrs , the program office is following a partially undocumented , manual process for collecting and storing test results and defects that involves a database and layers of documentation .

as explained by program officials and documentation , the dio initially documents defects and completed test case results manually on paper forms , and once the defect is approved by the test lead , it is input into a database .

however , it does not have written procedures governing the entire process , and thus key controls , such as assigned levels of authority for database read / write access , are not clearly defined .

moreover , once the test results and defects are input into the database , traceability back to the original test data for data integrity checks cannot be established because the program office does not retain these original data sets .

program officials acknowledged these internal control weaknesses and stated that they intend to adopt a new test management tool that will allow them to capture in a single database test cases , test results , and test defects .

furthermore , the dio's process for analyzing and resolving test defects has limitations .

according to relevant guidance and the draft sortsrep test and evaluation master plan ( temp ) , defects should be analyzed and prioritized .

however , the program office has not established a standard definition for defect priority levels identified during testing .

for example , the various release / subrelease test reports ( dated through january 2009 ) prioritize defects on a scale of 1-3 , where a priority 2 means critical but with a viable workaround .

in contrast , the sortsrep temp ( dated january 2009 ) prioritizes defects on a scale of 1-5 , where a priority 2 means an error that adversely affects the accomplishment of an operational or mission essential function in accordance with official requirements so as to degrade performance and for which no alternative work around solution exists .

by not using standard priority definitions for categorizing defects , the program office cannot ensure that it has an accurate and useful understanding of the scope and magnitude of the problems it is facing at any given time , and it will not know if it is addressing the highest priority issues first .

in addition , the dio has not ensured that critical defects are corrected prior to concluding a given test event .

according to relevant guidance and the draft sortsrep temp , all critical and high defects should be resolved prior to the conclusion of a test event , and all test results should be reviewed for validity and completeness .

however , the drrs release / subrelease test reports show that the dio concluded five test events even though each had at least 11 open critical defects ( priority 1 defects with no workaround ) .

moreover , these numbers of open critical defects are potentially higher because they do not include defects for which a solution was identified but the solution failed during regression testing and do not include defects that were dismissed because the program official was unable to recreate it .

until the dio adequately documents and reports the test results , and ensures that severe problems discovered are corrected prior to concluding a given test event , the probability of incomplete test coverage , and insufficient and invalid test results , is increased , thus unnecessarily increasing the risk of drrs not meeting mission needs or otherwise not performing as intended .

the dio does not have an effective test management structure , to include a well - defined overall test management plan that clearly assigns test management roles and responsibilities , a designated test management lead and a supporting working group , and a reliable schedule of planned test events .

according to relevant guidance , these aspects of test management are essential to adequately planning , executing , and reporting a program's series of test events .

although the program has been underway for 8 years , it did not have an overarching drrs temp until very recently ( february 2009 ) , and this plan is still in draft and has yet to be approved .

further , this draft temp does not clearly define drrs test management roles and responsibilities , such as those of the test manager , and it does not include a reliable schedule of test events that reflect the program's recent restructuring of its software releases / subreleases into 10 modules .

according to dio officials , they recently decided not to approve this overarching temp .

instead , they said that they now intend to have individual temps for each of the recently defined 10 modules , and to have supporting test plans for each module's respective developmental and operational test events .

according to program documentation , three individual temps are under development ( i.e. , sortsrep tool and the mission readiness and readiness review modules ) .

however , drafts of these temps also do not clearly define test entrance and exit criteria , test funding requirements , an integrated test program schedule , and the respective test management roles and responsibilities .

for example , while the draft sortsrep temp identifies the roles and responsibilities of some players , such as the test manager , the personnel or organization that is to be responsible is not always identified .

in addition , while the various players in the user community are identified ( i.e. , military services , combatant commands ) , their associated roles or responsibilities are not .

furthermore , the dio has yet to designate a test management lead and establish an effective test management working group .

according to relevant guidance , test management responsibility and authority should be assigned to an individual , and this individual should be supported by a working integrated product team that includes program office and operational testing representatives .

among other things , the working integrated product team is to develop an overall system test strategy .

however , dio officials told us that the test manager position has been vacant , and this position is now being temporarily filled by the program's chief engineer , who is a contractor .

furthermore , although drrs system development began prior to 2004 , a charter for a test and evaluation working integrated product team was not issued until february 2009 .

according to dio officials , the delay in establishing the team has not had any impact because of corresponding delays in finalizing the program's overall test strategy .

however , this statement is not consistent with the defense acquisition guidebook , which states that two of the key products of the working integrated product team are the program's test strategy and temp .

further , jitc officials stated that the lack of a test manager and an active test and evaluation working integrated product team have reduced the effectiveness of drrs testing activities .

as a result , they stated that they have had to compensate by conducting individual meetings with the user community to discuss and receive documentation to support their operational and interoperability test planning efforts .

moreover , the dio has yet to establish a reliable schedule of planned test events .

for example , the schedule in the temps is not consistent with either the integrated master schedule or the developmental test plans .

specifically , the draft sortsrep temp ( last updated in january 2009 ) identifies sortsrep developmental testing occurring through january 2009 and ending in early february 2009 , while the integrated master schedule ( last updated in april 2009 ) shows sortsrep development testing as occurring in the july / august 2009 time frame .

in addition , while program officials said that development testing for sortsrep has occurred , the associated development test plans ( eg , system integration and system acceptance test plans ) had no established dates for test execution , and are still in draft .

as another example , a module referred to as “mission readiness” had no established dates for test execution in its temp , and while program documentation indicates that this module completed development testing in december 2008 , the associated development test plans ( eg , system integration and system acceptance test plans ) do not exist .

in addition , the dio has yet to define in its draft temps how the development and testing to date of at least 63 subreleases relate to the development and testing of the recently established 10 system modules .

according to joint staff and jitc officials , they do not know how the releases / subreleases relate to the modules , and attributed this to a lack of an approved description for each module that includes what functionality each is intended to provide .

furthermore , the high - level schedule in the temp does not describe what test events for the drrs releases / subreleases that have already been developed and deployed relate to the development test efforts planned for the respective modules .

these problems in linking release / subrelease test events to module test events limit the dio and jitc in leveraging the testing already completed , which in turn will impact the program's ability to meet cost , schedule , and performance expectations .

collectively , the weaknesses in this program's test management structure increase the chances that the deployed system will not meet certification and operational requirements , and will not perform as intended .

the success of any program depends in part on having a reliable schedule that defines , among other things , when work activities will occur , how long they will take , and how they are related to one another .

as such , the schedule not only provides a road map for the systematic execution of a program , but also provides the means by which to gauge progress , identify and address potential problems , and promote accountability .

from its inception in 2002 until november 2008 , the dio did not have an integrated master schedule .

moreover , the only milestone that we could identify for the program prior to november 2008 was the date that drrs was to achieve full operational capability , which was originally estimated to occur in fiscal year 2007 , but later slipped to fiscal year 2008 and then fiscal year 2011 , and is now fiscal year 2014 — a 7-year delay .

in addition , the drrs integrated master schedule that was developed in november 2008 , and was updated in january 2009 and again in april 2009 to address limitations that we identified and shared with the program office , is still not reliable .

specifically , our research has identified nine practices associated with developing and maintaining a reliable schedule .

these practices are ( 1 ) capturing all key activities , ( 2 ) sequencing all key activities , ( 3 ) assigning resources to all key activities , ( 4 ) integrating all key activities horizontally and vertically , ( 5 ) establishing the duration of all key activities , ( 6 ) establishing the critical path for all key activities , ( 7 ) identifying float between key activities , ( 8 ) conducting a schedule risk analysis , and ( 9 ) updating the schedule using logic and durations to determine the dates for all key activities .

however , the program's latest integrated master schedule does not address three of the practices and only partially addresses the remaining six .

for example , the schedule does not establish a critical path for all key activities , include a schedule risk analysis , and it is not being updated using logic and durations to determine the dates for all key activities .

further , it does not fully capture , sequence , and establish the duration of all key work activities ; fully assign resources to all key work activities ; fully integrate all of these activities horizontally and vertically ; and fully identify the amount of float — the time that a predecessor activity can slip before the delay affects successor activities — between these activities .

these practices are fundamental to producing a sufficiently reliable schedule baseline that can be used to measure progress and forecast slippages .

 ( see table 3 for the results of our analyses relative to each of the nine practices. ) .

the limitations in the program's latest integrated master schedule , coupled with the program's 7-year slippage to date , make it likely that drrs will incur further delays .

compounding these limitations is the considerable concurrency in the key activities and events in the schedule associated with the 10 recently identified system modules ( see fig .

2 ) .

for example , in 2010 alone , the program office plans to complete development testing on 2 modules and operational testing on 3 modules , while also reaching initial operational capability on 3 modules and full operational capability on 2 modules .

by way of comparison , the program office had almost no concurrency across a considerably more modest set of activities and events over the last 5 years , but nevertheless has fallen 7 years behind schedule .

as previously reported , such significant overlap and concurrency among major program activities can create contention for limited resources and thus introduce considerable cost , schedule , and performance risks .

in addition , the schedule remains unstable as evidenced by the degree of change it has experienced in just the past few months .

for example , the january 2009 schedule had a full operational capability milestone of october 2011 .

by contrast , the april 2009 schedule has a december 2013 milestone ( see fig .

3 below ) .

moreover , some milestones are now to occur much earlier than they were a few months ago .

for example , the january 2009 schedule shows initial operational capability for “readiness reviews” to be june 2010 .

however , the april 2009 schedule shows that that this milestone was attained in august 2007 .

overall , multiple milestones for four modules were extended by at least 1 year , including two milestones that were extended by more than 2 years .

such change in the schedule in but a few months suggests a large degree of uncertainty , and illustrates the ut a few months suggests a large degree of uncertainty , and illustrates the importance of ensuring that the schedule is developed in accordance with importance of e nsuring that the schedule is developed in accordance with best practices .

best practices .

as we have previously reported , effective human capital management is an essential ingredient to achieving successful program outcomes .

among other things , effective human capital management involves a number of actions to proactively understand and address any shortfalls in meeting a program's current and future workforce needs .

these include an assessment of the core competencies and essential knowledge , skills , and abilities needed to perform key program management functions , an inventory of the program's existing workforce capabilities , and an analysis of the gap between the assessed needs and the existing capabilities .

moreover , they include explicitly defined strategies and actions for filling identified gaps , such as strategies for hiring new staff , training existing staff , and contracting for support services .

the dio is responsible for performing a number of fundamental drrs program management functions .

for example , it is responsible for acquisition planning , performance management , requirements development and management , test management , contractor tracking and oversight , quality management , and configuration management .

to effectively perform such functions , program offices , such as the dio , need to have not only well - defined policies and procedures and support tools for each of these functions , but also sufficient human capital to implement the processes and use the tools throughout the program's life cycle .

without sufficient human capital , it is unlikely that a program office can effectively perform its basic program management functions , which in turn increases the risk that the program will not deliver promised system capabilities and benefits on time and on budget .

the dio does not currently have adequate staff to fulfill its system acquisition and deployment responsibilities .

in particular , the dio is staffed with a single full - time government employee — the dio director .

all other key program office functions are staffed by either contractor staff or staff temporarily detailed , on an as - needed basis , from other dod organizations ( referred to as “matrixed” staff ) .

as a result , program management positions that the dio itself has identified as critical to the program's success , such as configuration manager and security manager , are being staffed by contractors .

moreover , these contractor staff report to program management positions also staffed by contractors .

other key positions , such as those for performing acquisition management , requirements development and management , and performance management , have not even been established within the dio .

furthermore , key positions , such as test manager , are vacant .

these human capital limitations were acknowledged by the drrs executive committee in november 2008 .

according to dio and contractor officials , they recognize that additional program management staffing is needed .

they also stated that while drrs has been endorsed by usd ( p&r ) leadership and received funding support , past requests for additional staff have not been approved by usd ( p&r ) due to other competing demands for staffing .

further , dio officials stated that the requests for staff were not based on a strategic gap analysis of its workforce needs and existing capabilities .

specifically , the program has not assessed its human capital needs and the gap between these needs and its onboard workforce capabilities .

until the program office adopts a strategic , proactive approach to managing its human capital needs , it is unlikely that it will have an adequate basis for obtaining the people it needs to effectively and efficiently manage drrs .

in addition to the contacts named above , key contributors to this report were michael ferren ( assistant director ) , neelaxi lakhmani ( assistant director ) , april baugh , mathew butler , richard j. hagerman , nicole harms , james houtz , john lee , stephen pruitt , terry richardson , karen richey , karl seifert , and kristy williams .

