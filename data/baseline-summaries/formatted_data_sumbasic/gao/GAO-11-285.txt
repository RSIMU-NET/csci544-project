across the country , the workforce development systems' one - stop centers serve as the key access point for services that are crucial in today's economy — services that include unemployment insurance ( ui ) benefits , job training , and employment assistance .

the department of labor's ( labor ) employment and training administration ( eta ) is responsible for guiding the nearly $13 billion public workforce development system .

its mission is to help make the u.s. labor market function more efficiently by developing policies that lead to high - quality job training , employment , labor market information , and income maintenance services .

to help shape its policies , eta conducts evaluations and research studies on a range of employment - related topics .

congress appropriated about $103 million to eta's research and evaluation budget line items for 2010 .

over the last decade , however , eta's research and evaluation program has fallen short in its efforts to conduct research that can help answer urgent workforce policy questions .

in 2008 and again in 2009 , we faulted eta for failing to conduct research and evaluations that would lead to an understanding of what works and what doesn't .

for example , in january 2010 , we reported on shortcomings in eta's research structure and processes .

we found that eta's research and evaluation center , the office of policy development and research ( opdr ) , lacked independent authority to make key decisions about its research ; maintained processes that were unclear and that lacked transparency lacked a standard process for ensuring stakeholder involvement or other strategies to ensure that research addressed key national priorities ; and had been slow to distribute its research findings and slow to respond to its statutory mandate to evaluate the workforce investment act of 1998 ( wia ) .

based on these findings , we made several recommendations to labor to improve eta's research program .

 ( for information on the status of those recommendations , see app .

i. ) .

as eta's leadership moves forward to help the nation meet its current employment challenges , questions remain about how well eta's research has prepared the workforce development system for the challenges of today .

against this backdrop , you asked us to build upon our january 2010 review and further examine eta's research program .

specifically , we answered the following questions: 1 .

to what extent do eta's research priorities reflect key national employment and training issues and how useful were the studies funded under them ? .

2 .

what steps has eta taken to improve its research program ? .

3 .

how has eta improved , if at all , the availability of its research since our last review in january 2010 and what other steps could eta take to further ensure its research findings are readily available ? .

to address our objectives , we reviewed the 58 research and evaluation reports that eta disseminated between january 2008 and march 2010 and assessed the methodological soundness of the 11 completed studies that cost $1 million or more .

in addition , we reviewed the 10 ongoing studies costing $2 million or more to determine if research practices or the soundness of research designs had changed over time .

in addition , we convened a virtual ( delphi ) expert panel of academics , researchers , and consultants to obtain their opinions on eta's research priorities and dissemination methods .

to learn how eta determines what research to conduct , we interviewed labor officials and reviewed relevant agency and budget documents .

in addition , we conducted site visits to two local workforce agencies in pennsylvania and virginia that are implementing eta's ongoing research studies to obtain information about challenges and issues associated with participating in studies .

to evaluate the availability of eta's research , we analyzed dissemination time frames for all publications released between january 2008 and march 2010 and we tested the ability of eta's research database to support searches generally available to users of research databases .

 ( see app .

ii for more details on our objectives , scope , and methodology. ) .

we conducted this performance audit from march 2009 through march 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

wia sets forth various requirements for the secretary of labor relating to research and evaluation of federally funded employment - related programs and activities .

the law calls upon the secretary of labor to publish in the federal register every 2 years a plan that describes its pilot , demonstration , and research priorities for the next 5 years regarding employment and training .

specifically , wia requires the secretary to develop the research plan after consulting with states , localities , and send the plan to the appropriate committees of congress ; and take into account such factors as the likelihood that the results of the projects will be useful to policymakers and stakeholders in addressing employment and training problems .

within eta , opdr's division of research and evaluation plans , conducts , and disseminates employment and training - related research and evaluations .

nearly all of the agency's research and evaluation studies are conducted under contract ; these contractors represent a range of research organizations and academic institutions .

furthermore , opdr plans and conducts its research and evaluation activities in consultation with eta's program offices , such as the office of workforce investment and the office of trade adjustment assistance .

eta's research and evaluation funding is divided into two separate budget line items: pilots , demonstrations , and research .

efforts in this category are focused on developing and testing new ways to approach problems and to deliver services .

under wia , pilots and demonstrations shall be carried out “for the purpose of developing and implementing techniques and approaches , and demonstrating the effectiveness of specialized methods , in addressing employment and training needs.” wia also states that the secretary shall “carry out research projects that will contribute to the solution of employment and training problems in the united states.” evaluations .

efforts in this category are focused on continuing evaluations of certain programs and activities carried out under wia .

these evaluations must address the effectiveness of these programs and activities carried out under wia in relation to their cost ; the effectiveness of the performance measures relating to these programs and activities ; the effectiveness of the structure and mechanisms for delivery of services through these programs and activities ; the impact of the programs and activities on the community and participants involved , and on related programs and activities ; the extent to which such programs and activities meet the needs of various demographic groups ; and such other factors as may be appropriate .

in program year 2010 , eta's combined budget appropriation for conducting evaluations and pilots , demonstrations , and research was about $103 million — or nearly $34 million above what the agency requested .

 ( see fig .

1. ) .

about $84 million of the 2010 funds were designated by the congress for specific projects , including $30 million for transitional jobs activities for ex - offenders , and another $5.5 million for competitive grants addressing the employment and training needs of young parents .

according to agency documents , in 2008 and 2009 , the congress similarly increased eta's requested budget for pilots , demonstrations , and research , at the same time specifically designating how the majority of those funds would be used , including $4.9 million in 2008 and $5 million in 2009 for the young parents' demonstration .

while there is no single or ideal way for government agencies to conduct research , several leading national organizations have developed guidelines that identify key elements that promote a sound research program .

these guidelines identify five elements as key: agency resources , professional competence , independence , evaluation policies and procedures , and evaluation plans .

resources .

research should be supported through stable , continuous funding sources and through special one - time funds for evaluation projects of interest to executive branch and congressional policymakers .

professional competence .

research should be performed by professionals with appropriate training and experience for the evaluation activity ( such as performing a study , planning an evaluation agenda , reviewing evaluation results , or performing a statistical analysis ) .

independence .

although the heads of federal agencies and their component organizations should participate in establishing evaluation agendas , budgets , schedules , and priorities , the independence of evaluators must be maintained with respect to the design , conduct , and results of their evaluation studies .

evaluation policy and procedures .

each federal agency and its evaluation centers should publish policies and procedures and adopt quality standards to guide evaluations within its purview .

such policies and procedures should identify the kinds of evaluations to be performed and the criteria and administrative steps for developing evaluation plans and setting priorities , including selecting evaluation approaches to use , consulting experts , ensuring evaluation product quality , and publishing reports .

evaluation plans .

each federal agency should require its major program components to prepare annual and multiyear evaluation plans and to update these plans annually .

the planning should take into account the need for evaluation results to inform program budgeting , reauthorization , agency strategic plans , program management , and responses to critical issues concerning program effectiveness .

these plans should include an appropriate mix of short - and long - term studies to produce results for short - or long - term policy or management decisions .

to the extent practical , the plans should be developed in consultation with program stakeholders .

furthermore , leading organizations , including the american evaluation association and the national academy of sciences , emphasize the need for research programs to establish specific policies and procedures to guide research activities .

based on several key elements identified by these organizations , we developed a framework comprised of five phases — agenda setting , selecting research , designing research , conducting research , and disseminating research results .

 ( see fig .

2. ) .

agenda setting .

agencies should establish a structured process for developing their research priorities .

the process should identify how agencies set research priority areas and provide for updating the areas on a regular basis .

the process should also allow for the consideration of critical issues and state how internal and external stakeholders will be included in developing the plan .

selecting research .

at this phase , the process should identify how the research program's staff identifies and selects studies to fund , including the criteria it uses to make those decisions .

steps might describe how the staff assembles a list of potential studies , works with internal program offices , and makes final decisions .

designing research .

during the design phase , the process should identify steps taken to select appropriate research approaches and methods and the safeguards in place to ensure appropriate tradeoffs are made between what is desirable and what is practical and between the relative strengths and weaknesses of different methods .

conducting research .

at this stage , the process should include policies and procedures to guide the conduct of research .

the process should ensure that key events , activities , and time frames are specified and that knowledgeable staff in the sponsoring agency monitor the implementation of the research .

disseminating research .

this process should describe how research findings are made available to the public and disseminated to all potential users .

these dissemination methods should include safeguards to ensure research findings are disseminated in a timely manner and are accessible through the internet with user - friendly search and retrieval technologies .

in this report , we use several technical terms in describing eta's research designs and study characteristics .

 ( see table 1. ) .

our expert panel generally considered eta's research areas to be the right ones for the period the research plan covered .

about three - fourths of the panel members reported that eta's 2007 to 2012 research agenda reflected key employment and training issues to at least a moderate extent .

however , a few experts commented that some of eta's research areas may be too broad and lack specificity .

the areas in eta's 2007 to 2012 research plan covered a range of issues , from job training to postsecondary education .

table 2 illustrates the scope of eta's research areas .

with regard to the specific studies within these research areas , eta invested most of its research and evaluation resources in work that focused on increasing the labor market participation of underutilized workers and on ui .

of the estimated $96 million that supported the 58 research reports published between january 2008 and march 2010 , more than half — about $56 million — funded research that addressed these two research areas .

other areas received far less funding .

for example , funding for studies addressing the methods of expanding u.s. workforce skills and using state - level administrative data to measure progress and outcomes accounted for about $6.5 million , or about 6.7 percent of the cost of studies published during the period we examined .

 ( see table 3. ) .

overall , the individual studies that eta funded addressed a wide variety of issues and ranged in cost from about $15,000 to a high of about $22 million .

in addition to the research areas covered in eta's 2007 to 2012 research plan , experts from our virtual panel suggested that eta incorporate additional research areas in its future research agenda .

of the research areas identified , over half of our experts ( 28 of 39 ) ranked the identification of employment and training approaches that work , and for whom , as one of the top areas that eta's future research should address .

 ( see fig .

3. ) .

without such focus , experts commented that it will be difficult to know how to improve the nation's workforce system .

other issues ranked at the top by experts included research on job creation strategies and the impact of long - term and short - term training .

 ( see app .

iii for more information on issue - area rankings. ) .

in addition to identifying overall employment and training areas , including issues related to ui , experts also identified specific aspects of the ui system that could be examined in eta's future research .

in particular , most experts ( 34 of 39 respondents ) reported that it would be at least moderately important , in the future , for eta to research the linkage between ui and employment and safety net programs , such as temporary assistance for needy families or the supplemental nutrition assistance program .

 ( see fig .

4. ) .

this area of research may be particularly important given the role that these programs play in supporting individuals during economic downturns .

in addition , many experts ( 24 of 39 respondents ) mentioned that eta should make the examination of the incentives and disincentives in the ui system a research priority , given the challenge of supporting unemployed workers during difficult economic times , while promoting self - sufficiency through employment .

experts also reported that it is important to fund research on what works for selected population groups .

of the population groups identified , the experts on our virtual panel most often ranked the long - term unemployed , economically disadvantaged workers , and adults with low basic skills as the top populations on which to focus future research .

specifically , several experts commented that research could help to identify the challenges some of these groups face , as well as identify effective strategies that may help these population groups obtain employment .

 ( see app .

iii for a complete list of responses to these items. ) .

in addition to population groups , experts also identified several employment and training programs that they believe warrant research attention .

in particular , experts most often ranked three components of the wia program — wia adult , wia dislocated workers , and wia youth — as key to evaluate in eta's future research .

among those three , wia adult was ranked the highest .

 ( see app .

iii for a complete list of experts' responses on employment and training programs to evaluate. ) .

research organizations and academic institutions with responsibility for implementing eta - funded research generally used methodologies appropriate for the questions posed , but the studies were not always useful for informing policy and practice .

from january 2008 through march 2010 , eta published 17 large research and evaluation reports — 14 evaluations and 3 research reports — that each cost $1 million or more .

four of these reports were designed to demonstrate what works and for whom .

each of these four reports compared the employment - related outcomes of individuals or regions who participated in training or employment programs with the employment outcomes of similar individuals who did not participate in the programs .

the remaining 13 reports were descriptive and were not designed to assess program outcomes .

in several studies we examined that cost $1 million or more , we found that , for a number of reasons , eta's research studies were limited in their usefulness and in their ability to inform policy and practices .

for example , in a study of the prisoner re - entry initiative , shortcomings in the data collection phase limited the strength of the findings and , as a result , limited the study's opportunity to influence policy directions .

among other things , while the study provided information on employment - centered opportunities for ex - offenders , the study relied on self - reported baseline data , did not account for differences across sites where services were received , lacked the capacity to record differences in the intensity of those services , and researchers failed to ensure that data collectors were properly trained .

in another study , researchers did not control for bias in selecting participants , compromising their ability to draw conclusions about the cause and effect of program outcomes .

authors of this study on the workforce innovations in regional economic development ( wired ) initiative acknowledged that the study would be unable to attribute outcomes to program services because it did not use random assignment in selecting participating regions .

we have previously criticized eta for failing to adequately provide for evaluating the effectiveness of its wired initiative .

moreover , some studies were limited due to observation periods that did not match the needs of the studies' objectives .

for example , an evaluation of an entrepreneurship training project was unable to assess the effectiveness of the project in meeting its long - term goals of increasing business ownership and self - sufficiency because the time frames for the study were too short .

in this study , data collection was limited to 18 months after participants were randomly assigned , a period far shorter than the 60-month period recommended by experts .

 ( see app .

iv for additional information on the methodological characteristics of these studies. ) .

experts generally agreed that eta's research had limited usefulness in informing policy and practice .

over one - third of the 39 experts reported that over the past 5 years , eta's research informed employment and training policy and state and local practices to a little extent or not at all .

 ( see fig .

5. ) .

some experts commented that the design of these studies and the length of time to complete them and disseminate results reduced their usefulness .

for example , many of the reports that we reviewed costing $1 million or more were multiyear projects that took , in most cases , about 3 to 5 years to complete .

some experts commented that the inclusion of shorter - length studies may be useful in times of rapidly changing economic conditions .

at least one expert noted that some mixed - methods studies would be useful — studies that would allow for short - term interim findings that could facilitate changes in practice during the course of the research study .

members of our expert panel stressed the importance of eta incorporating varied methodological approaches into its future research proposals to best position the agency to address key employment and training issues .

twenty - seven of the 39 experts reported it was very important that eta evaluate its pilots and demonstrations .

twenty - three reported that it was very important that more randomized experimental research designs be integrated into eta's future research .

 ( see fig .

6. ) .

while several experts noted that these randomized experiments will allow eta to identify the effectiveness of particular interventions or strategies , at least one expert suggested that eta should be strategic in choosing the interventions it tests more rigorously , basing those decisions on what appears most promising in preliminary studies .

furthermore , 16 of the 39 experts also reported that it is very important for eta to consider including more quasi - experimental studies in the future .

as previously discussed , such studies would include designs that compare outcomes between groups with similar characteristics , but do not use random assignment .

by including more quasi - experimental designs , eta may be able to better understand the link between services and outcomes in those settings where random assignment is not possible , ethical , or practical .

labor has taken several steps designed to improve the way it conducts research , both at the department level and within eta .

department - level efforts .

labor has changed the organizational structure of research within the department .

in 2010 , acknowledging the need for better and more rigorous evaluations to inform its policy , labor established the chief evaluation office to oversee the department's research and evaluation efforts .

the office , which resides within the office of the assistant secretary for policy , has no authority to direct research within labor's agencies , according to officials .

it does , however , manage evaluations supported by funds from a departmentwide account , oversee departmentwide evaluations , and provide consultation to labor agencies , including eta .

specifically , the office is responsible for creating and maintaining a comprehensive inventory of past , ongoing , and planned evaluation activities within labor and for ensuring that labor's evaluation program and findings are transparent , credible , and accessible to the public .

in fiscal year 2010 , the chief evaluation office had an estimated budget of $8.5 million , and two of its four staff were on board by the beginning of fiscal year 2011 .

eta efforts .

eta has recently made changes to some of its research practices — chief among them is the involvement of stakeholders and outside experts in the research process .

we previously criticized eta for failing to consistently involve a broad range of stakeholders , outside experts , or the general public in deciding what areas of research it should undertake .

we recommended that eta take steps to routinely involve outside experts in the research agenda - setting process .

for the upcoming 2010 to 2015 research plan , eta has awarded a grant to the heldrich center at rutgers university to convene an expert panel to help inform the research plan .

the center is expected to issue a report in may 2011 that outlines the panel's recommendations for research areas to include in the plan .

in addition , eta will work with other labor agencies , as well as the departments of education and health and human services , before finalizing its research agenda .

officials told us that they will also solicit public comments before the research plan is finalized .

in addition to engaging stakeholders , eta has also established a formal research process .

as we previously reported , eta developed and documented its research process in 2007 .

the agency's actions were in response to a request by the office of management and budget ( omb ) to establish more formal policies and procedures to guide its research — a request that came out of omb's concerns about the manner in which eta's research was being carried out .

prior to 2007 , eta lacked a documented research process , and its research was often conducted in an ad hoc manner .

eta's current research process identifies the steps , activities , and time frames it uses to carry out its research .

figure 7 illustrates critical components of eta's 8-step research process .

eta's process contains several of the key elements identified by leading organizations as important for guiding research activities .

for example , the process includes specific steps the agency should take to identify the types of evaluations it will perform , as well as the administrative steps it should take to develop evaluation plans and select the research projects to fund .

in addition , the process also specifies key events and time frames , and provides for monitoring the implementation of the research .

for example , the process stipulates that eta should alert omb of research reports that have not been approved for dissemination within 9 months of being submitted and allows contractors to publicly release their research reports within those same time frames .

despite eta's efforts , more action is needed to improve its research program .

while eta has taken steps to document its research process , its process lacks specific details in some areas , creating ambiguities that could undermine efforts to adhere to a formal process .

for example , as we previously reported , its process lacks clear criteria , such as a dollar threshold or a particular methodological design feature , for determining which projects require peer review .

and while the process specifies the actions project officers should take if reports are not released in a timely manner , it does not specify the consequences for failing to do so .

we previously recommended that eta establish more specific processes , including time frames for disseminating research reports .

eta has taken some action , such as revising the performance standards for project officers to hold them accountable for meeting time frames , but these steps do not fully satisfy the recommendation because the changes are not yet reflected in the formal research process .

moreover , eta's process is missing some critical elements that are needed to ensure that the current improvements become routine practices .

consulting with the chief evaluation officer .

eta's process lacks a formal provision requiring consultation with the newly established chief evaluation officer at important points in the research process .

for example , it contains no provision for consulting with the chief evaluation officer when developing its annual list of research projects or when determining how eta will invest its research and evaluation resources .

such consultation could help labor better coordinate its research and evaluation efforts and better leverage its research funding .

moreover , the process contains no provision for involving the chief evaluation officer in the early stages of developing its research projects .

in the recent past , labor officials told us that eta has had difficulty developing requests for research and evaluation proposals that can pass omb technical reviews .

in particular , omb has been critical of eta's research designs because they failed to provide for adequate sample size and appropriate methodologies that are needed to obtain useful results .

in addition , omb has also expressed concerns with eta's reliance on process evaluations rather than focusing on outcomes .

these difficulties have resulted in delays in the research process .

eta has begun to consult with the chief evaluation officer ; however , these consultations are not a routine component in the formal process .

setting the research agenda .

eta's current process , as documented , begins with phase two — selecting specific research studies — and misses the important first step of setting the overall research agenda .

this first phase of the process should include the steps that eta will take to establish its research priorities and to update them on a regular basis .

it should also include provisions for ensuring critical issues are considered and internal and external stakeholders are included in developing the plan .

officials noted that they plan to incorporate the agenda - setting phase into its formal process , but have not yet done so .

setting the research agenda is key to ensuring that an appropriate mix of studies is included in future research .

failing to make this phase part of the formal process , including the specific steps to involve outside stakeholders that are currently under way , may leave eta with little assurance that these efforts will continue in the future .

beyond eta's process for conducting research , current research practices fall short of ensuring research transparency and accountability — essential elements of a sound research and evaluation program .

the research program has few , if any , safeguards to protect it from undue influence .

according to officials , at times in the past decade , many key research decisions have been made outside of the office that is responsible for research .

for example , decisions about which research studies would and would not be publicly released were made at the highest levels within eta , and the criteria used to make those decisions were unclear .

of the 34 reports that eta released to the public in 2008 , 20 had waited between 2 and 5 years to be approved for public release .

several reports that had experienced long delays had relatively positive and potentially useful findings for the workforce system , according to our analysis .

among the studies delayed by almost 5 years was an evaluation of labor exchange services in the one - stop system that found certain employment services to be highly cost - effective in some situations .

another study , delayed for about 3.5 years , was a compendium of past and ongoing experimental studies of the workforce system , including early findings and recommendations for future research .

in our previous report , we noted that eta's research and evaluation center lacked a specific mechanism to insulate it from undue influence .

we reported that other federal agencies , such as the department of education's institute of education sciences and the national science foundation , engage advisory bodies in the research process .

while not without tradeoffs in terms of additional time and effort , such an approach may serve to protect the research program from undue influence and improve accountability .

eta is currently involving outside experts in setting the research agenda for 2010 to 2015 , but is not involving experts more broadly on research policy and practices .

eta has recently begun to include more rigorous studies in its ongoing research .

of the 10 large , ongoing studies costing $2 million or more that began during the period of our review , three — the wia gold standard evaluation of the adult and dislocated worker programs , the impact evaluation of the young parents demonstration , and the evaluation of project growing america through entrepreneurship ii ( project gate ii ) — use experimental design with random assignment , as recommended by our experts .

these ongoing studies — which range in cost from $2 million to nearly $23 million — have the potential to determine the effectiveness of some of the program services .

table 4 outlines some key characteristics of these three studies .

experimental designs with random assignment are an important means to understand whether various program components or services are effective , but they are also often difficult to design and implement in real - world settings .

for example , in doing evaluations of employment and training programs , researchers often have difficulty in recruiting sample sizes large enough to detect meaningful outcomes .

because employment and training services may vary by location , and participants and their socio - economic environments are diverse , researchers must find ways to standardize procedures and treatment or service options .

this often means recruiting relatively large samples .

however , studies can be intrusive , often requiring program sites to change how they operate or to increase the resources available to participants .

as a result , recruiting sites and sufficient numbers of participants may be difficult .

some of eta's ongoing research studies face challenges in recruiting sample sizes large enough to meet the studies' objectives .

for example , based on an omb review , it was determined that the sample size for the impact evaluation of the young parents demonstration had to be much larger in order to be able to assess the effectiveness of the program .

at that time , eta had already awarded two phases of grants .

after consulting with the new chief evaluation officer , eta changed the number of participants required for the third phase from 100 to 400 to obtain a sample large enough to address omb's concerns and provide reliable estimates .

however , grantees found it difficult to recruit even the 100 participants in the smaller sample , and it remains unclear whether they will be able to recruit all of the needed participants for the expanded design .

the wia gold standard evaluation illustrates eta's difficulties in planning and executing large - scale , rigorous random assignment studies .

wia required that the secretary of labor conduct at least one multi - site control - group evaluation of the services and programs under wia by the end of fiscal year 2005 .

eta , however , delayed executing such a study , finally soliciting proposals in november 2007 and awarding the contract in june 2008 .

the contractor submitted the initial design report in january 2009 and provided eta with design revisions in may 2010 .

officials tell us researchers will soon begin randomly assigning participants .

eta expects to receive the first report ( on implementation ) during the winter of 2012- 2013 and the final report in 2015 — 10 years later than the wia - mandated time frame .

an omb - selected panel of government experts — a technical working group composed of experts chosen by eta , the evaluation contractor , and omb staff — reviewed the original design for this study .

reviewers agreed the design contained many strengths , including the selection of an experimental design and a net impact approach ; the addition of a process or implementation study to evaluate differences among sites and other implementation and data collection issues ; the use of administrative and survey data ; the collection of information on services received by participants in the control group ; and the collection of a wide range of outcome data for participants .

however , reviewers raised several concerns regarding the design .

for example , they were skeptical that the researchers would be able to obtain a sufficiently large and representative sample to draw meaningful conclusions about the effectiveness of the national workforce system .

in order to maximize participation , officials told us that the assistant secretary of eta made personal phone calls to all selected sites to emphasize the importance of the study , offered an open door policy to site officials to discuss issues , and followed up with an appreciation letter .

furthermore , eta required the evaluation contractor to provide reimbursement payments to each site to offset implementation costs .

reviewers also had several other concerns regarding which groups would be included in the study and which groups would not .

for example , some experts raised concerns about getting accurate information on the youth program because of the large , one - time infusion of funds the program received from the american recovery and reinvestment act of 2009 .

reviewers were further concerned about the appropriateness of the evaluation objectives , the adequacy of steps taken to account for the effect of variation in services across sites on evaluation outcomes , and the external validity or generalizability of the study .

in order to address these concerns , eta made substantial adjustments to the original design .

specifically , eta officials told us that based on an agreement with omb , they instructed the contractor to drop the youth component from the evaluation and to focus only on the adult and dislocated worker programs .

while we received information on the new design and time frames for the wia gold standard evaluation , a finalized design plan is not yet available .

according to officials , a finalized design is being prepared and will be available in june 2011 .

eta has recently improved the timeliness with which it disseminates its research reports .

in our last review in january 2010 , we found that 20 of the 34 reports that eta disseminated in 2008 had been waiting 2 to 5 years to be publicly released .

the 34 research reports published by eta in 2008 took , on average , 804 days from the time the report was submitted to eta until the time it was posted to eta's research database .

by contrast , from 2009 through the first quarter of 2010 , the average time between submission and public release was 76 days , which represents a more than 90 percent improvement in dissemination time compared with 2008 .

additionally , there were no research reports in 2009 that were delayed for more than 6 months .

further , the average time to dissemination improved significantly even when we excluded such outliers as the 20 research reports that were delayed for 2 years or more .

without these outliers , average time to dissemination for reports in 2008 was 100 days , indicating that time to dissemination in 2009 through the first quarter of 2010 still improved by 24 percent .

in 2010 , eta updated its online , web - based search page in order to improve the usability of its research database — the primary tool for making eta research available to policymakers and the general public .

officials told us that eta's old web - based search page was so error - prone and difficult to use that they opted to substitute it with one that had not yet completed internal testing .

our review of the old web - based search page confirmed that it had serious limitations and did not consistently return the same results .

for example , when we searched the database by title for a known eta research report titled registered apprenticeship , we successfully retrieved that report once .

one month later , when we entered the exact same search terms , we were unable to retrieve the report .

 ( for a more complete description of our analysis of eta's search capability , see app .

ii. ) .

in our review of the updated web - based search page , we found that the updates make the research database more useable .

labor officials told us they have taken other steps , as well , in efforts to improve its web - based search page .

for example , they have developed a project plan that articulates the steps labor will take to update eta's web - based search page .

in addition , they have assigned a database administrator whose responsibilities include performing daily quality control spot checks in order to monitor performance and address technical problems .

although these changes have the potential to improve the usability of eta's database , labor has not developed a formal plan for assessing the overall effectiveness of its web - based search page , including user satisfaction .

labor has made a number of changes to the way the page operates , but it has not provided users with tips on how to use the search functions , even though it is an industry standard to do so .

even skilled users who were familiar with the old web - based search page may need guidance on the exact meaning of new terms and functions now available on the new page .

for example , the old web - based search page gave users the option of searching by “key word,” which is no longer an option in the new page .

instead , “key word” searches have been replaced with a variety of other options , including the ability to search the full text or abstract of a research report .

however , there is no guidance on the web site on how to use these new search options .

industry best practices suggest that a web site evaluation plan that incorporates data from routine reviews of web site performance and that assesses user satisfaction can help agencies ensure the usability of their web sites .

eta currently has no plans to do such assessments .

at present , eta's research database is the primary method that eta uses to make its research reports publicly available , according to officials .

in order to call attention to new reports available in that database , eta sends a training and employment notice , also commonly known as a ten , to an e - mail list of the more than 40,000 subscribers who have signed up to receive them .

eta's research process specifies that for each new research report that is approved for dissemination , eta must draft a ten and an abstract before it is posted to eta's web site .

beyond posting reports to its database , eta also distributes hard copies of some of its research reports .

in addition to electronic distribution , eta also organizes various presentations to disseminate its research findings .

these presentations , however , are done on an ad hoc basis .

as mentioned in our prior report , eta hosted a research conference in 2009 to present some of its research findings , renewing a practice that had been discontinued in 2003 .

as eta looks to the future , officials tell us they will plan and organize similar research conferences as resources permit .

in addition to these research conferences , eta's regional offices occasionally hold smaller , regional conferences as well .

beyond these formal conferences , eta also hosts an internal briefing series at labor headquarters where research contractors present their findings to various officials .

for each of these briefings , eta has a list of stakeholders that it invites , including various labor officials , outside agency officials , congressional staff , and other outside stakeholders .

experts who participated in our virtual panel provided their views on the effectiveness of different methods for disseminating research reports , and several of those rated more highly are methods currently employed by eta .

 ( see fig .

8. ) .

most of the experts ( 30 of the 39 respondents ) in our panel reported that using e - mail notifications , a searchable database of eta papers , and briefings at eta for external audiences ( including stakeholders and policymakers ) would be very effective or extremely effective approaches for disseminating research .

in addition , a majority of the experts ( 26 of the 39 respondents ) in our panel reported that publishing one - page summaries of research findings , not currently done by eta , would be very or extremely effective .

eta plays an important role in developing workforce policies and helping to identify the most effective and efficient ways to train and employ workers for jobs in the twenty - first century .

with the current economic crisis and high unemployment rates , eta's role has become even more critical .

the agency has made some improvements in its research program , even since our last review a year ago .

but officials can do more to ensure that the progress continues in the years to come .

eta has taken a major step forward in establishing a formal research process — one that documents most actions that must be taken in the life cycle of a research or evaluation project .

but , it is missing some key elements that could help ensure the continuation of current practices .

while eta is currently using outside advisory bodies to help it establish its research agenda , the formal process does not include the agenda - setting phase .

officials tell us they have plans to incorporate this phase in the future , and we urge them to do so .

without a formalized agenda - setting phase , eta may miss opportunities to ensure that its research agenda addresses the most critical employment and training issues and that outside stakeholders are routinely involved .

moreover , eta's process has not formalized the now ad hoc advisory role of the chief evaluation officer .

absent the routine involvement of the chief evaluation officer at key steps in the process , eta may find it difficult to ensure that research proposals are asking the right questions , are methodologically sound , and that they can quickly pass critical omb reviews .

eta's research findings are now available to the public on its web site in far less time than it took in 2008 .

despite this clear improvement , eta has not taken the necessary steps to ensure that research products remain readily available to the public .

the decision regarding what and when to make research publicly available is left in the hands of too few , and the process lacks needed safeguards to ensure transparency and accountability .

absent safeguards , key research decisions may again be made in ways that harm the credibility of the program and prevent important research findings from being used to inform policy and practice .

eta's web - based search page is the primary means eta uses to make the research studies it funds readily available to the public .

and , while eta has improved the functionality of its web site , no effort has been made to ensure that the problems that plagued the system in the past do not recur .

absent such efforts , eta will have little assurance that its research findings are actually available to users .

to improve eta's research program , we recommend that the secretary of labor require eta to take the following three actions: formally incorporate into its research process the routine involvement of the chief evaluation officer at key milestones , including at the development of eta's annual research agenda and spending priorities , as well as at the early stages of developing specific research projects .

develop a mechanism to enhance the transparency and accountability of eta's research program .

for example , such a mechanism might include involving advisory bodies or other entities outside eta , in efforts to develop eta's research policies and processes .

develop a formal plan for ensuring that eta's research products are easily accessible to stakeholders and to the general public through its searchable database .

such a plan could involve requiring labor to assess the overall effectiveness of its web - based search page , including user satisfaction with search features .

we provided a draft of this report to the department of labor for review and comment .

labor provided written comments , which are reproduced in appendix vii .

in addition , eta provided technical comments , which we incorporated where appropriate .

in its response , labor generally agreed with our findings and all of our recommendations , noting its ongoing efforts in support of the recommendations .

regarding our recommendation to formally incorporate into its research process the routine involvement of the chief evaluation officer at key research milestones , labor noted that it is currently taking steps to do so .

officials reported that they have worked closely with this office in various aspects of its research , including discussing research , demonstration projects , and evaluations in the early stages of development and plans to continue this collaboration in the future .

however , eta's comments did not discuss plans to update its documentation on the formal research process .

we found in our review that involving the chief evaluation officer was not an official component of eta's documented research process , and it occurred on an ad hoc basis .

as eta moves forward , we urge the agency to modify its current research process and document the involvement of the chief evaluation officer at critical research milestones .

regarding our recommendation for eta to develop a mechanism to enhance the transparency and accountability of its research program , officials cited several steps they are taking to improve the program , including involving outside experts in the development of their 5-year research plan and establishing advisory and peer review groups to review major evaluations .

while officials note they plan to engage outside experts in broader research policies and processes , we encourage eta to formalize this involvement .

moreover , we encourage eta to continue to move forward in its efforts to further clarify components of its research process that are not well defined , including , for example , the criteria to be used when deciding when a peer review should be performed .

regarding our recommendation to develop a formal plan to ensure that disseminated research is easily accessible to stakeholders and the general public , officials cited specific steps the agency has taken to improve its web - based research database .

while these actions are a step in the right direction , we believe that it is still important for labor to develop a formal and comprehensive plan to ensure that disseminated research continues to be accessible to the public .

furthermore , labor expressed concerns about how we characterized the agency's budget for pilots , demonstrations , and research .

recognizing these concerns , we made changes to the report to better capture the amount of funding eta has available for research .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to the appropriate congressional committees , the secretary of labor , and other interested parties .

the report will also be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staffs have any questions about this report , please contact me at ( 202 ) 512-7215 or scottg@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix viii .

appendix i: status of prior gao recommendations to the department of labor , as of january 2011 department of labor's response the department of labor ( labor ) does not agree with this recommendation as written .

according to labor officials , the administrator of opdr currently reports to the deputy assistant secretary , not directly to eta's assistant secretary .

however , labor officials acknowledge that important functions such as research and evaluation should not have too many intermediary reporting layers .

to facilitate communication , officials further noted that the opdr administrator , the deputy assistant secretary , and the chief evaluation officer meet on a monthly basis with the assistant secretary to discuss evaluations .

labor agrees with this recommendation , but authority to make key decisions still resides with the office of the assistant secretary for eta .

opdr currently provides recommendations to this office regarding plans for conducting and disseminating research .

in an effort to improve evaluations departmentwide , the secretary of labor recently established the chief evaluation office to monitor evaluation efforts across the department .

opdr has begun to work informally with the chief evaluation officer and the chief economist to design and implement research and evaluation projects .

labor agrees with this recommendation .

eta reports that it has taken some steps to establish more specific processes regarding dissemination of research , citing changes in performance standards for project officers .

however , our recommendation would make broader changes to their research process and no such changes are reflected in the documents the agency provided .

labor's actions do not completely satisfy recommendation … create an information system to track research projects at all phases to ensure timely completion and dissemination .

labor agrees with this recommendation .

officials report that they have begun working on a centralized , electronic tracking system for its research projects .

however , the work is still under way and no time frames have been provided for its completion .

currently , opdr uses an excel document to keep inventory of all research , demonstration , and evaluation projects .

labor's actions do not completely satisfy recommendation … instruct eta's research and evaluation center to develop processes to routinely involve outside experts in setting its research agenda and to the extent required , do so consistent with the federal advisory committee act .

labor agrees with this recommendation .

opdr has taken steps to engage outside experts in setting its 5-year research plan for 2011 and collaborate with the research and evaluation centers of other federal agencies , such as the departments of education and health and human services .

opdr also plans to convene an expert panel , solicit public comments , and incorporate feedback from its 2009 reemployment research conference and its 2010 eta reemployment summit .

however , despite these current efforts , opdr has not formally incorporated them in its standard research process .

we were asked to review the employment and training administration's ( eta ) research program to better understand its approach to conducting and disseminating research .

specifically , we answered the following research questions: ( 1 ) to what extent do eta's research priorities reflect key national employment and training issues and how useful were the studies funded under them ? .

 ( 2 ) what steps has eta taken to improve its research program ? .

 ( 3 ) how has eta improved , if at all , the availability of its research since our last review in january 2010 and what other steps could eta take to further ensure its research findings are readily available ? .

to answer our research questions , we convened a virtual panel using a modified delphi technique to obtain selected employment and training experts' opinions on eta's research priorities and dissemination methods .

we also visited two workforce agencies in pennsylvania and virginia that are implementing two of eta's ongoing research studies to learn about implementation issues and how research is being conducted .

in addition , we reviewed 58 eta - funded research and evaluation reports disseminated between january 2008 and march 2010 and assessed the methodological soundness of completed studies that cost $1 million or more .

we also reviewed eta's ongoing studies that cost $2 million or more .

to determine the availability of eta's research , we measured the time between when the final version of a research report was submitted to eta's office of policy development and research ( odpr ) and when it was posted on eta's web site .

we also conducted a series of systematic searches to test the reliability of eta's research database .

furthermore , we interviewed department of labor ( labor ) and eta officials to better understand eta's research capacity , processes , and the use of research findings to inform policy and practice .

lastly , we reviewed relevant agency documents and policies , as well as relevant federal laws .

we convened a nongeneralizable web - based virtual panel of 41 employment and training experts to obtain their opinions on eta's research priorities and dissemination methods .

we employed a modified version of the delphi method to organize and gather these experts' opinions .

to encourage participation by our experts , we promised that responses would not be individually identifiable and that results would generally be provided in summary form .

to select the panel , we asked several employment and training experts , on the basis of their experience and expertise , to identify other experts who were knowledgeable of eta and the research it conducts and disseminates .

after receiving nominations from experts , we reviewed the list to ensure that it reflected a range of perspectives and backgrounds , including academics , researchers , and consultants .

our delphi process entailed two survey phases .

 ( see app .

v for a copy of our phase i and phase ii questionnaires. ) .

in phase i , which ran from june 22 , 2010 , to august 9 , 2010 , we asked the panel to respond to five open - ended questions about eta's research priorities and dissemination methods .

we developed these questions based on our study objectives and pretested them with four experts by phone to ensure the questionnaire was clear , unbiased , and did not place an undue burden on respondents .

all relevant changes were made before we deployed the first web - based questionnaire to experts .

after the experts completed the open - ended questions in the first questionnaire , we performed a content analysis of the responses in order to identify the most important issues raised by our experts .

two members of our team categorized experts' responses to each of the questions .

any disagreements were discussed until consensus was reached .

thirty - six of the 41 panelists selected completed phase i of the survey ( about an 88 percent response rate ) .

those that did not complete phase i were allowed to participate in phase ii .

 ( for a list of experts who participated in phase i and phase ii , see app .

vi. ) .

the experts' responses to phase i were used to create the questions for phase ii .

in phase ii , we gathered more specific information on eta's research and dissemination practices .

phase ii , which ran from october 29 , 2010 , to december 14 , 2010 , consisted of 16 follow - up questions where panelists were asked to either rank or rate the responses from phase i .

we pretested the questionnaire for the second phase with three experts to ensure the clarity of the instrument .

we conducted two of our expert pretests in - person and one by phone .

thirty - nine of the 41 experts completed phase ii ( about a 95 percent response rate ) .

to further enhance our understanding of how eta conducts its research , we visited two workforce agencies that are implementing eta's ongoing research studies .

first , we visited the lancaster county workforce investment board in lancaster , pa. , which received funding from eta to implement the young parents demonstration project .

this project provides educational and occupational skills training to promote employment and economic self - sufficiency for mothers , fathers , and expectant mothers ages 16 to 24 .

second , we visited the northern virginia workforce investment board in falls church , va. , which received funding from eta to implement the second round of the project growing america through entrepreneurship , also referred to as project gate ii .

this grant helps dislocated workers aged 50 and over obtain information , classroom training , one - to - one technical assistance , counseling , and financial assistance to establish new businesses in order to help them start and sustain successful self - employment .

we selected these workforce agencies because they were identified by eta as having active research projects in the implementation stage .

these sites also required minimum travel expenditure .

during our site visits , we toured each workforce agencies' facilities and used a semistructured interview protocol to interview the project director and staff about their role and responsibilities , the extent to which they communicate with eta , and whether or not they face challenges with regards to implementation .

at the lancaster county site , we participated in an informal on - site lunch forum where local community programs that the agency partners with talked with us about their collaboration with the program .

at the northern virginia gate ii site , we observed a focus group operated by the program to facilitate information - sharing among participants .

after our site visits , we conducted phone interviews with the contractors that received funding from eta to evaluate the outcomes of two research projects .

specifically , we interviewed the urban institute , which evaluates the young parents demonstration project , and impaq international , which evaluates project gate ii .

both projects include an experimental component with control and comparison groups to determine the effects of program interventions on participants .

during our interviews we used a semistructured questionnaire and asked questions to better understand their roles and responsibilities for the project , the extent to which they communicate with eta , and whether or not they experience methodological and implementation challenges .

we reviewed the 58 research and evaluation reports that eta disseminated between january 2008 and march 2010 and assessed the methodological soundness of 11 completed studies that cost $1 million or more .

in addition , we reviewed 10 ongoing studies costing $2 million or more to determine if research practices or the soundness of research designs had changed over time .

we categorized the 58 studies disseminated between january 2008 and march 2010 by study type , cost , and research area .

for the larger studies costing $1 million or more , we analyzed key characteristics including design features , scope , generalizability , and the appropriateness of analytical approaches and statistical procedures .

these studies were analyzed independently by two analysts and the agreement between their ratings was 100 percent .

 ( for results of this analysis , see app .

vi. ) .

to evaluate the availability of eta's research , we measured the time between when the final version of a research report was submitted to odpr and when it was posted on eta's web site .

specifically , we measured the dissemination time frames for reports posted in 2008 and compared that with the dissemination time frames for reports issued between january 2009 through march 2010 .

in addition , we conducted a series of systematic searches to test the reliability of eta's web - based research database .

to perform our searches , we selected a random sample of 30 reports from the 312 reports available on eta's research database at the time of our review .

specifically , we tested a variety of search functions available at the time of our review to determine the extent to which research reports could be easily retrieved on eta's research database .

these functions included searches by title , keywords , author , and / or dates .

we classified a report as retrievable if it appeared anywhere in our search results .

we conducted our initial searches between june 30 , 2010 , and july 6 , 2010 .

a second round of searches was conducted between august 6 , 2010 , and august 10 , 2010 .

further , we interviewed labor and eta officials to learn more about the search capabilities of eta's research database and the processes used to address errors and implement changes .

finally , we interviewed officials to gather information about eta's dissemination methods , including its current techniques and future plans for disseminating research reports .

to better understand the agency's research capacity , we interviewed eta officials and reviewed relevant agency and budget documentation .

similarly , to obtain information on eta's research process and how research findings are used to inform employment and training policy and practice , we interviewed officials and reviewed agency documentation , including relevant policies and procedures that guide eta's research .

we also reviewed relevant federal laws .

we conducted this performance audit from march 2009 through march 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in our delphi phase ii web - based questionnaire , we asked the panel of experts to rate and rank the key employment and training issues , populations , and programs that eta should address in its future research .

these issues were identified by the panel during phase i .

for our analysis , we calculated basic descriptive statistics on these issues , which are presented in tables 5 through 7 .

other ( comparison v. treatment group using propensity score matching ) increasing the labor market participation of underutilized populations characteristics collected before the intervention , but these were not used to make comparisons ) integration of the workforce and regional economic development network analysis ) integration of the workforce and regional economic development network analysis ) experimental ( randomized control trials ) increasing the labor market participation of underutilized populations collected on characteristics upon entry and outcome characteristics collected after completion ) .

trachtenberg school of public policy and public administration , george washington university abt associates inc. mathematica policy research , inc. w.e .

upjohn institute for employment research national bureau of economic research robert m. lafollette school of public affairs , university of wisconsin - madison w.e .

upjohn institute for employment research abt associates inc. john j. heldrich center for workforce development , rutgers , the state university of new jersey department of economics , college of arts and sciences , american university mathematica policy research , inc. department of economics , university of missouri - columbia humphrey school of public affairs , university of minnesota minnesota department of employment and economic development w.e .

upjohn institute for employment research institute for policy studies , johns hopkins university center for law and social policy peterson institute for international economics mathematica policy research , inc .

in addition to the contact listed above , dianne blank , assistant director , and kathleen white , analyst - in - charge , managed all phases of the engagement .

ashanta williams assisted in managing many aspects of the work and was responsible for final report preparation .

lucas alvarez and benjamin collins made significant contributions to all aspects of this report .

in addition , amanda miller assisted with study and questionnaire design ; joanna chan performed the data analysis ; stephanie shipman advised on evaluation approaches ; james bennett provided graphics assistance ; david chrisinger provided writing assistance ; alex galuten and sheila mccoy provided legal support ; and sheranda campbell and ryan siegel verified our findings .

program evaluation: experienced agencies follow a similar model for prioritizing research .

gao - 11-176 .

washington , d.c.: january 14 , 2011 .

employment and training administration: increased authority and accountability could improve research program .

gao - 10-243 .

washington , d.c.: january 29 , 2010 .

workforce investment act: labor has made progress in addressing areas of concern , but more focus needed on understanding what works and what doesn't .

gao - 09-396t .

washington , d.c.: february 26 , 2009 .

employment and training program grants: evaluating impacts and enhanced monitoring would improve accountability .

gao - 08-486 .

washington , d.c.: may 7 , 2008 .

federal research: policies guiding the dissemination of scientific research from selected agencies should be clarified and better communicated .

gao - 07-653 .

washington , d.c.: may 17 , 2007 .

data quality: expanded use of key dissemination practices would further safeguard the integrity of federal statistical data .

gao - 06-607 .

washington , d.c.: may 31 , 2006 .

workforce investment act: substantial funds are used for training , but little is known nationally about training outcomes .

gao - 05-650 .

washington , d.c.: june 29 , 2005 .

program evaluation: an evaluation culture and collaborative partnerships help build agency capacity .

gao - 03-454 .

washington , d.c.: may 2 , 2003 .

workforce investment act: improvements needed in performance measures to provide a more accurate picture of wia's effectiveness .

gao - 02-275 .

washington , d.c.: february 1 , 2002 .

