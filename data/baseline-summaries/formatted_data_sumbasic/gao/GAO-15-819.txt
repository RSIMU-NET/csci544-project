the federal government is one of the world's largest and most complex entities , with about $3.5 trillion in outlays in fiscal year 2014 funding a vast array of programs and operations .

it faces a number of significant budget , management , and performance challenges as it seeks to achieve diverse and complex results .

for example , since 2011 , our series of annual reports has identified more than 200 areas in which actions are needed to address fragmentation , overlap , and duplication ; achieve other cost savings ; or enhance revenue .

in addition , weaknesses in management capacity , both government - wide and in individual agencies , impair efficient and effective government operations .

since 1990 , our high - risk list has identified 57 areas that are vulnerable to fraud , waste , abuse , or in need of broad - based transformation , of which 32 remain on the current list .

addressing these challenges will require tough choices in setting priorities and reforming programs and management practices .

we previously reported that the performance planning and reporting framework originally put into place by the government performance results act of 1993 ( gpra ) , and significantly enhanced by the gpra modernization act of 2010 ( gprama ) , provides important tools that can help decision makers address challenges facing the federal government .

full and effective implementation of gprama will be also be instrumental in addressing these pressing governance issues in anticipation of the transition to the next presidential administration in 2017 .

congress included a statutory provision for gao to evaluate and report on ( 1 ) how implementation of gprama is affecting performance management at the chief financial officers ( cfo ) act agencies , including whether performance management is being used to improve the efficiency and effectiveness of agency programs ; and ( 2 ) crosscutting goal implementation , not later than september 30 , 2015 .

based on our issued and ongoing work since our previous assessment of the initial implementation of gprama required by the act in june 2013 , for this report we reviewed progress in four key areas: ( 1 ) addressing cross - cutting efforts ; ( 2 ) ensuring performance information is useful and used ; ( 3 ) aligning daily operations with results ; and ( 4 ) communicating performance information .

we also reviewed the status of recommendations made to the office of management and budget ( omb ) and agencies as part of our issued work on gprama from 2012 through september 2015 .

to perform this work , we reviewed gprama , omb guidance , and our recent and ongoing work related to our four key areas , including the status of our recommendations .

we also interviewed omb and performance improvement council ( pic ) staff .

our recent work under gprama , both ongoing and issued since june 2013 , covered the 24 cfo act agencies and the army corps of engineers - civil works .

eight of our 12 reviews that are the basis of this report used selected agencies as case illustrations .

half of the 12 reports included government - wide reviews , some involving surveys of all or most of the cfo act agencies .

figure 1 shows the agencies covered in our recent and ongoing work under gprama .

this report also includes some results from our ongoing work on cross - agency priority ( cap ) goals and on major management challenges , on which we plan to issue reports at the end of 2015 .

for our ongoing work on cap goals , we selected 7 of the 15 cap goals established in march 2014 for review , interviewed agency officials with responsibility for implementing these goals , and reviewed relevant guidance and documentation .

for our ongoing work on the progress agencies are making in using gprama to address their major management challenges , we evaluated agency reporting against gprama requirements and omb guidance and interviewed omb staff and relevant agency officials .

appendix i provides additional information about our objectives , scope , and methodology .

we conducted this performance audit from april 2015 to september 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

gprama is a significant enhancement of gpra , which was the centerpiece of a statutory framework that congress put in place during the 1990s to help resolve long - standing performance and management problems in the federal government and provide greater accountability for results .

gprama was likewise intended to address a number of federal performance management challenges , including focusing attention on crosscutting issues , enhancing the use and usefulness of performance information , increasing transparency , and ensuring leadership commitment and attention to improving performance .

our june 2013 report assessing the initial government - wide implementation of gprama described some of the important steps omb and agencies had taken to implement key provisions of gprama .

these included developing agency - level and government - wide goals , designating officials to key leadership roles , and using the performance improvement council ( pic ) to facilitate the exchange of information to strengthen agency performance management .

gprama revises existing provisions and adds new requirements , including the following: gprama includes requirements that omb and agencies establish different types of government - wide and agency - level performance goals .

these include: government - wide: cross - agency priority ( cap ) goals .

omb is required to coordinate with agencies to establish federal government priority goals — otherwise referred to as cap goals — that include outcome - oriented goals covering a limited number of policy areas as well as goals for management improvements needed across the government .

the act also requires that omb — with agencies — develop annual federal government performance plans to , among other things , define the level of performance to be achieved through each of the cap goals .

omb established the first set of cap goals for a 2-year interim period in february 2012 .

in march 2014 , omb identified the next set of cap goals , which it is to update every 4 years .

agency - level: agency priority goals ( apg ) .

every 2 years , gprama requires the heads of certain agencies , in consultation with omb , to identify a subset of agency goals to be identified as apgs .

these goals are to reflect the highest priorities of each of these agencies , and to be informed by the cap goals as well as consultations with relevant congressional committees and other interested parties .

twenty - three agencies identified a total of 91 apgs covering fiscal years 2014 through 2015 .

gprama provided a statutory basis for selected senior leadership positions that had been created by executive orders , presidential memorandums , or omb guidance .

gprama established these positions in law , provided responsibilities for various aspects of performance improvement , and elevated some of them as described below .

chief operating officer ( coo ) .

the deputy agency head , or equivalent , is designated coo , with overall responsibility for improving agency management and performance .

performance improvement officer ( pio ) .

agencies are required to designate a senior executive within the agency as pio , who reports directly to the coo and has responsibilities to assist the agency head and coo with performance management activities .

goal leaders .

gprama requires that goal leaders be designated for cap goals and apgs and omb guidance requires goal leaders be designated for strategic objectives .

they are designated for cap goals , strategic objectives , and apgs .

cap goals have at least two goal leaders — one from the executive office of the president and the other from a key responsible agency .

apgs have a goal leader , and omb guidance directs agencies to designate a deputy goal leader to support the goal leader .

gprama also established the pic in law and included additional responsibilities .

originally created by a 2007 executive order , the pic is charged with assisting omb to improve the performance of the federal government and achieve the cap goals .

among its other responsibilities , the pic is to facilitate the exchange among agencies of useful performance improvement practices and work to resolve government - wide or crosscutting performance issues .

the pic is chaired by the deputy director for management at omb and includes agency pios from each of the 24 cfo act agencies as well as other pios and individuals designated by the chair .

gprama and related omb guidance require the regular review of progress in achieving goals and objectives through performance reviews .

strategic reviews .

omb's 2012 guidance implementing gprama established a strategic review process in which agencies , beginning in 2014 , were to conduct leadership - driven , annual reviews of their progress toward achieving each strategic objective — the outcome or impact the agency is intending to achieve through its various programs and initiatives — established in their strategic plans ( and updated in their annual performance plans ) .

data - driven reviews .

data - driven performance reviews are regularly scheduled — at least quarterly — structured meetings used by organizational leaders and managers to review and analyze data on progress toward key performance goals and other management - improvement priorities .

for each apg , gprama requires agencies to conduct reviews to assess progress toward the goal and risk of not meeting it , and develop strategies to improve performance , as needed .

these reviews are to be led by the agency head and coo , with the support of the pio , and include relevant goal leaders .

coordination with relevant parties both within and outside the agency that contribute to goal accomplishment is also required .

gprama also requires that the director of omb , with the support of the pic , review progress toward each cap goal with the appropriate lead government official at least quarterly .

specifically , these reviews should examine the progress made over the most recent quarter , overall trends , the likelihood of meeting the planned level of performance and , if necessary , strategies to improve performance .

gprama includes several provisions related to reporting of performance information .

performance.gov .

omb is required to develop a single , government - wide performance website to communicate government - wide and agency performance information .

the website — implemented by omb as performance.gov — is required to make available information on apgs and cap goals , updated on a quarterly basis ; agency strategic plans , annual performance plans , and annual performance reports ; and an inventory of all federal programs .

program inventory .

omb is required to make publicly available , on a central government - wide website , a list of all federal programs identified by agencies , along with related budget and performance information .

performance information quality .

agencies are required to describe how they are ensuring the accuracy and reliability of the data used to measure progress toward apgs and performance goals , including an identification of the following five areas: o the means used to verify and validate ; o the sources for the data ; o the level of accuracy required for the intended use of the data ; o any limitations to the data at the required level of accuracy ; o how the agency will compensate for such limitations ( if needed ) to reach the required level of accuracy .

agencies are required to provide information to omb that addresses all five requirements for each of their apgs for publication on performance.gov .

agencies also must address all five requirements for performance goals in their performance plans and reports .

major management challenges .

agencies are required to address major management challenges in their performance plans .

these challenges may include programs or management functions that have greater vulnerability to fraud , waste , abuse , and mismanagement , such as those issues included in our high - risk list or identified by inspectors general , where a failure to perform well could seriously affect an agency's ability to achieve its mission or goals .

the concepts described above and their relationships to each other are represented in figure 2 , which summarizes them and highlights areas in which our recent work has focused .

many of the meaningful results that the federal government seeks to achieve , such as those related to protecting the environment , promoting public health , and providing homeland security , require the coordinated efforts of more than one federal agency , level of government , or sector .

even with sustained leadership , crosscutting issues are difficult to address because they may require agencies and congress to reexamine ( within and across various mission areas ) the fundamental structure , operation , funding , and performance of a number of long - standing federal programs or activities .

collaboration and improved working relationships across agencies are critical tools for addressing the issues of fragmentation , overlap , and duplication our recent work has highlighted .

additionally , they are fundamental to addressing many of the issues that we have designated as high risk due to their vulnerabilities to fraud , waste , abuse , and mismanagement or most in need of broad - based transformation .

we have found that resolving many of these issues requires better collaboration among agencies , levels of government , and sectors .

for more than two decades , we have reported on agencies' missed opportunities for improved collaboration through the effective implementation of gpra and , more recently , gprama .

now , more than 20 years since gpra's passage , our work continues to demonstrate that the needed collaboration is not sufficiently widespread .

the examples in the textbox below show areas from our high - risk list – improving and modernizing federal disability programs and improving federal oversight of food safety – that demonstrate the need for greater collaboration on crosscutting issues .

examples of 2015 high - risk areas demonstrating the continued need to address crosscutting issues federal disability programs remain fragmented and a high - risk federal disability programs across government remain fragmented and in need of modernization .

numerous federal programs provide a patchwork of services and supports to people with disabilities and work independently without a unified vision and strategy or set of goals to guide their outcomes .

our 2015 update to our high - risk series found that progress in improving and modernizing disability programs has been mixed .

omb has made some progress toward enhancing coordination across programs that support employment for people with disabilities , but it has not established a larger vision for disability programs that include appropriate government - wide goals and strategies for achieving those goals .

omb needs a government - wide action plan that describes how federal agencies will improve coordination and set measurable goals that support employment for people with disabilities beyond the public sector .

such a plan should identify additional opportunities to build capacity and leverage existing government resources .

continued planning , management focus , and coordination can improve and modernize federal disability programs .

federal food safety is a high - risk area and in need of improved foodborne illness is a common , costly , yet largely preventable public health concern .

according to the centers for disease control and prevention , each year nearly 50 million people in the united states get sick and roughly 3,000 die due to foodborne illness .

for more than a decade , we have reported on the fragmented federal food safety system and we added federal oversight of food safety to our high - risk areas because of risks to the economy and public health and safety .

in december 2014 we reported that the department of health and human services ( hhs ) and the u.s. department of agriculture ( usda ) have taken steps to implement gprama requirements , but could more fully address crosscutting food safety efforts .

hhs and usda vary in the amount of detail they provide on their crosscutting food safety efforts and they do not include several relevant crosscutting efforts in their strategic and performance planning documents .

hhs and usda have mechanisms in place to facilitate interagency coordination on food safety that focus on specific issues , but they do not provide for broad - based , centralized collaboration .

a centralized collaborative mechanism on food safety is important to foster effective interagency collaboration and enhance food safety oversight .

we recommended that hhs and usda build upon their efforts to implement gprama requirements to fully address crosscutting food safety efforts .

we asked congress to consider directing omb to develop a government - wide food safety performance plan and formalize the food safety working group through statute to help ensure sustained leadership across food safety agencies over time .

hhs and usda agreed with our recommendation and in february 2015 , hhs updated its strategic plan to more fully describe how it is working with other agencies to achieve its food - safety - related goals and objectives .

as of august 2015 , our recommendation to usda and matters for congressional consideration remain unimplemented .

our annual reports on areas where opportunities exist for executive branch agencies or congress to reduce , eliminate , or better manage fragmentation , overlap , or duplication ; to achieve cost savings ; or to enhance revenue have also included areas in which greater collaboration is needed to address crosscutting issues .

see textbox below .

examples of crosscutting issues identified in gao's 2015 annual report on fragmentation , overlap , and duplication fragmentation , overlap , and potential for duplication exists among nonemergency medical transportation programs access to transportation services is essential for millions of americans to fully participate in society and access human services , including medical care .

our april 2015 report on opportunities to reduce fragmentation , overlap , and duplication reported that 42 programs across six federal departments provide funding for nonemergency medical transportation ( nemt ) to individuals who cannot provide their own transportation due to age , disability , or income constraints .

coordination of nemt programs at the federal level is limited and there is fragmentation , overlap , and potential for duplication across these programs .

an interagency coordinating council was developed to enhance federal , state , and local coordination activities , and it has taken some actions to address program coordination .

however , the council has provided limited leadership and has not convened since 2008 .

to improve efficiency , we recommended that the department of transportation , which chairs the council , take steps to enhance coordination among the programs that provide nemt .

the department agreed that more work is needed , and said that the federal transit administration is asking its technical assistance centers to assist in developing responses to nemt challenges .

in addition , as of june 1 , 2015 , the federal transit administration reported working to develop a new 2-year strategy for addressing nemt coordination among federal agencies and plans to develop and propose a cost - sharing model that can be applied to federal programs that provide funding for nemt .

strengthened coordination could increase efficiency and effectiveness of consumer product safety oversight the oversight of consumer product safety is a complex system involving a number of federal agencies .

however , as our april 2015 report on opportunities to reduce fragmentation , overlap , and duplication highlighted , oversight of consumer product safety is fragmented across agencies , overlaps jurisdictions , or is unclear for certain products .

in some cases agencies regulate different components of or carry out different regulatory activities for the same product .

agencies reported that they collaborate to address specific consumer product safety topics .

for example , officials from the coast guard , which regulates safety standards for recreational boats , said they work informally with the consumer product safety commission when the need arises .

however , we did not identify a formal mechanism for addressing such issues more comprehensively , and no single entity or mechanism exists to help the agencies that collectively oversee consumer product safety .

without this , agencies may miss opportunities to leverage resources and address challenges , including those related to fragmentation and overlap .

in response to our recommendation that the coast guard and the consumer product safety commission establish a formal coordination mechanism , in may 2015 the two agencies signed a formal policy document establishing such a mechanism .

we also recommended that congress should establish a formal collaboration mechanism to address comprehensive oversight and inefficiencies related to fragmentation and overlap .

as of august 2015 , no formal collaboration mechanism had been established .

the textbox below shows additional examples of areas in which we have identified the need for additional work to address crosscutting issues .

examples from gao's work from 2013-2015 of continued challenges in addressing crosscutting issues additional leadership needed to achieve interagency efforts for department of agriculture and department of health and human services ( hhs ) veterinarians perform critical work for public and animal health and for emergency response to economically devastating or highly contagious animal diseases .

in may 2015 , we reported that the office of personnel management ( opm ) and other federal agencies have taken steps toward achieving the goals outlined in opm's government - wide strategic plan for the veterinarian workforce , primarily through an interagency group opm created .

however , in each of the three goals , the group did not follow through on next steps and made limited progress .

according to opm officials , the group did not consistently monitor progress toward goals in part because it did not have sufficient leadership support from participating agencies .

opm agreed with our recommendation that it obtain leadership support for achieving its goals , and stated that it designed and will aid in establishing a veterinary medical officer executive steering committee that will , among other things , provide leadership and ensure progress toward stated goals .

federal strategy needed to ensure efficient and effective delivery of services for older adults in may 2015 , we reported that five federal agencies across four departments had one or more programs that operate within a system of home and community - based services ( hcbs ) and related supports that older adults often require to live as independently as possible in their homes and communities .

the older americans act of 1965 requires the administration on aging , within hhs , to facilitate collaboration among federal agencies ; however , the five agencies that fund these services and supports do so , for the most part , independently .

to help ensure that agencies' resources for hcbs and supports are used efficiently and effectively , we recommended that hhs facilitate development of a cross - agency federal strategy .

hhs agreed with our recommendation .

while much of our recent work has focused on the need for improved collaboration to address crosscutting issues , we have also reported on areas , including one high - risk area , in which agencies have made progress or are generally effectively coordinating .

the text box below discusses two of these examples .

examples from gao's work from 2013-2015 of areas in which agencies are doing well or making progress in addressing crosscutting issues coordination of dod's and nnsa's nuclear weapons stockpile responsibilities is generally consistent with key practices the nuclear weapons council ( council ) serves as the focal point of department of defense ( dod ) and national nuclear security administration ( nnsa ) interagency activities to maintain the u.s. nuclear weapons stockpile .

in may 2015 , we reported that the council's actions to coordinate dod's and nnsa's nuclear weapons stockpile responsibilities are generally consistent with most of the key practices we have identified for collaborating across agency boundaries .

for example , according to council documents , the council and its support committees meet on a regular basis to monitor , evaluate , and report on nuclear weapons stockpile issues .

these meetings include periodic oversight briefings on nuclear weapon refurbishment programs .

we made recommendations to the secretaries of defense and energy to address two areas in which actions could be enhanced: ( 1 ) having up - to - date , written agreements and guidance that establish compatible policies , procedures , and other means to operate across agency boundaries and defines roles and responsibilities and ( 2 ) regularly including all relevant participants .

the departments generally agreed with our recommendations .

progress made on high - risk area of sharing and managing the federal government has made significant progress in promoting the sharing of information on terrorist threats , an area we designated as high risk in 2005 .

in february 2015 , we reported that significant progress was made in this area by developing a more structured approach to achieving the information sharing environment ( environment ) and by defining the highest priority initiatives to accomplish .

in december 2012 , the president signed the national strategy for information sharing and safeguarding ( strategy ) , which provides guidance on the implementation of policies , standards , and technologies that promote secure and responsible national security information sharing .

in 2013 , in response to the strategy , the program manager for the environment released the strategic implementation plan for the national strategy for information sharing and safeguarding ( implementation plan ) .

the implementation plan provides a roadmap for the implementation of the priority objectives in the strategy , assigns stewards to coordinate each priority objective , and provides time frames and milestones for achieving the outcomes in each objective .

the steward is responsible for ensuring that participating agencies communicate and collaborate to complete the objective , while also raising to senior management any issues that might hinder progress .

although progress has been made , more work remains to be done to fully address the issues identified in this high - risk area .

if fully and effectively implemented , gprama and the digital accountability and transparency act of 2014 ( data act ) hold promise for helping to address crosscutting issues .

for example , gprama establishes a framework aimed at taking a more crosscutting and integrated approach to focusing on results and improving government performance .

effective implementation of gprama could help clarify desired outcomes , address program performance spanning multiple organizations , and facilitate future actions to reduce , eliminate , or better manage fragmentation , overlap , and duplication .

the data act also offers the potential to help address crosscutting issues , as it requires agencies to publicly report information about any funding made available to , or expended by , an agency .

these actions would allow executive branch agencies and congress to accurately measure the costs and magnitude of federal investments .

as we have previously reported , the data act holds great promise for improving the efficiency and effectiveness of the federal government and for addressing persistent government management challenges .

for example , as our annual reports on fragmentation , overlap , and duplication have highlighted , a complete picture of federal programs , along with related funding and performance information , is critical for addressing these issues .

data - driven reviews .

data - driven reviews have had a positive effect on collaboration among officials within agencies , but agencies are still missing opportunities to include stakeholders from other federal agencies and thus promote collaboration across agencies .

specifically , in our july 2015 report on data - driven reviews , we found that 21 of the 22 agencies we surveyed that reported holding in - person data - driven reviews said that their data - driven reviews have had a positive effect on collaboration among officials from different offices or programs within the agency .

despite the positive effects of reviews on internal collaboration , most agencies reported that relevant contributors from other federal agencies did not participate in their reviews .

this situation has not changed since our july 2014 report on the role of the agency priority goal leader , in which we found that some goal leaders reported that goal contributors from other federal agencies , and even different components within the same federal agency , were not included in their data - driven reviews .

as we previously reported in 2013 , failing to include all goal contributors may lead to missed opportunities to have all the relevant parties apply their knowledge of the issues and participate in developing solutions to performance problems .

as a result , in that 2013 report , we recommended that omb work with the pic and other relevant groups to identify and share promising practices to help agencies extend their performance reviews to include , as relevant , representatives from outside organizations that contribute to achieving their agency performance goals .

as of june 2015 , omb had not taken action in response to this recommendation .

omb staff said that while agencies have found that at times it is useful to engage external stakeholders in improving program delivery , officials view data - driven reviews as internal agency management meetings and believe it would not always be appropriate to regularly include external representatives .

we continue to believe that more active involvement from external contributors is needed , as appropriate , and continue to urge omb to implement our recommended actions .

strategic reviews .

effective implementation of strategic reviews could help identify opportunities to reduce , eliminate , or better manage instances of fragmentation , overlap , and duplication because agencies are to identify the various organizations , program activities , regulations , tax expenditures , policies , and other activities that contribute to each objective , both within and outside the agency .

where progress in achieving an objective is lagging , the reviews are intended to identify strategies for improvement , such as strengthening collaboration to better address crosscutting challenges or using evidence to identify and implement more effective program designs .

if successfully implemented in a way that is open , inclusive , and transparent — to congress , delivery partners , and a full range of stakeholders — this approach could help decision makers assess the relative contributions of various programs to a given objective .

successful strategic reviews could also help decision makers identify and assess the interplay of public policy tools that are being used to ensure that those tools are effective and mutually reinforcing and that results are being efficiently achieved .

to that end , in july 2015 we reported on seven practices that can help ensure agencies conduct effective strategic reviews .

these practices include identifying the various strategies and other factors that influence outcomes and determining which are most important , identifying key stakeholders to participate in the review , and assessing the effectiveness in achieving strategic objectives and identifying actions to improve implementation and impact .

program inventories .

one of the gprama provisions that has the potential to help in addressing crosscutting issues is the requirement that agencies develop inventories of their programs , though in october 2014 we reported that several issues limit the usefulness of the inventories .

as our prior work has highlighted , creating a comprehensive list , or program inventory , of federal programs , along with related performance and funding information , could provide decision makers with critical information that could be used to better address crosscutting issues .

however , in developing the inventory omb allowed agencies to define their programs using different approaches , but within a broad definition of what constituted a program consistent with several characteristics .

moreover , omb's guidance presents five possible approaches agencies could take to define their programs and notes that agencies could use one or more of those approaches in doing so .

as a result , we found that the use of inconsistent approaches by agencies to define their programs limits the comparability of programs within agencies as well as government - wide .

to illustrate the shortcomings of the inventory , in our report on program inventories we compared relevant agencies' inventories for various science , technology , engineering , and mathematics education and nuclear nonproliferation programs to programs identified in our past work .

we were unable to identify in the inventories a large majority of the programs previously identified in our work: 9 of the 179 programs matched exactly and 51 others were identified based on program descriptions .

according to omb staff , agencies used different approaches for valid and legitimate reasons and a one - size - fits - all approach would not work for all agency inventories .

while this may be true , omb could do more to direct agencies to find common ground on similar programs .

one of omb's stated purposes for the inventories is to facilitate coordination across programs that contribute to similar outcomes .

however , as we discovered through our interviews with agency officials involved with the inventory efforts , none of the agencies sought input from other agencies on how they defined and identified their programs .

we concluded that if agencies worked together to more consistently define their programs , it could also help them identify where they have programs that contribute to similar outcomes , and therefore have opportunities to collaborate .

we made several recommendations to omb aimed at presenting a more coherent picture of all federal programs in agency inventories , but omb has not yet taken action to address these items .

omb planned to publish updated inventories in may 2014 .

however , omb put the plans for updating the inventories on indefinite hold and agencies have not published updated inventories with program - level budget information , in part due to the enactment of the data act .

omb staff told us that they are considering how implementation of data act requirements can be tied to the program inventories .

agency reporting for both sets of requirements is web - based , which could more easily enable linkages between the two or facilitate incorporating information from each other .

the house and senate versions of the taxpayers right - to - know act would require that program inventories also include , to the extent practicable , financial information required to be reported under the data act for each program activity .

if enacted , the taxpayers right - to - know act could result in detailed financial and performance information for federal programs , all in one place .

in our july 2015 testimony on the implementation of the data act , we recommended that omb accelerate efforts to determine how best to merge data act purposes and requirements with the gprama requirement to produce a federal program inventory .

omb and treasury did not comment on this recommendation .

however , the acting deputy director for management and controller at omb stated at the july 2015 hearing that , because the staff that would be involved in working on the program inventories were heavily involved in data act implementation , he would not expect an update of the program inventories to happen before may 2017 .

gprama's provisions for establishing and managing achievement toward cross - agency priority ( cap ) goals make up another area in which the act offers the potential to address crosscutting issues .

cap goals , which gprama requires omb to develop in coordination with agencies , are intended to cover areas where increased cross - agency collaboration is needed to improve progress toward shared , complex policy or management objectives , such as improving our nation's cybersecurity .

omb established the first set of cap goals for a 2-year interim period in february 2012 .

in march 2014 , omb released the next set of cap goals , which it will update every 4 years , as required by gprama .

omb is required to coordinate with agencies to publish progress updates on performance.gov on a quarterly basis for each cap goal .

of the 15 current goals , 7 are mission - oriented goals and 8 are management - focused goals ( see figure 3 ) .

when selecting the current set of cap goals , omb staff told us they considered factors such as the administration's priorities , gprama requirements , and our prior work .

according to documents we reviewed and agency officials we spoke with , omb also consulted with government - wide councils , agencies , and congressional committees when developing potential cap goals .

for example , omb staff told us that they added the insider threat and security clearance reform cap goal based on congressional input .

we are conducting an ongoing assessment of the current set of cap goals , shown in figure 3 above , and selected 7 goals for our review .

the objectives of this review are to ( 1 ) assess the extent to which lessons learned from implementing the interim cap goals were incorporated into the governance of the current cap goals ; ( 2 ) assess the extent to which gprama requirements for assessing and reporting on cap goal progress are included in the selected cap goal quarterly progress updates ; and ( 3 ) assess the initial progress in implementing the selected cap goals .

we plan to issue this work at the end of 2015 and will provide updated information on selected cap goals' progress at that time .

cap goal leaders or their teams from each of the seven selected cap goals told us that the cap goal designation led to increased emphasis and leadership attention within their agencies and the executive office of the president ( eop ) on the cap goal area .

omb has made several improvements to its cap goal guidance , in part in response to our prior work , which found that omb should strengthen cap goal reviews .

our june 2014 report on cap goal reviews found , among other things , that all reviews did not meet leading practices for leadership involvement , participation by key officials , and follow - up .

in response to our recommendations , omb and the pic took several actions , including updating the guidance to cap goal teams and outlining the role of omb leadership and the pic .

for example , the guidance specifies that omb's deputy director for management will chair implementation - focused meetings for the 8 management cap goals approximately three times a year and omb's deputy director for budget will chair meetings for the 7 mission - focused cap goals , as necessary .

omb staff confirmed that as of august 2015 , such meetings had been held for 11 of the 15 cap goals .

omb staff also told us that regular senior - level meetings , consistent with omb's guidance for cap goal reviews , also took place for 3 additional goals — cybersecurity , infrastructure permitting modernization , and insider threat and security clearance .

no such reviews have yet taken place for the climate change cap goal , according to omb staff .

omb also changed the cap goal governance structure to build capacity for goal implementation .

for the interim cap goal process , each cap goal was assigned a goal leader from the eop .

for the current cap goals , in addition to the eop goal leader , omb assigned a co - goal leader from key agencies to jointly manage and oversee the goal .

for example , the customer service cap goal leaders are omb's associate director for personnel and performance and the acting commissioner of the social security administration ( ssa ) .

according to omb , this new governance structure reflects agency leadership and expertise in cap goal subject areas and more effectively leverages agency resources for crosscutting efforts and to promote greater coordination across multiple agencies .

for example , the omb goal leader for the customer service goal told us that it is helpful to have ssa in a leadership role because the agency provides its perspective on implementation efforts related to improving customer service , including piloting new activities before they are implemented government - wide .

according to the second quarterly update of fiscal year 2015 , ssa helped omb to launch a pilot for a customer service regional community of practice in denver , colorado , to help field staff work across agencies and identify opportunities for joint trainings and joint recruiting efforts .

we found that omb and the pic have also implemented strategies intended to build agency capacity to work across agencies .

these strategies include: providing ongoing guidance and assistance to cap goal teams .

the seven cap goal teams we spoke with told us that omb and the pic staff are available on a regular basis to provide them with ongoing support , such as assisting with the regular collection of performance data and updating performance.gov .

for example , the science technology engineering and mathematics ( stem ) education cap goal leader from the national science foundation told us the pic facilitated meetings to assist the goal team in developing milestones and performance indicators and to define actionable next steps .

developing a template to enhance reporting and management .

omb provided cap goal teams with a simplified reporting template to use for managing implementation of the goal and to meet quarterly reporting requirements on performance.gov .

according to omb staff and the seven cap goal teams we spoke with , the new reporting template makes it easier for goal leaders to review the updates and track progress from one quarter to the next .

piloting a government - wide white house leadership development program .

in december 2014 , the president announced a white house leadership development program which is designed to provide selected civil servants ( i.e. , gs - 15 level or equivalent ) with rotational assignments across agencies to focus on managing cap goals .

according to omb staff , the program will begin in october 2015 and the participants will spend the next year helping the white house and agencies work on implementing the cap goals .

during the first year of implementation of the current set of cap goals , cap goal teams reported initial progress to the extent performance data were available .

for example , in its progress update for the second quarter of fiscal year 2015 , published in june 2015 , the open data cap goal team reported an increase in the use of open government data as indicated by a 25 percent quarterly increase in the number of visits to data.gov , the government's platform for publishing its data .

the goal team has identified this indicator as a way to measure progress toward one of their goals to fuel economic growth and innovation .

however , a few of the cap goal teams we spoke with told us that in some cases performance data are not always available and developing meaningful performance indicators to assess progress is a challenge .

our june 2014 assessment of the interim cap goal process found that 6 of the 14 interim cap goals did not report performance data , in some cases because data needed to assess and report progress toward the goals were unavailable .

as a result , we recommended that omb direct cap goal leaders to develop plans to identify , collect , and report data necessary to demonstrate progress being made toward each goal .

omb and the pic updated guidance directing cap goal teams to establish performance targets and report on any performance indicators that are under development .

according to the second quarterly update for fiscal year 2015 , 5 of the 7 cap goals we reviewed have indicators under development for some of their goals .

for example , the customer service cap goal team reported that they are developing a standardized performance indicator to measure improvements in citizen satisfaction across government , but the progress update does not provide any information on intermediate deliverables , roles and responsibilities , or time frames for completion .

on the other hand , another progress update we examined did include information on steps the cap goal team is taking to develop a government - wide performance indicator .

the stem education cap goal team reported that one of its working groups is developing common evaluation elements to be used across federal agencies , with an expected completion date in early 2016 .

the goal team also provided information on currently available data , near - term and long - term steps they are taking , and additional research needs .

our june 2014 recommendation remains open because omb's updated guidance did not direct cap goal teams to report on the steps they are taking to develop indicators and associated time frames .

we have previously reported that tracking and monitoring progress for cross - agency activities is difficult for a number of reasons such as competing mission priorities , incompatible processes and systems across agencies , resources , and staffing .

given these challenges , when developing performance indicators for government - wide activities , it is important that cap goal teams provide information on the steps they plan to take to successfully develop meaningful indicators , which will enable them to better track progress over time and hold contributors accountable for implementation .

for over 20 years , we have recommended greater scrutiny of the performance of tax expenditures — reductions in a taxpayer's tax liability that are the result of special exemptions and exclusions from taxation , deductions , credits , deferrals of tax liability , or preferential tax rates .

if the department of the treasury's ( treasury ) estimates are summed , approximately $1.2 trillion in revenue was forgone from the 169 tax expenditures reported for fiscal year 2014 , nearly the same as discretionary spending that year .

in june 1994 and again in september 2005 , we recommended that omb develop a framework for reviewing tax expenditure performance .

periodic reviews could help determine how well specific tax expenditures work to achieve their stated purposes and how their benefits and costs compare to those of spending programs with similar goals .

given the significant investment tax expenditures represent , such reviews could help identify the most effective approaches for achieving results — vital information for federal decision makers in an era of scarce resources .

despite the strong case to evaluate the performance of tax expenditures , omb has not yet developed a framework for doing so .

fully implementing gprama requirements could provide the foundation of such a framework .

gprama requires omb to identify tax expenditures that contribute to the cap goals .

in addition , since 2012 , omb's guidance has directed agencies to identify tax expenditures that contribute to their apgs .

our past work reviewing initial gprama implementation in 2012 and 2013 found that omb and agencies rarely identified tax expenditures as contributors to these goals .

as a result , we made several recommendations to improve efforts to identify and assess the contributions of tax expenditures toward executive branch goals .

to date , omb and agencies have taken little action to address these recommendations .

omb has directed agencies , beginning with its 2013 update to its guidance , to identify tax expenditures that contribute to each of their strategic objectives , in response to a recommendation we made in june 2013 .

however , our work reviewing gprama implementation continues to find that omb and agencies have not adequately identified the contributions of tax expenditures to cap goals and apgs .

for example , we found in june 2014 that although the goal leader for the broadband interim cap goal told us he was aware that tax deductions available to businesses making capital investments contributed to the goal by incentivizing investments in broadband , the deductions were not identified as contributors on performance.gov .

given their government - wide purview and familiarity with administering the tax code , omb and treasury , respectively , are well positioned to assist agencies in identifying tax expenditures that relate to their goals .

to that end , omb's 2013 and 2014 circular a - 11 guidance noted that it would work with treasury and agencies to identify where tax expenditures align with their goals and this information was to be published on performance.gov and included in relevant agency plans , beginning in february 2014 .

however , as we found in october 2014 , according to omb staff , they did not begin to engage treasury on this effort until after agency plans were published and the website was updated .

omb staff told us in august 2015 that they had not yet made any progress on this effort .

moreover , omb removed the language about working with treasury and agencies to align tax expenditures with agency goals in the june 2015 update to its guidance .

although omb staff told us they intend to focus on this effort , they did not provide us with any plans or time frames for doing so .

we have previously identified additional steps omb could take to help agencies consider the contributions of tax expenditures to the achievement of their goals .

we recommended in our october 2014 report on the federal program inventory required under gprama that omb should include tax expenditures .

the federal program inventory is the primary tool for agencies to identify programs that contribute to their goals , according to omb's guidance .

by including tax expenditures in the inventory , omb could help ensure that agencies are properly identifying the contributions of tax expenditures to the achievement of their goals .

omb staff neither agreed nor disagreed with these tax expenditure recommendations , and told us that until they had firmer plans on how program inventory and data act implementation would be merged , they could not determine if implementing these recommendations would be feasible .

as previously described , program inventory implementation remains on hold and omb has not taken any actions to address these recommendations , according to omb staff .

without including tax expenditures in the inventory , omb forgoes an important opportunity to increase the transparency of tax expenditures and the outcomes to which they contribute .

we have long reported that agencies are better equipped to address management and performance challenges when managers effectively use performance information for decision making .

unfortunately , agencies continue to struggle to do so .

our work has found that federal agencies can use performance information to identify performance improvement opportunities , improve program implementation and organizational processes , and make other important management and resource allocation decisions .

however , our recent work shows that agencies continue to have problems effectively using performance information .

our september 2014 report on trends in agencies' use of performance information compared agencies' reported use of performance information from our 2007 and 2013 federal managers surveys .

to analyze use , we developed a use of performance information index , based on a set of survey questions in both surveys that reflected the extent to which managers reported that their agencies used performance information for various management activities and decision making .

the index runs from 1 to 5 , where a 1 reflects that managers feel the agency engages “to no extent” and a 5 reflects that managers feel the agency engages “to a very great extent” in the use of performance information activities .

most agencies showed no statistically significant change in use during this period .

as shown in figure 4 , only two agencies experienced a statistically significant improvement in the use of performance information , while four agencies experienced a statistically significant decline .

we have previously reported that in order for performance information to be useful , it must have certain characteristics .

specifically , agencies should ensure that performance information meets various users' needs for completeness , accuracy , consistency , timeliness , validity , and ease of use .

without complete and reliable performance information , congress , other decision makers , and stakeholders at all levels of government are hampered in their ability to set priorities , identify improvement opportunities , and allocate resources .

our work over the past 2 years has identified weaknesses in each of the areas that affect the usefulness of performance information .

for example , our september 2015 report on affordable rental assistance programs identified an incomplete picture of performance of these programs as a problem .

we found that federal , state , and local jurisdictions involved in these efforts reported their performance to varying extents , but that there was incomplete information on their collective performance .

accordingly , we recommended that the department of housing and urban development ( hud ) , in consultation with an interagency working group on rental policy , should work with states and localities to develop an approach for compiling and reporting on the collective performance of federal , state , and local rental assistance programs .

treasury and irs , which are agency members of this working group , did not comment on this recommendation .

hud was concerned that compiling and reporting collective performance information would require significant funding and resources .

we continue to believe the overall recommendation is valid .

specifically , we noted that ( 1 ) our recommendation is to develop an approach for compiling and reporting such data as a first step , and ( 2 ) that our recommendation is purposefully not prescriptive and allows hud , in consultation with the working group , to design an approach .

additional examples of problems that affect the usefulness of performance information are illustrated in table 1 .

performance reviews required under gprama and other guidance by their nature promote the use of performance information , as they focus on assessing performance in order to determine progress toward meeting goals and objectives .

data - driven reviews .

gprama requires that reviews of progress of agency priority goals ( apg ) be held at least quarterly ; as this requirement is more fully implemented , the use of performance information for decision making should improve .

omb emphasized that frequent , data - driven performance reviews provide a mechanism for agency leaders to use data to assess the organization's performance , diagnose performance problems and identify improvement opportunities , and decide on next steps to improve performance .

these practices are designed to shift the emphasis away from the passive collection and reporting of performance information to a model where performance information is actively used by agency officials to inform decision - making , which is more likely to lead to performance improvements .

in our july 2015 report on data - driven reviews , we found that pios had reported that the reviews had positive effects on their agencies' use of performance information .

nearly all of the 22 agencies that reported holding in - person reviews responded that they always or often use their review meetings to assess progress on apgs and to identify goals at risk and strategies for performance improvement .

additionally , as shown in figure 5 , nearly all of these agencies also reported that their data - driven review meetings have had a positive effect on progress toward the achievement of their agency's goals and on their ability to identify and mitigate risks to goal achievement .

in our discussions with officials from selected agencies , data - driven review meetings were described as venues for agency leaders and managers to assess progress toward key goals and milestones , the status of ongoing initiatives and planned actions , potential solutions for problems or challenges hindering progress , and additional support or resources needed to improve performance .

agency officials emphasized that discussions in their review meetings tend to focus on those goals or issues most in need of attention , where the achievement of a goal or milestone is at risk .

in this way , reviews can serve as early warning systems and facilitate focused discussions on external , technical , or operational obstacles that may be hindering progress and the specific actions that should be taken to overcome them ( see sidebar on the following page ) .

increasing online registration through my social security the social security administration ( ssa ) has an apg to increase the number of registrations for its “my social security” portal by 15 percent per year in fiscal years 2014 and 2015 .

however , we reported in july 2015 that during ssa's 2014 third quarter review meeting , it became apparent to ssa leadership that the agency was not on track to achieve its target for this goal .

ssa shifted focus to what could be done by offices throughout the agency to support efforts to increase the number of registrations using currently available or attainable resources and technology .

to achieve this , ssa leadership had different offices within the agency specify the contributions they would make to help increase the number of registrations .

since then , the agency's quarterly review meetings have been used to review and reinforce the commitments each office made .

while ssa was unable to meet the registration goal for fiscal year 2014 , according to ssa officials , these efforts recently undertaken as a result of the review process have helped generate an increase in registrations .

data from ssa's fiscal year 2015 first quarter review show that there was a 46 percent increase in new account registrations in october 2014 compared to the number of new registrations in october 2013 , and a 26 percent increase in december 2014 relative to december 2013. performance reviews .

reexamining the benefits and costs achieved after a regulation is implemented could provide useful data for these reviews .

despite the potential to leverage retrospective review information , agencies reported mixed experiences linking retrospective analyses to apgs .

agencies typically selected rules to review based on criteria such as the number of complaints or comments from regulated parties and the public .

including whether a regulation contributed to an apg as one of these criteria would help agencies prioritize retrospective analyses that could contribute useful information to apg assessments .

we recommended that omb's office of information and regulatory affairs direct in guidance that agencies take actions to ensure that contributions made by regulations toward the achievement of apgs are properly considered and improve how retrospective regulatory reviews can be used to help inform assessments of progress toward these apgs .

omb staff agreed with this recommendation and stated that the agency was working on strategies to help facilitate agencies' ability to use retrospective reviews to inform apgs , but as of june 2015 they have not provided additional details on their actions .

strategic reviews .

like data - driven reviews of apgs , and retrospective reviews of regulations , agencies' annual strategic reviews also have potential to increase the use of performance information .

for example , we reported in july 2015 that , to ensure effective strategic reviews , participants should use relevant performance information and evidence to assess whether strategies are being implemented as planned and whether they are having the desired effect , and to identify areas where action is needed to improve or enhance implementation and impact .

where progress in achieving an objective is lagging , the reviews are intended to identify strategies for improvement , such as strengthening collaboration to better address crosscutting challenges , building skills and capacity , or using evidence to identify and implement more effective program designs .

strategic reviews can also be used to identify any evidence gaps or areas where additional analyses of performance data are needed to determine effectiveness or to help set priorities .

for example , we reported that for the department of homeland security's ( dhs ) goal to safeguard and expedite lawful trade and travel , officials determined that sufficient progress was being made but identified gaps in monitoring efforts , such as a lack of performance measures related to travel .

as a result , dhs officials are taking steps to develop measures to address the gaps .

we have in the past identified leading practices , such as demonstrating management commitment , that can enhance and facilitate the use of performance information .

our recent periodic survey of federal managers found that specific practices were related to greater use of performance information .

as described previously , we developed a use of performance information index , composed of questions from the 2007 and 2013 surveys , to analyze responses to our surveys of federal managers .

we used statistical testing to determine if the relationship between additional survey questions , shown in figure 6 , from the 2013 survey and an agency's use of performance information index was statistically significant .

we found that an agency's average use of performance information index score increased when managers reported their agencies engaged to a greater extent in these practices as reflected in the survey questions .

the questions that were statistically and positively related to the use of performance information index are also shown in figure 6 .

for example , we found that the strongest driver of the use of performance information was whether federal managers had confidence in that information's validity .

a greater focus on these practices may help agencies improve their use of performance information .

prompted by our work , several agencies — including the departments of the treasury and labor , the national aeronautics and space administration , and the nuclear regulatory commission — asked us to provide them with underlying data for their agencies from the 2007 and 2013 managers' surveys , so that they could conduct additional analyses of their agencies' use of performance information .

some of the practices reflected in these questions are ones that we have identified elsewhere in our work as important .

for example , demonstrated leadership commitment is an area we have emphasized in our work on government operations we identify as high risk .

our high - risk program serves to identify and help resolve serious weaknesses in areas that involve substantial resources and provide critical services to the public .

our experience with the high - risk list over the past 25 years has shown that one of the key elements needed to make progress in high - risk areas is demonstrated strong commitment and top leadership support .

additionally , providing , arranging , or paying for training may also be related to employee engagement .

in july 2015 , we reported that career development and training — as measured by the federal employee viewpoint survey question “i am given a real opportunity to improve my skills in my organization” — is one of the six practices that are key drivers of employee engagement .

in addition , evidence - based tools — such as program evaluations and “pay for success” funding mechanisms — can also facilitate the use of performance information .

program evaluations .

our recent work on program evaluations — systematic studies of program performance — found that agencies have varying levels of evaluation capacity .

omb has encouraged agencies to strengthen their program evaluations and expand their use in management and policy making , but our 2014 examination of agencies' ability to conduct and use program evaluations found it to be uneven .

as part of our work , we surveyed and received responses from the pios at the 24 cfo act agencies .

about half ( 11 ) of the 24 agencies reported committing resources to obtain evaluations by establishing a central office responsible for evaluating agency programs , operations , or projects ; on the other hand , 7 reported having no recent evaluations for any of their performance goals .

although agencies may not have many evaluations , more than a third reported using them to a moderate to a very great extent to support several aspects of program management and policy making .

while agency program evaluation capacity is mixed , some agencies reported increasing use of evaluations and capacity - building activities after gprama was enacted .

about half of agencies reported increasing their use of evaluations for various activities , as shown in figure 7 , since gprama was enacted .

additionally , half of the pios we surveyed reported that efforts to improve their capacity to conduct credible evaluations had increased at least somewhat over this time .

our work found that implementing certain gprama requirements are among the reported actions agencies can take to improve their capacity to conduct evaluations and make use of evaluation information .

about two - thirds of agencies ( 15 ) reported hiring staff with research analysis and expertise , and nearly half ( 11 ) reported that doing so was useful for improving agency capacity to conduct credible evaluations .

additionally , about half of pios reported that conducting data - driven reviews of apgs and holding goal leaders accountable for progress on apgs , both of which are required under gprama , were moderately to very useful for improving agency capacity to make use of evaluation information in decision making .

engaging program staff was also rated very useful .

furthermore , in our june 2013 report on strategies to facilitate agencies' use of evaluation , we identified three strategies to facilitate the influence of evaluations on program management and policy: demonstrating leadership support of evaluation for accountability and program improvement ; building a strong body of evidence ; and engaging stakeholders throughout the evaluation process .

pay for success .

another evidence - based tool that promotes the use of performance information is pay for success ( pfs ) , also known as social impact bonds .

pfs is a new contracting mechanism to fund prevention programs , where investors provide capital to implement a social service , for example , to reduce recidivism by former prisoners .

if the service provider achieves agreed upon outcomes , the government pays the investor , usually with a rate of return , based on savings from decreased use of more costly remedial services , such as incarceration .

in september 2015 , we reported that stakeholders from the 10 pfs projects we reviewed said that pfs offers potential benefits to all parties to the project .

for example , governments can implement prevention programs that potentially lead to reduced spending on social services and transfer the risk of failing to achieve outcomes to investors .

figure 8 shows the roles of organizations involved in pfs projects .

pfs emphasizes the use of performance information because the government contracts for specific performance outcomes and generally includes a requirement that a program's impact be independently evaluated .

while the structures of the pfs examples we reviewed in our september 2015 report varied , stakeholders we interviewed reported that pfs oversight bodies established in the projects' contracts regularly reviewed performance data during service delivery .

additionally , stakeholders told us that intermediaries and investors can bring performance management expertise to service providers and provide a rigorous focus on performance management and accountability .

for example , an official we interviewed from one service provider noted that her organization invested in data entry and data analyst positions and has a team that collects , analyzes , and processes data that it submits to the intermediary .

as federal agencies consider expanding their involvement in pfs , it becomes increasingly important for officials at all levels of government to collaborate to share knowledge and experiences .

we found that while the federal government could play a role in addressing challenges in implementing pfs at the state and local levels of government , a formal means to collaborate and share lessons learned does not exist .

we recommended in our september 2015 report that omb establish a formal means for federal agencies to collaborate on pfs .

having such a mechanism as the field grows would allow agencies to leverage the experience of early federal actors in the pfs field and would decrease the potential for missteps in developing projects due to information gaps and failure to learn from experience with this evolving tool of government .

omb concurred with this recommendation and is working with agencies to explore options for continued collaboration on pfs .

our previous work has highlighted the importance of creating a “line of sight” showing how unit and individual performance can contribute to overall organizational goals .

at the individual level , an explicit alignment of daily activities with broader results is one of the defining features of effective performance management systems .

this link reinforces employee engagement and accountability for achieving goals .

gprama and related guidance provide several mechanisms that can help individuals and agencies see this connection and help them contribute to agency and government - wide goals .

gprama requires agencies to identify an individual — the goal leader — responsible for the achievement of each apg , and related omb guidance more recently directed agencies to identify a deputy goal leader to support each goal leader , as we had recommended in our july 2014 report on the role of the agency priority goal leader .

additionally , data - driven reviews required under the act offer the opportunity to hold responsible officials , such as the goal leaders , personally accountable for addressing problems and identifying strategies for improvement .

agency priority goal leaders .

our july 2014 review of the agency priority goal leader role found that most of the 46 goal leaders we interviewed thought the goal leader designation had positive effects , including providing accountability .

other benefits goal leaders identified as resulting from the position include that it provided greater visibility for the goal , facilitated coordination , heightened focus on the goal , and improved access to resources .

goal leaders told us that several mechanisms promote accountability for goal progress , including personal reputation and accountability to agency and other leadership .

for example , the assistant secretary of labor for occupational safety and health , who was the goal leader for two apgs for 2012 and 2013 ( reduce worker fatalities and develop a model safety and return - to - work program ) , told us that the interest of congress and the department of labor's inspector general , in their respective oversight roles , both operated to hold him accountable .

deputy goal leaders supported day - to - day goal management and provided continuity during times of goal leader transition .

however , we found that nearly a quarter ( 11 of 46 ) of the goal leaders we interviewed had not assigned an official to the deputy goal leader position , a designation that provides clear responsibility and additional accountability for goal achievement .

because of the importance of this position , we recommended that omb work with agencies to ensure that they appointed a deputy goal leader to support each agency priority goal leader .

in response to our recommendation , in april 2015 , the director of omb issued a memorandum stating that , in addition to identifying a goal leader for each apg , agencies must identify a senior career leader to support implementation .

omb also updated its 2015 circular a - 11 guidance to reflect this requirement .

data - driven and strategic reviews .

our work has found that regular , in - person data - driven reviews are an effective mechanism for holding goal leaders and other goal contributors individually accountable for goal progress .

in our july 2015 examination of data - driven reviews , we reported that 22 agencies we surveyed reported holding in - person data - driven reviews .

of these , 21 reported that their data - driven reviews have had a positive effect on their agency's ability to hold goal leaders and other officials accountable for progress toward goals and milestones .

according to officials from selected agencies , the transparency of performance information and a review process that ensures it receives appropriate scrutiny produce an increased sense of personal accountability for results .

for example , officials from the department of commerce told us that they are using their regular review meetings with bureau heads and goal leaders to support a cultural change throughout the agency and reinforce accountability for performance at multiple levels of the organization .

nasa officials report that strategic reviews encourage accountability we reported in july 2015 that the national aeronautics and space administration's ( nasa ) experience reviewing strategic objectives illustrates their potential for promoting accountability .

agency officials told us that their chief operating officer ( coo ) determines final ratings for strategic objectives during a briefing attended by the agency's performance improvement officer , strategic objective leaders , and relevant program staff .

according to nasa officials , the personal involvement of the coo encouraged accountability for results and performance improvements .

similar to data - driven reviews , our work on agency strategic reviews noted that they also have potential for promoting individual accountability for organizational results .

in our july 2015 report , in which we identified and illustrated practices that facilitate effective strategic reviews , we reported that accountability for results is one of the key features for planning reviews .

specifically , we stated that the focus of accountability should be on the responsible objective leader's role in using evidence to credibly assess progress in achieving strategic objectives .

agency leaders should hold objective leaders and other responsible managers accountable for knowing the progress being made in achieving outcomes and , if progress is insufficient , understanding why and having a plan for improvement .

if evidence is insufficient for assessing progress , managers should be held accountable for improving the availability and quality of the evidence so that it can be used effectively for decision making .

managers should also be held accountable for identifying and replicating effective practices to improve performance ( see sidebar ) .

employee engagement .

research on both private - and public - sector organizations has found that increased levels of engagement — generally defined as the sense of purpose and commitment employees feel toward their employer and its mission — can lead to better organizational performance .

our july 2015 report on employee engagement identified specific practices that drive employee engagement .

specifically , our regression analysis of selected federal employee viewpoint survey ( fevs ) questions identified six practices as key drivers of employee engagement , as measured by opm's employee engagement index .

these practices are detailed in figure 9 .

of these six , having constructive performance conversations is the strongest driver of employee engagement .

performance appraisal systems , which include performance plans , are a powerful mechanism for promoting alignment with and accountability for organizational goals .

there are several benefits to aligning performance with results , including increased use of performance information .

as shown in the textbox , our work has found problems with oversight and accountability in the department of veterans affairs' ( va ) health care system .

in response to these and other problems , congress has taken action , such as passing the veterans access , choice , and accountability act of 2014 , to hold senior va leadership accountable for performance and is considering other means of increasing accountability .

inadequate oversight and accountability in va's health care system despite substantial budget increases in recent years , for more than a decade there have been numerous reports — by gao , va's office of the inspector general , and others — of va facilities failing to provide timely health care .

in some cases , the delays in care or va's failure to provide care at all have reportedly resulted in harm to veterans .

these and other serious and long - standing problems with the timeliness , cost - effectiveness , quality , and safety of veterans' health care led to our designation of va's health care system as a high - risk area in 2015 .

to facilitate accountability for achieving its organizational goal of ensuring that veterans have timely access to health care , va included measures related to wait times for primary and specialty care appointments ( 1 ) in the performance contracts for senior leaders and ( 2 ) in the agency's annual budget submissions and performance and accountability reports .

however , we found that data used to monitor performance on these measures were unreliable and that inconsistent implementation of va's scheduling policies may have resulted in increased wait times or delays in scheduling outpatient medical appointments at va facilities .

scheduling staff in some locations told us that they had changed desired dates for medical appointment to show that wait times were within va's performance goals .

the va office of the inspector general has published reports with similar findings .

va has since announced that it has modified its performance measures that relate to wait times and removed measures related to wait times from senior leaders' performance contracts .

goal leaders' senior executive service performance plans .

although goal leaders told us that the designation provides accountability , we found their senior executive service ( ses ) performance plans generally did not reflect their responsibility for goal achievement .

as part of our work on the role of the agency priority goal leader in july 2014 , we reviewed the performance plans of all of the goal leaders and deputy goal leaders for the 47 apgs in our sample , where applicable .

these performance plans covered a range of responsibilities , but many did not reference the apgs for which the goal leaders and deputies were responsible .

additionally , the vast majority ( all but one of the 32 goal leader plans and one of the 35 deputy goal leader plans ) failed to link performance standards to goal outcomes .

failing to fully reflect goal achievement in performance plans is a missed opportunity to ensure that goal leaders and deputies are held accountable for goal progress and to reinforce links .

because apgs by definition reflect the highest priorities of each agency , accountability for goal achievement is especially important .

to ensure goal leader and deputy goal leader accountability , we recommended that the director of omb work with agencies to ensure that goal leader and deputy goal leader performance plans demonstrate a clear connection with apgs .

as of june 2015 , omb had not yet taken action in response to this recommendation .

senior executive ratings .

our recent work has also raised questions about agency processes for rating senior executive performance , which can promote alignment with and accountability for organizational goals .

for our january 2015 report on ses ratings and performance awards , we reviewed performance award data from the 24 cfo act agencies and we examined performance appraisal systems at five case study agencies .

specifically , we looked at the performance appraisal system that the office of personnel management ( opm ) and other agency representatives developed in 2012 .

this system is intended to provide a more consistent and uniform framework for ses evaluation .

we found that the five agencies we studied in detail had all linked ses performance plans with agency goals , a key practice for effective performance management systems , and a feature that promotes the line of sight between individual performance and organizational goals .

disparities in agencies' executive ratings distributions we reported in january 2015 that there is wide variation in ses ratings distributions among agencies .

for example , in fiscal year 2013 the department of defense rated 30.6 percent of its ses employees at the highest rating level , while the department of justice rated 73.6 percent of its ses employees at this level .

as we have previously reported , one of the key practices in promoting a line of sight between individual performance and organizational goals is making meaningful distinctions in performance .

however , although one of the primary purposes for establishing the new appraisal system included increasing equity in ratings across agencies , we found disparities in rating distributions and pay ( see sidebar ) .

this disparity in ratings between agencies raises questions about whether agencies are consistently applying performance definitions and whether performance ratings are meaningful .

we recommended that the director of opm , which certifies — with omb concurrence — ses performance appraisal systems , should consider the need for refinements to the performance certifications guidelines addressing distinctions in performance and pay differentiation .

opm partially concurred with the recommendation , though we maintain that additional action should be considered to ensure equity in ratings and performance awards across agencies .

as of june 2015 , opm officials said that they had convened a cross - agency working group that developed several recommendations that are intended to make agencies' justifications for high ses ratings more transparent .

senior executives' use of performance information for decision making .

aligning ses performance with results is a key feature of effective performance management , and our recent work has found that it also may promote use of performance information .

as we found in our september 2014 report on trends in the use of performance information , managers' responses to a question we asked them on aligning an agency's goals , objectives , and measures was significantly related to the use of performance information , controlling for other factors .

specifically , an increase in the extent to which managers aligned performance measures with agency - wide goals and objectives was associated with an increase on the five - point scale we used for our use index .

however , our analysis also found that there was a gap between ses and non - ses managers in reported use of performance information .

ses managers government - wide and at nine agencies scored statistically significantly higher than the non - ses managers at those agencies .

as shown in figure 10 , ses and non - ses managers from the departments of homeland security and veterans affairs had the largest gap in use of performance information between their ses and non - ses managers .

a critical element in an organization's efforts to manage for results is its ability to set meaningful goals for performance and to measure progress toward these goals .

gprama reinforces the need to set meaningful goals by directing agencies to establish a balanced set of performance measures , such as output , outcome , customer service , and efficiency , across program areas .

agencies have been responsible for measuring program outcomes since gpra was enacted in 1993 , but still have difficulty developing and using performance measures .

as we reported nearly 20 years ago , performance measures should demonstrate to each organizational level how well it is achieving its goals .

as shown in the illustrations in the textbox , however , agencies continue to make insufficient progress in establishing and using outcome - oriented performance measures .

examples of agency difficulties in developing and using outcome measures outcome - oriented metrics and goals are needed to gauge dod's and va's progress in achieving interoperability of electronic health records systems the departments of defense ( dod ) and veterans affairs ( va ) operate two of the nation's largest health care systems , serving approximately 16 million veterans and active duty service members and their beneficiaries , at a cost of more than $100 billion a year .

with guidance from the interagency program office ( ipo ) that is tasked with facilitating the departments' efforts to share health information , the two agencies have taken actions to increase interoperability between their electronic health record systems .

developing electronic health records is particularly important for optimizing the health care provided to military personnel and veterans , as they and their families tend to be highly mobile and may have health records residing at multiple medical facilities .

in august 2015 , we reported that the ipo had taken steps to develop process metrics intended to monitor progress of these efforts , but had not yet specified outcome - oriented metrics or established related goals that are important to gauging the impact that interoperability capabilities have on improving health care services for shared patients .

using outcome - based metrics could provide dod and va a more accurate , ongoing picture of their progress toward achieving interoperability and the value and benefits generated .

we recommended that dod and va , working with the ipo , establish a time frame for identifying outcome - oriented metrics ; define related goals to provide a basis for assessing and reporting on the status of interoperability ; and update ipo guidance to reflect the metrics and goals identified .

dod and va concurred with our recommendations .

measuring progress in addressing incarceration challenges the federal inmate population has increased more than eight - fold since 1980 , and the department of justice ( doj ) has identified prison crowding as a critical issue since 2006 .

in june 2015 , we reported that doj had implemented three key initiatives to address the federal incarceration challenges of overcrowding , rising costs , and offender recidivism .

the department had several early efforts underway to measure the success of these initiatives , but we concluded that its current approach could be enhanced .

for example , the clemency initiative is intended to encourage federal inmates who meet criteria that doj established to apply to have their sentences commuted ( reduced ) by the president .

doj tracked some statistics related to this initiative , such as the number of petitions received and the disposition of each , but it did not track how long , on average , it took for petitions to clear each step in its review process .

such tracking would help doj identify processes that might be contributing to any delays .

without this tracking , doj cannot be sure about the extent to which the additional resources it is dedicating to this effort are helping to identify inmate petitions that meet doj's criteria and expedite their review .

we recommended that the attorney general direct the office of the pardon attorney to ( 1 ) track how long it takes , on average , for commutation of sentence petitions to clear each step in the review process under doj's control , and ( 2 ) identify and address , to the extent possible , any processes that may contribute to unnecessary delays .

doj concurred with the recommendation and stated that it would consider our findings and recommendations during the course of its ongoing efficiency reviews .

measuring effectiveness of military sexual assault prevention efforts our recent work has identified issues in establishing goals and metrics to measure the effectiveness of efforts to reduce incidents of sexual assault in the military , which according to the department of defense ( dod ) represent a significant and persistent problem within the department .

for example , in march 2015 , we reported that dod had not established goals or metrics to gauge sexual assault - related issues for male service members .

dod's sexual assault prevention and response office had three different general officers in the director position since 2011 .

given this high level of turnover , we stated that establishing goals and metrics is key to institutionalizing efforts to address sexual assault of male service members .

we recommended that dod develop clear goals and associated metrics to drive the changes needed to address sexual assaults of males and articulate these goals .

dod agreed with this recommendation .

measuring the performance of different program types — such as grants , regulations and tax expenditures — is a significant and long - standing government - wide challenge and one we have addressed in our previous work .

in our june 2013 report on initial gprama implementation , we also reported that agencies have experienced common issues in measuring various types of programs .

we recommended that the director of omb work with the pic to develop a detailed approach to examine these difficulties , including identifying and sharing any promising practices .

additionally , our july 2014 report on the role of the agency priority goal leader noted that several apgs we examined identified certain program types , such as grants , as key contributors to their goals .

however , goal leaders and their deputies lacked the means to identify and share information with other goal leaders who were facing similar challenges or were interested in similar topics .

we recommended that the director of omb work with the pic to further involve agency priority goal leaders and their deputies in sharing information on common challenges and practices related to apg management .

omb and pic staff told us in june 2015 that they have taken some actions to facilitate information sharing on common topics .

for example , the pic developed a law enforcement working group , which aims to address challenges in measuring law enforcement functions .

despite these steps , additional actions are needed to fully implement these recommendations and address this long - standing issue .

we will continue to monitor omb's and the pic's efforts .

illustrative examples from our recent work that show how agencies need to make better progress in measuring certain program types are provided in table 2 .

one program type — direct service — is one of the areas in which our recent work has highlighted problems with agencies' performance measurement in multiple agencies .

our october 2014 report on customer service standards examined how selected agencies are using customer service standards and measuring performance against those standards .

we reviewed the customer services standards for six federal programs and compared them to key elements of effective customer services standards , which we identified based on our review of gprama and executive orders that focused on providing greater accountability , oversight , and transparency .

two of the key elements of customer services standards are that they ( 1 ) include targets or goals for performance , and ( 2 ) include performance measures .

we found that three of the six programs did not have customer services standards that met these two elements .

for example , we reported that because the national park service ( nps ) did not have performance goals or measures directly linked to those goals , the agency is unable to determine the extent to which the standards are being met agency - wide or strategies to close performance gaps .

we made several recommendations related to improving the nps's and other agencies' customer service standards , including that the department of the interior ( of which nps is a part ) to ensure nps standards include ( 1 ) performance targets or goals , and ( 2 ) performance measures .

in july 2015 , nps officials reported that they had made plans to implement these recommendations .

additionally , omb is focusing on improving the federal government's customer service by developing a related cap goal .

according to information on performance.gov , as part of its work on the customer service cap goal , the administration is working to streamline transactions , develop standards for high impact services , and utilize technology to improve the customer experience .

we will be assessing omb's progress in implementing this cap goal as part of our ongoing review .

to operate as effectively and efficiently as possible and to make difficult decisions to address the federal government's fiscal and performance challenges , congress , the administration , and federal managers must have ready access to reliable and complete financial and performance information — both for individual federal entities and for the federal government as a whole .

however , in our work since 2013 we have identified areas in which agencies have not clearly reported information related to billions of dollars in government spending ( see textbox ) .

examples of agencies not clearly reporting information on government spending agencies fail to properly report over $600 billion in assistance awards the federal funding accountability and transparency act ( ffata ) was enacted in 2006 to increase the accountability and transparency over the more than $1 trillion spent by the federal government on contracts , grants , loans , and other awards annually .

the act required omb to establish a website that contains data on federal awards and guidance on agency reporting requirements for the website , usaspending.gov .

the website is to promote transparency in government spending by providing the public with the ability to track where and how federal funds are spent .

however , we reported in june 2014 that although agencies generally reported required contract information , they did not properly report information on assistance awards ( eg , grants or loans ) , totaling approximately $619 billion in fiscal year 2012 .

in addition , we found that few awards on the website contained information that was fully consistent with agency records .

we estimated with 95 percent confidence that between 2 and 7 percent of the awards contained information that was fully consistent with agencies' records for all 21 data elements examined .

we concluded that without accurate data , the usefulness of usaspending.gov will be hampered .

to improve the reliability of information on usaspending.gov , we recommended that omb ( 1 ) clarify guidance on reporting award information and maintaining supporting records , and ( 2 ) develop and implement oversight processes to ensure that award data are consistent with agency records .

omb generally agreed with our recommendations but , as of august 2015 , had not taken actions to address them .

full implementation of the data act , which amended ffata and which omb is currently working on , may address these recommendations .

usda performance reporting does not reflect effects of approximately $3 billion in spending on broadband access to affordable broadband internet is seen as vital to economic growth and improved quality of life , yet deployment in rural areas tends to lag behind urban and suburban areas .

the american recovery and reinvestment act of 2009 ( recovery act ) appropriated funding for the broadband initiatives program ( bip ) , a department of agriculture ( usda ) rural utilities service ( rus ) program to fund broadband projects primarily to serve rural areas .

however , we reported in june 2014 that rus has reported limited information on bip's impact since awarding funds to projects , and that bip results are not tracked in usda's annual performance reporting .

as a result , rus has not shown how much the program's approximately $3 billion in project funding has affected broadband availability .

gprama directs agencies to establish performance goals in annual performance plans and to report on progress made toward these goals in annual performance reports .

however , usda did not update or include bip results as compared to the related performance goals in its annual performance reports .

we concluded that without an updated performance goal and regular information reported on the results of bip projects , it is difficult for usda , rus , and policymakers to determine the impact of recovery act funds or bip's progress on improving broadband availability .

we recommended that the secretary of agriculture include bip performance information as part of usda's annual performance plan and report by comparing actual results achieved against the current subscribership goal .

usda agreed with our recommendation , and stated that it planned to modify its next annual performance plan and report to include the number of subscribers receiving new and improved service as a result of the program .

our work has also identified other problems with transparency .

as described in the textbox below , only one of the six federal services for which we reviewed customer service standards had standards that were easily publicly available .

most agencies reviewed did not make customer service standards easily publicly available our recent work has also found issues with transparency related to agencies' customer service standards .

in october 2014 we identified key elements of customer service standards — which should inform customers as to what they have a right to expect when they request services — that would allow agencies to better serve the needs of their customers by providing greater accountability , oversight , and transparency .

one of the elements that we identified is that customer service standards be easily publicly available .

easily available standards help customers know what to expect , when to expect it , and from whom .

as part of our work , we assessed the extent to which customer service standards at six services within five federal agencies ( including two services within one of those agencies ) included key elements , including easily publicly accessible standards .

we found that only one of these services had standards that were easily available to the public .

that service — customs and border protection ( cbp ) inspection of individuals — posts its standards on its website as well as at points of service in entry ports , field offices , and headquarters , according to cbp officials .

the other five services , we found , did not make their standards easily accessible to the public .

for example , we had reported in 2010 that the forest service did not make its customer service standards available to its customers because officials felt that the standards would not be helpful to the visitors who evaluate such things as the cleanliness of restrooms against their own standards and not those set forth by the forest service .

in 2014 , forest service officials told us that there has been no change since 2010 .

however , according to executive orders and guidance , standards are specifically intended to inform the public , and should be publicly available .

we recommended that the department of agriculture ( of which the forest service is a part ) ensure that the forest service's standards are easily publicly available , among other things .

in addition , we made recommendations to the other five services that had not made their standards easily accessible to the public .

although gprama requirements have the potential to increase transparency of performance information , we have found mixed progress in implementing these requirements .

program inventories .

gprama's requirements for program inventories have the potential to improve transparency of performance information , but , as previously described , our october 2014 report identified several issues that affect these inventories' usefulness .

for example , although gprama requires agencies to describe each program's contribution to the agency's goals , we found instances where agencies omitted that information .

ensuring agencies illustrate this alignment would better explain how programs support the results agencies are achieving .

as stated earlier , omb has put plans for updating the inventories on hold , in part due to the enactment of the data act , which is intended to increase accountability and transparency in federal spending by requiring agencies to publicly report information about any funding made available to , or expended by , an agency .

as noted in our july 2015 testimony on data act implementation , effective implementation of both the data act and gprama's program inventory provisions , especially the ability to crosswalk spending data to individual programs , could provide vital information to assist federal decision makers in addressing the significant challenges the government faces .

we identified a potential approach omb could take in merging program inventory efforts with data act implementation .

that is , omb could explore ways to improve the comparability of program data by using tagging or similar approaches that allow users to search by key words or terms and combine elements based on the user's interests and needs .

this merging could help ensure consistency in the reporting of related program - level spending information .

as mentioned previously , omb does not expect an update of program inventories to happen before may 2017 .

other planned changes to the program inventories could also improve the transparency of their information .

for example , omb staff told us that they also planned to present the 24 program inventories during the planned may 2014 update in a more dynamic , web - based format .

this approach , too , has been put on hold .

a web - based approach could make it easier to tag and sort related or similar programs .

for instance , omb plans to have agencies tag each of their programs by one or more program type in a future iteration of the inventory to provide a sorting capability for identifying the same type of program .

by providing a sorting mechanism by program type , omb could help address one of our open recommendations , described previously , that omb work with the pic to develop a detailed approach to examine common , long - standing difficulties agencies face in measuring the performance of various types of federal programs and activities .

a sorting mechanism could help by identifying ( 1 ) all programs in a given type , and ( 2 ) of those programs , any of which have developed strategies to effectively overcome measurement challenges .

additionally , in its guidance for the 2014 update before it was put on hold , omb intended for agencies to link each program to the existing web pages on performance.gov for strategic goals , strategic objectives , apgs , and cap goals .

according to omb staff , once they move forward with the next inventory update and move to a web - based presentation , users will be able to sort programs by the goals to which they contribute .

this approach also would allow users to identify programs that contribute to broader themes on performance.gov .

the themes generally align with budget functions from the president's budget and include administration of justice ; general science , space , and technology ; national defense ; and transportation , among others .

currently , the themes can be used to sort goals on performance.gov that contribute to those broad themes .

major management challenges .

another area in which our work has identified problems with transparency and communication of performance information is related to the gprama requirement that agencies report in their annual performance plans key performance information related to their major management challenges , including performance goals , milestones , indicators , and planned actions that they have developed to address such challenges .

major management challenges include programs or management functions , within or across agencies , that have greater vulnerability to fraud , waste , abuse , and mismanagement , such as those issues identified by gao as high risk , where a failure to perform well could seriously affect an agency's or the government's ability to achieve its mission or goals .

we have ongoing work , which we plan to issue in late 2015 , which is examining how federal agencies are addressing their major management challenges .

as of august 2015 , we found that agencies generally did not report key performance information about their major management challenges in their annual performance plans and reports in a transparent manner .

for example , while some agencies told us that they had internal plans for addressing their major management challenges , 12 of 24 agencies that issued agency performance plans or similar documents for fiscal year 2015 did not publicly report planned actions for addressing their major management challenges .

while the reasons for why agencies did not report complete information varied , such as readability and redundancy with other similar topics in the performance plan , agencies told us that omb's guidance appeared to give them flexibility on what information they needed to report .

we will provide updated information on major management challenges in our forthcoming report .

cap goals .

another area in which transparency is important is in communicating progress on performance goals , but our june 2014 report on cap goal reviews found that the quarterly updates for the 14 interim cap goals did not always provide a complete picture of progress .

for each of the cap goals , gprama requires omb to coordinate with agencies to establish annual and quarterly performance targets and milestones and to report quarterly the results achieved compared to the targets .

the updates we reviewed were inconsistent , and some were missing key performance information , such as performance targets , milestone due dates , and key contributors to the goals , that was needed to track progress toward the goals .

in one case , we were told that the data needed to track progress toward a goal were not available .

staff from the real property interim cap goal team told us that they did not have data available for tracking progress toward the goal of holding the federal real property footprint at its fiscal year 2012 baseline level .

in addition , we found that in some cases information on the organizations and program types that contributed to an interim cap goal , such as relevant tax expenditures , was missing .

we concluded that the incomplete information in many of the updates provided a limited basis for ensuring accountability for progress toward targets and milestones for those interim cap goals and recommended that omb take a number of actions to ensure that all key contributors were identified and that quarterly and overall progress toward cap goals could be fully reported .

these included identifying all key contributors to the achievement of the goals ; identifying annual planned levels of performance and quarterly targets for each of the goals ; developing plans to identify , collect , and report data necessary to demonstrate progress being made toward each of the goals or developing an alternative approach for tracking and reporting on progress quarterly ; and reporting the time frames for the completion of milestones , the status of milestones , and how milestones are aligned with strategies or initiatives that support the achievement of each goal .

as described previously , omb has increased its emphasis on cap goal governance for the current set of cap goals , and omb has taken actions to address concerns our work raised about cap goal reviews .

one of the actions omb has taken , together with the pic , was to develop revised guidance , in the form of a template , for cap goal teams to use to report quarterly progress updates for these goals .

this template responded to three of our recommendations related to cap goal progress reporting by including a section for the cap goal teams to identify programs that contribute to their goals ; directing the teams to list targets for the key indicators they use to track progress ; and directing the teams to establish work plans with a list of specific milestones that should include milestone due dates and information on milestone status .

the template also indicated that goal teams can organize milestones by each identified sub - goal , aligning specific activities with the objectives to which they contribute .

in addition , the pic provided guidance in january 2015 that further addressed two of our recommendations .

the guidance directs cap goal teams to report all agencies , organizations , programs , activities , regulations , tax expenditures , policies , and other activities that contribute to each goal .

it also specifically notes that gprama requires the teams to report on performance against targets and states that quarterly progress updates should identify areas where progress has exceeded expectations or been slower than expected or where targets for performance measures have been missed .

the actions that omb and the pic have taken to address our recommendations have helped to improve the transparency of the cap goal progress updates .

for example , nearly all of the quarterly updates released in june 2014 for the second quarter of fiscal year 2014 included milestone due dates and information on their status .

many ( 8 of 15 ) of the lists of milestones aligned with specific sub - goals .

quality of performance information .

gprama requirements for reporting on the quality of performance information also have the potential to increase transparency , as they require agencies to publicly report on how they are ensuring the accuracy and reliability of the performance information they use to measure progress toward apgs and performance goals .

specifically , for each apg , agencies must provide information addressing five requirements to omb for publication on performance.gov .

additionally , agencies must address all five requirements for performance goals , which include apgs .

our september 2015 report on the quality of publicly reported performance information found limited information on performance.gov on the quality of performance information used to assess progress on six selected agencies' 23 apgs .

in response to our review , omb updated its a - 11 guidance in june 2015 to direct agencies to either provide this information for publication on performance.gov on how they are ensuring the quality of performance information for their apgs , or provide a hyperlink from performance.gov to an appendix in their performance report that discusses the quality of their performance information .

omb staff stated that this information will likely not be available until agencies start reporting on the next set of apgs ( for fiscal years 2016 and 2017 ) .

this is because omb will need to update a template that agencies complete for their performance.gov updates .

further , the agencies we reviewed generally did not describe how they addressed all five requirements for their individual apgs in their performance plans and reports .

while all six agencies described how they ensured the quality of their performance information overall , we found that only dhs's performance plans and reports included discussions about performance information quality addressing all five gprama requirements , as shown in table 3 and described in more detail in the textbox below .

dhs addressed gprama requirements in explaining how performance information quality is ensured for all agency priority goals in september 2015 , we reported that of the 23 apgs in our sample from six agencies , we could only find discussions about performance information quality that addressed all five of the gprama requirements for three apgs , which belonged to dhs .

dhs presented information about performance information quality for all three of its apgs in its performance plans and reports .

specifically , dhs published an appendix to its performance plans and reports with detailed discussions of performance information quality for 10 performance measures used to measure progress on these apgs .

for each measure , dhs's appendix described the related program , the scope of the data , the source and collection methodology for the data , and an assessment of data reliability .

in our september 2015 report , we recommended that all six of the agencies in our review work with omb to describe on performance.gov how they are ensuring the quality of their apgs' performance information and that the agencies , except for dhs , also describe this in their annual performance plans and reports .

we also noted that to help improve the reliability and quality of performance information , omb and the pic established the data quality cross - agency working group in february 2015 .

the group could serve as a vehicle for disseminating good practices in public reporting on data quality .

as a result , we also recommended that omb , working with the pic , focus on ways the pic's data quality working group can improve public reporting for apgs .

omb did not comment on the recommendations , but the six agencies generally concurred or identified actions they planned to take to implement them .

our work examining aspects of gprama implementation and its effects on agency performance management has identified a number of areas in which improvements are needed .

since gprama was enacted in january 2011 , we have made a total of 69 recommendations to omb and agencies aimed at improving its implementation .

omb and the agencies have generally agreed with the recommendations we have made thus far , and have implemented some of them .

however , of the 69 recommendations we have made , 55 ( about 80 percent ) have not yet been implemented , while 14 recommendations ( about 20 percent ) have been implemented .

additional details on these recommendations and their status are included in appendixes ii , iii and iv .

we made 21 recommendations to omb and agencies between may 2012 , when we issued our first report on gprama implementation , and june 2013 , when we issued our previous summary report .

fourteen ( about 67 percent ) of these recommendations have not been implemented .

between july 2013 and september 2015 , we made 48 additional recommendations to omb and the agencies .

forty - one ( about 85 percent ) of these recommendations have not been implemented .

figure 11 shows the number of recommendations we have made , by year , and the number that have been implemented .

omb , which has been the focus of most of our recommendations , has implemented just over one - third ( 14 ) of the 38 recommendations we have made to it .

because of the agency's central role in implementing gprama , we made more recommendations to omb in our work under the act than to all other agencies combined .

most of the actions omb has taken to implement our recommendations involve updating or issuing new guidance .

agencies have yet to implement any of the 31 recommendations we have made , although we made most ( 23 ) of these recommendations in reports that we have issued since july 2015 .

specifically , these 23 recommendations were included in our recent work on data - driven reviews and the quality of performance information , and they focus on ensuring that agency data - driven review processes and reporting on the quality of performance information are consistent with gprama requirements , omb guidance , and leading practices .

while omb has implemented some of our recommendations , some of those that have yet to be implemented focus on long - standing and significant issues .

for example , as described previously , we have made several recommendations to identify and assess the contributions of tax expenditures toward executive branch goals , but omb and agencies have taken little action to address these recommendations .

additionally , we have reported that agencies have difficulty measuring the performance of different program types – such as grants and regulations .

we have identified individual examples of these problems , but our work has also shown that some areas — such as customer service — are common problems across multiple agencies .

agencies have not yet implemented recommendations we made in our october 2014 report on agency customer service standards .

we have also made numerous recommendations aimed at improving the effectiveness of various aspects of gprama implementation .

these recommendations focus on a range of areas , including making federal program inventories more useful , strengthening data - driven review practices , and improving goal leader accountability mechanisms .

as we have stated , effective gprama implementation has the potential to improve performance management across government and can help address crosscutting issues , promote the use of performance information , increase alignment of performance with results , and improve transparency .

we will continue to monitor omb's and agencies' actions to implement our recommendations .

we provided a copy of this draft report to the director of the office of management and budget for its review and comment .

on september 18 , 2015 , omb staff provided us with oral comments on the report .

omb staff generally agreed with the information presented in the report , and provided us with technical clarifications , which we have incorporated as appropriate .

we are sending copies of this report to interested congressional committees , the director of the office of management and budget , and other interested parties .

this report will also be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contract points for our offices of congressional relations and public affairs may be found on the last page of our report .

key contributors to this report are listed in appendix vi .

the gpra modernization act of 2010 ( gprama ) includes a provision for us to review implementation of the act at several critical junctures and provide recommendations for improvements to implementation .

specifically , we are required to evaluate and report on how implementation of the act is affecting performance management at the agencies subject to the chief financial officers act of 1990 , as amended , and to evaluate the implementation of cross - agency priority ( cap ) goals , federal government performance plans , and related reporting by september 2015 .

this report pulls together findings from our work related to the act and on federal performance and coordination issues , focusing on ongoing work and work issued since our last summary report on gprama was issued in june 2013 , as well as some results from our work on two ongoing engagements .

our objectives for this report were to evaluate how gprama implementation has affected progress in addressing four areas: ( 1 ) crosscutting issues ; ( 2 ) the extent to which performance information is useful and used ; ( 3 ) aligning daily operations with results ; and ( 4 ) communication of performance information .

to address these objectives , we reviewed gprama , office of management and budget ( omb ) guidance , and our past and recent work related to managing for results and the act .

we also interviewed omb and performance improvement council staff .

our recent work under gprama , both ongoing and issued since june 2013 , covered the 24 cfo act agencies and the army corps of engineers - civil works .

most ( 8 ) of the 12 reports that are the focus of this report used selected agencies as case illustrations .

half of the 12 reports included government - wide reviews , and in some cases involved surveys of all or most of the cfo act agencies .

this report also includes some results from our ongoing work examining the implementation of cap goals , which we plan to issue at the end of 2015 .

we identified lessons learned from the interim cap goal period , and we assessed initial progress implementing the current set of cap goals .

to do this , we selected 7 of the 15 cap goals for examination , interviewed officials with responsibility for implementing these goals , and reviewed relevant guidance and documentation .

in order to provide some insight into both interim and new cap goals , the team initially randomly selected 2 of each , resulting in selecting open data and stem education , which were also interim cap goals , and job - creating investment and lab - to - market , which are new cap goals .

because gao did recent work on three additional cap goals — customer service , people and culture , and the smarter it delivery — we also selected those goals .

we interviewed omb and pic staff responsible for management and implementation of the current cap goals and responsible agency officials , including cap goal leaders and members of the seven cap goal implementation teams .

we reviewed omb and pic guidance , relevant documentation , and quarterly progress updates published on performance.gov from the second quarter of fiscal year 2014 through the second quarter of fiscal year 2015 , published in june 2015 .

this report also reflects some results from our ongoing work on major management challenges , which we also plan to issue at the end of 2015 .

we compared information reported in 24 agency performance plans and reports against gprama requirements and omb circular a - 11 guidance to identify agency activities and reporting related to major management challenges .

we interviewed omb staff about their guidance related to major management challenges .

we also interviewed 24 cfo act agency performance officials , including performance improvement officers , program offices officials , and , when appropriate , officials from agencies' chief financial officer and chief human capital offices to understand how agencies defined and addressed their major management challenges .

we conducted this performance audit from april 2015 to september 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

table 4 shows recommendations we have made as part of our work under the gpra modernization act ( gprama ) that the office of management and budget ( omb ) has implemented .

table 5 shows recommendations we have made to the office of management and budget ( omb ) as part of our work under the gpra modernization act ( gprama ) that have not been implemented .

table 6 shows recommendations we have made to agencies as part of our work under the gpra modernization act ( gprama ) that have not been implemented .

in addition to the above contact , sarah e. veale ( assistant director ) and kathleen padulchick supervised this review and the development of the resulting report .

margaret m. adams , shea bader , lisette baylor , peter beck , elizabeth curda , dewi djunaidy , deirdre duffy , karin fangman , jennifer m. felder , farrah graham , emily gruenwald , jonathan harmatz , jennifer kamara , barbara lancaster , dainia lawes , benjamin t. licht , adam miles , michael o'neill , lisa pearson , steven putansu , marylynn sergent , stephanie shipman , matthew sweeney , and dan webb also made key contributions .

managing for results: greater transparency needed in public reporting on the quality of performance information for selected agencies' priority goals .

gao - 15-788 .

washington , d.c.: september 10 , 2015 .

data act: progress made in initial implementation but challenges must be addressed as efforts proceed .

gao - 15-752t .

washington , d.c.: july 29 , 2015 .

managing for results: practices for effective agency strategic review. , gao - 15-602 .

washington , d.c.: july 29 , 2015 .

managing for results: agencies report positive effects of data - driven reviews on performance but some should strengthen practices .

gao - 15-579 .

washington , d.c.: july 7 , 2015 .

2015 annual report: additional opportunities exist to reduce fragmentation , overlap , and duplication and achieve other financial benefits .

gao - 15-404sp .

washington d.c.: april 14 , 2015 .

fragmentation , overlap , and duplication: an evaluation and management guide .

gao - 15-49sp .

washington , d.c.: april 14 , 2015 .

government efficiency and effectiveness: opportunities to reduce fragmentation , overlap , and duplication and achieve other financial benefits .

gao - 15-522t .

washington , d.c. april 14 , 2015 .

high - risk series: an update .

gao - 15-290 .

washington , d.c.: february 11 , 2015 .

federal data transparency: effective implementation of the data act would help address government - wide management challenges and improve oversight .

gao - 15-241t .

washington , d.c.: december 3 , 2014 .

program evaluation: some agencies reported that networking , hiring , and involving program staff help build capacity .

gao - 15-25 .

washington , d.c.: november 13 , 2014 .

government efficiency and effectiveness: inconsistent definitions and information limit the usefulness of federal program inventories .

gao - 15-83 .

washington , d.c.: october 31 , 2014 .

managing for results: selected agencies need to take additional efforts to improve customer service .

gao - 15-84 .

washington , d.c.: october 24 , 2014 .

managing for results: agencies' trends in the use of performance information to make decisions .

gao - 14-747 .

washington , d.c.: september 26 , 2014 .

managing for results: enhanced goal leader accountability and collaboration could further improve agency performance .

gao - 14-639 .

washington , d.c.: july 22 , 2014 .

managing for results: omb should strengthen reviews of cross - agency goals .

gao - 14-526 .

washington , d.c.: june 10 , 2014 .

government efficiency and effectiveness: views on the progress and plans for addressing government - wide management challenges .

gao - 14-436t .

washington , d.c.: march 12 , 2014 .

managing for results: implementation approaches used to enhance collaboration in interagency groups .

gao - 14-220 .

washington , d.c.: february 14 , 2014 .

financial and performance management: more reliable and complete information needed to address federal management and fiscal challenges .

gao - 13-752t .

washington , d.c.: july 10 , 2013 .

managing for results: executive branch should more fully implement the gpra modernization act to address pressing governance challenges .

gao - 13-518 .

washington , d.c.: june 26 , 2013 .

