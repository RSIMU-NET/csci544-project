the census bureau ( bureau ) is relying on both the acquisition of new systems and the enhancement of existing legacy systems for conducting operations for the 2010 decennial census .

as you know , the census is mandated by the u.s. constitution and provides data that are vital to the nation .

these data are used , for example , to reapportion and redistrict the seats of the u.s. house of representatives ; realign the boundaries of the legislative districts of each state ; allocate billions of dollars in federal financial assistance ; and provide a social , demographic , and economic profile of the nation's people to guide policy decisions at each level of government .

the bureau is required to take a population count as of april 1 , 2010 ( census day ) , and the secretary of commerce is required to report to the president on the tabulation of total population by state within nine months of that date .

carrying out the census is the responsibility of the department of commerce's census bureau , which is relying on automation and technology to improve the coverage , accuracy , and efficiency of the 2010 census .

because the accuracy of the 2010 census depends , in part , on the proper functioning of these systems , both individually and when integrated , thorough testing of these systems before their actual use is critical to the success of the census .

in march 2008 , we designated the 2010 decennial census as a high - risk area , citing a number of long - standing and emerging challenges , including weaknesses in the bureau's management of its information technology ( it ) systems and operations .

the 2010 decennial census remained as one of our high - risk areas in our recent high - risk update issued in january 2009 .

given the importance of comprehensive testing prior to the 2010 census , you asked us to determine the status of and plans for the testing of key decennial systems .

to address this objective , we analyzed documentation related to system , integration , and end - to - end testing , including plans , schedules , and results , and interviewed bureau officials and contractors .

we then compared the bureau's practices with those identified in our testing guide and other best practices .

we conducted this performance audit from june 2008 to february 2009 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

appendix i contains further details about our scope and methodology .

the bureau's mission is to provide comprehensive data about the nation's people and economy .

its core activities include conducting decennial , economic , and government censuses ; conducting demographic and economic surveys ; managing international demographic and socioeconomic databases ; providing technical advisory services to foreign governments ; and performing other activities such as producing official population estimates and projections .

conducting the decennial census is a major undertaking that includes the following major activities: establishing where to count .

this includes identifying and correcting addresses for all known living quarters in the united states ( address canvassing ) and validating addresses identified as potential group quarters , such as college residence halls and group homes ( group quarters validation ) .

collecting and integrating respondent information .

this includes delivering questionnaires to housing units by mail and other methods , processing the returned questionnaires , and following up with nonrespondents through personal interviews ( nonresponse follow - up ) .

it also includes enumerating residents of group quarters ( group quarters enumeration ) and occupied transitional living quarters ( enumeration of transitory locations ) , such as recreational vehicle parks , campgrounds , and hotels .

it also includes a final check of housing unit status ( field verification ) where bureau workers verify potential duplicate housing units identified during response processing .

providing census results .

this includes processes to tabulate and summarize census data and disseminate the results to the public .

figure 1 illustrates key decennial activities .

the 2010 census enumerates the number and location of people on census day , which is april 1 , 2010 .

however , census operations begin long before census day and continue afterward .

for example , address canvassing for the 2010 census will begin in april 2009 , while tabulated census data must be distributed to the president by december 31 , 2010 , and to state legislatures by march 31 , 2011 .

figure 2 presents a timeline of key decennial operations .

automation and it are to play a critical role in the success of the 2010 census by supporting data collection , analysis , and dissemination .

several systems will be used in the 2010 census .

for example , enumeration “universes,” which serve as the basis for enumeration operations and response data collection , are organized by the universe control and management ( uc&m ) system , and response data are received and edited to help eliminate duplicate responses using the response processing system ( rps ) .

both uc&m and rps are legacy systems that are collectively called the headquarters processing systems .

geographic information and support to aid the bureau in establishing where to count u.s. citizens are provided by the master address file / topologically integrated geographic encoding and referencing ( maf / tiger ) system .

the decennial response integration system ( dris ) is to provide a system for collecting and integrating census responses from all sources , including forms and telephone interviews .

the field data collection automation ( fdca ) program includes the development of handheld computers for the address canvassing operation and the systems , equipment , and infrastructure that field staff will use to collect the data .

paper - based operations ( pbo ) was established in august 2008 , primarily to handle some of the operations that were originally part of fdca .

pbo includes it systems and infrastructure needed to support the use of paper forms for operations such as group quarters enumeration activities , nonresponse follow - up activities , enumeration at transitory locations activities , and field verification activities .

these activities were originally to be conducted using it systems and infrastructure developed by the fdca program .

finally , the data access and dissemination system ii ( dads ii ) is to replace legacy systems for tabulating and publicly disseminating data .

table 1 describes the key systems supporting the 2010 census , as well as the offices responsible for their development .

figure 3 shows the timeframes when each of the systems for the 2010 census are to be operational , according to the census bureau .

we have reported long - standing weaknesses in the bureau's management of its it systems .

for example , in october 2007 , we reported on the status and plans of key 2010 census it acquisitions and whether the bureau was adequately managing associated risks .

we identified critical weaknesses in the bureau's risk management practices , including those associated with risk identification , mitigation , and executive - level oversight .

further , operational testing planned during the census dress rehearsal would take place without the full complement of systems and functionality that was originally planned , the bureau had not finalized its plans for testing all the systems , and it was unclear whether the plans would include testing to address all interrelated systems and functionality .

we recommended that the bureau develop a comprehensive plan to conduct an end - to - end test of its systems under census - like conditions .

in march 2008 , we designated the 2010 census as a high - risk area , citing several long - standing and emerging challenges .

these challenges included , among other things , weaknesses in risk management and system testing , elimination of several operations from the 2008 dress rehearsal , and questions surrounding the performance of handheld computers developed for the 2010 census .

we have also testified on significant risks facing the 2010 census .

for example , in march 2008 , we testified that the fdca program was experiencing significant problems , including schedule delays and cost increases resulting from changes to system requirements , which required additional work and staffing .

shortly thereafter , in april 2008 , we testified on the bureau's efforts to implement risk reduction strategies , including the decision to drop the use of handheld computers during the nonresponse follow - up operation and revert to a paper - based operation .

further , in june 2008 , we testified that the bureau had taken important steps to plan for a paper - based nonresponse follow - up operation , but several aspects remained uncertain .

we concluded that it was critical to test capabilities for supporting the nonresponse follow - up operation .

in july 2008 , we reported that continued planning and testing of the handheld computers would be critical to the address canvassing operation .

specifically , the bureau had developed a testing plan that included a limited operational field test , but the plan did not specify the basis for determining whether the fdca solution was ready for address canvassing and when and how this determination would occur .

in response to our findings and recommendations , the bureau has taken several steps to improve its management of the 2010 decennial census .

for example , the bureau has sought external assessments of its activities from independent research organizations , implemented a new management structure and management processes , brought in experienced personnel in key positions , and established improved reporting processes and metrics .

as stated in our testing guide and the institute of electrical and electronics engineers ( ieee ) standards , complete and thorough testing is essential for providing reasonable assurance that new or modified it systems will perform as intended .

to be effective , testing should be planned and conducted in a structured and disciplined fashion that includes processes to control each incremental level of testing , including testing of individual systems , the integration of those systems , and testing to address all interrelated systems and functionality in an operational environment .

system testing: verifies that the complete system ( i.e. , the full complement of application software running on the target hardware and systems software infrastructure ) meets specified requirements .

it allows for the identification and correction of potential problems within an individual system , prior to integration with other systems .

integration testing: verifies that systems , when combined , work together as intended .

effective integration testing ensures that external interfaces work correctly and that the integrated systems meet specified requirements .

end - to - end testing: verifies that a defined set of interrelated systems , which collectively support an organization's core business area or function , interoperate as intended in an operational environment .

the interrelated systems include not only those owned and managed by the organization , but also the external systems with which they interface .

to be effective , this testing should be planned and scheduled in a structured and disciplined fashion .

comprehensive testing that is effectively planned and scheduled can provide the basis for identifying key tasks and requirements and better ensure that a system meets these specified requirements and functions as intended in an operational environment .

in preparation for the 2010 census , the bureau planned what it refers to as the dress rehearsal .

the dress rehearsal is managed by the bureau's decennial management division , in collaboration with other bureau divisions ( including the program offices , shown in table 1 , which are responsible for developing and testing each of the systems ) .

the dress rehearsal includes systems and integration testing , as well as end - to - end testing of key operations in a census - like environment .

during the dress rehearsal period , running from february 2006 through june 2009 , the bureau is developing and testing systems and operations , and it held a mock census day on may 1 , 2008 .

the dress rehearsal activities , which are still under way , are a subset of the activities planned for the actual 2010 census and include testing of both it and non - it related functions , such as opening offices and hiring staff .

the dress rehearsal tested several activities involving key systems .

for example , the bureau tested key systems with address canvassing and group quarters validation operations , including fdca handheld computers and the maf / tiger system .

in addition , the bureau used the uc&m system and maf / tiger to provide an initial list of housing unit addresses for the dress rehearsal test sites .

questionnaires were mailed to these housing units in april 2008 .

subsequently , a mock census day was held on may 1 , 2008 — 1 month later than originally planned .

the mock census day was delayed , in part , to focus greater attention on testing the technology being used .

the dress rehearsal identified significant technical problems during the address canvassing operations .

for example , the bureau had originally planned to use handheld computers , developed under the fdca program , for operations such as address canvassing and non - response followup .

however , from may 2007 to june 2007 , the bureau tested the handhelds under census - like conditions for the first time during the dress rehearsal address canvassing operation .

bureau officials observed a number of performance problems with the handheld computers , such as slow and inconsistent data transmissions .

in addition , help desk logs revealed that users had frequently reported problems , such as the devices freezing up or users having difficulties collecting mapping coordinates and working with large blocks ( geographic areas with large numbers of housing units , more often found in urban areas ) .

the bureau also found system problems during testing of the group quarters validation operation , in which field staff validate addresses as group quarters and collect information required for their later enumeration .

as part of this operation , the bureau tested the operations control system — designed to manage field operations that rely on paper , as well as those that rely on the handheld computers — and the system was found to be unreliable .

as a result , the workload for these operations had to be supplemented with additional paper - based efforts by local census office staff , instead of being performed electronically , as intended .

as a result of the problems observed with the handheld computers and operations control system , cost overruns and schedule slippage in the fdca program , and other issues , the bureau removed the planned testing of key operations from the dress rehearsal as follows: update / leave ( that is , after enumerators update addresses , they leave questionnaires at housing units ; this occurs mainly in rural areas lacking street names , house numbers , or both ) , enumeration of transitory locations , group quarters enumeration , and field verification .

furthermore , in april 2008 , the secretary of commerce announced a redesign of the 2010 decennial census , including the fdca program .

specifically , the bureau would no longer use handheld computers for nonresponse follow - up ( its largest field operation ) , but would conduct paper - based nonresponse follow - up , as in previous censuses .

it would , however , continue to use the handheld computers for the address canvassing operations .

in may 2008 , the bureau issued a plan that detailed key components of the paper - based operation and described processes for managing it and other operations .

it later established the pbo office to manage designing , developing , and testing paper - based operations , as well as to prepare related training materials .

in addition to the planned dress rehearsal testing , the bureau is planning supplementary testing to prepare for the 2010 decennial census .

this testing includes system , integration , and end - to - end testing of changes resulting from the dress rehearsal , operations or features that were not tested during the dress rehearsal , and additional features or enhancements that are to be added after the dress rehearsal .

the bureau has made progress in conducting system , integration , and end - to - end testing for the 2010 census , but much remains to be done .

significant testing remains to be done , and many plans for the remaining testing activities have not been developed .

the weaknesses in the bureau's it testing can be attributed , in part , to a lack of sufficient executive - level oversight and guidance on testing .

without comprehensive oversight and guidance , the bureau cannot ensure that it is thoroughly testing its systems before the 2010 decennial census .

through the dress rehearsal and other testing activities , the bureau has completed key system tests , but significant testing has yet to be performed , and planning for this is not complete .

for example , the headquarters processing systems ( uc&m and rps ) are still completing system testing related to the dress rehearsal , and the program office is planning for further testing .

for dris , on the other hand , system testing related to the dress rehearsal is complete , and additional 2010 system testing is under way .

table 2 summarizes the status and plans for system testing .

for both headquarters processing systems ( uc&m and rps ) , system testing for the dress rehearsal has been partially completed , as shown in table 3 .

for uc&m , dress rehearsal system testing is divided into three phases , as shown .

these phases include a total of 19 products ( used to control and track dress rehearsal enumeration activities ) .

the completed phases ( 1 and 2 ) included the development and testing of 14 products .

program officials had planned to complete testing of the remaining 5 products for uc&m by october 2008 , but as of december 2008 , the program had not yet completed this testing .

for rps , dress rehearsal system testing is being done by component — eight components perform functions for key activities such as data integration — where response data are integrated before processing .

according to program officials , development and testing of four components are complete , and the remaining four components are planned to be completed by march 2009 .

in addition to ongoing dress rehearsal system testing , the program office intends to perform system testing for 2010 census operations , but plans for this testing have not yet been developed .

according to program officials , they have not developed testing plans and schedules for additional testing for the 2010 census because bureau management has not yet finalized the requirements for 2010 operations .

finalizing these requirements may involve both changes to existing requirements and new requirements .

program officials stated that they do not anticipate substantial changes in uc&m and rps system requirements for 2010 census operations and plan to have them finalized by may 2009 .

in commenting on a draft of this report , the bureau provided an initial test plan and schedule , but did not provide the finalized baselined requirements for the headquarters processing systems for 2010 operations .

until the baseline requirements are established , it is unclear whether the amount of additional testing necessary for 2010 census operations will be significant .

according to industry best practices , defining requirements for a system is important because they provide a baseline for development and testing activities and are used to establish test plans , which define schedule activities , roles and responsibilities , resources , and system testing priorities .

the absence of finalized requirements increases the risk that there may not be sufficient time and resources to adequately test the systems , which are critical to ensuring that address files are accurately organized into enumeration universes and that duplicate responses are eliminated .

system testing has been partially completed for maf / tiger products ( that is , extracts from the maf / tiger system ) required for the 2010 census .

for maf / tiger , testing activities are defined by products needed for key activities , such as address canvassing .

during dress rehearsal system testing , the program office completed testing for a subset of maf / tiger products for address canvassing , group quarters validation , and other activities .

additional system testing is planned for the 2010 census .

according to program officials , as of december 2008 , the bureau had defined requirements and completed testing for 6 of approximately 60 products needed for 2010 operations ( these 6 products are related to address canvassing ) .

the program office has also developed detailed test plans and schedules through april 2009 , but these do not cover all of the remaining products needed to support the 2010 census .

table 4 is a summary of the status of maf / tiger 2010 testing and plans .

according to program officials , the detailed test plans for the remaining products will be developed after the requirements for each are finalized .

as mentioned , establishing defined requirements is important because these provide a baseline for development and testing activities and define the basic functions of a product .

the officials stated that they were estimating the number of products needed , but would only know the exact number when the requirements for the 2010 census operations are determined .

the officials added that , by january 2009 , they plan to have a detailed schedule and a list of the products needed through december 2009 .

without knowing the total number of products , related requirements , and when the products are needed for operations , the bureau risks both not sufficiently defining what each product needs to do and not being able to effectively measure the progress of maf / tiger testing activities , which therefore increases the risk that there may not be sufficient time and resources to adequately test the system and that the system may not perform as intended .

system testing has been partially completed for dris components , including paper , workflow control and management , and telephony .

portions of the functionality in each dris component are being developed and tested across five increments for 2010 operations .

as of november 2008 , the program had planned and completed testing of increments 1 , 2 , and 3 .

testing of increment 4 is currently ongoing .

table 5 is a summary of the status of dris testing for 2010 operations .

 ( in addition , system testing of a subset of dris functionality , including the integration of certain response data , took place during the dress rehearsal. ) .

the dris program has developed a detailed testing plan and schedule , including the remaining testing for increment 5 .

for example , detailed testing plans have been developed for all 558 functional requirements for dris .

according to program officials , most of the 558 functional requirements will be fully tested during increments 4 and 5 .

as of november 2008 , 22 of the 558 requirements had been tested .

although plans and schedules were completed , the change from handheld computers to paper processes for nonresponse follow - up has caused changes to dris processing requirements .

for example , dris program officials stated that they now need to process an additional 40 million paper forms generated as a result of this switch .

although dris program officials stated that they are prepared to adjust their test schedule and plan to accommodate this change , they cannot do so until the requirements have been finalized for the switch to paper processes .

 ( this responsibility is primarily that of the pbo program office. ) .

furthermore , based on the switch to paper , dris may not be able to conduct a test using these operational systems and live data .

this increases the risk that the bureau could experience problems with these systems and the processing of paper forms during the 2010 census .

the dris program office is addressing this risk by developing alternative strategies for testing and providing additional resources as contingencies for activities that may not be fully tested before 2010 operations begin .

fdca testing has been partially completed , but much more work remains .

system testing for fdca took place during the dress rehearsal , but problems encountered during this testing led to the removal of key operations from the dress rehearsal and the april 2008 redesign , as described earlier .

going forward , fdca development and testing for 2010 operations are being organized based on key census activities .

for example , fdca testing for the address canvassing and group quarters validation operations was completed in december 2008 .

the fdca contractor is currently developing and testing a backup system ( known as the continuity of operations system ) for address canvassing and group quarters validation , and is currently testing another system ( known as map printing ) to provide printed maps for paper - based field operations , such as nonresponse follow - up .

table 6 summarizes the fdca test status .

although system testing for address canvassing and group quarters validation was recently completed , program officials have not demonstrated that all current system requirements have been fully tested .

as part of a contract revision required by the april 2008 redesign , the fdca program established a revised baseline for system requirements on november 20 , 2008 .

according to program officials , this revision included both modifications to previous requirements and removal of requirements that were part of activities transferred to pbo .

as of december 2008 , program officials stated that detailed testing plans for many of the requirements exist , but need to be revised to address the newly baselined requirements .

furthermore , as of december 2008 , the fdca program had not finalized detailed testing plans and schedules for the continuity of operations and map printing systems .

according to program officials , they had not yet developed detailed testing plans and schedules for testing systems' requirements because their focus has been on testing the system for address canvassing and group quarters validation .

officials added that they plan to begin testing the requirements for the continuity of operations system in january 2009 , and for the map printing system in february 2009 .

however , without having established testing plans and schedules for these systems , it is unclear what amount of testing will be needed and whether sufficient time has been allocated in the schedule to fully test these systems before they are needed for operations .

testing has only recently started for pbo because it is still in the preliminary phase of program planning and initial system development as the bureau shifts responsibility for certain operations from the fdca program office to pbo .

because this office has only recently been created , it is currently hiring staff , developing a schedule for several iterations of development , establishing a means to store and trace requirements , developing testing plans , and establishing a configuration management process .

according to program officials , development will occur in five releases , numbered 0 through 4 .

the first release , release 0 , is planned to contain functionality for the nonresponse follow - up and group quarters enumeration activities .

table 7 provides the current status of pbo release 0 test activities .

however , the bureau still has not yet determined when detailed testing plans and schedules for pbo systems will be developed .

officials stated that a more detailed schedule for release 0 and development schedules for the remaining releases are under development , and that they plan to have the majority of the schedules developed by the end of january 2009 .

in commenting on a draft of this report , the bureau provided a partial schedule for pbo test activities .

furthermore , officials stated they had not yet fully defined which requirements pbo would be accountable for , and which of these requirements will be addressed in each iteration of development .

the officials did state that the requirements will be based on those requirements transferred from fdca as part of the reorganization .

bureau officials stated they had not yet completed these activities because responsibility for the requirements was only formally transferred as of october 2008 .

the program office expects to have its first iteration of requirements traceable to test cases by march 2009 .

however , officials did not know what percentage of program requirements will be included in this first iteration .

although progress has been made in establishing the pbo program office , numerous critical system development activities need to be planned and executed in a limited amount of time .

because of the compressed schedule and the large amount of planning that remains , pbo risks not having its systems developed and tested in time for the 2010 decennial census .

testing is critical to ensure that the paper forms used to enumerate residents of households who do not mail back their questionnaires , group quarters , and transitional living quarters are processed accurately .

the dads system ( which had been used in the 2000 census ) is currently being tested during the dress rehearsal , which is scheduled to be completed in march 2009 .

however , the bureau intends to replace dads with dads ii , which is currently being developed and tested for 2010 operations .

dads ii is still in the early part of its life cycle , and the program office has only recently started system testing activities .

the two main dads ii components , the replacement tabulation system ( rts ) and replacement dissemination system ( rds ) , are being developed and tested across a series of three iterations .

as of december 2008 , the program had begun iterations 1 and 2 for rts , and iteration 1 for rds .

table 8 summarizes the rts and rds testing status .

the dads ii program office has developed a high - level test plan for rts and rds system testing , but has not yet defined detailed testing plans and a schedule for testing system requirements .

system requirements for the new system have been baselined , with 202 requirements for rts and 318 requirements for rds .

according to program officials , the program office is planning to develop detailed testing plans for the system requirements for both rts and rds .

effective integration testing ensures that external interfaces work correctly and that the integrated systems meet specified requirements .

this testing should be planned and scheduled in a disciplined fashion according to defined priorities .

for the 2010 census , each program office is responsible for and has made progress in defining system interfaces and conducting integration testing , which includes testing of these interfaces .

however , significant activities remain in order for comprehensive integration testing to be completed by the date that the systems are needed for 2010 operations .

for example , dris has conducted integration testing with some systems , such as fdca , uc&m , and rps , and is scheduled to complete integration testing by february 2010 .

the fdca program office has also tested interfaces related to the address canvassing operation scheduled to begin in april 2009 .

however , for many other systems , such as pbo , interfaces have not been fully defined , and other interfaces have been defined but have not been tested .

table 9 provides the status of integration testing among key systems .

in addition , the bureau has not established a master list of interfaces between key systems , or plans and schedules for integration testing of these interfaces .

a master list of system interfaces is an important tool for ensuring that all interfaces are tested appropriately and that the priorities for testing are set correctly .

although the bureau had established a list of interfaces in 2007 , according to bureau officials , it was not updated because of resource limitations at the time and other management priorities .

as of october 2008 , the bureau had begun efforts to update this list , but it has not provided a date when this list will be completed .

without a completed master list , the bureau cannot develop comprehensive plans and schedules for conducting systems integration testing that indicate how the testing of these interfaces will be prioritized .

this is important because a prioritized master list of system interfaces , combined with comprehensive plans and schedules to test the interfaces , would allow for tracking the progress of this testing .

with the limited amount of time remaining before systems are needed for 2010 operations , the lack of comprehensive plans and schedules increases the risk that the bureau may not be able to adequately test system interfaces , and that interfaced systems may not work together as intended .

the dress rehearsal was originally conceived to provide a comprehensive end - to - end test of key 2010 census operations ; however , as mentioned earlier , because of the problems encountered with the handheld devices , among other things , testing was curtailed .

as a result , although several critical operations underwent end - to - end testing in the dress rehearsal , others did not .

according to the associate director for the 2010 census , the bureau tested approximately 23 of 44 key operations during the dress rehearsal .

examples of key operations that underwent end - to - end testing during the dress rehearsal are address canvassing and group quarters validation .

an example of a key operation that was not tested is the largest field operation — nonresponse follow - up .

although the bureau recently conducted additional testing of the handhelds , this test was not a robust end - to - end test .

in december 2008 , after additional development and improvements to the handheld computers , the bureau conducted a limited field test for address canvassing , intended to assess software functionality in an operational environment .

we observed this test and determined that users were generally satisfied with the performance of the handhelds .

according to bureau officials , the performance of the handheld computers has substantially improved from previous tests .

however , the test was not designed to test all the functionality of the handhelds in a robust end - to - end test — rather , it included only a limited subset of functionality to be used during the 2009 address canvassing operations .

further , the field test did not validate that the fdca system fully met specified requirements for the address canvassing operation .

bureau officials stated that additional testing of the fdca system , such as performance testing , mitigated the limitations of this field test .

nonetheless , the lack of robustness of the field test poses several risks for 2010 operations .

specifically , without testing all the fdca system's requirements in a robust operational environment , it is unclear whether the system can perform as intended when the address canvassing operation begins in april 2009 .

furthermore , as of december 2008 , the bureau has neither established testing plans nor schedules to perform end - to - end testing of the key operations that were removed from the dress rehearsal , nor has it determined when these plans will be completed .

as previously mentioned , these operations include enumeration of transitory locations , group quarters enumeration , and field verification .

although the bureau has established a high - level strategy for testing these operations , which provides details about the operations to be tested , bureau officials stated that they have not developed testing plans and schedules because they are giving priority to tests for operations that are needed in early 2009 .

in addition , key systems needed to test these operations are not ready to be tested because they are either still in development or have not completed system testing .

until system and integration testing activities are complete , the bureau cannot effectively plan and schedule end - to - end testing activities .

without sufficient end - to - end testing , operational problems can go undiscovered , and the opportunity to improve these operations will be lost .

the decreasing time available for completing end - to - end testing increases the risk that testing of key operations will not take place before the required deadline .

bureau officials have acknowledged this risk in briefings to the office of management and budget .

the bureau is in the process of identifying risks associated with incomplete testing and developing mitigation plans , which it had planned to have completed by november 2008 .

however , as of january 2009 , the bureau had not completed these mitigation plans .

according to the bureau , the plans are still being reviewed by senior management .

without plans to mitigate the risks associated with limited end - to - end testing , the bureau may not be able to respond effectively if systems do not perform as intended .

as stated in our testing guide and ieee standards , oversight of testing activities includes both planning and ongoing monitoring of testing activities .

ongoing monitoring entails collecting and assessing status and progress reports to determine , for example , whether specific test activities are on schedule .

using this information , management can effectively determine whether corrective action is needed and , if so , what action should be taken .

in addition , comprehensive guidance should describe each level of testing ( for example , system , integration , or end - to - end ) , criteria for each level , and the type of test products expected .

the guidance should also address test preparation and oversight activities .

although the 2010 decennial census is managed by the decennial management division , the oversight and management of key census it systems is performed on a decentralized basis .

dris , fdca , and dads ii each have a separate program office within the decennial automation contracts management office ; headquarters processing and pbo are managed within the decennial system and processing office ; and maf / tiger is managed within the geography division .

each program has its own program management reviews and develops plans and tracks metrics related to testing .

these offices and divisions collectively report to the associate director for the 2010 census .

according to the bureau , the associate director chairs biweekly meetings where the officials responsible for these systems meet to review the status of key systems development and testing efforts .

in addition , in response to prior recommendations , the bureau took initial steps to enhance its programwide oversight ; however , these steps have not been sufficient to establish adequate executive - level oversight .

in june 2008 , the bureau established an inventory of all testing activities specific to all key decennial operations .

this inventory showed that , as of may 2008 , 18 percent of about 1049 system testing activities had not been planned .

 ( see fig .

4. ) .

in addition , approximately 67 percent of about 836 operational testing activities had not been planned .

although officials from the decennial system and processing office described the inventory effort as a means of improving executive - level oversight , the inventory has not been updated since may 2008 , and officials have no plans for further updates .

instead , officials stated that they plan to track testing progress as part of the bureau's detailed master schedule of census activities .

however , this schedule does not provide comprehensive status information on testing .

in another effort to improve executive - level oversight , the decennial management division began producing ( as of july 2008 ) a weekly executive alert report and has established ( as of october 2008 ) monthly dashboard and reporting indicators .

however , these products do not provide comprehensive status information on the testing progress of key systems and interfaces .

for example , the executive alert report does not include the progress of testing activities , and although the dashboard provides a high - level , qualitative assessment of testing for key operations and selected systems , it does not provide information on the testing progress of all key systems and interfaces .

further , the assessment of testing progress has not been based on quantitative and specific metrics .

for example , the status of testing key operations removed from the dress rehearsal was marked as acceptable , or “green,” although the bureau does not yet have plans for testing these activities .

bureau officials stated that they marked these activities as acceptable because , based on past experience , they felt comfortable that a plan would be developed in time to adequately test these operations .

the lack of quantitative and specific metrics to track progress limits the bureau's ability to accurately assess the status and progress of testing activities .

in commenting on a draft of this report , the bureau provided selected examples in which they had begun to use more detailed metrics to track the progress of end - to - end testing activities .

finally , although the bureau announced in august 2008 that it was planning to hire a senior manager who would have primary responsibility for monitoring testing across all decennial systems and programs , the position had not been filled as of january 2009 .

instead , agency officials stated that the role is being filled by another manager from the decennial statistical studies division , who has numerous other responsibilities .

the bureau also has weaknesses in its testing guidance ; it has not established comprehensive guidance for system testing .

according to the associate director for the 2010 census , the bureau did establish a policy strongly encouraging offices responsible for decennial systems to use best practices in software development and testing , as specified in level 2 of carnegie mellon's capability maturity model® integration .

however , beyond this general guidance , there is no additional guidance on key testing activities such as criteria for each level or the type of test products expected .

standardized policies and procedures help to ensure comprehensive processes across an organization and allow for effective executive - level oversight .

the lack of guidance has led to an ad hoc — and , at times — less than desirable approach to testing .

while the bureau's program offices have made progress in testing key decennial systems , much work remains to ensure that systems operate as intended for conducting an accurate and timely 2010 census .

several program offices have yet to prepare and execute system test plans and schedules and ensure that system requirements are fully tested .

in addition , the bureau has not developed a master list of interfaces , which is necessary to prioritize testing and to develop comprehensive integration test plans and schedules .

additionally , end - to - end testing plans for key operations have not been finalized or executed based on established priorities to help ensure that systems will support census operations .

weaknesses in the bureau's it testing can be attributed , in part , to a lack of sufficient executive - level oversight and guidance .

more detailed metrics and status reports would help the bureau to better monitor testing progress and identify and address problems .

giving accountability for testing to a senior - level official would also provide the focus and attention needed to complete critical testing .

also , completing risk mitigation plans will help ensure that actions are in place to address potential problems with systems .

given the rapidly approaching deadlines of the 2010 census , completing these important tests and establishing stronger executive - level oversight and guidance are critical to ensuring that systems perform as intended when they are needed .

to ensure that testing activities for key systems for the 2010 census are completed , we are making 10 recommendations .

we recommend that the secretary of commerce require the director of the census bureau to expeditiously implement the following recommendations: for the headquarters uc&m and rps , finalize requirements for 2010 census operations and complete testing plans and schedules for 2010 operations that trace to baselined system requirements .

for maf / tiger , establish the number of products required , define related requirements , and establish a testing plan and schedule for 2010 operations .

for fdca , establish testing plans for the continuity of operations and map printing systems that trace to baselined system requirements .

for pbo , develop baseline requirements and complete testing plans and schedules for 2010 operations .

establish a master list of system interfaces ; prioritize the list , based on system criticality and need date ; define all interfaces ; and develop integration testing plans and schedules for tracking the progress of testing these interfaces .

establish a date for completing testing plans for the operations removed from the dress rehearsal operations and prioritize testing activities for these operations .

finalize risk mitigation plans detailing actions to address system problems that are identified during testing .

establish specific testing metrics and detailed status reports to monitor testing progress and better determine whether corrective action is needed for all key testing activities .

designate a senior manager with primary responsibility for monitoring testing and overseeing testing across the bureau .

in addition , after the 2010 census , we recommend that the bureau establish comprehensive systems and integration testing guidance to guide future testing of systems .

the associate under secretary for management of the department of commerce provided written comments on a draft of this report .

the department's letter and general comments are reprinted in appendix ii .

in the comments , the department and bureau stated they had no significant disagreements with our recommendations .

however , the department and bureau added that since the fdca replan last year , their testing strategy has been to focus on those things they have not done before , and to demonstrate to their own satisfaction that new software and systems will work in production .

the department added that it has successfully conducted census operations before , and was focusing “on testing the new things for 2010 — not things that have worked before.” while we acknowledge that the bureau has conducted key census operations before , the systems and infrastructure in place to conduct these operations have changed substantially since the 2000 census .

for example , while the bureau has conducted paper - based nonresponse followup during previous censuses , it will be using newly developed systems which have not yet been fully tested in a census - like environment to integrate responses and manage the nonresponse followup work load .

in addition , new procedures , such as one to remove questionnaires that were mailed in late from the nonresponse followup operation , have not been tested with these systems .

any significant change to an existing it system introduces the risk that the system may not work as intended ; therefore , testing all systems after changes have been made to ensure the systems work as intended is critical to the success of the 2010 census .

in addition , the department and bureau provided technical comments , such as noting draft plans that had been developed after the conclusion of our work , that we have incorporated where appropriate .

we are sending copies of this report to the secretary of commerce , the director of the u.s. census bureau , and other appropriate congressional committees .

the report also is available at no charge on the gao web site at http: / / www.gao.gov .

if you have any questions about this report , please contact david powner at ( 202 ) 512-9286 or pownerd@gao.gov .

gao staff who made contributions to this report are listed in appendix iii .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

to determine the status of and plans for testing key decennial systems , we analyzed documentation related to system , integration , and end - to - end testing .

for system testing , we analyzed documentation related to each key decennial system , including system test plans , schedules , requirements , results , and other test - related documents .

we then compared the bureau's practices with those identified in our testing guide and institute of electrical and electronics engineers ( ieee ) standards to determine the extent to which the bureau had incorporated best practices in testing .

we also interviewed program officials and contractors of key decennial systems to obtain information on the current status of and plans for testing activities .

for integration testing , we analyzed interface control documents , interface testing plans , and schedules .

we also analyzed documentation of the census bureau's ( bureau ) oversight of integration testing activities , including efforts to issue integration testing guidance and monitor the progress of integration testing activities .

we interviewed program officials at each key decennial system program office to obtain information on the current status of and plans for integration testing and interviewed program officials at the decennial systems processing office to obtain information on the executive - level oversight of integration testing activities .

we compared the bureau's practices with those identified in our testing guide and ieee guidance .

for end - to - end testing , we analyzed documentation related to the testing of key census operations during the bureau's dress rehearsal , additional testing conducted for the address canvassing operation , and efforts to establish testing plans and schedules for operations removed from the dress rehearsal .

we also observed the bureau's operational field test , held in december 2008 in fayetteville , north carolina .

we interviewed program officials at the decennial systems processing office to obtain information on the current status and plans for end - to - end testing activities .

we compared the bureau's practices with those identified in our testing guide and ieee guidance .

we also analyzed documentation of the bureau's overall oversight of testing , including its executive alert reports and monthly dashboard reports .

in addition , we assessed system testing guidance and interviewed the associate director for the decennial census to obtain information on the overall oversight of testing activities .

we conducted this performance audit from june 2008 to february 2009 in the washington , d.c. , and fayetteville , north carolina , areas , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

in addition to the contact name above , individuals making contributions to this report included cynthia scott ( assistant director ) , sher`rie bacon , barbara collier , neil doherty , vijay d'souza , nancy glover , lee mccracken , jonathan ticehurst , melissa schermerhorn , and karl seifert .

