servicemembers who are injured in war or as the result of accidents and illnesses may face a difficult transition as they leave the military and become veterans .

in response to concerns that wounded , ill , or injured servicemembers had to undergo two complex disability evaluations — first by the department of defense ( dod ) then by the department of veterans affairs ( va ) — dod and va jointly designed a new integrated disability evaluation process to expedite the delivery of benefits to servicemembers .

in november 2007 , dod and va began pilot testing the integrated disability evaluation system ( ides ) at three military treatment facilities in the washington , d.c. area , and expanded the number of sites over time .

as of october 1 , 2011 , ides had replaced the military services' existing — or “legacy” — disability evaluation systems for almost all new disability cases .

past gao work highlighted challenges dod and va experienced while piloting the ides and recommended a number of improvements .

for instance , we reported in december 2010 that insufficient staff and logistical challenges contributed to delays in completing ides cases and recommended the agencies take steps to ensure adequate staffing levels and develop a systematic process for monitoring caseloads .

in response to ongoing concerns with ides performance , this report provides information on ( 1 ) the extent to which dod and va are meeting ides performance goals , and ( 2 ) steps dod and va are taking to improve ides performance .

in conducting our work , we obtained dod timeliness and customer satisfaction data from the inception of ides in 2007 to december 2011 .

we assessed the reliability of these data and analyzed them to look for changes in performance over time ; factors that may help or hinder performance ; and relationships between servicemember satisfaction and case outcomes and timeliness .

we supplemented these analyses with site visits to six military treatment facilities , where we spoke with dod and va staff as well as some servicemembers involved in the ides process .

we selected these facilities to obtain perspectives from sites in different military services and geographical regions and with varying caseloads and performance outcomes .

for both research objectives , we interviewed key officials involved with ides at dod , va , and each of the military services , and reviewed pertinent reports , guidance , plans , relevant federal laws , regulations , directives , and other documents .

we conducted this performance audit from may 2011 to august 2012 , in accordance with generally accepted government auditing standards .

these standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the ides process begins at a military treatment facility when a physician identifies one or more conditions that may interfere with a servicemember's ability to perform his or her duties .

the process involves four main phases: the medical evaluation board ( meb ) , the physical evaluation board ( peb ) , transition out of military service ( transition ) , and va benefits .

meb phase: in this phase , medical examinations are conducted and decisions are made by the meb regarding a servicemember's ability to continue to serve in the military .

this phase involves four stages: ( 1 ) the servicemember is counseled by a dod board liaison on what to expect during the ides process ; ( 2 ) the servicemember is counseled by a va case manager on what to expect during the ides process and medical exams are scheduled ; ( 3 ) medical exams are conducted according to va standards for exams for disability compensation by va , dod , or contractor physicians , and ( 4 ) exam results are used by the meb to identify conditions that limit the servicemember's ability to serve in the military.an impartial medical review by a physician not on the meb , or both .

also during this stage , a servicemember can seek a rebuttal , or peb phase: in this subsequent phase , decisions are made about the servicemember's fitness for duty , disability rating and dod and va disability benefits , and the servicemember has opportunities to appeal those decisions .

this includes: ( 1 ) the informal peb stage , an administrative review of the case file by the relevant military branch's peb without the presence of the servicemember ; ( 2 ) va rating stage , where a va rating specialist prepares a rating that covers the conditions that dod determined made a servicemember unfit for duty and any other conditions claimed by the servicemember to va .

this rating is prepared for use by both agencies in determining disability benefits .

in addition , servicemembers have several opportunities to appeal different aspects of their disability evaluations: a servicemember dissatisfied with the decision on whether he or she is fit for duty may request a hearing with a “formal” peb ; a servicemember who disagrees with the formal peb fitness decision can , under certain conditions , appeal to the reviewing authority of the peb ; and a servicemember can ask for va to reconsider its rating , but only for conditions found unfitting by the peb .

transition phase: if the servicemember is found unfit to serve , he or she enters the transition phase and begins the process of separating from the military .

during this time , the servicemember may take accrued leave .

also , dod board liaisons and va case managers provide counseling on available benefits and services , such as job assistance .

va benefits phase: a servicemember found unfit and separated from service becomes a veteran and enters the va benefits phase .

va finalizes its disability rating after receiving evidence of the servicemember's separation from military service .

va then starts to award monthly disability compensation to the veteran .

dod and va established timeliness goals for the ides process to provide va benefits to active duty servicemembers within 295 days of being referred into the process , and to reserve component members within 305 days ( see fig .

1 ) .

dod and va also established interim timeliness goals for each phase and stage of the ides process .

the overall timeframes are intended to represent an improvement over the legacy disability evaluation system , which was estimated to take 540 days to complete .

in addition to timeliness , the agencies also established a performance goal of having 80 percent of servicemembers satisfied with the ides process .

dod measures satisfaction through surveys conducted after the completion of the meb , peb , and transition phases .

each survey consists of approximately 30 questions , including 4 questions that ask about the servicemember's satisfaction with the overall ides process up to that point .

reported satisfaction rates for each phase are based on an average of responses to these four questions , and reported overall satisfaction with ides ( which is used to track the percent satisfied under the performance goal ) is an average of satisfaction rates for the three phases .

from the original 3 pilot military treatment facilities in the washington , the ides has expanded to 139 military treatment facilities in d.c. , area,the u.s. and several other countries .

dod and va first added 24 military treatment facilities to the pilot in fiscal years 2009 and 2010 , bringing the pilot total to 27 .

in 2010 , dod and va leadership decided to implement the ides world - wide , and did so in 4 stages between october 2010 and september 2011 , adding 112 military treatment facilities .

as ides expanded , the number of new cases enrolled in ides has also increased , totaling 18,651 in fiscal year 2011 ( see fig .

2 ) .

ides caseloads vary by service , but the army manages the bulk of ides cases .

of new cases referred to ides in fiscal year 2011 , about 64 percent were in the army , and much of the growth in caseload has been in the army .

additionally , active duty servicemembers make up the majority of ides cases , with about 88 percent of new cases in fiscal year 2011 involving this group ( see fig .

3 ) .

ides timeliness has worsened since the inception of the program .

since fiscal year 2008 , the average number of days for servicemember cases to be processed and receive benefits increased from 283 to 394 for active duty cases ( compared to the goal of 295 days ) and from 297 to 420 , for reserve component cases ( compared to the goal of 305 days ) ( see fig .

4 ) .

along with increasing average processing times , the percent of ides cases awarded benefits within timeliness goals has steadily declined .

dod's and va's current goal is to complete 60 percent of ides cases on time .

in fiscal year 2008 , an average of 63 percent of cases for active duty servicemembers and 65 percent for reservists completed the process and received benefits within the timeliness goals ; by fiscal year 2011 this was down to 19 and 18 percent respectively ( see fig .

5 ) .

these trends also hold when considering all cases that completed the ides process regardless of outcome , although overall processing times were shorter .

 ( see app .

iii for more information on case processing times regardless of outcome. ) .

when examining timeliness across the four phases that make up ides , data show that average processing time regularly fell short of goals for three — meb , transition , and va benefits .

for example , for cases that completed the meb phase in fiscal year 2011 , active duty and reserve component members' cases took an average of 181 and 188 days respectively to be processed , compared to goals of 100 and 140 days .

for the peb phase , processing times increased over time , but were still within the established goal of 120 days .

along with increasing average processing times , the percentage of cases meeting goals for most phases has generally declined ( see fig .

6 ) .

in particular , the meb and transition phases have lower percentages of cases meeting goals than the other phases in most years , especially for active duty cases .

as noted above , the meb phase was a key contributor to increases in overall processing times between 2008 and 2011 for both active duty servicemembers and reservists for cases that have completed the ides process regardless of outcome ( table 1 ) .

to obtain a better understanding of more recent timeliness trends within the meb phase , gao analyzed meb timeliness of all cases — all fiscal years combined — that completed the meb process by sorting them into two groups: ( 1 ) those that completed the entire ides process , and ( 2 ) those that had not yet completed ides but completed the meb phase .

as shown in figure 7 , for the group that completed ides , 30 percent of active duty servicemembers and 18 percent of reservists missed the goal by more than 90 days .

for those still in ides , representing more recent data , the picture is slightly better for active duty servicemembers with 37 percent of cases meeting the meb goal and 25 percent missing the goal by more than 90 days .

however , the percentage of reserve component members who missed the goal by more than 90 days increased from 18 to 28 percent .

for those servicemembers who were still enrolled in the meb phase as of december 2011 , the data show that 41 percent of active duty and 33 percent of reserve component servicemember cases had already missed the goal processing times ( see fig .

8 ) .

of these , 15 percent of active duty and 10 percent of reservist component servicemember cases missed the goal by more than 90 days .

within the meb phase , significant delays have occurred in completing medical examinations ( medical exam stage ) and delivering an meb decision ( the meb stage ) .

for cases completing the meb phase in fiscal year 2011 , 31 percent of active duty and 29 percent of reservist cases met the 45-day goal for the medical exam stage and 20 percent of active duty and 17 percent of reservist cases met the 35-day goal for the meb stage .

officials at some sites we visited told us that meb phase goals were difficult to meet and not realistic given current resources .

for example: some military officials noted that they did not have sufficient numbers of doctors to write the narrative summaries of exam results needed to complete the meb stage in a timely manner .

one facility noted that while they have 7 doctors , they would need 11 additional doctors and 10 technician assistants to process cases through the initial medical exam and other additional disability specific examinations in a timely manner .

further , officials at another army base we visited noted that there was a shortage of doctors and dod board liaisons and that they had difficulty recruiting such staff due to the remote location of the base .

at all the facilities we visited , officials told us dod board liaisons and va case managers had large case loads .

while dod has established a goal of 1 board liaison for every 20 servicemembers , the ratios varied widely by military treatment facility with a range from 1:1 as the lowest to the highest of 1:75 according to recent data .

because of high case loads and a reported increase in the complexity of cases , staff at one facility reported a liaison to servicemember ratio of 1:80 and noted that liaisons must often prioritize cases to deal with the most pressing issues first .

as a result , cases that might otherwise be quick to process take longer simply because they are waiting to be processed .

liaisons are often working overtime and weekends to keep up with cases .

monthly data produced by dod subsequent to the data we analyzed show significantly improved timeliness for the medical exam stage ( 66 percent of active duty cases met the goal in june 2012 ) and some improvement for the meb stage ( 40 percent of active duty cases met the goal in the month of june 2012 ) .

however , it is too early to tell whether these improvements will continue going forward .

 ( see app .

iii for dod reported monthly data , october 2011 – june 2012. ) .

since fiscal year 2008 , the majority of cases have completed the peb phase under the goal of 120 days , however , peb timeliness has still worsened over time .

in 2011 , 78 percent of active duty and 62 percent of reservist cases that completed the entire ides process met the peb goal .

the average processing time was 93 days for active duty servicemembers and 116 for reservists ( see table 2 ) .

despite meeting the overall peb goal in fiscal year 2011 , established goals were not met for any of the interim peb stages , including the informal peb and va rating stages which are the two stages all servicemembers must complete .

for all cases that completed the peb phase in fiscal year 2011 , only 38 percent of active duty and 38 percent of reservists' cases received an informal peb decision within the 15 days allotted .

further , only 32 percent of active duty and 27 percent of reservist cases received a preliminary va rating within the 15-day goal .

 ( see table 3 ) .

regarding delays with the va rating , va officials told us that staffing has been a challenge at their ides rating sites and that this has slowed case processing .

monthly data produced by dod subsequent to the data we analyzed show similar trends for the informal peb and va preliminary rating stages .

as of june 2012 ( most recent data available ) , active duty cases showed slight improvements in timeliness for the informal peb stage ( 41 percent of cases meeting the goal and processing times averaging 24 days ) .

the va rating stage , on the other hand , showed slight declines in timeliness ( 31 percent of cases meeting the established goal and processing times of 35 days ) relative to fy 2011 averages for active duty servicemembers .

however , as noted before , it is too early to tell the extent to which such trends will continue .

 ( see app .

iii for dod reported monthly data , october – june 2012. ) .

also during this phase , ides planners allocated the majority of overall peb processing time ( 75 out of the 120 days ) for appeals — including a formal peb hearing and a reconsideration of the va ratings .

according to officials , while the three appeal stages do not happen for every case , appeals can significantly increase processing times for any one case .

however , only 20 percent of cases completed in fiscal year 2011 actually had any appeals ; calling into question dod and va's assumption on the prevalence and average effect of appeals , and potentially masking processing delays in other mandatory parts of the peb phase .

the transition phase has consistently taken longer than its 45-day goal — almost twice as long on average .

while processing times improved slightly for cases that completed this phase in fiscal year 2011 ( from 79 days in fiscal year 2010 to 76 days in fiscal year 2011 for active duty cases ) , timeliness has remained consistently problematic since fiscal year 2008 ( see table 4 ) .

dod lacks comprehensive data on how servicemembers spend their time in the transition phase , which includes many different activities related to separation from the military .

these activities vary widely depending on the case .

for example , during this phase servicemembers receive mandatory training such as job training through the transition assistance program and may also receive counseling such as pre - discharge vocational rehabilitation and employment counseling .

in addition , servicemembers may be placed on temporary duty while house hunting , or to allow for a servicemember's children to complete the school year before moving .

servicemembers may also take earned leave time — to which they are entitled — before separating from the service .

for example , an army official said that army policy allows servicemembers to take up to 90 days of earned leave prior to separating , and that average leave time was about 80 days .

because many of these activities can occur simultaneously or in small intermittent segments of time , dod officials said it is difficult to track which activities servicemembers participate in or determine how much time each activity takes .

dod is exploring options for better tracking how time is spent in this phase .

because a potentially substantial amount of the time in this phase may be for the personal benefit of servicemembers , dod recently began reporting time in ides with and without the transition phase included .

processing time improved somewhat for the benefits phase ( 48 days in fiscal year 2010 to 38 days in fiscal year 2011 ) , but continued to exceed the 30-day goal for active duty servicemembers ( see table 5 ) .

several factors may contribute to delays in this final phase .

va officials told us that cases cannot be closed without the proper discharge forms and that sometimes they do not receive this information in a timely manner from the military services .

additionally , if data are missing from the ides tracking system ( eg , the servicemember already separated , but this was not recorded in the database ) , processing time will continue to accrue for cases that remain open in the system .

officials could not provide data on the extent to which these factors had an impact on processing times for pending cases , but said that once errors are detected and addressed , reported processing times are also corrected .

in addition to timeliness , dod and va evaluate ides performance using the results of servicemember satisfaction surveys .

in principle , all members have an opportunity to complete satisfaction surveys at the end of the meb , peb , and transition phases ; however , under current survey procedures servicemembers become ineligible to complete a survey for either the peb or transition phases if they did not complete a survey in an earlier phase .

additionally , servicemembers who start but do not complete a phase are not surveyed .

as such , dod may be missing opportunities to obtain input from servicemembers who did not complete a prior survey or exited ides in the middle of a phase .

further , response rates may be affected because dod does not survey servicemembers once they separate from the service and become veterans .

while it is not necessary for dod to survey all servicemembers at the end of every phase , the percentage and characteristics of servicemembers covered by the survey ( i.e. , who completed a phase and were ultimately interviewed ) may be insufficient to establish that the survey results are representative of servicemember satisfaction , especially for later phases .

 ( see table 6 for response and coverage rates. ) .

dod officials recently told us that they will consider alternative survey eligibility requirements , including working with the office of management and budget for permission to interview veterans .

 ( for additional information regarding the timing of the survey , see app .

ii ) .

in addition , alternate survey measures show lower satisfaction rates than those reported by dod .

using dod's measure , we found an overall satisfaction rate of about 67 percent since the inception of ides .

dod defines a servicemember as satisfied if the average of his or her responses across several surveys is above 3 on a 5-point scale , with 3 denoting neither satisfied nor dissatisfied .

however , using our alternate measure that defines servicemembers as satisfied only when all of their responses are 4 or above , we calculated the satisfaction rate to be about 24 percent ( see fig .

9 ) .

our calculation is a more conservative measure of satisfaction , because it rules out the possibility that a servicemember is deemed “satisfied” even when he or she is dissatisfied on one or more questions in the scale .

while not incorrect , dod's scale can mask pockets of servicemember dissatisfaction .

for example , an individual may indicate that he or she is very dissatisfied with one phase of the program , but satisfied with other phases , and the overall satisfaction score can be the same as one for a servicemember who is generally satisfied across all phases of the process .

measuring satisfaction , or even dissatisfaction , in different ways may provide a more complete picture of satisfaction and how it varies in different circumstances , and thus may reveal areas where dod could focus on improving management and performance .

finally , using either dod's or our calculated measure , we found that overall satisfaction did not vary much according to differences in the experiences of servicemembers .

for example , our model estimated that satisfaction varied by no more than approximately five percentage points across branch , component , disenrollment outcome , sex , meb exam provider , enlisted and officer personnel classes , and the number of claimed and referred conditions .

while lack of variation could be a positive outcome signaling consistent treatment , it could equally mean that the survey does not measure opinions in enough detail to discriminate among servicemembers' experiences .

either way , such results provide little insight into identifying areas for improvement or effective practices .

further , while we found some association between servicemembers satisfaction and the timeliness of their case processing , we also found many servicemembers were highly dissatisfied even when their cases were completed on time , and many were highly satisfied even when their cases were not .

for example , 68 percent of those who said that peb timeliness was “very poor” completed the phase on time , and 55 percent of those who said that meb timeliness was “very good” did not complete on time .

the lack of variation and / or correlation between satisfaction and experiences of servicemembers — coupled with low coverage rates — raise questions about the value of the survey results as a performance measure and program evaluation tool .

 ( see app .

ii for more information on servicemember satisfaction results. ) .

dod is reconsidering its options for measuring customer satisfaction , but has yet to select a particular approach .

as noted above , possible changes might include widening the criteria for who is eligible for the survey , modifying survey questions , changing when and how the survey is delivered , and changing how satisfaction is calculated .

officials already concluded that the survey , in its current form , is not a useful management tool for determining what changes are needed in ides and said that it is expensive to administer — costing approximately $4.3 million in total since the start of the ides pilot .

navy officials told us they believed that the satisfaction surveys could be made more useful if they knew whether servicemember's satisfaction was actually influenced by the servicemember's desired or actual outcome of the ides process .

further , army officials already determined that the dod survey is of limited value , and are proceeding with plans to field their own survey in the hopes of obtaining more detailed information at the facility level .

because of fiscal constraints , dod suspended the survey in december 2011 , but officials told us that they hope to resume collecting data in fiscal year 2013 .

we identified two potential alternatives to assessing servicemember experiences .

surveying a sample of servicemembers: while a census gives each servicemember a chance to describe his or her experiences with ides , dod could collect the same data at a lower cost by surveying a probability sample of servicemembers .

if appropriately designed and executed , a sample would accurately represent all groups of servicemembers and produce the necessary data for important subgroups , such as facilities or branches .

since the cost of administering a survey is strongly related to the number of people surveyed , probability sampling could also allow dod to assess servicemember experiences while substantially reducing data collection costs .

exit interviews: in - depth interviews with servicemembers , completed at disenrollment from ides , could also yield more detailed and actionable information about the program .

although the current survey includes open - ended questions , it is primarily designed to collect standardized , quantitative measures of satisfaction with broad aspects of ides , such as fairness and the performance of dod board liaisons and va case managers .

as a result , the survey provides a limited amount of detailed feedback on particular facilities , staff members , and stages of the process that managers might use to improve the servicemember experience , decrease processing times , or reduce cost .

in contrast , semi - structured exit interviews would allow servicemembers to provide this type of qualitative , detailed feedback .

interviewing servicemembers at the end of the process would also allow servicemembers to assess their overall experiences with ides rather than at an earlier stage , without having completed the entire process .

exit interviews could also reach servicemembers who exit ides without completing the process such as those who are returned to duty .

exit interviews , however , have the potential to be labor intensive and expensive .

dod and va have undertaken a number of actions to address ides challenges — many of which we identified in our past work .

some actions — such as increased oversight and staffing — represent important steps in the right direction , but progress is uneven in some areas .

increased monitoring and oversight: we identified the need for agency leadership to provide continuous oversight of ides in 2008 and the need for system - wide monitoring mechanisms in 2010 .

since then , agency leadership has established mechanisms to improve communication , monitoring , and accountability .

the secretaries of dod and va have met several times since february 2011 to discuss progress in improving ides timeliness and have tasked their agencies to find ways to streamline the process so that the timeliness goals can be shortened .

the secretaries also tasked their agencies to expand the use of expedited disability evaluations for severely combat - wounded servicemembers ; and develop a system to electronically transfer case files between dod and va locations .

senior army and navy officials regularly hold conferences to assess performance and address performance issues , including at specific facilities .

with respect to the army , meetings are led by the army's vice - chief of staff and va's chief of staff , and include reviews of performance where regional and local facility commanders provide feedback on best practices and challenges .

for example , recent army - va conferences focused on delays in completion of preliminary ratings for army pebs by va's seattle rating site , efforts by the army to increase meb staffing , development of army - wide ides standardization guidance , and army - va electronic records interchange .

periodic meetings are also held between senior navy medical and va officials to discuss performance issues at navy military treatment facilities .

va holds its own biweekly conferences with local staff responsible for va's portion of the process .

these conferences are supplemented by a bi - weekly ides “dashboard” that tracks performance data for portions of the ides for which va is responsible .

according to va officials , in addition to identifying best practices , these conferences focus on sites with performance problems and identify potential corrective actions .

for example , officials said a recent conference addressed delays at fort benning , georgia , and discussed how they could be reduced .

va officials noted that examiner staff were reassigned to this site and worked on weekends to address the problems at this site .

in addition , senior va health care officials hold periodic conferences with officials responsible for exams at ides sites , to monitor performance .

ensuring sufficient medical exam resources: in our december 2010 report , we noted that va struggled to provide enough medical examiners ( both va employees and contractors ) to meet demand and deliver exam summaries within its 45-day goal .

for example , significant deficiencies in examiner staffing ( particularly for mental health exams ) at fort carson contributed to exams for active duty members taking an average of 140 days .

to improve exam timeliness , va hired more examiners and is devoting more resources at those sites where va clinicians perform ides exams .

in addition , in july 2011 , va awarded a revised compensation and pension ( including ides ) contract that provides more flexibility for va to have contractors perform ides exams at sites needing additional resources .

as a result , va can use contractors to conduct exams for regional offices beyond the 10 offices for which the contractor normally provides services .

also , va contracted with 5 companies to provide short - term exam assistance at ides sites needing it .

further , va procedures allow reserve component servicemembers in remote locations to receive exams close to their homes .

va exam timeliness has improved and the agency met its 45-day goal for active component members in every month from august 2011 through june 2012 .

va officials attributed improved exam timeliness , in part , to additional exam resources provided to ides sites .

 ( see app .

iii for additional information on fiscal year 2012 timeliness. ) .

ensuring sufficient exam summaries: in our december 2010 report , we noted that some cases were delayed because va medical exam summaries were not complete and clear enough for use in making rating and fitness decisions and needed to be sent back to examiners for additional work .

va officials told us that they have been reinforcing the importance of training and communication between rating staff and medical examiners as ways to improve exam summary sufficiency .

for example , va identified types of information which , if missing from an exam summary , would cause it to be insufficient , and has been training examiners to include such information .

additionally , va noted that vta now has the ability to track cases with insufficient exams by allowing staff to annotate information on exam summaries .

however , staff are not required to provide this information and rules and procedures for its use have not been established .

ensuring sufficient meb staffing: in our december 2010 report , we noted that some sites had insufficient meb physicians , leading to delays in completing the meb phase .

at that time , most of the 27 pilot sites were not meeting the 35-day goal , with average times for active component cases as high as 109 days .

meanwhile , dod did not have sufficient board liaison staff to handle ides caseloads .

the army is in the midst of a major hiring initiative intended to more than double staffing for its mebs over its october 2011 level , which will include additional board liaison and meb physician positions .

the army reported having 610 full - time equivalent meb staff positions in october 2011 , and planned to hire up to 1,410 ; this would include 172 meb physician and 513 board liaison positions .

the army also planned to hire an average of one contact representative per board liaison ; these staff members assist the board liaisons with clerical functions , freeing more of the liaisons' time for counseling servicemembers .

as of june 2012 , the army had filled 1,219 ( 86 percent ) of the planned 1,410 positions .

ensuring sufficient va rating staff: in our december 2010 report , we noted that va had insufficient staff at one of its rating sites to handle the demand for preliminary ratings , rating reconsiderations , and final va benefit decisions .

va officials said that the agency has more than tripled the staffing at its ides rating sites – from 78 to 262 positions .

further , va has moved staff resources to ides rating sites from other va regional offices to provide short - term help in working down rating backlogs .

recent monthly data show an increase in the number of preliminary va ratings completed , and a slight improvement in processing times .

however , as noted before , it is too early to tell the extent to which such trends will continue .

 ( see app .

iii for additional information on fiscal year 2012 timeliness. ) .

improving completeness of reserve component members' records: service officials noted that incomplete medical records and administrative documentation , especially for reserve component members , often contribute to delays in the early ides stages , including the va exam stage .

for example , a reserve unit may not have complete medical records for a member who received care from a private provider .

when the servicemember enters the ides , a board liaison is responsible for obtaining the private provider records before handing off the case to va for exams .

to address issues with reserve component servicemembers' records , the army established an interim office in pinellas park , florida in january 2011 .

for reserve component servicemembers who may require ides referral , this office is tasked with obtaining records from the member's reserve unit ; reviewing them to identify missing information ; and , if necessary , requiring the reserve unit to obtain additional records to complete the case file .

staff at this office also determine whether the member needs ides referral .

army officials indicated that this office is expected to help reduce the backlog of army reserve component cases in the ides .

however , army officials noted that they are providing training to reserve units to improve their ability to maintain complete records on their servicemembers and eventually , the army may discontinue this office if no longer needed .

improving meb documentation and decisions: in response to delays in completing the meb stage , the navy and army have initiatives underway to help ensure the timely completion of narrative summaries and meb decisions .

for example , the navy piloted electronic narrative summary preparation at naval hospital camp lejeune , north carolina .

in may 2012 , after determining that the piloted process led to improved meb completion timeliness , the navy deployed electronic narrative summary preparation navy - wide .

in march 2011 , the army also deployed an abbreviated meb narrative summary format , intended to provide better information for meb and peb decision making while helping reduce delays in the completion of these summaries by meb clinicians .

incorporating feedback from its mebs and pebs , the army expects the revised ides template to reduce redundant information , make summaries simpler and easier to use , and standardize summary preparation across their sites .

resolving diagnostic differences: in our december 2010 report , we identified differences in diagnoses between dod physicians and va examiners , especially regarding mental health conditions , as a potential source of delay in ides .

we also noted inconsistencies among services in providing guidance and a lack of a tracking mechanism for determining the extent of diagnostic differences .

in response to our recommendation , dod commissioned a study on the subject .

the resulting report confirmed the lack of data on the extent and nature of such differences , and noted that the army has established guidance more comprehensive than the guidance dod was developing .

it also recommended that dod or the other services develop similar guidance .

a dod official told us that consistent guidance across the services , similar to the army's , was included in dod's december 2011 ides manual .

also , in response to our recommendation , va took steps to modify the vta database used to track ides to collect information on diagnostic differences .

the vta upgrade was completed in june 2012 after several delays .

the report also recommended that dod and va establish a committee to improve the accuracy of posttraumatic stress disorder ratings .

dod noted that training on diagnostic differences has been incorporated into its continuing medical education curriculum for military clinicians , but dod considers the issue of posttraumatic stress disorder ratings largely resolved .

meanwhile , the army's new ides narrative summary template includes a section where the meb clinician identifies any inconsistencies in the case record , including any diagnostic differences with va examiners .

dod and va are working to remedy shortcomings in information systems that support the ides process .

these shortcomings include vta's lack of capability for local sites to track cases , and the potential for erroneous and missing data in vta , affecting timeliness measurement .

however , some efforts related to information systems are causing work inefficiencies , are still in progress , or otherwise are limited .

improving local ides reporting capability: dod and va are implementing solutions to improve the ability of local military treatment facilities to track their ides cases , but multiple initiatives may result in redundant work efforts .

officials told us that the vta — which is the primary means of tracking the completion of ides cases — has limited reporting capabilities and staff at local facilities are unable to use it for monitoring the cases for which they are responsible .

dod and va developed vta improvements that will allow dod board liaisons and va case managers — and their supervisors — to track the status of their cases .

va included these operational reporting improvements in its june 2012 vta upgrade .

in the meantime , staff at many ides sites have been using their own local systems to track cases and alleviate limitations in vta .

further , the military services have been moving ahead with their own solutions .

for instance , the army has deployed its own information system for mebs and pebs army - wide .

in addition , dod has also been piloting its own tracking system at 9 ides sites .

as a result , staff at ides sites we visited reported having to enter the same data into multiple systems .

for example , board liaisons at army mebs fort meade and joint base lewis - mcchord reported entering data into vta and the army's new system , while board liaisons at andrews air force base reported entering data into vta and dod's pilot data system .

improving ides data quality: dod is taking steps to improve the quality of data in vta .

our analysis of vta data identified erroneous or missing dates in at least 4 percent of the cases reviewed .

officials told us that vta lacks adequate controls to prevent erroneous data entry , and that incorrect dates may be entered , or dates may not be entered at all , which can result in inaccurate timeliness data .

for example , army officials noted that some cases shown in vta as very old were actually closed , but were missing key dates .

in september 2011 , dod began a focused effort with the services to correct erroneous and missing case data in vta .

officials noted that the air force and navy completed substantial efforts to correct the issues identified at that time , but army efforts continue .

dod and army officials noted that additional staff resources are being devoted to cleaning up army vta data .

while improved local tracking and reporting capabilities will help facilities identify and correct erroneous data , keeping vta data accurate will be an ongoing challenge due to a lack of data entry controls .

while dod is currently assisting the services , dod officials said they expect that eventually the services will be responsible for identifying and fixing data errors .

dod and va are also pursuing options to allow them to save time by replacing the shipping of paper case files among facilities with electronic file transfers .

requirements for an electronic case file transfer solution have been completed and dod and va officials expect to begin piloting it in august 2012 .

as a short - term solution , the army and va began using an army file transfer web site to move ides records between the army's pebs and the seattle va rating site in march 2012 .

according to va officials , this could save several days currently spent shipping paper files between these offices .

va officials noted that the same web site is being used for transfers between the navy peb and providence rating site .

meanwhile , the secretaries of defense and veterans affairs tasked their staffs to develop standards for electronic ides case files by july 2012 .

based on concerns of the secretaries of dod and va about ides delays , the departments have undertaken additional initiatives to achieve time savings for servicemembers .

for example , in response to the secretaries' february 2011 directive to streamline the process , dod and va officials proposed a remodeled ides process .

in december 2011 , senior agency leadership decided to postpone the pilot of a remodeled ides process , and instead tasked the agencies to explore other ways to streamline the process .

as a result , dod , with va's assistance , began a business process review to better understand how ides is operating and identify best practices for possible implementation .

this review incorporates several efforts , including visits to 8 ides sites to examine how the process was operating and identify best practices.includes the following: process simulation model: using data from site visits and vta , dod is developing a simulation model of the ides process .

according to a dod official , this process model will allow the agencies to assess the impact of potential situations or changes on ides processing times , such as surges in workloads or changes in staffing .

fusion diagram: dod is developing this diagram to identify the various sources of ides data — including va claim forms and narrative summaries — and different information technology systems that play a role in supporting the ides process .

officials said this diagram would allow them to better understand and identify overlaps and gaps in data systems .

ultimately , according to dod officials , this business process review could lead to short - and long - term recommendations to improve ides performance , potentially including changes to the different steps in the ides process , performance goals , and staffing levels ; and possibly the procurement of a new information system to support process improvements .

however , a dod official noted that these efforts are in their early stages , and thus there is no timetable yet for completing the review or providing recommendations to senior dod and va leadership .

dod officials indicated that they expect this to be a continuous ides improvement process , including further site visits .

finally , dod is also developing guidance to expand implementation of an expedited disability evaluation process for servicemembers with catastrophic , combat - related conditions by allowing it to be operated at more military treatment facilities .

dod created this expedited process in january 2009 for servicemembers who suffer catastrophic , combat - related disabilities .

under an agreement with va , the services can rate such members as 100 percent disabled without the need to use va's rating schedule .

however , according to dod officials , the services report that no eligible servicemembers are using this process .

instead , servicemembers are having their cases expedited through the ides informally .

the revisions to dod's policy would allow the expedited process to be used at additional military treatment facilities beyond the original 4 facilities.of a rewrite of dod's key guidance documents , and was undergoing review at the time of our review .

by merging two duplicative disability evaluation systems , ides shows promise for expediting the delivery of dod and va benefits to injured servicemembers and is considered by many to be an improvement over the legacy process it replaced .

however , nearly 5 years after its inception as a pilot , delays continue to affect the system and the contribution of various , complex factors to timeliness is not fully understood .

recent efforts by dod and va to better understand how different ides processes contribute to timeliness are promising and may provide the departments with an opportunity to reassess resource levels and timeframes , and to make adjustments if needed .

this information will also help to ensure that dod and va are making the best use of limited resources to improve ides performance .

however , it is not clear when these efforts will be complete or if any recommended actions will be implemented .

dod has also begun rethinking its approach to determining servicemember satisfaction with ides .

our analysis of customer satisfaction data suggests that there are opportunities for improving the representativeness of the survey information collected and reconsidering the cost - effectiveness of the current lengthy surveys .

finally , providing local facilities the capability to track and generate reports on the status of their cases is long overdue and may empower local staff to better address challenges .

however , tracking reports are only as good as the data that are entered into vta , and dod and va can ensure the quality of these data through continuous monitoring .

meanwhile , the dod - led business process review should identify and ultimately eliminate any redundant or inefficient information systems for tracking cases as well as for other ides purposes .

1 ) to ensure that servicemember cases are processed and are awarded benefits in a timely manner , we recommend that the secretaries of defense and veterans affairs work together to develop timeframes for completing the ides business process review and implementing any resulting recommendations .

2 ) to improve dod's ability to measure servicemembers' satisfaction with the ides process , we recommend that the secretary of defense develop alternative approaches for collecting more meaningful and representative information in a cost effective manner .

3 ) to ensure that ides management decisions continue to be based upon reliable and accurate data , we recommend that the secretaries of defense and veterans affairs work together to develop a strategy to continuously monitor and remedy issues with vta timeliness information .

this could include issuing guidance to facilities or developing best practices on preventing and correcting data entry errors ; and developing reporting capabilities in vta to alert facilities to potential issues with their data .

we provided a draft of this report to dod and va for review and comment .

in their written comments , which are reproduced in appendixes iv and v , dod and va both concurred with our recommendations .

va also provided technical comments , which we incorporated as appropriate .

while concurring with our recommendations , dod also commented that our discussion of ides surveys contained inaccuracies , but did not specify the inaccurate information in our draft report .

in a subsequent communication , dod officials noted that our draft inaccurately described dod's decision to not survey veterans .

we corrected this information accordingly .

further , while va concurred with our recommendation that it work with dod to develop timeframes for completing the ides business process review and implementing any resulting recommendations , va stated that dod is leading the business process review , and therefore should develop the timeframes for completing the review .

we have revised this report to clarify that dod is leading the business process review , but we did not alter the recommendation because we believe that it is important for va to work closely with dod , including in developing review timeframes .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to the appropriate congressional committees , the secretary of defense , the secretary of veterans affairs , and other interested parties .

the report is also available at no charge on the gao web site at www.gao.gov .

if you or your staff members have any questions about this report , please contact me at ( 202 ) 512-7215 or at bertonid@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

staff members who made key contributions to this report are listed in appendix vi .

in conducting our review of the integrated disability evaluation system ( ides ) , our objectives were to examine ( 1 ) the extent to which the departments of defense ( dod ) and veterans affairs ( va ) are meeting ides performance goals , and ( 2 ) steps dod and va are taking to improve ides performance .

we conducted this performance audit from may 2011 to august 2012 , in accordance with generally accepted government auditing standards .

these standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

to determine the extent to which ides is meeting established timeliness goals , we analyzed data collected through va's veterans tracking application ( vta ) database .

while va manages vta , both dod and va staff enter data into vta , and the evaluation of ides data is primarily conducted by staff at dod's office of warrior care policy ( wcp ) .

wcp provided us with a dataset that was current as of january 1 , 2012 and contained data spanning back to the inception of ides in late 2007 .

this data export included data on a total of 39,260 cases .

of these cases , 34,185 were active duty servicemembers and 5,068 were reserve / guard servicemembers .

this vta data set contained demographic data for each individual ides case as well as a record of dates for when servicemembers reached various milestones in ides .

overall and interim ides timeliness calculations are based on computing the number of days elapsed between appropriate milestone dates .

for example , overall timeliness for servicemembers that receive benefits is calculated as the number of days between the individual being referred into the ides and the date on which his or her va benefits letter is issued .

we met with staff at wcp to ensure we used appropriate variables when calculating timeliness .

we also met with officials at va to discuss the calculations used to determine the timeliness of cases .

we took a number of steps to assess the reliability of vta data and ultimately found the data to be sufficiently reliable for the purposes of our audit .

past gao worknumber of steps to follow up on past assessments of vta .

relied on vta data , and therefore we took a we interviewed dod and va and determined that internal controls on vta data had not changed substantially since our past review .

we conducted electronic testing of the vta data and generally found low rates of missing data or erroneous dates pertinent to our analysis — approximately 4 percent of cases .

for ides cases in which we found missing dates or dates out of sequence , we excluded those cases from all of our analyses .

while there were some instances in which the erroneous dates may be justified , we excluded the entire case from our analysis if any such dates appeared at any point in the vta database .

such data included cases in which ( 1 ) there was no meb referral date signifying the start of ides process , and ( 2 ) the ending date preceded the beginning date of the ides phase ( resulting in timeliness calculations appearing as a negative amount of time ) .

we also conducted a limited trace - to - file process to determine whether date fields in vta were an accurate reflection of the information in ides case files .

specifically , we compared vta dates in 15 ides cases completed in fiscal year 2011 against the dates in the corresponding paper files .

in comparing dates , we allowed for a discrepancy of 5 days in dates to allow for the possibility that dates may have been entered into the database after an event took place .

ninety - three percent of the dates we traced back to the original file documents were found to be accurate , that is falling within our 5 day allowance .

for the cases meeting our criteria for reliability , we analyzed timeliness data for those cases that had completed the entire ides process or had completed each of the four ides phases .

we specifically: identified the total number of cases enrolled each fiscal year from fy 2008 through 2011 , by active as well as national guard and reserve servicemembers , and by military branch of service .

identified the number of cases that completed the entire ides process for each fiscal year from fiscal year 2008 through fiscal year 2011 .

we analyzed completed cases in two different ways: ( 1 ) those who completed the process and received va benefits and ( 2 ) those who completed the ides with any outcome ( such as permanent retirement , temporary disability retirement list , return to duty , etc. ) .

in order to be able to make comparisons across cases with different outcomes for a given point in time , we defined fiscal year by using the vta variable “final disposition date” .

we did this because most completed cases — regardless of outcome — have a final disposition date in vta .

in contrast to our approach , va use the “va benefit date” variable to determine fiscal year of completion for cases resulting in benefits .

as such , their number of cases and timeliness calculations by fiscal year differed from ours , although overall trends are similar .

identified the number of cases that completed each phase of ides and the interim stages within each phase , again by fiscal year ( fiscal years 2008 through 2011 ) .

computed timeliness statistics for the completion of the ides process , phases , and stages against the performance goals set by dod and va , such as average days and percent meeting goals .

computed number and percent of cases where a servicemember appealed a decision made during the ides process , by fiscal year .

for the purposes of this report , gao opted to not include reserve component time spent in the va benefit phase in our calculations for overall time because the 30 days allotted for this phase is not included in the 305-day overall goal for the reserve component .

gao also performed analyses similar to those above , except that we grouped cases according to the year in which they were enrolled in ides .

 ( see app .

ii for more detail on these analyses. ) .

additionally , we analyzed timeliness for cases that had not yet completed the meb stage as of the date we received the vta data .

to determine the extent to which ides is meeting its customer satisfaction goals , we analyzed data collected from ides customer satisfaction surveys conducted at the end of three phases: meb , peb and transition .

these surveys are administered by telephone by contractors hired by dod .

the dataset we received contained survey responses for individual servicemembers from the beginning of the ides pilot to december 2011 , at which time administration of the survey was suspended .

additionally , we matched individual survey responses with information from vta to gain additional understanding into how customer satisfaction varied according to different factors such as timeliness and case outcome .

we matched survey and vta data using the unique case identifier attached to each ides case , maintaining the anonymity of the servicemembers .

see appendix ii for the results of additional analyses we conducted using survey data and survey data matched with vta data .

in the course of our review we concluded that the survey data were sufficiently reliable for our purposes .

we interviewed relevant officials at dod and their contractors about eligibility requirements and the administration of the surveys .

further , we met with dod and their contractors on multiple occasions to discuss the calculations used to determine response rates for the survey and servicemembers' level of satisfaction .

see appendix ii for more details on gao's review of response rates .

to identify challenges in implementing ides as well as steps taken to improve performance , we visited six military treatment facilities .

during the site visits , we interviewed officials involved in implementing ides from both dod and va , including military facility commanders and administrators , dod board liaisons , military physicians involved in meb determinations , dod legal staff , va case workers , va or contract examiners , and administrators at va medical clinics and regional offices .

additionally , we interviewed servicemembers who were currently enrolled in the ides process .

we selected the six facilities to obtain perspectives from sites in different military services , geographic areas , and their ability to meet timeliness goals for different phases of the process ( see table 7 ) .

in addition , we visited the air force's formal physical evaluation board at randolph air force base , texas .

during this visit we observed a hearing and met with board members to obtain a better understanding of the process .

this appendix provides additional information on the timeliness of the ides process and servicemember satisfaction with it .

first , we use timeliness data to examine whether changes over time in processing times and the percentage of cases meeting timeliness goals look any different when cases are grouped according to the fiscal year in which the cases were first enrolled rather than the fiscal year in which the cases were completed .

second , we use survey data to examine different measurements of servicemember satisfaction with ides , how satisfaction varied according to various servicemember characteristics , response and coverage rates for the servicemembers surveyed , and how the survey respondents differed from nonrespondents .

with respect to timeliness , we find generally similar trends for cases grouped by fiscal year of enrollment versus fiscal year of completion , with some key differences .

organizing cases by completion date results in shorter average processing times in 2008 , since only those cases that are processed quickly could be completed in the first year of ides .

as such , organizing cases by enrollment date provides a better estimate of the processing times for the early ides cases .

however , this approach results in shorter processing times in 2011 , the most recent full year of the program , since only cases that finish quickly can be analyzed .

with respect to satisfaction , we find that the particular index used to summarize servicemembers' responses can affect the proportion reported as being “satisfied” or “dissatisfied” with ides overall .

dod's index suggests that 67 percent of servicemembers have been satisfied since the ides program began , but a reasonable alternative measure we developed suggests that only 24 percent of servicemembers have been satisfied .

using this measure , satisfaction varies only slightly across many important groups of servicemembers , such as by disenrollment outcome , suggesting that available program data cannot precisely explain satisfaction outcomes .

also , servicemembers surveyed may not represent the servicemembers who completed the different phases of ides well enough to generalize to them , given the low response rates to the meb survey and fact that being selected for latter ( peb and transition ) surveys were conditional on completing the meb survey .

average ides processing times for completed cases resulting in benefits generally worsened since 2008 , especially for active duty cases , regardless of whether cases are grouped by the fiscal year in which they were completed ( fig .

10 ) or by the fiscal year in which they were enrolled ( fig .

11 ) .

the notable exception is when fiscal year 2011 is the year of enrollment .

however , caution must be used when examining cases enrolled in 2011 because over 15,600 service members of the 18,651 ( or at least 84 percent ) who entered ides in fiscal year 2011 did not have an outcome in 2011 and were enrolled in ides as of january 1 , 2012 , potentially changing the distribution of processing times as they proceed through ides .

we also examine average ides processing times according to year of completion ( see fig .

12 ) and year of enrollment for cases ( see fig .

13 ) for all completed cases regardless of outcome .

as with cases that resulted in benefits , for cases resulting in any outcome we find that average processing times increased since 2008 — again with the exception of fiscal year 2011 for reasons discussed earlier — although average processing times are somewhat shorter than when only servicemembers receiving benefits are included ( fig .

11 ) .

figures 14 and 15 show that regardless of whether cases are organized by year of completion or enrollment , the percent of completed cases resulting in benefits that were not timely increased between fiscal year 2008 and 2010 for both active duty servicemembers and members in the reserves or national guard .

as with the average processing times , caution must be used when examining cases enrolled in fiscal year 2011 ( fig .

15 ) , since only those cases that are processed quickly are observed in the last year .

similarly , caution also must be used when examining cases in 2008 ( fig .

14 ) , since the only cases that are included in the first year are those that completed ides quickly .

figures 16 and 17 show how average processing times for each of the four phases of ides have changed over the four fiscal years when cases are grouped by the fiscal year in which they completed a given phase and when cases are grouped by the fiscal year in which they were enrolled or started a given phase.according to the fiscal year in which the different phases were completed , processing times increased for all phases except the transition phase .

figure 17 shows a roughly similar pattern of increases in processing times in all but the transition phase , though processing times in 2011 are skewed for the reason mentioned above .

figures 18 and 19 show how the percentages meeting the timeliness goals for each of the four phases of ides have changed over the four fiscal years when cases are grouped by the fiscal year in which they completed a given phase and when cases figure 16 shows that when cases are grouped are grouped by the fiscal year in which they were enrolled or started a given phase .

figure 18 shows that the percent of cases meeting timeliness goals decreased over the four years for the meb and peb phases , although a high percent of cases met peb goals .

however , the transition and benefits phases fluctuated up and down and both were favorable across some years .

figure 19 also shows decreases in percentages of cases meeting timeliness goals at the meb and peb phases when cases are grouped by fiscal year of starting a given phase .

the fluctuations in the timeliness of the transition and benefits phases were more prevalent when cases were grouped in this manner .

low response and coverage rates for servicemember satisfaction surveys administered after each phase of ides raise concerns about how well the satisfaction survey results represented the larger population of servicemembers who completed one or more phases .

dod surveys servicemembers after they complete the meb , peb , and transition phases of ides .

the department attempts to survey all servicemembers who complete each phase , but only if they completed the prior surveys .

for example , the meb survey must be completed before a servicemember is eligible to complete the peb survey .

using the data available to us , and as table 8 below shows , we found that 9,604 of the 25,212 servicemembers who completed the meb phase were surveyed , for a 38 percent response and coverage rate.18,296 servicemembers who completed the peb phase , only 8,968 of them completed the prior meb survey and were eligible for the peb survey and of these only 4,795 were surveyed .

using dod's eligibility criteria , the response rate for the peb survey was roughly 54 percent ( 4,795 of 8,968 ) .

however , the coverage rate for all servicemembers who completed the peb phase ( regardless of whether they completed the prior survey ) was only 26 percent ( 4,795 of 18,296 ) .

similarly , the response rate for the transition survey was 72 percent while the coverage rate was only 23 percent ( see table 8 ) .

as table 9 below shows , there were some sizable differences between respondents and nonrespondents , especially for the peb and transition surveys .

for example , respondents to the transition survey spent more time than nonrespondents in the transition phase , were less likely to be separated with benefits , and were more likely to be placed on the permanent disability retired list .

these differences , combined with low response and coverage rates , raise the possibility of biased responses .

the particular measure used to assess servicemember satisfaction can affect the proportion reported as “satisfied” with the ides program .

depending on the measure used , satisfaction is about 2.8 times lower than what dod has reported , and many servicemembers classified as “satisfied” express moderate dissatisfaction with some aspects of the process .

dod has reported average servicemember satisfaction with ides overall and with three phases of the process , i.e. , meb , peb , and transition phases .

in so doing , dod has developed indices of satisfaction on several broad dimensions , such as satisfaction with the overall experience and fairness , which combine responses to selected survey questions .

although the number of questions used in each index vary depending on the number of phases completed , each index classifies servicemembers as “satisfied” or “dissatisfied” using the average of their responses across all questions in the index .

each question's scale ranges from 1 to 5 , with 1 denoting “very dissatisfied” ( or a similar negative response ) , 5 denoting “very satisfied,” and 3 denoting “neither satisfied nor dissatisfied.” dod reports that a servicemember is “satisfied” if his or her average response across all items in the scale exceeds 3 .

table 10 summarizes the responses to each question that dod uses in its overall satisfaction index at each phase ( as of august 2011 ) .

dod's indices are one reasonable method of summarizing servicemember opinions .

in quarterly performance reports , dod notes that it has used factor analysis , a form of latent variable statistical models to assess the reliability of its scales .

while we did not review dod's models , we independently found that dod's overall index of satisfaction with ides was highly reliable .

 ( specifically , using cronbach's alpha , the index was highly correlated with a single latent dimension at α = 0.92. ) .

this supports dod's choice to measure the single concept of “satisfaction” by averaging the ordinal servicemember responses .

nevertheless , the average survey response can obscure variation in the responses that make up the index .

for example , suppose that a servicemember said she was “very satisfied” ( response of 5 ) on two of the four questions in the index , “dissatisfied” on one ( response of 2 ) and “very dissatisfied” on the last one ( response of 1 ) .

with an average response over 3 , the dod measure would classify her as “satisfied,” despite the fact that she was “somewhat dissatisfied” or “very dissatisfied” with two of the four aspects of ides that dod considers important .

the grouping rule considers this servicemember equally happy with ides as someone who says they are “satisfied” with all four aspects in the index .

to assess the extent to which dod's index might mask dissatisfaction , we calculated the proportion of questions in the scale on which servicemembers whom dod classified as “satisfied” gave neutral or negative responses ( 1 , 2 , or 3 ) .

we found that half of these servicemembers gave neutral or negative answers to at least 25 percent of the items in the index , and a quarter gave such answers to at least 41 percent of the items .

for these servicemembers , the dod index may suggest more satisfaction than the underlying survey questions would support .

we further assessed the sensitivity of dod's index by comparing it against a different ( i.e. , gao's ) measure of satisfaction: whether a servicemember is “somewhat” or “very satisfied” ( or gives a similarly positive response ) on all items in dod's scale of overall ides satisfaction .

our measure is more conservative than dod's , because ours only includes positive responses and uses a broader cutpoint ( two response categories ) to distinguish between “satisfied” and “not satisfied” servicemembers .

 ( in contrast , dod calculates average satisfaction on an ordinal scale of 1 to 5 , and then uses a cutpoint at 3. ) .

our measure is not inherently more valid , however , and has its own weaknesses .

in particular , we classify a servicemember as “not satisfied” if she gives a neutral or negative response to just one of the four items in dod's scale .

when we analyzed overall satisfaction using both measures , we found that overall , servicemembers are 2.8 times less satisfied on our measure than on dod's ( i.e. , 23.8 versus 67 percent ) .

further , only about 20 to 30 percent of servicemembers are “satisfied” with each aspect of the ides process that dod considers important across most of the subgroups we analyzed , while dod classifies about 60 to 70 percent of such servicemembers as “satisfied” on average .

in the next section , we present further information on variation in satisfaction across servicemember groups .

although the servicemember survey provides numerous measures of satisfaction , it is also important to explain variation in satisfaction outcomes — i.e .

why some servicemembers are more satisfied than others .

explaining variation can connect dissatisfaction with poor program performance and help identify specific reforms to improve the experiences of servicemembers who typically have been less satisfied .

however , the available program data cannot precisely explain outcomes when used in this type of explanatory analysis .

using the available data , we could predict satisfaction only 1.9 percentage points better after controlling for multiple factors than what we would have achieved by chance ( 65.5 percent vs. 63.7 percent of satisfied responses predicted correctly ) .

in order to further explain variation in satisfaction , we matched the survey responses to the data that dod and va maintain on the processing of each servicemember's case , known as the vta data .

this database primarily measures the time it took servicemembers to complete each phase of the ides process .

a small number of other program and demographic variables are also available , such as service branch , component , and the number of conditions claimed and referred .

using the matched survey and vta data , we estimated the association between satisfaction and observable factors that could potentially explain variation in servicemembers' experiences .

table 11 below ( columns 2-4 ) presents these associations for both dod's and gao's overall measures of satisfaction .

the “raw data” estimates are simply the proportion of servicemembers in a particular group who were satisfied according to either measure .

in the fourth column ( “model estimates” ) , we estimate this proportion holding constant all of the other factors listed , using a statistical model .

specifically , the estimates are in - sample mean predicted probabilities of giving a satisfied response on the gao satisfaction index from a logistic model of satisfaction .

the covariates are given by indicators of whether the servicemember belonged to each group in column 4 .

the maximum likelihood estimators allowed the probability of satisfaction , given the covariates , to be dependent across observations within the 26 cross - classified groups of peb location and meb medical treatment facility .

this adjusted for the possibility that servicemembers were similarly satisfied if they were processed in the same locations , given similar values on the observed covariates .

regardless of which measure is used ( dod's or gao's ) , satisfaction varied only modestly across many important groups of servicemembers .

our model estimates that the gao measure of satisfaction varied by no more than approximately five percentage points across branch , component , disenrollment outcome , sex , meb exam provider , enlisted and officer personnel classes , and the number of claimed and referred conditions , although differences across meb treatment facilities and peb locations were larger .

this can be seen as a positive outcome , if this correlation implies that dod and va administer the program consistently across servicemembers and locations .

however , the lack of variation also could suggest that the survey items do not measure opinions in enough detail to discriminate among servicemembers' experiences .

also shown in table 11 , satisfaction had a stronger association with case processing time ( time spent in ides ) than some of the other factors we examined .

servicemembers whose case processing times were among the quickest 25 percent were about 2.3 times as likely to be satisfied ( on the gao scale ) than those whose times were among the 25 percent of cases with the longest overall timeframes ( i.e. , 41 versus 18 percent ) .

nevertheless , only 41 percent of those servicemembers whose cases were processed most quickly were satisfied ( holding constant the other factors ) .

this suggests that servicemembers' opinions about ides may be only loosely related to the amount of time they spent in ides , as discussed in the next section below .

although the average case processing time has generally increased since 2008 , when we look at satisfaction by fiscal year , servicemember satisfaction shows evidence of improvement since fiscal year 2008 .

specifically , our measure of satisfaction from the model increased by 15 percentage points since 2008 , roughly doubling from 13 to 28 .

because the model estimates control for various other factors , these results suggest that servicemember views of the ides process have improved over time , rather than the possibility that ides has simply processed different types of cases .

satisfaction does not vary by a large amount across many meb treatment facilities , but there are exceptions .

our model estimates that about 18 to 26 percent of servicemembers were satisfied at most facilities .

however , there were pockets of greater satisfaction .

specifically , servicemembers had more positive experiences at forts belvoir , bragg , campbell , drum , hood , and polk , with satisfaction estimated to have ranged from 28 to 45 percentage points .

fort meade had the lowest satisfaction at 15 percent .

these estimates hold constant time spent in ides and other factors in column 4 and , thus , partially account for the types of cases each facility processes .

dod and va measure ides timeliness directly in vta and as part of the overall servicemember satisfaction scale .

these overlapping measures let us compare servicemembers' opinions to their actual experiences in the program .

to do this , we calculated processing times at each phase of ides for servicemembers who expressed varying degrees of satisfaction with the timeliness of their case processing at that phase .

in addition , we analyzed whether servicemembers who were satisfied with the overall ides process were more or less likely to meet timeliness goals .

table 12 provides these statistics .

as shown in table 12 , satisfaction generally stayed the same or decreased as processing times increased .

the median days spent in the meb and peb phases were 35 and 38 percent lower , respectively , among those servicemembers who said that meb and peb timeliness was “very good” as compared to those who said it was “very poor.” the former group was 170 percent more likely to have met the meb timeliness goal and 30 percent more likely to have met the peb timeliness goal .

similarly , the case for a median servicemember — whom we classified as “satisfied” with the overall ides process — was completed 15 percent more quickly and was 49 percent more likely to have met the timeliness goal than the median servicemember who was “dissatisfied.” the model estimates in table 11 confirm that the gao measure of satisfaction and timeliness ( time spent in ides ) are negatively related even when holding constant several other variables .

perceived and actual timeliness had little association at the transition phase .

across all levels of satisfaction with timeliness , the median processing time varied by no more than 4 days , and the proportion meeting the timeliness goal varied by no more than 4 percentage points .

the use of personal leave is one plausible explanation for the unresponsiveness of servicemember satisfaction to actual processing times in the transition phase .

a servicemember might not have been dissatisfied with delays if taking leave was the reason , rather than the ides process itself .

despite the associations between actual and perceived timeliness at the meb and peb phases , there were many servicemembers who were satisfied or dissatisfied with timeliness that spent similar amounts of time in the program .

for example , 68 percent of those who said that peb timeliness was “very poor” completed the phase on time , and 55 percent of those who said that meb timeliness was “very good” did not complete on time .

among servicemembers who said that meb timeliness was “very good,” the middle 80 percent of processing times ranged from 62 days to 223 days .

the same range for servicemembers who said meb timeliness was “very poor” was 88 to 323 days .

as table 12 shows , a similar pattern holds for the peb phase .

although servicemembers tend to be more satisfied in meb and peb when their cases take less time , many of them are highly dissatisfied even when their cases take an unusually short amount of time ( and vice versa ) .

in the transition phase , however , 40 percent of servicemembers who said that timeliness was “very good” were processed in 91 to 657 days — a more lengthy range than at the other phases .

the large range and relationship with satisfaction may reflect the use of servicemember leave .

the fact that many servicemembers are similarly satisfied with timeliness , even though they can have widely different processing times , has broader implications for measuring the performance of ides .

dod's timeliness goals may not be meaningful to servicemembers or necessarily reflect high - quality service .

alternatively , servicemembers may not use reasonable standards to assess the time required to process their cases , or they may not accurately perceive the time they have spent in the program .

in these scenarios , the value of servicemember satisfaction as a performance measure becomes less certain .

the relationship between perceived and actual timeliness may simply reflect a large amount of unobserved heterogeneity across servicemembers .

for example , a servicemember whose case has been in ides for an extremely long time might still be highly satisfied with timeliness if the case was complex or personal leave was taken during the process .

neither the survey nor the vta data measure these or other such characteristics that might affect the program's key performance measures .

the lack of variation in satisfaction across servicemember groups and according case timeliness might be seen as a positive outcome , and may suggest that dod and va administer the program consistently across servicemembers and locations .

however , the lack of variation also could suggest shortcomings in the design and administration of the survey , or in data limitations that , alone or together , may reduce the usefulness of survey data for program evaluation .

for example: survey questions: the survey questions may not be sufficiently detailed to measure important differences among servicemembers' experiences .

for example , the survey includes 12 questions ( 4 per survey ) that measure broad opinions about ides , and dod subsequently averages these responses together .

this approach may limit the survey's capacity to describe ides experiences in sufficient detail .

precision of dod indices: dod reports measures of overall satisfaction with ides for each phase , using the questions in table 9 .

however , these measures include one question that asks respondents to “evaluate their overall experience since entering the ides process,” which could be influenced by experiences in prior phases .

consequently , the satisfaction measures reported for each phase could represent a combination of servicemembers' experiences in that phase and prior phases .

completing two surveys at once: dod officials told us that a servicemember may be surveyed for the peb and transition phases in one session .

in these instances a large amount of time may have passed since the servicemember completed the peb phase and it may be more difficult for the servicemember to isolate his or her satisfaction with a particular phase .

survey design: the satisfaction survey is primarily designed to measure performance , not explain it .

the survey includes many highly correlated questions measuring satisfaction with the overall process or broad components of it , such as dod board liaisons , va case managers , or timeliness .

while multiple questions can improve the statistical reliability and validity of dod's performance measures , they require costly survey administration time that could be used for other purposes , such as to measure a larger number of variables that could explain servicemember satisfaction or case processing times .

vta data limitations: the vta administrative data that we matched to survey data primarily measure processing times and basic servicemember demographics , such as service branch , component , and treatment facility .

the data support detailed reporting of performance measures , but they do not measure similarly detailed information on the nature of each case that might allow dod and va to understand the reasons for lengthy case processing times or to identify cases that might become delayed and ensure that they remain on schedule .

for example , the database does not measure the type or severity of referred medical conditions in detail , the nature of delays experienced early in the process , or the use of servicemember leave .

in addition , little information is available on staffing at or caseloads for meb and peb locations , dod board liaisons , or va case managers , which might help to explain or predict performance .

low response and coverage rates: the response and coverage rates of the satisfaction survey further limit the degree to which dod can generalize the data obtained to the population of servicemembers who participate in ides .

in particular , the survey does not assess the views of servicemembers who disenroll from the process before finishing a stage or those who do not complete prior waves of the survey .

including servicemembers who do not complete all waves would complicate longitudinal analysis , however .

table 13 presents data reported by dod on average processing time for active duty cases completed during part of fiscal year 2012 — oct .

2011 to june 2012 .

dod's data are provided as a supplement the analyses gao conducted for fiscal years 2008 through 2011 .

we did not evaluate the reliability of these data and cannot predict the extent to which any trends will continue for the rest of the fiscal year .

michele grgich ( assistant director ) , daniel concepcion , melissa jaynes , and greg whitney made significant contributions to all aspects of this report .

also contributing to this report were bonnie anderson , james bennett , mark bird , joanna chan , brenda farrell , jamila jones kennedy , douglas sloane , almeta spencer , vanessa taylor , jeffrey tessin , roger thomas , walter vance , kathleen van gelder , and sonya vartivarian .

military disability system: preliminary observations on efforts to improve performance .

gao - 12-718t ( washington , d.c.: may 23 , 2012 ) .

military and veterans disability system: worldwide deployment of integrated system warrants careful monitoring .

gao - 11-633t ( washington , d.c.: may 4 , 2011 ) .

military and veterans disability system: pilot has achieved some goals , but further planning and monitoring needed .

gao - 11-69 ( washington , d.c.: december 6 , 2010 ) .

military and veterans disability system: preliminary observations on evaluation and planned expansion of dod / va pilot .

gao - 11-191t ( washington , d.c.: november 18 , 2010 ) .

veterans' disability benefits: further evaluation of ongoing initiatives could help identify effective approaches for improving claims processing .

gao - 10-213 ( washington , d.c.: january 29 , 2010 ) .

recovering servicemembers: dod and va have jointly developed the majority of required policies but challenges remain .

gao - 09-728 ( washington , d.c.: july 8 , 2009 ) .

recovering servicemembers: dod and va have made progress to jointly develop required policies but additional challenges remain .

gao - 09-540t ( washington , d.c.: april 29 , 2009 ) .

military disability system: increased supports for servicemembers and better pilot planning could improve the disability evaluation process .

gao - 08-1137 ( washington , d.c.: september 24 , 2008 ) .

dod and va: preliminary observations on efforts to improve care management and disability evaluations for servicemembers .

gao - 08-514t ( washington , d.c.: february 27 , 2008 ) .

dod and va: preliminary observations on efforts to improve health care and disability evaluations for returning servicemembers .

gao - 07-1256t ( washington , d.c.: september 26 , 2007 ) .

military disability system: improved oversight needed to ensure consistent and timely outcomes for reserve and active duty service members .

gao - 06-362 ( washington , d.c.: march 31 , 2006 ) .

