for some time now , you have expressed concern about the large amount of money spent annually on information technology ( it ) by federal agencies and whether agencies have processes in place to ensure it is being spent on the right projects and is producing meaningful results .

this report responds to your request that we assess the it investment practices of a small group of federal agencies and compare them to those used by leading private and public sector organizations .

this report also highlights the implications of our findings for the office of management and budget ( omb ) as it responds to investment management requirements of the recent information technology management reform act .

we are sending copies of this report to the secretaries of commerce and transportation ; the administrators of the national aeronautics and space administration and environmental protection agency ; the commissioner of internal revenue ; the director of the office of management and budget ; the ranking minority member of the subcommittee on oversight of government management and the district of columbia , senate committee on governmental affairs ; the chairmen and ranking minority members of the cognizant subcommittees of the senate and house appropriations committees ; and other interested congressional committees .

we will also make copies available to others upon request .

please contact me at ( 202 ) 512-6406 if you have any questions about this report .

there is an increasing demand , coming from the congress and the public , for a smaller government that works better and costs less .

having valuable , accurate , and accessible financial and programmatic information is a critical element for any improvement effort to succeed .

furthermore , increasing the quality and speed of service delivery while reducing costs will require the government to make significant investments in three fundamental assets — personnel , knowledge , and capital property / fixed assets .

investments in information technology ( it ) projects can dramatically affect all three of these assets .

indeed , the government's ability to improve performance and reduce costs in the information age will depend , to a large degree , on how well it selects and uses information systems investments to modernize its often outdated operations .

however , the impact of information technology is not necessarily dependent on the amount of money spent , but rather on how the investments are selected and managed .

this , in essence , is the challenge facing federal executives: increasing the return on money spent on it projects by spending money wiser , not faster .

it projects , however , are often poorly managed .

for example , one market research group estimates that about a third of all u.s .

it projects are canceled , at a estimated cost in 1995 of over $81 billion .

in the last 12 years , the federal government has obligated at least $200 billion for information management with mixed results at best .

yet despite this huge investment , government operations continue to be hampered by inaccurate data and inadequate systems .

too often , it projects cost much more and produce much less than what was originally envisioned .

even worse , often these systems do not significantly improve mission performance or they provide only a fraction of the expected benefits .

of 18 major federal agencies , 7 have an it effort that has been identified as high risk by either the office of management and budget ( omb ) or us .

some private and public sector organizations , on the other hand , have designed and managed it to improve their organizational performance .

in a 1994 report , we analyzed the information management practices of several leading private and state organizations .

these leading organizations were identified as such by their peers and independent researchers because of their progress in managing information to improve service quality , reduce costs , and increase work force productivity and effectiveness .

from this analysis , we derived 11 fundamental it management practices that , when taken together , provide the basis for the successful outcomes that we found in leading organizations .

 ( see figure 1.1. ) .

one of the best practices exhibited by leading organizations was that they manage information systems projects as investments .

this particular practice offers organizations great potential for gaining better control over their it expenditures .

in the short term ( within 2 years ) , this practice serves as a powerful tool for carefully managing and controlling it expenditures and better understanding the explicit costs and projected returns for each it project .

in the long term ( from 3 to 5 years ) , this practice serves as an effective process for linking it projects to organizational goals and objectives .

however , managing it projects as investments works most effectively when implemented as part of an integrated set of management practices .

for example , project management systems must also be in place , reengineering improvements analyzed , and planning processes linked to mission goals .

while the specific processes used to implement an investment approach may vary depending upon the structure of the organization ( eg , centralized versus decentralized operations ) , we nonetheless found that the leading organizations we studied shared several common management practices related to the strategic use of information and information technologies .

specifically , they maintained a decision - making process consisting of three phases — selection , control , and evaluation — designed to minimize risks and maximize return on investment .

 ( see figure 1.2. ) .

the congress has passed several pieces of legislation that lay the groundwork for agencies to establish a investment approach for managing it .

for instance , revisions to the paperwork reduction act ( pra ) ( public law 104-13 ) have put more emphasis on evaluating the operational merits of information technology projects .

the chief financial officers ( cfo ) act ( public law 101-576 ) focuses on the need to significantly improve financial management and reporting practices of the federal government .

having accurate financial data is critical to establishing performance measures and assessing the returns on it investments .

finally , the government performance and results act ( gpra ) ( public law 103-62 ) requires agencies to set results - oriented goals , measure performance , and report on their accomplishments .

in addition , the recently passed information technology management reform act ( itmra ) ( division e of public law 104-106 ) requires federal agencies to focus more on the results achieved through it investments while streamlining the federal it procurement process .

specifically this act , which became effective august 8 of this year , introduces much more rigor and structure into how agencies approach the selection and management of it projects .

among other things , the head of each agency is required to implement a process for maximizing the value and assessing and managing the risks of the agency's it acquisitions .

appendix v summarizes the primary it investment provisions contained in itmra .

itmra also heightens the role of omb in supporting and overseeing agencies' it management activities .

the director of omb is now responsible for promoting and directing that federal agencies establish capital planning processes for it investment decisions .

the director is also responsible for evaluating the results of agency it investments and enforcing accountability .

the results of these decisions will be used to develop recommendations for the president's budget .

omb has begun to take action in these areas .

in november 1995 , omb , with substantial input from gao , published a guide designed to help federal agencies systematically manage and evaluate their it - related investments.this guide was based on the investment processes found at the leading organizations .

recent revisions to omb circular a - 130 on federal information resources management have also placed greater emphasis on managing information system projects as investments .

and the recently issued part 3 of omb circular a - 11 , which replaced omb bulletin 95-03 , “planning and budgeting for the acquisition of fixed assets,” provides additional guidance and information requirements for major fixed asset acquisitions .

the chairman , senate subcommittee on oversight of government management and the district of columbia , committee on governmental affairs and the chairman and ranking minority member , house committee on government reform and oversight , requested that we compare and contrast the management practices and decision processes used by leading organizations with a small sample of federal agencies .

the process used by leading organizations is embodied in omb's evaluating information technology investments: a practical guide and specific provisions contained in the information technology management reform act of 1996 .

the agencies we examined are the national aeronautics and space administration ( nasa ) ( $1.6 billion spent on it in fy 1994 ) , national oceanic and atmospheric administration ( noaa ) ( $296 million spent on it in fy 1994 ) , environmental protection agency ( epa ) ( $302 million spent on it in fy 1994 ) , coast guard ( $157 million spent on it in fy 1994 ) , and the internal revenue service ( irs ) ( $1.3 billion spent on it in fy 1994 ) .

we selected the federal agencies for our sample based on one or more of the following characteristics: ( 1 ) large it budgets , ( 2 ) expected it expenditure growth rates , and ( 3 ) programmatic risk as assessed by gao and omb .

in addition , the coast guard was selected because of its progress in implementing an investment process .

collectively , these agencies spent about $3.7 billion on it in fy 1994 — 16 percent of the total spent on it .

our review focused exclusively on how well these five agencies manage information technology as investments , one of the 11 practices used by leading organizations to improve mission performance , as described in our best practices report .

as such , our evaluation only focused on policies and practices used at the agencywide level ; we did not evaluate the agencies' performance in the 10 other practices .

in addition , we did not systematically examine the overall it track records of each agency .

during our review of agency it investment decision - making processes , we did the following: reviewed agencies' policies , practices , and procedures for managing it investments ; interviewed senior executives , program managers , and irm professionals ; and determined whether agencies followed practices similar to those used by leading organizations to manage information systems projects as investments .

we developed the attributes needed to manage information systems projects as investments from the paperwork reduction act , the federal acquisition streamlining act , omb circular a - 130 , gao's “best practices” report on strategic information management , gao's strategic information management toolkit , and omb's guide evaluating information technology investments: a practical guide .

many of the characteristics of this investment approach are contained in the information technology management reform act of 1996 ( as summarized in appendix v ) .

however , this law was not in effect at the time of our review .

to identify effects associated with the presence or absence of investment controls , we reviewed agencies' reports and documents , related gao and inspector general reports , and other external reports .

we also discussed the impact of the agencies' investment controls with senior executives , program managers , and irm professionals to get an agencywide perspective on the controls used to manage it investments .

additionally , we reviewed agency documentation dealing with it selection , budgetary development , and it project reviews .

to determine how much each agency spent on information technology , we asked each agency for information on spending , staffing , and their 10 largest it systems and projects .

the agencies used a variety of sources for the same data elements , which may make comparisons among agencies unreliable .

while data submitted by the agencies were validated by agency officials , we did not independently verify the accuracy of the data .

most of our work was conducted at agencies' headquarters in washington , d.c .

similarly , we visited noaa offices in rockville , maryland , and the national weather service in silver spring , maryland .

we also visited nasa program , financial , and irm officials at johnson space center in houston , texas , and ames research center in san francisco , california , to learn how they implement nasa policy on it management .

we performed the majority of our work from april 1995 through september 1995 , with selected updates through july 1996 , in accordance with generally accepted government auditing standards .

we updated our analyses of irs and nasa in conjunction with other related audit work .

in addition , several of the agencies provided us with updated information as part of their comments on a draft version of the report .

many of these changes have only recently occurred and we have not fully evaluated them to determine their effect on the agency's it investment process .

we provided and discussed a draft of this report with officials from omb , epa , nasa , noaa , irs , and the coast guard , and have incorporated their comments where appropriate .

omb's written comments , as well as our evaluation , are provided in appendix i .

appendix ii profiles each agency's it spending , personnel , and major projects .

appendix iii provides a brief description of an it investment process approach based on work by gao and omb .

appendix iv provides a brief overview of each agency's it management processes .

because of its relevance to this report , the investment provisions of the information technology management reform act of 1996 are summarized in appendix v. major contributors to this report are listed in appendix vi .

all of the agencies we studied — nasa , irs , the coast guard , noaa , and epa — had at least elements or portions of an it investment process in place .

for instance , the coast guard had a selection process with decision criteria that included an analysis of cost , risk , and return data ; epa had created a executive management group to address cross - agency it nasa and noaa utilized program control meetings to ensure senior management involvement in monitoring the progress of important ongoing it projects ; and irs had developed a systems investment evaluation review methodology and used it to conduct postimplementation reviews of some tax systems modernization projects .

however , none of these five agencies had implemented a complete , institutionalized investment approach that would fulfill requirements of pra and itmra .

consequently , it decision - making at these agencies was often inconsistent or based on the priorities of individual units rather than the organization as a whole .

additionally , cost - benefit and risk analyses were rarely updated as projects proceeded and were not used for managing project results .

also , the mission - related benefits of implemented systems were often difficult to determine since agencies rarely collected or compared data on anticipated versus actual costs and benefits .

in general , we found that the it investment control processes used at the case study agencies at the time of our review contained four main weaknesses .

while all four weaknesses may not have been present at each agency , in comparison to leading organizations , the case study agencies lacked a consistent process ( used at all levels of the agency ) for uniformly selecting and managing systems investments ; focused their selection processes on selected efforts , such as justifying new project funding or focusing on projects already under development , rather than managing all it projects — new , under development , and operational — as a portfolio of competing investments ; made funding decisions without giving adequate attention to management control or evaluation processes , and made funding decisions based on negotiations or undefined decision criteria and did not have the up - to - date , accurate data needed to support it investment decisions .

appendix iv provides a brief overview of how each agency's current processes for selecting , controlling , and evaluating it projects worked .

leading organizations use the selection , control , and evaluation decision - making processes in a consistent manner throughout different units .

this enables the organization , even one that is highly decentralized , to make trade - offs between projects , both within and across business units .

figure 2.1 illustrates how this process can be applied to the federal government where major cabinet departments may have several agencies under their purview .

it portfolio investment processes can exist at both the departmental and agency levels .

as with leading organizations , the key factor is being able to determine which it projects and resources are shared ( and should be reviewed at the departmental level ) and which are unique to each agency .

three common criteria used by leading organizations are applicable in the federal setting .

these threshold criteria include ( 1 ) high - dollar , high - risk it projects ( risk and dollar amounts having been already defined ) , ( 2 ) cross - functional projects ( two or more organizational units will benefit from the project ) , and ( 3 ) common infrastructure support ( hardware and telecommunications ) .

projects that meet these particular threshold criteria are discussed , reviewed , and decided upon at a departmentwide level .

the key to making this work is having clearly defined roles , responsibilities , and criteria for determining the types of projects that will be reviewed at the different organizational levels .

as described in itmra , agency heads are to implement a process for maximizing the value and assessing and managing the risks of it investments .

further , this process should be integrated with the agency's budget , financial , and program management process ( es ) .

whether highly centralized or decentralized , matrixed or hierarchial , agencies can most effectively reap the benefits of an investment process by developing and maintaining consistent processes within and across their organizations .

one of the agencies we reviewed — the coast guard — used common investment criteria for making cross - agency it decisions .

irs had defined some criteria , but was not yet using these criteria to make decisions .

the three other agencies — nasa , epa , and noaa — chose it projects based on inconsistent or nonexistent investment processes .

there was little or no uniformity in how risks , benefits , and costs of various it projects across offices and divisions within these three agencies were evaluated .

thus , cross - comparisons between systems of similar size , function , or organizational impact were difficult at best .

more important , management had no assurance that the most important mission objectives of the agency were being met by the suite of system investments that was selected .

nasa , for instance , allowed its centers and programs to make their own it funding decisions for mission - critical systems .

these decisions were made without an agencywide mechanism in place to identify low - value it projects or costs that could be avoided by capitalizing on opportunities for data sharing and system consolidation across nasa units .

as a result , identifying cross - functional system opportunities was problematic at best .

the scope of this problem became apparent as a result of a special nasa it review .

in response to budget pressures , nasa conducted an agencywide internal information systems review to identify cost savings .

the resulting march 1995 report described numerous instances of duplicate it resources , such as large - scale computing and wide area network services , that were providing similar functions .

a subsequent nasa inspector general's ( ig ) report , also issued in march 1995 , substantiated this special review , finding that at one center nasa managers had expended resources to purchase or develop information systems that were already available elsewhere , either within nasa or , in some cases , within that center itself .

while this special review prompted nasa to plan several consolidation efforts , such as consolidating its separate wide area networks ( for a nasa projected savings of $236 million over 5 years ) , the risk of purchasing duplicate it resources remained because of weaknesses in its current decentralized decision - making process .

for example , nasa created chief information officer ( cio ) positions for nasa headquarters and for each of its 23 centers .

these cios have a key role in improving agencywide it cooperation and coordination .

however , the cios have limited formal authority and to date have only exercised control over nasa's administrative systems — which account for about 10 percent of nasa's total it budget .

with more defined cio roles , responsibility , and authority , it is likely that additional opportunities for efficiencies will be identified .

nasa recently established a cio council to establish high - level policies and standards , approve information resources management plans , and address issues and initiatives .

the council will also serve as the it capital investment advisory group to the proposed nasa capital investment council .

nasa plans for this capital investment council to have responsibility for looking at all capital investments across nasa , including those for it .

nasa's proposed capital investment council may fill this need for identifying cross - functional opportunities ; however , it is too early to evaluate its impact .

by having consistent , quantitative , and analytical processes across nasa that address both mission - critical and administrative systems , nasa could more easily identify cross - functional opportunities .

nasa has already demonstrated that savings can be achieved by looking within mission - critical systems for cross - functional opportunities .

for instance , nasa estimated that $74 million was saved by developing a combined space station and space shuttle control center using largely commercial off - the - shelf software and a modular development approach , rather than the original plan of having two separate control centers that used mainframe technology and custom software .

epa , like nasa , followed a decentralized approach for making it investment decisions .

program offices have had control and discretion over their specific it budgets , regardless of project size or possible cross - office impact .

as we have previously reported , this has led to stovepiped systems that do not have standard data definitions or common interfaces , making it difficult to share environmental data across the agency .

this is important because sharing environmental data across the agency is crucial to implementing epa's strategic goals .

in 1994 , epa began to address this problem by creating a senior management executive steering committee ( esc ) charged with ensuring that investments in agencywide information resources are managed efficiently and effectively .

this committee , comprised of senior epa executives , has the responsibility to ( 1 ) recommend funding on major system development efforts and ( 2 ) allocate the it budget reserved for agencywide irm initiatives , such as geographical information systems ( gis ) support and data standards .

at the time of our review , the esc had not reviewed or made recommendations on any major information system development efforts .

instead , the esc focused its activity on spending funds allocated to it for agencywide irm policy initiatives , such as intra - agency data standards .

the esc met on june 26 , 1996 , to assess the impact of itmra upon epa's it management process .

in conducting their selection processes , leading organizations assess and manage the different types of it projects , such as mission - critical or infrastructure , at all different phases of their life cycle , in order to create a complete strategic investment portfolio .

 ( see figure 2.2. ) .

by scrutinizing and analyzing their entire it portfolio , managers can examine the costs of maintaining existing systems versus investing in new ones .

by continually and rigorously reevaluating the entire project portfolio based on mission priorities , organizations can reach decisions on systems based on overall contribution to organizational goals .

under itmra , agencies will need to compare and prioritize projects using explicit quantitative and qualitative decision criteria .

at the federal agencies we studied , some prioritization of projects was conducted , but none made managerial trade - offs across all types of projects .

irs , noaa , and the coast guard each conducted some type of portfolio analyses ; epa and nasa did not .

additionally , the portfolio analyses that were performed generally covered projects that were either high dollar , new , or under development .

for example , in 1995 we reported that irs executives were consistently maintaining that all 36 tsm projects , estimated to cost up to $10 billion through the year 2001 , were equally important and must all be completed for the modernization to succeed.this approach , as well as the accompanying initial failure to rank the tsm projects according to their prioritized needs and mission performance improvements , has meant that irs could not be sure that the most important projects were being developed first .

since our 1995 report , irs has begun to rank and prioritize all of the proposed tsm projects using cost , risk , and return decision criteria .

however , these decision criteria are largely qualitative , the data used for decisions were not validated or reliable , and analyses were not based on calculations of expected return on investment .

in addition , according to irs , its investment review board uses a separate process with different criteria for analyzing operational systems .

irs also said that the board does not review research and development ( r&d ) systems or field office systems .

using separate processes for some system types and not including all systems prevents irs from making comparisons and trade - offs as part of a complete it portfolio .

of all the agencies we reviewed , the coast guard had the most experience using a comprehensive selection phase .

in 1991 , the coast guard started a strategic information resources management process and shortly thereafter initiated an it investment process .

under this investment process , a coast guard working group from the irm office ranks and prioritizes new it projects and those under development based on explicit risk and return decision criteria .

a senior management board meets annually to rank the projects and decide on priorities .

the coast guard has derived benefits from its project selection process .

during the implementation of its it investment process , the coast guard identified opportunities for systems consolidation .

for example , the coast guard reported that five separate personnel systems are being incorporated into the personnel management information system / joint military pay system ii for a cost avoidance of $10.2 million .

the coast guard also identified other systems consolidation opportunities that , if implemented , could result in a total cost savings of $77.4 million .

however , at the time of our review , the coast guard's selection process was still incomplete .

for example , r&d projects and operational systems were not included in the prioritization process .

as a result , the coast guard could not make trade - offs between all types of proposed systems investments , creating a risk that new systems would be implemented that duplicate existing systems .

additionally , the coast guard was at risk of overemphasizing investments in one area , such as maintenance and enhancements for existing systems , at the expense of higher value investments in other areas , such as software applications development supporting multiple unit needs .

leading organizations continue to manage their investments once selection has occurred , maintaining a cycle of continual control and evaluation .

senior managers review the project at specific milestones as the project moves through its life cycle and as the dollar amounts spent on the project increase .

 ( see figure 2.3. ) .

at these milestones , the executives compare the expected costs , risks , and benefits of earlier phases with the actual costs incurred , risks encountered , and benefits realized to date .

this enables senior executives to ( 1 ) identify and focus on managing high - potential or high - risk projects , ( 2 ) reevaluate investment decisions early in a project's life cycle if problems arise , ( 3 ) be responsive to changing external and internal conditions in mission priorities and budgets , and ( 4 ) learn from past success and mistakes in order to make better decisions in the future .

the level of management attention focused on each of the three investment phases is proportional based on such factors as the relative importance of each project in the portfolio , the relative project risks , and the relative number of projects in different phases of the system development process .

the control phase focuses senior executive attention on ongoing projects to regularly monitor their interim progress against projected risks , cost , schedule , and performance .

the control phase requires projects to be modified , continued , accelerated , or terminated based on the results of those assessments .

in the evaluation phase , the attention is focused on implemented systems to give a final assessment of risks , costs , and returns .

this assessment is then used to improve the selection of future projects .

similarly in the federal government , gpra forces a shift in the focus of federal agencies — away from such traditional concerns as staffing and activity levels and towards one overriding issue: results .

gpra requires agencies to set goals , measure performance , and report on their accomplishments .

just as in leading organizations , gpra , in concert with the cfo act , is intended to bring a more disciplined , businesslike approach to the management of federal programs .

the agencies we reviewed focused most of their resources and attention on selecting projects and gave less attention to controlling or evaluating those projects .

while irs , nasa , and noaa had implemented control mechanisms , and irs had developed a postimplementation review methodology , none of the agencies had complete and comprehensive control and evaluation processes in place .

specifically , in the five case study agencies we evaluated , we found that control mechanisms were driven primarily by cost and schedule concerns without any focus on quantitative performance measures , evaluations of actual versus projected returns were rarely conducted , and information and lessons learned in either the control or evaluation phases were not systematically fed back to the selection phase to improve the project selection process .

leading organizations maintain control of a project throughout its life cycle by regularly measuring its progress against not only projected cost and schedule estimates , but also quantitative performance measures , such as benefits realized or demonstrated in pilot projects to date .

to do this , senior executives from the program , irm , and financial units continually monitor projects and systems for progress and identify problems .

when problems are identified , they take immediate action to resolve them , minimize their impact , or alter project expectations .

legislation now requires federal executives to conduct this type of rigorous project monitoring .

with the passage of itmra , agencies are required to demonstrate , through performance measures , how well it projects are improving agency operations and mission effectiveness .

senior managers are also to receive independently verifiable information on cost , technical and capability requirements , timeliness , and mission benefit data at project milestones .

furthermore , pursuant to the federal acquisition streamlining act of 1994 ( public law 103-355 ) , if a project deviates from cost , schedule , and performance goals , the agency head is required to conduct a timely review of the project and identify appropriate corrective action — to include project termination .

two of the agencies we reviewed — the coast guard and epa — did not use management control processes that focused on it systems projects .

the other three agencies — irs , noaa , and nasa — had management control processes that focused primarily on schedule and cost concerns , but not interim evaluations of performance and results .

rarely did we find examples in which anticipated benefits were compared to results at critical project milestones .

we also found few examples of lessons that were learned during the control phase being cycled back to improve the selection phase .

to illustrate , both irs and nasa used program control meetings ( pcms ) to keep senior executives informed of the status of their major systems by requiring reports , in the form of self - assessments , from the project managers .

however , these meetings did not focus on how projects were achieving interim , measurable improvement targets for quality , speed , and service that could form the basis for project decisions about major modifications or termination .

irs , for instance , used an implementation schedule to track different components of each of its major it projects under tsm .

based on our discussions with irs officials , the pcms focused on factors bearing on real or potential changes in project costs or schedule .

actual , verified data on interim application or system testing results — compared to projected improvements in operational , mission improvements — were not evaluated .

at nasa , senior program executives attended quarterly program management council ( pmc ) meetings to be kept informed of major programs and projects and to take action when problems arose .

while not focused exclusively on it issues , the pcms were part of a review process that looked at implementation issues of programs and projects that ( 1 ) were critical to fulfilling nasa's mission , particularly those that were assigned to two or more field installations , ( 2 ) involved the allocation of significant resources , defined as projects whose life - cycle costs were over $200 million , or ( 3 ) warranted special management attention , including those that required external agency reporting on a regular basis .

during the pmc meetings , senior executives reviewed self - assessments ( grades of green , yellow , and red ) , done by the responsible project manager , on the cost , schedule , and technical progress of the project .

using this color - coded grading scheme , nasa's control process focused largely on cost , schedule , and technical concerns , but not on assessing improvements to mission performance .

additionally , the grading scheme was not based on quantitative criteria , but instead was largely qualitative and subjective in nature .

for instance , projects were given a “green” rating if they were “in good shape and on track consistent with the baseline.” a “yellow” rating was defined as a “concern that is expected to be resolved within the schedule and budget margins,” and a “red” rating was defined as “a serious problem that is likely to require a change in the baseline determined at the beginning of the project.” however , the lack of quantitative criteria , benefit analysis , and performance data invited the possibility for widely divergent interpretations and a misunderstanding of the true value of the projects under review .

as of 1995 , three it systems had met nasa's review criteria and had been reviewed by the pmc .

these three systems constituted about 7 percent of nasa's total fiscal year 1994 it spending .

no similar centralized review process existed for lower dollar projects , which could have resulted in problem projects and systems that collectively added up to significant costs being overlooked .

for instance , in 1995 nasa terminated an automated accounting system project that had been under development for about 6 years , had cost about $45 million to date , and had an expected life - cycle cost of over $107 million .

in responding to a draft of this report , the nasa cio said that the current cost threshold of $200 million is being reduced to a lower level to ensure that most , if not all , agency it projects will be subject to pmc reviews .

in addition , the cio noted that nasa's internal policy directive on program / project management is being revised to ( 1 ) include it evaluation criteria that are aligned with itmra and executive - branch guidance and ( 2 ) clearly establish the scope and levels of review ( agency , lead center , or center ) for it investment decisions .

once projects have been implemented and become operational , leading organizations evaluate them to determine whether they have achieved the expected benefits , such as lowered cost , reduced cycle time , increased quality , or increased the speed of service delivery .

they do this by conducting project postimplementation reviews ( pirs ) to compare actual to planned cost , returns , and risks .

the pir results are used to calculate a final return on investment , determine whether any unanticipated modifications may be necessary to the system , and provide “lessons learned” input for changes to the organization's it investment processes and strategy .

itmra now requires agencies to report to omb on the performance benefits achieved by their it investments and how those benefits support the accomplishment of agency goals .

only one of the five federal agencies we reviewed — irs — systematically evaluated implemented it projects to determine actual costs , benefits , and risks .

indeed , we found that most of the agencies rarely evaluated implemented it projects at all .

in general , the agency review programs were insufficiently staffed and used poorly defined and inconsistent approaches .

in addition , in cases where evaluations were done , the findings were not used to consider improvements or revisions in the it investment decision - making process .

noaa , for instance , had no systematic process in place to ensure that it was achieving the planned benefits from its annual $300 million it expenditure .

for example , of the four major it projects that constitute the $4.5 billion national weather service ( nws ) modernization effort , only the benefits actually accruing from one of four — the nexrad radars — had been analyzed .

while not the only review mechanism used by the agency , noaa's central review program was poorly staffed .

noaa headquarters , with half a staff year devoted to this review program , generally conducted reviews in collaboration with other organizational units and had participated in only four it reviews over the last 3 fiscal years .

additionally , these reviews generally did not address the systems' projected versus actual cost , performance , and benefits .

irs had developed a pir methodology that it used to conduct five systems postimplementation reviews .

a standardized methodology is important because it makes the reviews consistent and adds rigor to the analytical steps used in the review process .

the irs used the june 1994 pir on the corporate files on - line ( cfol ) system as the model for this standardized methodology .

in december 1995 , irs used the pir methodology to complete a review of the service center recognition / image processing system ( scrips ) .

subsequently , three more pirs have been completed ( taxlink , the enforcement revenue information system , and the integrated collection system ) and five more are scheduled .

irs estimated that the five completed systems have an aggregate cost of about $845 million .

however , the pir methodology was not integrated into a cohesive investment process .

specifically , there were no mechanisms in place to take the lessons learned from the pirs and apply them to the decision criteria and other tools and techniques used in their investment process .

as a result , the pirs that were conducted did not meet one of their primary objectives — to ensure continual improvement based on lessons learned — and irs ran the risk of repeating past mistakes .

to help make continual decisions on it investments , leading organizations require all projects to have complete and up - to - date project information .

this information includes cost and benefit data , risk assessments , implementation plans , and initial performance measures .

 ( see figure 2.4 ) .

maintaining this information allows senior managers to rigorously evaluate the current status of projects .

in addition , it allows them to compare it projects across the organization ; consider continuation , delay , or cancellation trade - offs ; and take action accordingly .

itmra requires agencies to use quantitative and qualitative criteria to evaluate the risks and the returns of it investments .

as such , agencies need to collect and maintain accurate and reliable cost , benefit , risk , and performance data to support project selection and control decisions .

the requirement for accurate , reliable , and up - to - date financial and programmatic information is also a primary requirement of the cfo act and is essential to fulfilling agency requirements for evaluating program results and outcomes under gpra .

at the five case study agencies we evaluated , we found that , in general agency it investment decisions were based on undefined or implicit data on the project's cost , schedule , risks , and returns were not documented , defined , or kept up - to - date , and , in many cases , were not used to make investment decisions .

to ensure that all projects and operational systems are treated consistently , leading organizations define explicit risk and return decision criteria .

these criteria are then used to evaluate every it project or system .

risk criteria involve managerial , technical , resource , skill , security , and organizational factors , such as the size and scope of the project , the extent of use of new technology , the potential effects on the user organization , the project's technical complexity , and the project's level of dependency on other systems or projects .

return criteria are measured in financial and nonfinancial terms .

financial measurements can include return on investment and internal rate of return analyses while nonfinancial assessments can include improvements in operational efficiency , reductions in cycle time , and progress in better meeting customer needs .

of the five agencies in our sample , only the coast guard used a complete set of decision criteria .

these decision criteria included ( 1 ) risk assessments of schedule , cost , and technical feasibility dimensions , ( 2 ) cost - benefit impacts of the investment , ( 3 ) mission effectiveness measures , ( 4 ) degree of alignment with strategic goals and high - level interest ( such as congress or the president ) , and ( 5 ) organizational impact in the areas of personnel training , quality of work life , and increased scope of service .

the coast guard used these criteria to prioritize it projects and justify final selections .

the decision criteria were weighted and scored , and projects were evaluated to determine those with the greatest potential to improve mission performance .

generally , officials in other agencies stated that they determine which projects to fund based on the judgmental expertise of decisionmakers involved in the process .

noaa , for instance , had a board of senior executives that met annually to determine budget decisions across seven strategic goals .

working groups for each strategic goal met and each created a prioritized funding list , which was then submitted to the executive decision - making board .

these working groups did not have uniform criteria for selecting projects .

the executive board accepted the prioritized lists as submitted and made funding threshold decisions based on these lists .

as a result , the executive board could not easily make consistent , accurate trade - offs among the projects that were selected by these individual working groups on a repeatable basis .

in addition , to maximize funding for a specific working group , project rankings may not have been based on true risk or return .

according to a noaa senior manager and the chair of one of the noaa working groups , one group ranked high - visibility projects near the bottom of the list to encourage the senior decision - making board to draw the budgetary cut - off line below these high visibility projects .

few of these high - visibility projects were at the top of the list , despite being crucial to noaa and high on the list of the noaa administrator's priorities .

explicit decision criteria would eliminate this type of budgetary gamesmanship .

leading organizations consider project data the foundation by which they select , control , and evaluate their it investments .

without it , participants in an investment process cannot determine the value of any one project .

leading organizations use rigorous and up - to - date cost - benefit analyses , risk assessments , sensitivity analyses , and project specific data including current costs , staffing , and performance , to make funding decisions and project modifications based , whenever possible , on quantifiable data .

while the agencies in our sample developed documents in order to get project approvals , little effort was made to ensure that the information was kept accurate and up - to - date , and rarely were the data used to manage the project throughout its life cycle .

during our review , we asked each agency to supply us with basic data on its largest dollar it projects .

however , this information was not readily available and gathering it required agency officials to rely on a variety of sometimes incomparable sources for system cost , life - cycle phase , and staffing levels .

in addition , some of the agencies could not comparatively analyze it projects because they did not keep a comprehensive accounting of data on all of the it systems .

for example , epa had to conduct a special information collection to identify life - cycle cost estimates on its major systems and projects for this report .

while the individual system managers at epa did have system life - cycle cost estimates , the fact that this information was decentrally maintained made cross - system comparisons unlikely .

in a 1995 report , the nasa ig found that neither nasa headquarters nor any of the nasa centers had a complete inventory of all information systems for which they were responsible .

all of the agencies we reviewed conducted cost - benefit analyses for their major it projects .

however , these analyses were generally done to support decisions for project approval and were seldom kept current .

in addition , the cost - benefit projections were rarely used to evaluate actual project results .

the nws modernization , for instance , has a cost - benefit analysis that was done in 1992 .

this analysis covers the four major systems under the modernization .

to be effective , an analysis should include the costs and benefits of each project , alternatives to that project , and finally , a combined cost - benefit analysis for the entire modernization .

however , the cost - benefit analysis that was conducted only compares the aggregate costs and benefits of the nws modernization initiative against the current system .

it does not assess or analyze the costs and benefits of each system , nor does it examine alternatives to those systems .

as a result , nws does not know if each of the modernization projects is cost - beneficial , and cannot make trade - offs among them .

if using only this analysis , decision - makers are forced to choose either the status quo or all of the projects proposed under the modernization .

without updated cost - benefit data , informed management decisions become difficult .

we reported in april 1995 that nws was trying to assess user concerns related to the automated surface observing system ( asos ) , one of the nws modernization projects , but that nws did not have a complete estimate of what it would cost to address these concerns .

as we concluded in the report , without reliable estimates of what an enhanced or supplemented asos would cost , it would be difficult for nws to know whether continued investment in asos is cost - beneficial .

we provided and discussed a draft of this report with officials from epa , nasa , noaa , irs , and the coast guard , and have incorporated their comments where appropriate .

several of the agencies noted that they , in response to the issuance of omb's guidance on it investment decision - making and the passage of itmra , have made process changes and organizational modifications affecting it funding decisions .

we have incorporated this information into the report where applicable .

however , many of the process changes and modifications have occurred very recently , and we have not fully evaluated these changes or determined their effects .

officials from noaa and nasa also had reservations about the applicability of the investment portfolio approach to their organizations because their decentralized operating environments were not conducive to a single agencywide portfolio model with a fixed set of criteria .

because any organization , whether centralized or decentralized , has to operate within the parameters of a finite budget , priorities must still be set , across the organization , about where limited it dollars will be spent to achieve maximum mission benefits .

we agree that many it spending decisions can be made at the agency or program level .

however , there are some decisions — especially those involving projects that are ( 1 ) high - risk , high - dollar , ( 2 ) cross - functional , ( 3 ) or providing a common infrastructure ( eg , telecommunications ) — that should be made at a centralized , departmental level .

establishing a common , organizationwide focus , while still maintaining a flexible distribution of departmental and agency / program / site decision - making , can be achieved by implementing standard decision criteria .

these criteria help ensure that projects are assessed and evaluated consistently at lower levels , while still maintaining an enterprisewide portfolio of it investments .

buying information technology can be a high - risk , high - return undertaking that requires strong management commitment and a systematic process to ensure successful outcomes .

by using an investment - driven management approach , leading organizations have significantly increased the realized return on information technology investments , reduced the risk of cost overruns and schedule delays , and made better decisions about how their limited it dollar should be spent .

adopting such an investment - driven approach can provide federal agencies with similar opportunities to achieve greater benefits from their it investments on a more consistent basis .

however , the federal case study agencies we examined used decision - making processes that lacked many essential components associated with an investment approach .

critical weaknesses included the absence of reliable , quantitative cost figures , net return on investment calculations , rigorous decision criteria , and postimplementation project reviews .

with sustained management attention and substantive improvements to existing processes , these agencies should be able to meet the investment - related provisions of itmra .

implementing and refining an it investment process , however , is not an easy undertaking and cannot be accomplished overnight .

maximizing the returns and minimizing the risks on the billions of dollars that are spent each year for it will require continued efforts on two fronts .

first , agencies must fundamentally change how they select and manage their it projects .

they must develop and begin using a structured it investment approach that encompasses all aspects of the investment process — selection , control , and evaluation .

second , oversight attention far beyond current levels must be given to agencies' management processes and to actual results that are being produced .

such attention should include the development of policies and guidance as well as selective evaluations of processes and results .

these evaluations should have a dual focus: they should identify and address deficiencies that are occurring , but they should also highlight positive results in order to share lessons learned and speed success .

omb's established leadership role , as well as the policy development and oversight responsibilities that it was given under itmra , place it in a key position to provide such oversight .

omb has already initiated several changes to governmentwide guidance to encourage the investment approach to it decision - making , and has drawn upon the assistance of several key interagency working groups comprised of senior agency officials .

such efforts should be continued and expanded , to ensure that the federal government gets the most return for its information technology investments .

given its significant leadership responsibility in supporting agencies' improvement efforts and responding to requirements of itmra , it is imperative that omb continue to clearly define expectations for agencies and for itself to successfully implement investment decision - making approaches .

as such , we are recommending four specific actions for the director of omb to take .

omb's first challenge is to help agencies improve their investment management processes .

with effective processes in place , agencies should be in much stronger positions to make informed decisions about the relative benefits and risks of proposed it spending .

without them , agencies will continue to be vulnerable to risks associated with excessively costly projects that produce questionable mission - related improvements .

under sections 5112 and 5113 of the information technology management reform act , the director of omb has responsibility for promoting and directing that federal agencies establish capital planning processes for information technology investment decisions .

in designing governmentwide guidance for this process , we recommend that the director of the office of management and budget require agencies to: implement it investment decision - making processes that use explicitly defined , complete , and consistent criteria applied to all projects , regardless of whether project decisions are made at the departmental , bureau , or program level .

with criteria that reflect cost , benefit , and risk considerations , applied consistently , agencies should be able to make more reasonable and better informed trade - offs between competing projects in order to achieve the maximum economic impact for their scarce investment dollars .

periodically analyze their entire portfolios of it investments — at a minimum new projects , as well as projects in development and operations and maintenance expenditures — to determine which projects to approve , cancel or delay .

with development and maintenance efforts competing directly with one another for funding , agencies will be better able to gauge the best proportion of investment in each category of spending to move away from their legacy bases of systems with excessive maintenance costs .

design control and evaluation processes that include cost , schedule , and quantitative performance assessments of projected versus actual improvement in mission outcomes .

as a result , they should increase their capacity to both assess actual project results and learn from their experience which operational areas produce the highest returns and how well they estimate projects and deliver final results .

advise agencies in setting minimum quality standards for data used to assess ( qualitatively and quantitatively ) cost , benefit , and risks decisions on it investments .

agencies should demonstrate that all it funding proposals include only data meeting these quality requirements and that projected versus actual results are assessed at critical project milestones .

the audited data required by the cfo act should help produce this accurate , reliable cost information .

higher quality information should result in better and more consistent decisions on complex information systems investments .

omb's second challenge is to use the results produced by the improved investment processes to develop recommendations for the president's budget that reflect an agency's actual track record in delivering mission performance for it funds expended .

under section 5113 of itmra , the director of omb is charged with evaluating the results of agency it investments and enforcing accountability — including increases or reductions in agency it funding proposals — through the annual budget process .

in carrying out these responsibilities , we recommend that the director of the office of management and budget: evaluate information system project cost , benefit , and risk data when analyzing the results of agency it investments .

such analyses should produce agency track records that clearly and definitively show what improvements in mission performance have been achieved for the it dollars expended .

ensure that the agency investment control process are in compliance with omb's governmentwide guidance , and if not , assess strengths and weaknesses and recommend actions and timetables for improvements .

when results are questionable or difficult to determine , monitoring agency investment processes will help omb diagnose problem causes by determining the degree of agency control and the quality of decisions being made .

use omb's evaluation of each agency's it investment control processes and it performance results as a basis for recommended budget decisions to the president .

this direct linkage should give agencies a strong , much needed incentive to maximize the returns and minimize the risks of their scarce it investments .

to effectively implement improved investment management processes and make the appropriate linkages between agency track records and budget recommendations , omb also has a third challenge .

it will need to marshal the resources and skills to execute the new types of analysis required to make sound investment decisions on agency portfolios .

specifically , we recommend that the director of the office of management and budget: organize an interagency group comprised of budget , program , financial , and it professionals to develop , refine and transfer guidance and knowledge on best practices in it investment management .

such a core group can serve as an ongoing source of practical knowledge and experience on the state of the practice for the federal government .

obtain expertise on an advisory basis to assist these professionals in implementing complete and effective investment management systems .

agency senior irm management could benefit greatly from a high quality , easily accessible means to solicit advice from capital planning and investment experts outside the federal government .

identify the type and amount of skills required for omb to execute it portfolio analyses , determine the degree to which these needs are currently satisfied , specify the gap and both design and implement a plan , with timeframes and goals , to close the gap .

given existing workloads and the resilience of the omb culture , without a determined effort to build the necessary skills , omb will have little impact on the quality of it investment decision - making .

if necessary to augment its own staff resources , omb should consider the option of obtaining outside support to help perform such assessments .

finally , as part of its internal implementation strategy , the director of the office of management and budget should consider developing an approach to assessing omb's own performance in executing oversight responsibilities under the itmra capital planning and investment provisions .

such a process could focus on whether omb reviews of agency processes and results have an impact on reducing risk or increasing the returns on information technology investments — both within and across federal agencies .

in its written comments on a draft of our report , omb generally supported our recommendations and said that it is working towards implementing many aspects of the recommendations as part of the fiscal year 1998 budget review process of fixed capital assets .

omb also provided observations or suggestions in two additional areas .

first , omb stated that given itmra's emphasis on agencies being responsible for it investment results , it did not plan to validate or verify that each agency's investment control process is in compliance with omb's guidance contained in its management circulars .

as discussed in our more detailed evaluation of omb's comments in appendix i , conducting selective evaluations is an important aspect of an overall oversight and leadership role because it can help identify management deficiencies that are contributing to poor it investment results .

second , omb noted that the relationship of it investment processes between a cabinet department and bureaus or agencies within the department was not fully evaluated and that additional attention would be needed as more data on this issue become available .

we agree that our focus was on assessing agencywide processes and that continued attention to the relationships between departments , bureaus , and agencies will contribute to increased understanding across the government and will ultimately improve itmra's chances of success .

this issue is discussed in more detail in our response to comments provided by the five agencies we reviewed ( summarized at the end of chapter 2 ) .

the following are gao's comments on the office of management and budget's letter dated july 26 , 1996 .

1 .

as stated in the scope and methodology section of the report , we focused our analysis on agencywide processes .

we agree that continued attention to this issue will contribute to increased understanding across the government and will ultimately improve itmra's chances of success .

as noted in our response to comments received from the agencies we reviewed ( provided at the end of chapter 2 ) , we believe that a flexible distribution of departmental and agency / program / site it decision - making is possible and can best be achieved by implementing standard decision criteria for all projects .

in addition , we note that particular types of it decisions , such as those with unusually high - risk , cross - functional impact or that provide common infrastructure needs , are more appropriately decided at a centralized , departmental level .

experience gained during implementation of the chief financial officers ( cfo ) act showed that departmental - level cfos needed time to build effective working relationships with their agency - or bureau - level counterparts .

we believe the same will be true for chief information officers ( cios ) established by itmra and that establishing and maintaining this bureau - level focus will be integral for ensuring the act's success .

2 .

itmra does squarely place responsibility and accountability for it investment results with the head of each agency .

nevertheless , itmra clearly requires that omb provide a key policy leadership and implementation oversight role .

while we agree that it may not be feasible to validate and verify every agency's investment processes , it is still essential that selected evaluations be conducted on a regular basis .

these evaluations can effectively support omb's performance and results - based approach .

they can help to identify and understand problems that are contributing to poor investment outcomes and also help perpetuate success by providing increased learning and sharing about what is and is not working .

in order to develop a profile of each agency's it environment , we asked the agencies to provide us information on the following: total it expenditures for fiscal year 1990 through fiscal year 1994 ; total number of staff devoted to irm functions and activities for fiscal year 1990 through fiscal year 1994 ; and costs for the 10 largest it projects for fiscal year 1994 ( as measured by total project life - cycle cost ) .

to gather this information , we developed a data collection instrument and submitted it to responsible agency officials .

information supplied by the agencies is summarized in the following tables .

we did not independently verify the accuracy of this information .

moreover , comparison of figures across the agencies is difficult because agency officials used different sources ( such as budget data , irm strategic plans , etc. ) .

for the same data elements .

u.d .

provides an organizationwide microcomputer infrastructure and is the primary source for acquiring desktop , server and portable hardware ; operating system and office automation system software ; utilities and peripherals , training , personnel support , and cabling .

op .

provides continued support for the coast guard's existing microcomputer infrastructure .

op .

provides a consolidated accounting and pay system .

u.d .

a configuration of sensors , communication links , personnel , and decision support tools that will modernize and expand the systems in three cities by incorporating radar sensor information overlaid on digital nautical charts as well as improved decision support systems .

u.d .

provides an automated and consolidated communication system .

u.d .

merges two maintenance systems for tracking and recording scheduled aviation maintenance actions .

u.d .

reprograms most of the existing coast guard developed applications to comply with the national institute of standards and technology's application portability profile .

op .

provides safety performance histories of vessels and involved parties and is used as a decision support tool for the commercial vessel safety program .

op .

provides aviation technical publications in electronic format .

u.d .

consolidated into the coast guard standard workstation iii system .

op .

performs funds control from commitments through payment ; updates all ledgers and tables as transactions are processed ; provides a standard means of data entry , edit , and inquiry ; and provides a single set of reference and control files .

op .

contains data submitted to epa under the emergency planning and community right to know act for chemicals and chemical categories listed by the agency .

data include chemical identity , amount of on - site users , release and off - site transfers , on - site treatment , minimization / prevention actions .

public access is provided by the national library of medicine .

op .

supports management and administration of chemical samples from superfund sites that are analyzed under agency contracts with chemical laboratories .

the system schedules and tracks samples from site collection , through analysis , to delivery to the agency .

op .

stores air quality , point source emissions , and area / mobile source data required by federal regulations from the 50 states .

op .

superfund's official source of planning and accomplishment data .

serves as the primary basis for strategic decision - making and site - by - site tracking of cleanup activities .

op .

contains a set of computer applications and a major relational database which is used to support regulation development , air quality analysis , compliance audits , investigations , assembly line testing , in - use compliance , legislation development , and environmental initiatives .

op .

maintains basic data identifying and describing hazardous waste handlers ; detailed information about hazardous waste treatment storage and disposal processes , environmental permitting , information on inspections , violations , and enforcement actions ; and tracks specific corrective action information needed to regulate facilities with hazardous waste releases .

op .

supports the national pollutant discharge elimination system , a clean water act program that issues permits and tracks facilities that discharge pollutants into our navigable waters .

 ( continued ) u.d .

a replacement for the existing comprehensive environmental response , compensation , and liability information system described above .

op .

a pc lan version of the comprehensive environmental response , compensation , and liability information system database used by epa regional offices for data input and local analysis needs .

u.d .

acquire and install tax system modernization host - tier computers at three computing centers .

u.d .

integrates five systems that control , assign , prioritize , and track taxpayer inquiries ; provides office automation , case folder review and inventories , and display and manipulation of case inquiry folders ; automates collection cases ; provide access to current tax return information ; automates case preparation and closure ; and provides standardized hardware and custom software to the criminal investigation function on a nationwide basis .

u.d .

integrates six systems that will receive and control information being transmitted to or from irs ; automates remittance processing activities ; scans paper tax returns and correspondence for processing in an automated database ; provides automated telephone assistance to customers ; permits individual and business tax returns to be filed by utilizing a touch - tone phone ; and provides access to all electronically filed returns that have been scored for potential fraud .

op .

provides case tracking , expanded legal research , a document management system for briefs , an integrated office system , time reporting , issue tracking , litigation support , and a decision support system .

 ( continued ) u.d .

integrates three systems that provide application programs to query , search , update , analyze and extract information from a database ; aggregates tax information into electronic case folders and distributes them to field locations ; and provides the security infrastructure to support all components of the tax system modernization .

u.d .

provides a variety of workstation models , monitors , printers , operating systems and related equipment ; provides for standardization of the small and medium - scale computers used by front line programs in the national and field offices and service centers .

op .

provides funding for ( 1 ) the mainframe and miscellaneous peripherals at each service center , ( 2 ) magnetic media and adp supplies for all service centers , ( 3 ) lease and maintenance for support equipment , and ( 4 ) on - line access to taxpayer information and account status .

u.d .

provides an interim hardware platform at two computing centers to support master file processing and full implementation of the cfol data retrieval / delivery system .

u.d .

provides upgradable software development workstations and workbench tools , including automated analysis and design tools ; requirements traceability tools ; construction kits with smart editors , compilers , animators , and debuggers ; and static analyzers .

u.d .

integrates four systems that provide for ordering and delivery of telecommunication systems and services for treasury bureaus ; serves as a government open systems interconnection profile prototype ; provides centralized network and operations management and will acquire about 14,000 workstations .

u.d .

receives , processes , archives , and distributes earth science research data from u.s. , european , and japanese polar platforms , selected earth probes , the synthetic aperture radar free flyer , selected existing databases , and other sources of related data .

op .

provides telecommunications and computation services for marshall space flight center .

op .

supports most data systems , networks , user workstations and telecommunications systems and provides maintenance , operations , software development , engineering , and customer support functions at johnson space center .

op .

provides a family of compatible computing systems covering a broad performance range that will provide ground - based mission operations systems support .

op .

op .

provides continuity of base operations , including federal information processing resources of sustaining engineering , computer operations , and communications services for kennedy space center .

op .

acquisition of seven classes of scientific and engineering workstations plus supporting equipment .

op .

furnishes , installs , and tests the advanced computer generated image system ; provides direct computational analysis and programming support to specific research disciplines and flight projects ; provides for the analysis , programming , engineering , and maintenance services for the flight simulation facilities .

also provides support for the central scientific and computing complex operation and systems maintenance as well as complex - wide communications systems support and system administration of distributed computing and data reduction systems .

op .

provides a wide array of supporting services , including computational , professional , technical , administrative , engineering , and operations at the lewis research center .

 ( continued ) u.d .

u.d .

an information system including workstations , associated data processing , and communications , designed to integrate data from several national weather service information systems , as well as from field offices , regional and national centers , and other sources .

op .

an initiative to acquire supercomputers necessary to run large complex numeric models as a key component of the weather forecast system .

op .

a distributed - processing system architecture designed to acquire , process , and distribute satellite data and products .

op .

an effort to replace a variety of obsolete technology in the national marine fisheries service with a common computing infrastructure that supports distributed processing in an open system environment .

the system stores , integrates , analyzes , and disseminates large quantities of living marine resource data .

op .

procurement of a high - performance computer system to provide support services for climate and weather research activities .

geostationary operational environmental satellite ( goes i - m ) op .

ground system consisting of minicomputers with associated peripherals and satellite - dependent customized applications software to provide the monitoring , supervision , and data acquisition and processing functions for the goes - next satellites .

op .

a system designed to support weather radars and associated display systems .

 ( continued ) op .

an effort to replace old mainframes as well as the associated channel - connected architecture with an open systems architecture .

op .

ground system consisting of minicomputers with associated peripherals and satellite - dependent customized applications software intended to provide the monitoring , supervision , and data acquisition and processing functions for the polar satellites .

imp .

a system of sensors , computers , display units , and communications equipment to automatically collect and process basic data on surface weather conditions , including temperature , pressure , wind , visibility , clouds , and precipitation .

this appendix is a compilation of work done by omb and us on how federal agencies should manage information systems using an investment process .

it is based upon analysis of the it management best practices found in leading private and public sector organizations and is explained in greater detail in omb's evaluating information technology investments: a practical guide .

how do you know you have selected the best projects ? .

based on your evaluation , did the systems deliver what you expected ? .

key question: how can you select the right mix of it projects that best meets mission needs and improvement priorities ? .

the goal of the selection phase is to assess and prioritize current and proposed it projects and then create a portfolio of it projects .

in doing so , this phase helps ensure that the organization ( 1 ) selects those it projects that will best support mission needs and ( 2 ) identifies and analyzes a project's risks and returns before spending a significant amount of project funds .

a critical element of this phase is that a group of senior executives makes project selection and prioritization decisions based on a consistent set of decision criteria that compares costs , benefits , risks , and potential returns of the various it projects .

key question: what controls are you using to ensure that the selected projects deliver the projected benefits at the right time and the right price ? .

once the it projects have been selected , senior executives periodically assess the progress of the projects against their projected cost , schedule , milestones , and expected mission benefits .

the type and frequency of the reviews associated with this monitoring activity are usually based on the analysis of risk , complexity , and cost that went into selecting the project and that are performed at critical project milestones .

if a project is late , over cost , or not meeting performance expectations , senior executives decide whether it should be continued , modified , or canceled .

steps of the control phase use a set of performance measures to monitor the developmental progress for each it project to identify problems .

take action to correct discovered problems .

key question: based on your evaluation , did the system deliver what was expected ? .

the evaluation phase provides a mechanism for constantly improving the organization's it investment process .

the goal of this phase is to measure , analyze , and record results , based on the data collected throughout each phase .

senior executives assess the degree to which each project met its planned cost and schedule goals and fulfilled its projected contribution to the organization's mission .

the primary tool in this phase is the postimplementation review ( pir ) , which should be conducted once a project has been completed .

pirs help senior managers assess whether a project's proposed benefits were achieved and refine the it selection criteria .

the following sections briefly describe the information technology management processes at each of the five agencies we reviewed .

these descriptions are intended to characterize the general workings of the agency processes at the time of our review .

we used the selection / control / evaluation model ( as summarized in appendix iii and described in detail in omb's evaluating information technology investments: a practical guide ) as a template for describing each agency's it management process .

the coast guard had an it investment process used to select it projects for funding .

it project proposals were screened , evaluated , and ranked by a group of senior irm managers using explicit decision criteria that took into account project costs , expected benefits , and risk assessments .

the ranked list with recommended levels of funding for each project was submitted for review to a board of senior coast guard officers and then forwarded to the coast guard chief of staff for final approval .

epa used a decentralized it project initiation , selection , and funding process .

under this broad process , program offices independently selected and funded it projects on a case - by - case basis as the need for the system was identified .

epa had irm policy and guidance for it project data and analysis requirements — such as a project - level risk assessment and a cost - benefit study — that the program offices had to identify in order to proceed with system development .

epa did not have a consistent set of decision criteria for selecting it projects .

it selection and funding activities within irs differed depending on whether the project was part of the tax system modernization ( tsm ) or an operational system .

in 1995 , irs created a senior - level board for selecting , controlling , and evaluating information technology investments and began to rank all of the proposed tsm projects using its cost , risk , and return decision criteria .

however , these criteria were largely qualitative , data used were not validated or reliable , and the analyses were not based on calculations of expected return on investment .

according to irs , its investment review board used a separate process with different criteria for evaluating operational systems .

the board did not review research and development systems or field office systems .

irs did not compare the results of its different evaluation processes .

within nasa , it project selection and funding decisions were made by domain - specific program managers .

nasa had two general types of it funding — program expenditures and administrative spending .

most of nasa's it funding was embedded within program - specific budgets .

managers of these programs had autonomy to make system - level and system support it selection decisions .

administrative it systems were generally managed by the cognizant nasa program office or center .

nasa has recently established a cio council to establish high - level policies and standards , approve information resources management plans , and address issues and initiatives .

the council will also serve as the it capital investment advisory group to the proposed nasa capital investment council .

nasa plans for this capital investment council to have responsibility for looking at all capital investments across nasa , including those for it .

while this capital investment council may fill the need for identifying cross - functional opportunities , it is not yet operational .

it project selection and funding decisions at noaa were made as part of its strategic management and budgeting process .

noaa had seven work teams — each supporting a noaa strategic goal — that prioritized incoming funding requests .

managers on these work teams negotiated to determine it project funding priorities within the scope of their respective strategic goals .

these prioritization requests were then submitted to noaa's executive management board , which had final agency decision authority over all expenditures .

a key decision criterion used by the work teams was the project's contribution to the agency's strategic goals ; however , no standard set of decision criteria was used in the prioritization decisions .

other data , such as cost - benefit analyses , were also sometimes used to evaluate it project proposals , although use of these data sources was not mandatory .

the coast guard conducted internal system reviews , but these reviews were not used to monitor the progress of it projects .

the review efforts were designed to address ways to improve efficiency , reduce project cost , and reduce project risk .

cost , benefit , and schedule data were also collected annually for some new it projects , but the coast guard did not measure mission benefits derived from each of its projects .

epa had a decentralized managerial review process for monitoring it projects .

epa's irm policy set requirements for the minimum level of review activity that program offices had to conduct , but program offices had primary responsibility for overseeing the progress of their it projects .

in an effort to provide a forum for senior managerial review of it projects , epa , in 1994 , created the executive steering committee ( esc ) for irm to guide epa's agencywide irm activities .

the esc was chartered to review irm projects that are large , important , or cross - organizational .

the committee's first major system review was scheduled for some time in 1996 .

epa is currently formulating the data submission requirements for the esc reviews .

irs regularly conducted senior management program control meetings ( pcm ) to review the cost and schedule activity of tsm projects .

irs had two types of pcms .

the four tsm sites — submission processing , computing center , customer service , and district office — conducted pcms to monitor the tsm activity under their purview .

also , irs could hold “combined pcms” to resolve issues that spanned across the tsm sites .

irs did not conduct pcms to monitor the performance of operational systems .

to date , ( 1 ) working procedures , ( 2 ) required decision documents , ( 3 ) reliable cost , benefit , and return data , ( 4 ) and explicit quantitative decision criteria needed for an effective investment control process are not in place for the irs investment review board .

nasa senior executives regularly reviewed the cost and schedule performance of major programs and projects , but they reviewed only the largest it projects .

no central irm review has been conducted since 1993 .

nasa put senior - level cios in place for each nasa center , but these cios exercised limited control over mission - related systems and had limited authority to enforce it standards or architecture policies .

nasa's proposed capital investment council , which is intended to supplement the program management council by reviewing major capital investments , may address this concern once the investment council is operational .

noaa conducted quarterly senior - level program status meetings to review the progress and performance of major systems and programs , such as those in the nws modernization .

noaa had defined performance measures to gauge the progress toward its strategic goals , but did not have specific performance measures for individual it systems .

also , while some offices had made limited comparisons of actual to expected it project benefits , noaa did not require the collection or assessment of mission benefit accrual information on it projects .

the coast guard did not conduct any postimplementation reviews of it projects .

instead the coast guard focused its review activity on systems that were currently under development .

epa did not conduct any centralized postimplementation reviews .

epa did conduct postimplementation reviews as part of the general services administration's ( gsa ) triennial review requirement , but curtailed this activity in 1992 when the gsa requirement was lifted .

irs directives required that postimplementation reviews be conducted 6 months after an it system is implemented .

at the time of our review , irs had conducted five postimplementation reviews and had developed a standard postimplementation review methodology .

however , no mechanisms were in place to ensure that the results of these irs investment evaluation reviews were used to modify the irs selection and control decision - making processes or alter funding decisions for individual projects .

nasa did not conduct or require any centralized project postimplementation reviews .

nasa stopped conducting centralized irm reviews in 1993 and now instead urges programs to conduct irm self - assessments .

while the agency conducted other reviews , noaa's irm office has participated in only four irm reviews over the last 3 years .

these reviews tended to focus on specific it problems , such as evaluating the merits of electronic bulletin board systems or difficulties being encountered digitizing nautical navigation maps .

no postimplementation reviews had been conducted over the past 3 years .

on february 10 , 1996 , the information technology management reform act of 1996 ( division e of public law 104-106 ) was signed into law .

this appendix is a summary of the information technology investment - related provisions from this act , it is not the actual language contained in the law .

information technology ( it ) is defined as any equipment , or interconnected system or subsystem of equipment , that is used in the automatic acquisition , storage , manipulation , management , movement , control , display , switching , interchange , transmission , or reception of data or information .

it may include equipment used by contractors .

the omb director is to promote and be responsible for improving the acquisition , use , and disposal of it by federal agencies the omb director is to develop a process ( as part of the budget process ) for analyzing , tracking , and evaluating the risks and results of major capital investments for information systems ; the process shall include explicit criteria for analyzing the projected and actual costs , benefits , and risks associated with the investments over the life of each system .

the omb director is to report to the congress ( at the same time the budget is submitted ) on the net program performance benefits achieved by major capital investments in information systems and how the benefits relate to the accomplishment of agency goals .

the omb director shall designate ( as appropriate ) agency heads as executive agents to acquire it for governmentwide use .

the omb director shall encourage agencies to develop and use “best practices” in acquiring it .

the omb director shall direct that agency heads ( 1 ) establish effective and efficient capital planning processes for selecting , managing , and evaluating information systems investments , ( 2 ) before investing in new information systems , determine whether a government function should be performed by the private sector , the government , or government contractor , and ( 3 ) analyze their agencys' missions and revise the mission - related and administrative processes ( as appropriate ) before making significant investments in it .

through the budget process , the omb director is to review selected agency irm activities to determine the efficiency and effectiveness of it investments in improving agency performance .

agency heads are to design and implement a process for maximizing the value and assessing and managing the risks of it investments .

the agency process is to ( 1 ) provide for the selection , management , and evaluation of it investments , ( 2 ) be integrated with the processes for making budget , financial , and program management decisions , ( 3 ) include minimum criteria for selecting it investments and specific quantitative and qualitative criteria for comparing and prioritizing projects , ( 4 ) provide for identifying potential it investments that would result in shared benefits with other federal , state , or local governments , ( 5 ) provide for identifying quantifiable measurements for determining the net benefits and risks of it investments , and ( 6 ) provide the means for senior agency managers to obtain timely development progress information , including a system of milestones for measuring progress , on an independently verifiable basis , in terms of cost , capability of the system to meet specified requirements , timeliness , and quality .

agency heads are to ensure that performance measurements are prescribed for it and that the performance measurements measure how well the it supports agency programs .

 ( continued ) where comparable processes and organizations exist in either the public or private sectors , agency heads are to quantitatively benchmark agency process performance against such processes in terms of cost , speed , productivity , and quality of outputs and outcomes .

agency heads may acquire it as authorized by law ( the brooks act — 40 u. s. c. 759 — is repealed by sec .

5101 ) except that the gsa administrator will continue to manage the fts 2000 and follow - on to that program ( sec .

5124 ( b ) ) .

agency heads are to designate chief information officers ( in lieu of designating irm officials — as a result of amending the paperwork reduction act appointment provision ) .

agency chief information officers ( cios ) are responsible for ( 1 ) providing advice and assistance to agency heads and senior management to ensure that it is acquired and information resources are managed in a manner that implements the policies and procedures of the information technology management reform act of 1996 , is consistent with the paperwork reduction act , and is consistent with the priorities established by the agency head , ( 2 ) developing , maintaining , and facilitating the implementation of a sound and integrated agency it architecture , and ( 3 ) promoting effective and efficient design and operation of major irm processes .

agency heads ( in consultation with the cio and cfo ) are to establish policies and procedures that ( 1 ) ensure accounting , financial , and asset management systems and other information systems are designed , developed , maintained , and used effectively to provide financial or program performance data for agency financial statements , ( 2 ) ensure that financial and related program performance data are provided to agency financial management systems on a reliable , consistent , and timely basis , and ( 3 ) ensure that financial statements support the assessment and revision of agency mission - related and administrative processes and the measurement of performance of agency investments in information systems .

agency heads are to identify ( in their irm plans required under the paperwork reduction act ) major it acquisition programs that have significantly deviated from the cost , performance , or schedule goals established for the program ( the goals are to be established under title v of the federal acquisition streamlining act of 1994 ) .

this section establishes which provisions of the title apply to “national security systems.” “national security systems” are defined as any telecommunications or information system operated by the united states government that ( 1 ) involves intelligence activities , ( 2 ) involves cryptologic activities related to national security , ( 3 ) involves command and control of military forces , ( 4 ) involves equipment that is an integral part of a weapon or weapon system , or ( 5 ) is critical to the direct fulfillment of military or intelligence missions .

this section requires the gsa administrator to provide ( through the federal acquisition computer network established under the federal acquisition streamlining act of 1994 or another automated system ) not later than january 1 , 1998 , governmentwide on - line computer access to information on products and services available for ordering under the multiple award schedules .

the information technology management reform act takes effect 180 days from the date of enactment ( february 10 , 1996 ) .

david mcclure , assistant director danny r. latta , adviser alicia wright , senior business process analyst bill dunahay , senior evaluator john rehberger , information systems analyst shane hartzler , business process analyst eugene kudla , staff evaluator the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 6015 gaithersburg , md 20884-6015 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 301 ) 258-4066 , or tdd ( 301 ) 413-0006 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

