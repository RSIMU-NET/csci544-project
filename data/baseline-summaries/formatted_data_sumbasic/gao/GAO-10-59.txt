the constitution mandates that the federal government count the u.s. population every 10 years — a large and complex undertaking known as the decennial census .

to carry out the census , the u.s. census bureau ( bureau ) conducts numerous operations and coordinates a sequence of thousands of interdependent activities .

given mandated deadlines that the bureau faces for delivering census tabulations , the bureau uses a master activity schedule to help managers stay on track and alert them to potential delays .

with less than 5 months until census day , april 1 , 2010 , there is little time remaining for the bureau to deal with any unexpected problems that may arise , so early recognition of potential delays is essential .

the largest census field operation is nonresponse follow - up ( nrfu ) , where field staff visit households that have not mailed back their census forms .

the bureau will rely on a paper - based operations control system ( pbocs ) to manage this follow - up operation and other activities scheduled to be completed in the coming months .

field office managers will use pbocs to assign work to roughly 1.2 million temporary employees in about 500 offices nationwide .

among other activities , field office managers will also track response data collected by field staff from an estimated 47 million census forms from households that did not return their forms by mail .

another key part of managing upcoming census field operations is ensuring the quality of the data collected .

the quality control operation helps the bureau determine whether the first interviews held to collect census data were done correctly , look for workers who may need additional training , and identify workers who are falsifying census data .

for some operations , the quality control workload will be determined by matching and coding software that will identify cases that need follow - up and provide that case selection information to pbocs .

while the bureau developed a similar operations control system for the 2000 census and has used the core functionality of the matching software in census - like tests before , neither pbocs nor the matching software was used in a “dress rehearsal” for the decennial held in 2008 .

pbocs and its integration with the matching software has not been fully tested in a census - like environment .

because of the importance of staying on schedule to meet mandated deadlines , effectively managing field operations , and ensuring the quality of the data collected , as you requested , we examined ( 1 ) the bureau's use of scheduling tools to maintain and monitor progress and ( 2 ) the status of two systems critical to conducting field data collection: the control system the bureau will use to manage the workflow for paper - based operations including nrfu , and the system used to manage quality control of two major field operations .

to meet these objectives , we reviewed bureau planning documents , evaluations , schedules , scheduling best practices , and operational and software specifications .

we also interviewed bureau and contractor staff regarding the planning , scheduling , and status of selected areas related to field office workflow management .

to address the first objective , we examined the schedule , comparing its estimates to relevant best practices and testing its data for reliability using methods described in detail in appendix i .

to address the second objective , we compared the schedule of bureau operations to the testing schedule of systems needed to conduct those operations , examined previous evaluations of those systems , and evaluated the status of operational and software specifications .

we also reviewed past gao reports as appropriate to meet our objectives .

we conducted this performance audit from october 2008 to november 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the secretary of commerce is legally required to ( 1 ) conduct the census on april 1 of the decennial year , ( 2 ) report the state population counts to the president for purposes of congressional apportionment by december 31 of the decennial year , and ( 3 ) send population tabulations to the states for purposes of redistricting no later than april 1 of the year following census day .

the bureau has defined over 40 different operations in its high - level requirements document , describing all of the planned operations and systems needed to meet these mandates .

for the 2010 census , the bureau is using a comprehensive master schedule to integrate the work to be carried out in the dozens of operations .

the schedule provides a high - level roadmap for bureau executives and is used to alert executives to activities that are behind schedule or experiencing issues , allowing problems to be addressed so the census can continue to proceed on track .

staying on schedule is crucial to accomplishing all of the tasks involved in conducting the census .

in fact , scheduling and planning are so important that the bureau has already established a high - level schedule for planning the 2020 census .

while the schedule can be used to manage census operations at a high level and dictates major time allocations and deadlines , local census offices across the nation require more detailed plans to conduct enumeration that exceed the detail included in the master schedule .

a successful census depends , in large part , on the field work carried out in these local census offices where employees on the ground in local communities build a list of where to count people and count people who do not return their census forms .

as we have previously reported , the bureau had initially planned to carry out major field data collection activities using hand held computing devices .

development and performance problems with the hand held device led the secretary of commerce in april 2008 to abandon using the device for most of its intended operations and resulted in the bureau removing the nrfu operation from the 2008 dress rehearsal .

as a result , the bureau was not able to use the dress rehearsal as a comprehensive end - to - end test of the interoperability of all of its planned systems , and the bureau has had to develop plans to support and conduct the affected operations on paper as it did for the 2000 census .

for the 2010 census , the bureau will manage remaining fieldwork activities with pbocs .

this system is intended to provide managers with essential real - time information , such as worker productivity and completion rates for field operations .

it also allows managers in the field to assign or reassign cases among workers .

if the system does not work as intended , it could hinder or delay field operations and introduce errors into files containing collected data .

another responsibility of field offices is implementing the quality control process established by the bureau to ensure that correct information is collected by field staff and data are not falsified .

to ensure data quality and consistency of quality control procedures , the bureau will manage , track , match , and review answers provided during re - interview operations using its census matching review and coding system ( census marcs ) .

this system is also to designate quality control assignments where selected households will be re - interviewed in order to determine that the original enumerator correctly conducted the interview .

census marcs is also to assist in identifying interviews where the data from the re - interview do not match the data from the original interview , indicating that a mistake has been made .

both pbocs and census marcs are key systems that have not been fully tested .

since 2005 , we have reported concerns with the bureau's management and testing of key information technology systems .

in march 2009 , we reviewed the status of and plans for the testing of key 2010 census systems .

we reported that while the bureau has made progress in conducting systems , integration , and end - to - end testing , critical testing against baseline requirements still remained to be performed before systems would be ready to support the 2010 census and the planning for the testing needed much improvement .

while the bureau has made noteworthy progress in gearing up for the enumeration , with less than a year remaining until census day , uncertainties surround the bureau's overall readiness for 2010 .

the bureau has implemented processes around its master schedule that comply with a number of scheduling process criteria that are important to maintaining a schedule that is a useful management tool .

such a schedule can provide a road map for systematic execution of a program and the means by which to gauge progress , identify and address potential problems , and promote accountability .

we have documented the importance of adhering to these criteria and of implementing associated best practices in our gao cost estimating and assessment guide .

according to these criteria , a schedule should be comprehensive , with logically sequenced activities spanning the scope of work to be performed so that the full picture is available to managers ; current , with the progress on ongoing activities updated regularly so that managers can readily know the status of the project ; and controlled , with a documented process for changes to the schedule so that the integrity of the schedule is assured .

the bureau's master schedule represents all 44 of the operations described by the broad requirements for the census in its 2010 census operational plan .

while the bureau continues to add activities to its central schedule , by including at least all the activities described in these broad requirements , the bureau is ensuring that it has a comprehensive schedule that will be less likely to miss critical interactions between operations .

the bureau ensured a complete scope of the schedule with input from stakeholders throughout the agency , with reviews of previous schedules , and building on a number of census tests during the decade .

as a result , the schedule is the primary source for senior managers , on a weekly basis , to determine what census activity is ahead of or behind schedule and provides a resource for determining any impact to the overall project of delays in major activities .

the bureau has documented and implemented a formal process for keeping the data in the schedule current .

staff within each bureau division are responsible for ensuring that schedule activities within their division have their status updated on a weekly basis .

staff update the actual start and finish dates , the percentage of an activity completed so far , and estimates of the time remaining to complete each activity in progress .

the bureau is recording status information on an average of more than 1,300 activities in the schedule ongoing during any given week , generating historical data that could provide valuable input to future schedule estimates .

finally , the bureau has implemented a formal change control process that preserves a baseline of the schedule so that progress can be meaningfully measured .

the bureau's criteria for justifying changes are clearly documented and require approval by a team of senior managers and acknowledgment of the impact by each affected team within the bureau .

since the master schedule was baselined in may 2008 , about 300 changes have been approved .

even corrections to the schedule for known errors , such as incorrect links between activities , must be approved through the change control process , helping to ensure the integrity of the schedule .

in addition to these practices , the bureau has positioned itself to monitor the schedule regularly to help ensure that the census is progressing and that work is being completed as planned .

a central team of staff working with the schedule implements a process that begins with the weekly updates of the schedule status and involves subject matter experts from multiple divisions , and monitors and resolves schedule - related issues , resulting in a weekly briefing to the deputy director and the director of the census , which includes documented explanations for critical activities scheduled to start late .

for example , the central team began reporting from the master schedule in march 2009 that the printing of questionnaires for a field operation to validate locations of group quarters in september and october 2009 might be running late .

the schedule showed that late printing of the questionnaires would trigger their late delivery and the late assembly of job assistance kits needed to support the operation , and thus put the timeliness of the operation in danger .

according to a bureau official , the bureau then addressed the issue by deciding to unlink the kit assembly from the questionnaire printing , allowing kits to begin assembly on time and having questionnaires delivered directly to field offices when they were ready , letting the operation begin on time .

when we began analyzing the bureau's master schedule , we discovered a significant number of activities in the schedule that had either missing or inaccurate information describing their relationships with other activities in the schedule .

we brought these to the bureau's attention , and the bureau has begun systematically identifying such activities and correcting their information in the schedule .

in accordance with scheduling best practices , activities in the schedule should be linked logically with relationships to other activities that precede or follow them , and they should be linked in the correct order .

since reports that the bureau uses to manage the census depend on the schedule having been built properly , inconsistent adherence to these scheduling practices has occasionally created false alarms about the schedule and created unnecessary work for those who have had to resolve them .

in our analysis of the bureau's schedule , we found that nearly all relationships between activities are generally in place in the schedule and some activities in the schedule do not need relationships .

however , many activities appeared in the schedule missing one of their logical relationships .

from january 2009 through august , an average of more than 1,200 of the more than 11,000 activities in the entire schedule were missing relationships to other activities from either their start or end dates .

each month , on average , over 1,100 of the over 6,100 as yet not completed activities were missing relationships .

for example , within the bureau's master schedule , an activity listed for receiving finished materials from the nrfu re - interview operation appeared in the schedule with no relationship to subsequent activities , making it appear that any delays in its completion would have no impact on subsequent census activities .

while the absence of such a relationship in the schedule does not imply that the bureau would miss the potential impact of any delays in the completion of this activity , the incidence of a large number of such missing relationships can confound attempts to trace the chain of impacts that any delays may have throughout the schedule .

similarly , we found a small number of activities in the schedule that had been linked together in the wrong order , so that one activity might appear to finish before a necessary prior activity had been completed .

such an incorrect relationship can unnecessarily complicate the use of the schedule to guide work or measure progress .

the number of such apparent out - of - sequence activities in the entire schedule has decreased from on average more than 100 each month in january through march to 60 in august .

since june 2009 , bureau staff have been running structured queries on the data supporting the master schedule to identify activities with missing or incorrect data ; researching each activity to determine what , if any , corrections to the data are needed ; forwarding proposed changes to affected activities , operation by operation , to program officials for review ; and submitting changes to the bureau's formal change control process .

the bureau reports that since this concerted effort began to correct such errors that affect activities in 37 different census operations , the bureau had completed research for activities in 15 of the operations and approved changes in 12 of them by early october 2009 .

the bureau also informed us that this review process involving the program officials responsible for the logic errors has provided an educational opportunity for the officials to see how their programs can directly affect others , and as a result has heightened awareness about the importance of getting schedule information keyed in correctly .

a schedule provides an estimate of how long a given work plan will take to complete .

since the duration of the work described by the activities listed in a schedule is generally uncertain , a schedule can be analyzed for the amount of risk that its underlying work plan is exposed to .

schedule risk analysis — the systematic analysis of the impact of a variety of “what if” scenarios — is an established best practice to help identify areas of a schedule that need additional management attention .

conducting a schedule risk analysis helps establish the level of confidence in meeting scheduled completion dates and the amount of contingency time needed for given levels of confidence , and helps identify high - priority risks to a schedule .

the bureau is tracking risks to the census and managing those risks on a regular basis , as documented in the 2010 census risk management plan , but these data are not being mapped into the schedule at a level that can be used for a systematic schedule risk analysis .

a well - defined schedule should help identify the amount of human capital and financial resources that are needed to execute the programs within the scope of the schedule , providing a real - time link between time and cost and helping to reduce uncertainty in cost estimates and the risk of cost overruns .

however , the bureau does not link within its schedule estimates of resource requirements — such as labor hours and materials — to respective activities .

having this information linked in a schedule enhances an organization's capability to monitor , manage , and understand resource productivity ; plan for the availability of required resources ; and understand and report cost and staffing requirements .

for example , if the bureau were to find itself behind schedule with major operations to be completed , and resource requirements were linked in the schedule , the bureau could then better assess the trade - offs between either adding more resources or reducing the scope of the operations .

in addition , when resources are linked to activities in the schedule , scheduling tools can identify periods of their peak usage and assist managers with reordering activities to level out demands on potentially scarce or costly resources .

when we met with bureau officials and discussed this , they pointed out that incorporating this schedule best practice would be difficult to do late in the preparations for 2010 , but they expressed interest in incorporating this schedule best practice as a step forward in the bureau's use of the schedule to manage decennial censuses .

finally , the bureau's use of a master schedule in 2010 that is , according to the bureau , more highly integrated into the management of the decennial census provides an opportunity to draw many potential lessons for 2020 .

the bureau learned lessons from its use of the 2000 master schedule as documented in a 2003 bureau management evaluation , in particular with its adoption of the formal change control process implemented for 2010 .

yet as noted in the evaluation , there were questions about the quality of the data maintained in the schedule .

without a reliable change control process , the schedule did not provide a reliable baseline , making evaluation of schedule and activity duration estimates difficult , if not impossible .

the bureau is generating a large amount of data — and experience — with its efforts in developing , maintaining , and using the 2010 master schedule .

unless the bureau prioritizes the need for documenting lessons learned from the current experience — as it did for the 2000 census — and formally puts in place an effort to capture and analyze schedule data , changes to baselines , and variances between estimated and actual durations , it runs the risk of missing out on another opportunity for using additional lessons some of its staff may already be learning .

the automated control system that the bureau plans to use to help manage the data collection operations of the decennial census still faces significant development and testing milestones , some of which are scheduled to be completed just before the system needs to be deployed before respective field operations begin .

as a result , should the bureau encounter any significant problems during final testing , there will be little time to make changes before systems are needed to support field operations .

pbocs will help manage both paper , and people , and it needs to exchange data successfully with several other bureau systems , such as one used for processing payroll .

the bureau plans to complete development and testing of pbocs in three major releases , grouping the releases of parts of pbocs together loosely by the timing of the field operations those parts are needed to support .

the bureau already completed a preliminary release of pbocs with limited functionality in june 2009 to support some initial testing .

figure 1 shows the development , testing , and operation periods for the three remaining releases and the operations that pbocs supports .

according to the baseline of the bureau's master schedule , pbocs should be deployed and operational anywhere from 1 to 6 weeks before each operation begins for operations leading up to and including nrfu .

according to the bureau , the system should ideally be ready for use during training periods so that managers can familiarize themselves with the system they will have to use and can begin using the system to assign work to new staff .

this also requires that pbocs be ready in time so that production data can be loaded into the system , as well as information about the employees to whom work will be assigned .

for example , pbocs for nrfu is to finish its final testing in march 2010 , about 9 weeks before nrfu is scheduled to begin on may 1 , 2010 , and deployment is scheduled to take place about 6 weeks before nrfu starts , leaving 3 weeks of contingency time in the event that unexpected problems arise during pbocs development .

this means that if any significant problems are identified during the testing phases of pbocs , there is generally little time to resolve the problems before the system needs to be deployed .

in addition , it will be more difficult for the bureau to integrate into pbocs training for users on any late changes in the pbocs software .

while the bureau relies on last - minute additions to training and procedures documents to communicate late changes to workers , bureau officials agreed that it can be difficult to incorporate such last - minute additions into training sessions and for users to learn them , and doing so should be avoided if possible .

the bureau also faces the significant challenge of developing the detailed specifications for the software to be developed .

as of early - september 2009 , the bureau had established high - level requirements for pbocs and has reported completing development of release 1 of pbocs .

the bureau reports that as of late - october its requirements development , system development , and system testing for phase 1 is largely completed .

however , the bureau has not yet finalized the detailed requirements for this release or for later releases .

high - level requirements describe in general terms what functions the system will accomplish , such as producing specific management reports on the progress of specific paper - based operations .

detailed requirements describe more specifically what needs to be done in order to accomplish such functions .

for example , a detailed requirement would specify the specific data that should be pulled from the specific data set to produce specific columns of a specific report .

while high - level requirements provide software programmers with general guidelines on , for example , what types of reports should be produced , without a clear understanding of the detailed requirements , the programmers cannot be sure that they are identifying the correct source of information for producing such reports , and reports can thus be inaccurate .

according to bureau officials , previous contract programmers with little decennial census experience and no involvement with current development efforts made erroneous assumptions about which data to use when preparing some quality control reports that became problematic in the dress rehearsal .

without detailed requirements , the bureau also cannot be sure how frequently such reports should be updated or which staff should have access to which reports .

further , software developers may not have the required information to meet the bureau's needs .

also , as we have previously reported , detailed operational requirements determine system development , and without well - defined requirements , systems are at risk of cost increases , schedule delays , or performance shortfalls .

as we have reported and testified numerous times , the bureau experienced this with an earlier contract to automate the support of its field data collection activity , which included the failed handheld computing device .

the bureau's pbocs development managers have told us that they are working closely with stakeholders in an iterative process of short development cycles to help mitigate pbocs development risks caused by not having detailed requirements written in advance .

embedding subject matter experts within the software development process can help mitigate risk inherent in the short time frame the bureau has remaining to develop and test pbocs .

yet the absence of well - documented and prioritized detailed requirements for pbocs , which still need to be developed and tested , remains among the most significant risks to getting pbocs ready on time .

furthermore , the bureau lacks reliable development progress measures that permit estimating which requirements may not get addressed and that are important to ensuring the visibility of the development program to bureau leadership .

aggressive monitoring of system development and testing progress and of the effort remaining will help ensure that program officials who will rely on these systems can anticipate what risks they face and what mitigation activities they may need for shortfalls in the final systems .

in recognition of the serious implications that a failed pbocs would have for the conduct of the 2010 census , and to see whether there were additional steps that could be taken to mitigate the outstanding risks to successful pbocs development and testing , in june 2009 the bureau chartered an assessment of pbocs .

the assessment team , chaired by the bureau's chief information officer ( cio ) , reported initially in late july 2009 and provided an update report in late august 2009 .

according to the august update and our discussion of it with the cio , the team increased its risk rating in two areas of pbocs that it is monitoring in part because of the absence of fully documented requirements , testing plans , progress measures , and deployment plans .

in its comments on the draft of this report , the department of commerce provided information describing several steps the bureau was taking to monitor the progress of pbocs development .

according to commerce , the bureau already does the following: daily project management standup meetings , which cover action item management , calendar review , activity sequencing , and any threats or action - blocking issues .

daily architecture review board and team leads meetings .

weekly program management review board meetings , and thrice - weekly product architecture review board meetings .

at least weekly review of progress by the pbocs internal assessment team — chaired by the cio .

the team briefs the bureau director at least monthly .

monthly quality assurance board meetings and twice - monthly risk review board meetings .

at the end of our review , the pbocs development team demonstrated two software tools it said it was using to help manage its iterative process of short development cycles .

we did not fully assess their use of the tools .

the progress measures they demonstrated with one of the tools predicted that the completion dates would be missed .

it also showed that the development team was underestimating the development effort required to achieve its iterative development goals .

when we noted this , the presenters told us that the information in their management system was not current .

until the bureau completes the detailed requirements for pbocs and prioritizes them and its pbocs development monitoring is relying on current and reliable progress measures , such as those the development team attempted to demonstrate to us for estimating the effort needed to complete remaining development , it will not be able to fully gauge the pbocs development progress and have reasonable assurance that pbocs will meet the program's needs .

the bureau is continuing to examine how improvements will be made .

the bureau has experienced delays in the development and testing of software that will play a key role along with pbocs in controlling and managing field data collection activity for the quality assurance programs of nrfu and update / enumerate .

census marcs will help manage the process of identifying systematic or regular violations in the door - to - door data collection procedures .

in particular , census marcs will be a tool to help target additional households needing reinterview as part of the quality assurance program for these two major census data collection operations in the field .

therefore , fully developing and testing census marcs will be important to the successful conduct of the census .

like other systems at the bureau , census marcs had to undergo design changes when the bureau made the april 2008 decision to switch to paper - based operations .

detailed performance requirements — such as the information to be included on reports and its sources , including performance metrics , such as the number of users the system is designed to handle — are documented and were baselined in may 2009 .

however , specifications have been added or clarified as software development has progressed and improvements have been suggested .

software development has at times been slower than expected , leading to delays in some testing .

test plans for census marcs software and interfaces are in place , having been documented in may 2009 .

according to those plans , the bureau is in the second of three stages of testing census marcs and is scheduled to complete its final stage in december 2009 , almost 2 months before its first deployment for the update / enumerate operation .

slower - than - expected development has delayed some parts of the second phase of testing , which will thus finish late according to the bureau , but bureau officials have indicated that they believe that delay can be absorbed into the schedule , and that the system will be delivered as scheduled in february 2010 .

the compressed testing schedule leaves little time for additional delays in writing software or conducting tests .

the bureau is working on additional plans to test the interfaces between systems like pbocs and census marcs to ensure that they work together , but those test plans have not yet been finalized .

since census marcs was not used in the dress rehearsal , and a full end - to - end system test — that is , a test of whether all interrelated systems collectively work together as intended in an operational environment — is not planned for in the time remaining before the system is required to be deployed , successful testing of the interfaces with other systems is critical to the system's readiness .

if development or testing delays persist , it will be more important than ever that system requirements be prioritized so that effort is spent on the “must haves” necessary for system operation .

our review of the bureau's master schedule for conducting the 2010 census and the processes the bureau uses to manage it suggests that it is doing a commendable job conducting such a large and complex undertaking consistent with leading scheduling practices .

furthermore , the bureau's systematic effort to correct errors that we have identified in the schedule will further improve the ability of the master schedule to support senior management oversight and decision making as 2010 approaches .

other improvements , such as embedding estimates of resource needs into the schedule , may take more time to implement .

yet , the bureau's generally well - defined and integrated schedule provides an essential road map for the systematic execution of the census and the means by which to gauge progress , identify and address potential problems , and promote accountability .

leveraging the bureau's experience with scheduling for 2010 by documenting it should provide lessons learned for similar efforts in 2020 as well .

moreover , since we testified on the status of the 2010 census in march 2009 , the bureau has made progress on a number of key elements needed to manage the work flow in field operations .

in particular , the bureau has made progress in developing and testing systems to support paper - based operations in the wake of the secretary of commerce's april 2008 decision to switch to paper - based operations for most field data collection activity .

that said , some delays are also occurring , and since so much still remains to be done in the months leading up to census day , the bureau has limited time to fix any potential problems that arise in systems that are not thoroughly tested .

while the bureau has made significant progress in developing test schedules for key systems , careful monitoring of the progress in addressing , and setting priorities among , the remaining detailed requirements for the control system supporting paper - based operations is critical for the bureau to anticipate what risks it faces and mitigations it may need for shortfalls in the final system .

based on the challenges faced in the earlier program implementing handheld computing devices , the bureau has already experienced the ill effect of having to change its plans when a system does not fully meet planned program needs .

with limited time before implementation , it is uncertain whether the bureau may be able to complete development and fully test all key aspects of its systems , like pbocs and census marcs , which are still under development .

continued aggressive monitoring by the bureau and improvements in the progress measures on system development and testing , of effort remaining , and of the risks relating to these efforts is needed .

such effort will help ensure that bureau leadership , as well as bureau program officials who will rely on these systems , have early warning on what , if any , desired system features will be unavailable in the final systems , maximizing time available to implement mitigation strategies as needed .

we recommend that the secretary of commerce require the director of the u.s. census bureau to take the following three actions: to improve the bureau's use of its master schedule to manage the 2020 decennial census: include estimates of the resources , such as labor , materials , and overhead costs , in the 2020 integrated schedule for each activity as the schedule is built , and prepare to carry out other steps as necessary to conduct systematic schedule risk analyses on the 2020 schedule .

take steps necessary to evaluate the accuracy of the bureau's baselined schedule and determine what improvements to the bureau's schedule development and management processes can be made for 2020 .

to improve the bureau's ability to manage paper - based field operations in the 2010 decennial census , finalize and prioritize detailed requirements and implement reliable progress reporting on the development of the paper - based operations control system , including estimates of effort needed to complete remaining development .

the secretary of commerce provided written comments on a draft of this report on november 3 , 2009 .

the comments are reprinted in appendix ii .

commerce did not comment on the first two recommendations , but provided additional information on steps it had already been taking to monitor progress of pbocs development related to the third recommendation .

commerce commented on how we characterized the status of system development and testing , and provided additional statements about the status .

commerce also made some suggestions where additional context or clarification was needed , and where appropriate we made those changes .

with respect to our third recommendation to finalize and prioritize detailed requirements and implement reliable progress reporting on pbocs , including estimates of effort needed to complete remaining development , commerce described numerous regular meetings that the development team holds , as well as efforts by others in the bureau to monitor and report on pbocs development progress .

we added references to these monitoring efforts within the report .

we agree that the monitoring efforts the department describes can help assess the progress being made ; however , the monitoring efforts can only assess the progress being reported to them .

first , a complete set of requirements is needed to understand the work that has been accomplished and the work remaining .

as we have noted , the bureau has not yet developed a full set of requirements .

second , the management tool's measurement of the development activity successfully addressing the requirements is directly related to the effectiveness of test cases developed for those requirements .

however , as we have previously noted , if a requirement has not been adequately defined , it is unlikely that a test will discover a defect .

accordingly , until the bureau completes the detailed requirements for pbocs and prioritizes them it is unable to use these tools to fully gauge its progress toward meeting the overall project's goals and objectives of system development .

we clarified our discussion of this in the report and reworded the recommendation to better focus on the need for reliable information .

commerce maintained that our draft report implied that no pbocs development had begun and that no testing would be completed until march 2010 .

the draft report described development and testing dates that clearly illustrate that both development and testing have been occurring over several months leading up to their conclusion .

we have included additional language in the text to clarify that development and testing take place over a period of time .

we have also included a statement from commerce that much of this activity has largely been completed for the first of three phases .

commerce commented on the accuracy of the dates used in the figures in the draft report .

we verified that the dates we used were the correct dates that the bureau had provided to us earlier .

we made minor adjustments to some of the gridlines in the graphic for presentation purposes .

the bureau also provided additional information on the prior testing of matching software , the context for nrfu being dropped from the dress rehearsal , how pbocs errors could potentially introduce errors into census files , and contract programmers we reported being involved in dress rehearsal pbocs not being the same ones helping the bureau with pbocs development now .

we revised the report as appropriate in response .

commerce commented on our discussion of the unreliability of information that the bureau pbocs development team provided us during a demonstration of two tools that it uses to help manage its iterative process of short development cycles .

commerce described the use of one of the two tools but not the one whose progress tracking measures were demonstrated to us and for which the team told us that data were not current .

we have revised the text to more clearly state that more than one tool was used to demonstrate how development and testing was being managed by the pbocs development team , and we have added additional language to describe the progress information that should have been current but that was not .

finally , as we had noted in the draft report , the bureau has already begun taking action to address errors we had identified in its master schedule .

since we sent a draft of this report to commerce , the bureau provided additional information on the status of that effort , and we have updated this report accordingly .

we are sending copies of this report to the secretary of commerce , the director of the u.s. census bureau , and interested congressional committees .

the report also is available at no charge on gao's web site at http: / / www.gao.gov .

if you have any questions about this report please contact me at ( 202 ) 512- 2757 or goldenkoffr@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

we reviewed the u.s. census bureau ( bureau ) program's schedule estimates and compared them with relevant best practices in gao cost estimating and assessment guide: best practices for developing and managing capital program cost to determine the extent to which they reflect key practices that are fundamental to having a reliable schedule .

these practices address whether the schedule is comprehensive , with logically sequenced activities spanning the scope of work to be performed so that the full picture is available to managers ; current , with progress on ongoing activities updated regularly so that managers can readily know the status of the project ; and controlled , with a documented process for changes to the schedule so that the integrity of the schedule is ensured .

in doing so , we independently assessed a copy of the program's integrated master schedule and its underlying schedules against our best practices .

we also interviewed knowledgeable program officials to discuss their use of best practices in creating the program's current schedule and we attended a schedule walk - through to better understand how the schedule was constructed and maintained .

we tested the bureau's schedule data for reliability by running a schedule check report in pertmaster which is a scheduling analysis software tool that identifies missing logic , constraints , and so forth ; using the schedule information from pertmaster , copying the schedule data into excel , and checking for specific problems that could hinder the schedule's ability to dynamically respond to changes ; examining whether there were any open - ended activities ( i.e. , activities with no predecessors , successors , or both ) ; searching for activities with poor logic ; identifying whether there were any lags or leads that should only be used to show how two tasks interact and not to represent work ; determining if activities were resource loaded , which helps to cost out the schedule and examine whether resources are overstretched or not available when needed ; examining whether the schedule was baselined , when it had its status updated , and what deviations there were from the plan ; and examining if there were any actual start or finish dates recorded in the future and whether there was any broken logic .

in addition to the contact named above , ty mitchell , assistant director ; virginia chanley ; vijay d'souza ; jason lee ; andrea levine ; donna miller ; crystal robinson ; jessica thomsen ; jonathon ticehurst ; and katherine wulff made key contributions to this report .

2010 census: census bureau continues to make progress in mitigating risks to a successful enumeration , but still faces various challenges .

gao - 10-132t .

washington , d.c.: october 7 , 2009 .

2010 census: fundamental building blocks of a successful enumeration face challenges .

gao - 09-430t .

washington , d.c.: march 5 , 2009 .

information technology: census bureau testing of 2010 decennial systems can be strengthened .

gao - 09-262 .

washington , d.c.: march 5 , 2009 .

census 2010: census bureau's decision to continue with handheld computers for address canvassing makes planning and testing critical .

gao - 08-936 .

washington , d.c.: july 31 , 2008 .

census 2010: census at critical juncture for implementing risk reduction strategies .

gao - 08-659t .

washington , d.c.: april 9 , 2008 .

information technology: census bureau needs to improve its risk management of decennial systems .

gao - 08-79 .

washington , d.c.: october 5 , 2007 .

2010 census: basic design has potential , but remaining challenges need prompt resolution .

gao - 05-9 .

washington , d.c.: january 12 , 2005 .

