the department of veterans affairs ( va ) provided care to about 6.9 million veterans and obligated about $65 billion for their care in fiscal year 2016 .

the majority of veterans utilizing va health care services receive care in va - operated medical facilities , including 170 va medical centers ( vamcs ) as of june 2017 .

comprehensive assessments of va health care have found that vamcs vary significantly in the quality of their care , just as non - va operated hospitals do .

as a result , the ability of veterans to make informed choices among va and non - va medical facilities depends in part on having access to valid , accurate , and understandable information on the relative quality of the care at these facilities .

congress passed the veterans access , choice , and accountability act of 2014 ( choice act ) to help ensure , among other goals , greater accountability and transparency within the va health care system .

section 206 of the choice act established requirements for va to publicly report information on the performance of vamcs on various health care quality measures , both on the department of health and human services' ( hhs ) hospital compare website and on va's own website .

hospital compare publicly posts health care quality measures for over 4,000 non - va hospitals that participate in medicare , which enables veterans and others the opportunity to compare the performance of non - va hospitals and vamcs on a common set of quality measures .

va also separately reports on its own website information on the performance of vamcs on a range of health care quality measures .

such information can assist veterans in making their health care decisions , particularly if that information is presented in a way that veterans can locate and understand and if the information is relevant for making informed health care decisions .

the choice act included a provision for gao to assess va's implementation of requirements for publicly reporting health care quality measures on hospital compare and va's own website .

in this report , we describe the health care quality measures va reports on hospital compare and its own website ; evaluate va's reporting of health care quality measures on its website , including the information's presentation and relevance to veterans ; and examine the extent to which va has assessed the accuracy of the health care quality measures it publicly reports .

to describe the health care quality measures va reports on hospital compare and its own website , we reviewed the quality measures that va central office — which has responsibility for va's public reporting of information on quality of care — has reported on hospital compare and va's own website as of june 2017 .

we also reviewed va documents describing the quality measures and interviewed va central office officials to understand the rationale behind va's decisions to report certain quality measures , and we examined va's plans to expand or adjust its public reporting of quality measures in the future .

specifically , we interviewed officials from entities within va's central office that are responsible for calculating and reporting the quality measures , including the office of reporting , analytics , performance , improvement & deployment .

we also spoke with hhs officials responsible for maintaining the hospital compare website about the measures on the website and the processes used to receive information from va. to evaluate va's reporting of health care quality measures on its website , including the information's presentation and relevance , we used 13 criteria to assess the webpages on va's website that contain information on quality measures .

we previously identified these criteria for assessing the effectiveness of websites that provide consumers with comparative information on the quality of care delivered by different health care providers .

six of the 13 criteria examine whether the information is presented in a way that enables the consumer to locate and interpret it and the remaining 7 criteria address the extent to which a website provides relevant , substantive information to consumers in making informed health care decisions .

additionally , we spoke with representatives of three veterans service organizations ( vso ) — the american legion , the veterans of foreign wars , and disabled american veterans — to obtain their perspectives about the presentation and relevance of the quality measures reported on va's website .

finally , we asked two external experts to review and comment on our assessments of the presentation and relevance of va's publicly reported quality measures on its website , and we incorporated their input as appropriate .

to examine the extent to which va has assessed the accuracy of the health care quality measures it publicly reports , we reviewed va documents related to health care clinical information management and interviewed va central office officials about the clinical information used to calculate vamcs' performance on these measures .

specifically , we interviewed va central office officials responsible for reporting of quality measures and for managing the process of recording clinical information .

we also examined an evaluation by outside experts who conducted an independent assessment of the processes that va has implemented to ensure the completeness and accuracy of the clinical information recorded in patients' medical records at vamcs .

further , we reviewed investigations of va's clinical documentation practices by the va office of inspector general and the report and recommendations the commission on care issued in 2016 .

we assessed va central office's efforts in the context of federal standards for internal control for information and monitoring .

we conducted this performance audit from august 2016 to september 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

health care quality measures are standard , evidence - based metrics designed to assess the performance of health care providers , such as hospitals , in providing care .

these measures are intended to ( 1 ) inform providers about opportunities for potential improvements in their delivery of care , ( 2 ) incentivize providers to consistently provide high quality care , and ( 3 ) inform consumers about which providers are most likely to deliver high quality care .

there are broad categories of clinical quality measures that address various aspects of quality of care .

see table 1 for a description of these broad categories of quality measures .

these broad measure categories can be further broken down into more specific groups of related measures .

for example , outcome measures can include measures of patient safety , such as the incidence of healthcare - associated infections ( hai ) or complications , as well as measures of hospital readmissions and mortality , and results obtained from ambulatory care , such as the proportion of patients with hypertension whose blood pressure is reduced to the normal range .

the data used to calculate the results of health care quality measures can come from a number of different sources .

some measures often require detailed clinical information obtained from patient medical records , such as process measures that indicate whether timely and effective care was provided in a specific situation , for example , or whether stroke patients received clot - dissolving medication appropriately .

other measures are designed to use information on patient demographics and diagnoses that can be obtained from more readily accessible sources , such as claims data or other administrative data that have already been collected for other purposes such as billing .

in addition , patients can be asked directly , usually through surveys , to report on their experiences receiving care .

one key method for disseminating information on health care quality measures to consumers — including which providers are delivering high or low quality care and the costs of care — is through websites that can convey this information to anyone with internet access .

hhs is one of the organizations that provide such information to consumers and others .

specifically , hhs's centers for medicare & medicaid services ( cms ) maintains a series of websites that provide information on health care quality , including a series of websites for hospitals , nursing homes , and certain other providers that participate in the medicare program .

since 2005 , cms has increased the number of health care quality measures it posts on one of its websites , known as hospital compare , that covers more than 4,000 hospitals that participate in the medicare program .

these hospitals supply data to cms for quality measures of inpatient and outpatient care in return for higher payments on their medicare claims .

each year hhs goes through a formal process , including receiving input from experts and stakeholders , to review and revise the mix of quality measures that these hospitals report .

the hospital compare website allows anyone with internet access to select up to three hospitals to compare their performance on each of these measures side - by - side .

cms uses contractors to collect and process the data submitted by individual hospitals , and it posts the results on each of the quality measures on the hospital compare website .

in implementing a requirement under the choice act , va reports some of the 79 possible health care quality measures reported by non - va hospitals on hhs's hospital compare website .

as of june 2017 , va was reporting 35 measures that it had determined were applicable to its individual medical centers .

of the remaining 44 measures that non - va hospitals report on hospital compare , va plans on reporting on 12 measures in future years and does not plan to report 32 measures .

see table 2 for a summary of va's reporting of quality measures on hospital compare .

the 35 measures va reported on hospital compare as of june 2017 include all of the available patient experience measures ( eg , patient perspectives on how well physicians communicated with them ) , most of the process measures ( eg , whether stroke patients received appropriate clot - dissolving drugs ) , and some of the outcome measures ( eg , whether pneumonia patients were readmitted to the hospital within 30 days ) .

 ( see app .

i for more detailed information on the specific measures va reports on hospital compare. ) .

va officials told us that when they began to report quality measures to hospital compare in 2010 , the measures they first decided to report included measures that va was already reporting for other purposes .

for example , these included process measures of timely and effective inpatient care that va reported for joint commission hospital accreditation .

according to va officials , choosing quality measures it already reported for other purposes minimized the additional resources needed to report the measures on hospital compare .

from 2017 through 2019 , va plans on reporting an additional 11 outcome measures on hospital compare .

these include 6 measures of various hais at vamcs , 4 measures of mortality and readmission rates associated with additional medical conditions and procedures , such as stroke and hip / knee replacement surgery , and 1 measure related to patient safety .

specifically: regarding the 6 additional hai measures , va officials told us that they will report these measures when they can develop a new data collection process that will allow them to meet the requirements for reporting hai measures in a way that minimizes demands on va resources .

according to va officials , complying with the existing process for collecting data for and reporting on hais would require infection control staff at each vamc to fill out forms with information on individual patients , whereas va currently collects its own information on hais based on aggregated data assembled by va central office staff .

va officials told us that their new data collection process for reporting hai measures is intended to meet those hospital compare requirements through automating much of the required data entry at the vamcs .

however , va officials noted that this new reporting process is still in an early stage of development , and they expressed uncertainty about how long it would take to implement the process .

regarding the 4 additional mortality and readmission measures , cms faces challenges in integrating va's clinical information into the hospital compare database .

according to both va and cms officials , va has had to implement new readmissions and mortality rate measures incrementally due to the limited capacity of one of cms's hospital compare contractors to develop the programming needed to calculate these measures using va patient data .

because the readmissions and mortality rate measures require complex programming to implement risk adjustments , based on a number of different diagnoses recorded in patient medical records over time , va and cms officials agreed to add only two new measures each year until va can report all 4 measures .

there are another 32 measures non - va hospitals report on the hospital compare website that va has no plans to report .

va officials determined that these measures are not relevant for va's health care system , given its distinctive funding sources , patient population , and health care delivery structure .

for example , va officials stated that they did not plan to report any of the hospital compare measures related to costs of care , such as medicare spending per beneficiary , because those measures are based on medicare payments and va does not receive any payments from medicare .

in other cases , va officials do not plan on reporting measures that relate to health care services that vamcs rarely , if ever , provide , such as obstetrics and hip and knee replacements .

in implementing a requirement under the choice act , va has posted on the hospital compare website a link to the notification on the quality measures va is not reporting .

the most recent notice , dated february 2015 , broadly explains that va faces three main challenges that affect the availability of certain measures .

according to the notice , these challenges relate to data quality , standard data collection processes , and funding to support the collecting and reporting of quality measures .

va officials told us that they plan on adding to the notification information about the specific measures va expects to report in the future and when it expects to report them .

as of june 2017 , va reported on its own website 110 health care quality measures for its vamcs .

these include measures in many of the same categories that va reports on hospital compare , but they also include several additional categories of quality measures that are not available on hospital compare .

these additional categories address quality issues va officials deem relevant for veterans , such as various measures of access to care .

for example , the additional measures include measures of how long veterans must wait to obtain care at vamcs and measures of the quality of care related to ambulatory care , such as colorectal cancer screening rates .

see table 3 for the categories of measures va reports on its website .

some of the specific measures reported on va's website are the same as those reported on hospital compare and some are entirely different .

for example , most of the patient experience measures reported on va's website are the same as those reported on hospital compare , while the access to care measures are only reported on va's website .

in addition , some measures reported on va's website are similar to , but not exactly the same as , hospital compare measures .

for example , among similar measures , the target population may be defined somewhat differently or the result may be calculated differently .

 ( see app .

i for more information on the specific measures va reports on its website. ) .

in implementing a requirement under the choice act , va has also posted a notification of unavailable measures on its website .

this is the same february 2015 notice va provided on the hospital compare website .

va publicly reports 110 health care quality measures on two separate webpages on its website — the “access and quality” webpage , launched in april 2017 , and the “quality of care” webpage , which va has used since 2008 .

the access and quality webpage is intended to be the primary source of information on quality of care at va for veterans , according to va officials .

va officials also told us that this webpage was developed to present quality of care information in ways that are easy for veterans and other stakeholders to understand .

a link to the primary access and quality webpage can be found on the homepage of va's website , making it relatively easy to find .

va officials also told us that they retained the older quality of care webpage in an effort to be transparent as well as provide historical data on the many quality measures it has tracked .

the older quality of care webpage is not linked to the homepage of va's website .

additionally , neither the primary access and quality webpage nor the older quality of care webpage provides any link or makes any mention of the other .

see fig .

1 and fig .

2 , which show the primary access and quality and older quality of care webpages , respectively .

on the two webpages , va reports its 110 health care quality measures across various subpages , with some measures reported on multiple subpages .

specifically , as of june , 2017: va reported 15 of its 110 quality measures on the primary access and quality webpage , which comprises three subpages .

two subpages focus on measures related to access ( which we refer to as the “wait times” and “experience with access” subpages ) and the third subpage compares how vamcs perform relative to non - va hospitals in their geographic area on a few selected hospital compare measures ( which we refer to as the “non - va hospital comparison” subpage ) .

according to va officials , the 15 quality of care measures on the primary access and quality webpage were selected because they provide information that is useful to veterans in making health care choices .

on the older quality of care webpage , which comprises four subpages , va reported 100 quality measures .

according to va officials , each of the four subpages of this webpage was created for a specific purpose and as a result , some measures are reported on multiple subpages .

we also found that va reported five of the same quality measures on both the primary and older webpages , which include measures reflecting patient ratings of their experience in the hospital and healthcare - associated infection rates .

va officials told us that they do not plan on consolidating the information currently reported on the two webpages since they serve different purposes , as explained earlier .

see table 4 for a summary of the health care quality measures reported on va's access and quality and quality of care webpages .

we found that va's primary access and quality webpage generally reports health care quality of care information in ways that are more accessible and understandable than va's older quality of care webpage .

we assessed both webpages using criteria we identified in prior work for evaluating how well websites present information on health care quality to the public .

specifically , we found that va's primary webpage meets four of six presentation criteria compared with va's older webpage , which met none of the criteria .

 ( see table 5. ) .

we found that va's primary webpage does especially well at presenting information on quality of care on the two subpages that focus on access to care .

in particular , these subpages: provide information written in plain language with accompanying enable consumers to customize the information presented , so that they can , for example , select the types of medical appointments of interest to them ; and allow users to rank order vamcs by level of performance on a given quality measure .

in contrast with va's primary webpage , we found that va's older quality of care webpage displays quality measures in ways that generally do not meet any of the presentation criteria .

in particular , we noted that the information presented across the four subpages on the quality of care webpage shares the following key limitations: none are written in plain language or use graphics to convey key none summarize related quality information or organize the data to show patterns , such as rank ordering vamcs on a given performance measure ; none enables comparison of multiple vamcs in one view , but instead requires users to look up information on vamcs one at a time ; none enables customization of how the information is displayed so that users can focus on the quality measures most relevant to them ; and none are advertised to potential users , with no indication provided on the va website homepage that the subpages exist , what information they provide , and where to find them .

representatives from two of the three vsos we spoke with said that the information on the quality of care webpage is not utilized by veterans because links to the webpage are not prominently displayed on the va website .

according to one vso official we spoke with , when va sought feedback from the veterans service organizations about its overall plans for its primary access and quality webpage , the official found that the main advantage of this webpage is that it presents information in a more visually compelling way that allows veterans to directly compare quality information for multiple vamcs .

according to officials from the three vsos with whom we spoke , this capability is important because veterans can be overwhelmed by information that is not conveyed in an easily digestible format .

we also assessed va's webpages using criteria we identified in prior work for evaluating the extent to which websites provide consumers with information relevant for making health care decisions .

using these criteria , we found that va's primary access and quality webpage does not provide veterans as much relevant information as va's older quality of care webpage .

our analysis shows that va's primary webpage meets only two of the seven relevance criteria .

in contrast , we found that va's older webpage performs relatively well on the relevance criteria — meeting six of the seven criteria .

 ( see table 6. ) .

as table 6 shows , va's primary webpage does not provide information on a broad range of health care services , nor does it highlight key differences in clinical quality of care .

for example , the primary webpage reports only on the incidence of two types of hais and not on any other types of outcomes , such as readmissions and mortality rates associated with different medical conditions .

additionally , va's primary webpage only provides limited information about key differences in patient experiences and the key strengths and limitations of the data reported .

although va officials told us that they expect to expand the number of reported measures in future and intend to make the webpage more comprehensive , they did not identify specific measures to be implemented .

in contrast , va's quality of care webpage performs better on the relevance criteria primarily because its four subpages together provide information on 100 different measures spread across a broader range of measure categories ( see tables 4 and 6 ) .

as a result , it provides information on a broad range of services and on key differences in clinical quality of care .

furthermore , representatives of two veterans service organizations told us that much of the information reported on va's older webpage would be relevant to veterans , including rates of hospital readmissions , mortality , and complications — none of which are included on va's primary webpage .

while va has improved how it presents information on quality to veterans through its primary access and quality webpage , there are still gaps in the relevance of information reported on the webpage .

specifically , while it may not be necessary for va to report on all 110 measures on its primary webpage to meet the relevance criteria , the 15 measures reported on the access and quality webpage ( of which 10 focus on access ) clearly do not provide the same breadth of relevant information relevant for veterans .

va officials told us that they began with these 15 quality measures that they deemed most useful to veterans for making health care choices , and in the short term , they are focused on improving the user experience of the webpage .

however , until va's website provides information on a broader range of health care services , highlights key differences in clinical quality of care , and reports this information in a manner that is easily accessible and understandable , va is missing an opportunity to provide veterans with relevant quality information that it has already collected to help veterans make informed decisions about their care .

studies and other evidence we reviewed indicate potential problems with the completeness and accuracy of the clinical information in patient records that is used to calculate va's publicly reported health care quality measures on hospital compare and its own website .

moreover , va central office — which has responsibility for calculating and reporting health care quality measures for each vamc — has not systematically assessed the completeness and accuracy of this clinical information across its vamcs and the effects , if any , on the accuracy of its health care quality measures .

studies and other evidence we reviewed indicate potential problems with the completeness and accuracy of the clinical information recorded in patient medical records at some vamcs ( eg , diagnoses given and treatments received ) .

this is significant because the accuracy of vamcs' performance on quality measures that are calculated and reported by va on hospital compare and its own website depends on the completeness and accuracy of this clinical information .

for example , va reports readmission measures on hospital compare and on its website that compare vamcs in terms of the proportion of their patients with a medical condition , such as heart failure , who are readmitted to a hospital within 30 days of an initial inpatient stay .

because vamcs as well as non - va hospitals that treat healthier patients are likely to have lower readmission rates independent of the quality of care that they provide , these readmission measures incorporate information about whether a hospital's patients have certain other diagnoses that indicate their overall level of health .

however , the calculation of hospital readmission rates will not work as intended if vamcs do not record in their medical records complete and accurate information on patient diagnoses .

in particular , if some vamcs record that information more completely and accurately than others , that may distort any comparison of the readmission rates among the vamcs .

however , according to an independent assessment of va's clinical documentation procedures conducted by mckinsey & company in 2015 , va as a whole is below industry standards .

the independent assessment's report looked specifically at va's implementation of clinical documentation improvement ( cdi ) programs , which typically provide a combination of provider education and assessments of provider performance to promote more complete and accurate clinical documentation .

the independent assessment noted that such cdi programs have been widely adopted by hospitals across the u.s. health care system and play a critical role in producing more complete and accurate clinical information .

however , the assessment found that only 62 of 134 vamcs examined had a cdi program as of 2014 .

the number of vamcs with cdi programs has increased since the study was completed ; va officials reported to us that as of july 2017 , 99 vamcs had a cdi program in place , and an additional 11 vamcs are planning to implement a cdi program .

moreover , the independent assessment found evidence suggesting that deficiencies in the completeness and accuracy of the clinical information recorded in va patient medical records may have affected the assessed quality performance of at least some vamcs .

specifically , the independent assessment cited multiple instances where vamc officials observed that their facility's assessed performance on va's quality measures markedly improved after the facility took steps to improve the completeness and accuracy of the clinical information recorded in the vamc's patient records .

we could not quantify the potential effect of incomplete and inaccurate clinical information on vamc assessed quality performance without conducting an audit of the clinical information recorded at each vamc .

other components of the independent assessment also highlighted longstanding issues with va's electronic health records ( ehr ) system , which stores the clinical information used to calculate vamc performance on the quality measures .

the summary report for all 12 independent assessments noted that va's ehr system is outdated .

the ehrs used at each vamc do not consistently use standard data elements and algorithms for recording clinical information , and this variability in underlying clinical information from one vamc to the next has led to an inability to convey consistent and complete information across the va health care system .

in 2016 , the commission on care determined that the deficiencies in va's clinical documentation were so pervasive that it recommended that va procure an entirely new ehr system that would replace its increasingly obsolete ehr system .

these findings are similar to other studies conducted by the va office of inspector general ( oig ) that found deficiencies in vamcs' operations that can lead to the recording of incomplete and inaccurate patient information .

for example , in july 2017 , a va oig investigation of colonoscopy practices concluded that more accurate and stringent clinical data collection would enable va to improve its monitoring of the quality of providers' colonoscopies .

additionally , in may 2012 , the oig found that some vamcs do not consistently conduct reviews intended to ensure that their clinicians properly enter information into ehrs , including ensuring the appropriate use of cut and paste functions .

in april 2010 , the oig also found that vamc emergency departments do not consistently document all of the information that they are required to report when transferring patients to other facilities .

furthermore , as one va official observed to us , because of the way in which they are funded , vamcs do not have the same financial incentives that non - va hospitals do to ensure that the clinical information stored in patient medical records is complete and accurate .

in addition , because most of va health care is funded through direct appropriations , rather than by payments of claims filed with an insurance company or with government programs such as medicare , va does not routinely produce the claims - based data on health care services provided to patients that non - va hospitals and clinics typically generate in the course of doing business .

thus , to calculate vamc performance on quality measures , va officials told us that va has to extract data from its patient medical records specifically for this purpose .

in contrast , non - va hospitals routinely collect this information — such as data on readmissions and mortality — as part of their claims processing and reimbursement from medicare and other health care payers .

within va , va central office is responsible for calculating and reporting the health care quality measures that va publicly reports for each of its vamcs and ensuring that these measures provide accurate information on the quality of care at these facilities .

however , while studies and other evidence we reviewed indicate potential problems with the clinical information recorded at some vamcs , va central office has not determined the extent to which these problems exist across vamcs and adversely affect the accuracy of the quality measures va publicly reports on hospital compare and its own website .

specifically , va central office has not conducted a systematic assessment of the completeness and accuracy of the clinical data recorded in va patient medical records across vamcs .

 ( there are models for this type of assessment ; see app .

iii for examples of methodologies that have been applied to different types of medical record systems. ) .

when asked why they have not conducted such an assessment , va central office officials told us that they have focused instead on improving the accuracy of medical coding and on specific clinical quality issues identified at individual vamcs .

however , accurate medical coding depends on the completeness and accuracy of the underlying clinical information in patient records .

moreover , va central office policy assigns responsibility for monitoring the completeness and accuracy of patients' clinical information to each individual vamc .

each vamc has a health record review committee that is charged with monitoring the clinical documentation practices at that facility .

this committee determines the frequency of medical record reviews at its facility , decides what focused reviews should occur , and identifies clinicians that record clinical information poorly until they improve to an acceptable rate of completeness and accuracy .

according to officials in va's central office who are responsible for health information management , the results of individual vamc health record review committee reviews are not reported to va central office .

as a result , va central office lacks information that could help it systematically determine whether or to what extent vamcs' clinical information is incomplete and inaccurate , and if so , the extent to which these deficiencies affect vamcs' reported performance on the quality measures va publicly reports .

the results of such a systematic analysis could also help identify the deficiencies , if any , in the recording of patient clinical information and what steps , if any , va central office may need to take to address them .

va central office's lack of a systematic assessment of the completeness and accuracy of clinical information recorded in patient medical records and the extent to which this affects the accuracy of its quality measures is inconsistent with federal standards for internal controls related to information and monitoring .

these standards call for agencies to use accurate information to achieve objectives , and to monitor agency activities and evaluate results .

without a systematic assessment of the completeness and accuracy of the clinical information recorded in vamc patient medical records and their potential effects on the health care quality measures va reports , va central office does not have reasonable assurance that differences in vamcs' performance on va's quality measures reflect true differences in the quality of care and not differences in the accuracy and completeness of the underlying clinical information .

this may hinder va central office's ability to appropriately assess vamc performance and offer accurate publicly available information on its website to veterans so that they can make informed choices about their care .

va uses hospital compare and its website to provide veterans with information on how vamcs perform on a range of health care quality measures .

by providing information on the quality of care at va facilities , the quality measures are intended to help veterans make informed decisions about their care .

however , two key limitations in va's efforts may hinder veterans' ability to use the information va provides to make informed decisions about their health care .

first , while va's primary access and quality webpage provides generally accessible and understandable information on the quality of va health care , the breadth of information it provides is too limited for veterans to make informed choice about their care .

va intends for this webpage to be veterans' primary source of information on the quality of care at vamcs , but the 15 measures reported on the webpage represent only a small subset of the 110 measures and only a few of the measure categories va makes available elsewhere on its website .

until va can provide information on a broader range of health care measures and services , highlight key differences in the quality of clinical care at vamcs , and present this information in a way that is easily accessible and understandable , va cannot ensure that its website is functioning as intended in helping veterans make informed choices about their care .

second , va does not have reasonable assurance that the health care quality measures it reports on hospital compare and its own website accurately assess the relative performance of vamcs .

this is because the quality measures are calculated using clinical information recorded in patient medical records , but va central office does not know to what extent this information has been accurately and completely recorded across vamcs .

this is because va has not conducted a systematic assessment and does not have reasonable assurance about the accuracy of the information that its quality measures are based on .

therefore , va lacks reasonable assurance that the quality measures reported on hospital compare and its own website provide accurate information to veterans so they can make informed choices about their care .

we are making two recommendations to the department of veterans affairs .

we recommend that the undersecretary for health take additional steps to ensure that va's website reports health care quality measures that cover a broad range of health care services , highlights key differences in the clinical quality of care , and presents this information in an easily accessible and understandable way ( recommendation 1 ) .

we recommend that the undersecretary for health direct va central office to conduct a systematic assessment of the completeness and accuracy of patient clinical information across vamcs that is used to calculate the health care quality measures va reports and address any deficiencies that affect the accuracy of these measures ( recommendation 2 ) .

we provided a draft of this report to va and hhs for review and comment .

hhs had no comments on this report .

va provided written comments , which we have reprinted in appendix iv , and provided technical comments , which we have incorporated as appropriate .

in its comments , va concurred with our first recommendation to take additional steps to ensure that va's website reports health care quality measures that cover a broad range of health care services , highlights key differences in the clinical quality of care , and presents this information in an easily accessible and understandable way .

va provided additional information about its plans to expand the quality measures reported on its access and quality webpage , including information on which additional measures it was planning to add to the webpage and the steps it planned to take to continue to enhance the presentation of this information for veterans .

va indicated that its target completion date for these activities was december 2017 .

in addition , va concurred in principle with our second recommendation for va central office to conduct a systematic assessment of the completeness and accuracy of patient clinical information across vamcs that is used to calculate the health care quality measures va reports and address any deficiencies that affect the accuracy of these measures .

in its comments , va acknowledged the importance of improving the reliability of data used to calculate its health care quality measures and described its plans to establish a workgroup under the deputy under secretary for health for organizational excellence to determine the best approach for conducting a systematic assessment of the completeness and accuracy of the patient clinical information across vamcs .

in its response , va described the workgroup's focus on determining whether current validation processes ensure completeness and accuracy and stated that this determination would be completed by december 2017 .

in conjunction with reviewing these validation processes , va should also assess the completeness and accuracy of clinical information recorded in patient records that is used to calculate the quality measures .

this will allow va to determine the extent to which the quality measures it reports accurately reflect vamcs' performance in delivering care to veterans .

we are sending copies of this report to the appropriate congressional committees , the secretary of veterans affairs , the secretary of health and human services , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-7114 or williamsonr@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix v .

this appendix presents the specific health care quality measures that the department of veterans affairs ( va ) publicly reports , plans to report , or does not report on the department of human and health services' ( hhs ) hospital compare website .

hhs's hospital compare website provides information on the quality of care at non - va hospitals and va medical centers ( vamcs ) .

this information is captured across various quality measures covering topics such as patient experience .

additionally , this appendix presents the specific health care quality measures that va reports on its own website about the quality of care at vamcs .

some of the specific measures reported on the hospital compare website are the same as those reported on va's website and some are entirely different .

in the table below , measures listed on the same row ( eg , patient experience measures ) are measures that we determined were the same , based on information we obtained from va and hhs .

measures that are grouped together but listed on different rows may address similar quality issues ( eg , hospital readmissions ) but are calculated differently .

additionally , a dash in a table cell indicates that the measure listed on that row is not reported on the webpage in question .

see table 7 .

the va website locations refer to subpages of va's two webpages with quality measures .

they are coded as: access and quality webpage: ( 1 ) wait times ; ( 2 ) experience with access ; and ( 3 ) non - va hospital comparison .

quality of care webpage: ( 1 ) va quality scores ( strategic analytics for improvement and learning ( sail ) ) ; ( 2 ) mcp - - medical center performance ; ( 3 ) wntb - - why not the best ; and ( 4 ) pt exp - - patient experience on the quality of care .

the 13 criteria we used to assess the presentation and relevance of information on quality of care reported on va's primary access and quality and older quality of care webpages were identified as part of gao's prior work on health care transparency .

these criteria identify the key characteristics that make websites effective in communicating information on health care quality of care to consumers .

we used a four - scale rating system — yes , no , limited or very limited — to determine if each criterion had been met .

a “limited” rating indicates that the webpage has discrete areas where the webpage has implemented the characteristic to some degree , but those areas are not representative of the webpage as a whole .

a rating of “very limited” indicates that the webpage has largely not implemented the characteristic ( with a few exceptions ) .

two analysts applied the criteria separately and reconciled any differences .

two external experts we also consulted generally agreed with our assessments of the presentation and relevance of va's publicly reported quality measures on its website .

six of the 13 characteristics of effective consumer websites focus on the extent to which a website presents its information in a way that enables the consumer to grasp and interpret it .

specifically , the research we reviewed shows that more effective websites: 1 .

use plain language with clear graphics .

effective consumer websites use labels and descriptions that make sense to consumers who typically are unfamiliar with clinical terminology and who often have difficulty interpreting numerical information .

graphics , including symbols , can help to readily convey information on relative provider performance , especially when they are designed to display a summary assessment of that performance as part of the symbol itself , for example one that incorporates the words “superior” or “poor.” 2 .

explain purpose and value of quality performance ratings to consumers .

effective consumer websites address prevalent misleading preconceptions by providing consumers coherent explanations of how different quality measures relate to the aspects of quality that consumers find relevant .

these explanations work best when they link individual measures to overarching categories indicating what is being achieved , such as effectiveness of care , safety , or patient - focused care .

3 .

summarize related information and organize data to highlight patterns and facilitate consumer interpretation .

two techniques that consumer websites can use to help consumers make sense of large amounts of information are ( a ) combining information from multiple related measures into summary or composite scores , and ( b ) structuring presentation of the data in ways that make patterns evident .

for example , listing providers in rank order on selected cost and quality dimensions greatly simplifies identification of high and low performers .

4 .

enable consumers to customize information selected for presentation to focus on what is most relevant to them .

consumers differ in the priority they assign to different aspects of quality .

websites that enable consumers to customize which quality information is presented help consumers filter out information of lesser consequence to them , and hone in on the information that they find most compelling .

for example , one consumer may choose to focus on providers' capacity to communicate well with patients , while another may focus on providers' rates of complications and infections .

5 .

enable consumers to compare quality performance of multiple providers in one view .

websites are most effective when they present side - by - side assessments of providers' performance on a given aspect of cost or quality , so consumers can most easily compare providers .

6 .

enable easy use and navigation of the tool .

unless consumers can quickly find information of interest to them , they are likely to quickly dismiss the potential utility of a consumer website and move on .

extensive testing with consumers can help public and private entities providing websites to develop intuitive , user - friendly approaches to website navigation and for manipulating how the data are presented .

seven of the 13 characteristics of effective websites we identified address the extent to which a website provides substantive quality and cost information of relevance to consumers .

specifically , the research we reviewed shows that more effective websites: 1. review a broad range of services so that more consumers' particular needs are included .

the more services that are covered by the website , the more likely it is that the website will have information relevant to the particular services of interest to any given consumer .

it is especially important to include services that are predictable and non - urgent , because these services are most likely to afford consumers the opportunity to evaluate cost and quality information before receiving the service .

2 .

cover a broad range of providers .

websites that provide information for all or most of the available providers in a given geographic area , regardless of network status or practice setting , give consumers more information about their full range of options .

for example , for procedures that can be conducted in either a hospital outpatient department or ambulatory surgical center , it helps consumers to provide comparable information for both settings , so that consumers can choose from a larger number of providers that offer those procedures .

3 .

describe key differences in clinical quality of care , particularly patient - reported outcomes .

assessments of the clinical quality of care that have been shown to have particular relevance to consumers are those that relate to long - term outcomes of the care experienced by other patients .

often this is best addressed by patient - reported outcomes , which tell consumers the eventual outcome of treatments , as reported by previous patients of a particular provider .

for example , patients receiving hip replacements can be asked , through such patient - reported outcomes , to rate their ability to climb stairs both before and after their procedures , which enables assessments of the procedures' effects on patients' mobility .

4 .

describe key differences in patient experiences with providers .

another outcome that matters is patients' assessment of their interactions with providers .

effective websites include information on how past patients have evaluated providers in terms of dimensions such as how well nurses communicate with patients , or the responsiveness of clinicians to patients' needs .

5 .

describe other information related to quality , where appropriate .

there may be other quality indicators that could have major significance to consumers for certain types of services .

for example , facility inspection results and staffing levels are of particular relevance to nursing home care .

6 .

provide timely information .

more recent data are intrinsically more relevant than data that are several years old .

because consumer websites necessarily rely on past data to assess likely cost and quality performance in the future , some lag in collecting , analyzing , and providing data is inevitable .

data that are no more than two years old are generally considered timely .

7 .

describe key strengths and limitations of the data .

although the research we reviewed shows that few consumers are inclined to delve into the many methodological issues that concern appropriate techniques for collecting , checking , and analyzing cost and quality data , effective websites can provide both summary assessments of strengths and limitations for most consumers , and links to more complete explanations for those wanting to pursue these issues in greater detail .

such information , along with identification of the organization responsible for the website , provides consumers a basis to judge the credibility of the cost and quality information provided .

in recent years , a number of researchers have examined methods for assessing the quality of clinical information recorded in electronic medical records , including data completeness and accuracy .

a wide variety of approaches have been proposed and used , depending on the clinical focus of the research and the institutional context in which the clinical information is recorded .

the following provides some examples drawn from this body of research .

1 .

nicole gray weiskopf and chunhua weng , “methods and dimensions of electronic health record data quality assessment: enabling reuse for clinical research,” journal of the american medical informatics association , vol .

20 , no .

1 ( 2013 ) , 144 – 151 .

this article identifies a range of different tests that had been applied by various researchers to determine both data completeness and data accuracy in electronic patient records .

most of these tests involved comparison with another data source , such as paper records , patient interviews , or other alternative data sources , that served as a “gold standard” for purposes of assessing the information recorded in the electronic medical record .

other tests involved comparison between different pieces of information recorded in the electronic medical record .

2 .

philip j.b. brown and victoria warmington , “data quality probes — exploiting and improving the quality of electronic patient record data and patient care,” international journal of medical informatics , vol .

68 ( 2002 ) , 91-98 .

the article describes the use of “data quality probes” to assess data quality by matching the result of database queries relative to established clinical knowledge for a given condition .

3 .

edwin r faulconer and simon de lusignan , “an eight - step method for assessing diagnostic data quality in practice: chronic obstructive pulmonary disease as an exemplar,” informatics in primary care , vol .

12 , no .

4 , ( 2004 ) , 243 – 53 .

this article describes an eight - step methodology for assessing the completeness and accuracy of diagnoses for a specific condition .

4 .

michael g. kahn , marsha a. raebel , jason m. glanz , karen riedlinger , and john f. steiner , “a pragmatic framework for single - site and multisite data quality assessment in electronic health record - based clinical research,” medical care , vol .

50 , no .

7 ( 2012 ) , s21-s29 .

this article proposes an approach that prioritizes assessment of selected variables and data quality dimensions ; iterative cycles of assessment within and between sites ; targeting assessment toward data domains known to be vulnerable to quality problems ; and detailed documentation of the rationale and outcomes of data quality assessments to inform data users .

it presents a comprehensive set of data quality rules that look for anomalies in data values and distributions as well as inconsistencies across related variables .

5 .

william r. hogan and michael m. wagner , “accuracy of data in computer - based patient records,” journal of the american medical informatics association , vol .

4 , no .

5 ( 1997 ) , 342 – 355 .

this article emphasizes the importance of assessing data completeness and accuracy in conjunction with each other .

it recommends adopting a continuous improvement approach to improving data quality based on regular cycles of monitoring , analysis of errors , and interventions to address the factors found to cause errors .

in addition to the contact named above , rashmi agarwal , assistant director ; eric peterson , analyst in charge ; dee abasute ; krister friday ; jacquelyn hamilton ; wati kadzai , and vikki porter made key contributions to this report .

va health care: improvements needed in data and monitoring of clinical productivity and efficiency .

gao - 17-480 ( washington , d.c.: may 24 , 2017 ) .

federal health care center: va and dod need to develop better information to monitor operations and improve efficiency .

gao - 17-197 ( washington , d.c.: jan. 23 , 2017 ) .

veterans health care: improvements needed in operationalizing strategic goals and objectives.gao - 17-50 ( washington , d.c.: oct. 21 , 2016 ) .

va primary care: improved oversight needed to better ensure timely access and efficient delivery of care .

gao - 16-83 ( washington , d.c.: oct. 8 , 2015 ) .

health care transparency: actions needed to improve cost and quality information for consumers .

gao - 15-11 ( washington , d.c.: oct. 20 , 2014 ) .

va health care management and oversight of consult process need improvement to help ensure veterans receive timely outpatient specialty care .

gao - 14-808 ( washington , d.c.: sep. 30 , 2014 ) .

