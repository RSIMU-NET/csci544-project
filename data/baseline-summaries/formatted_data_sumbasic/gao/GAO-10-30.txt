several recent congressional initiatives seek to focus funds in certain federal social programs on activities for which the evidence of effectiveness is rigorous — specifically , well - designed randomized controlled trials showing sizable , sustained benefits to program participants or society .

to help agencies , grantees , and others implement the relevant legislative provisions effectively , the private , nonprofit coalition for evidence - based policy launched the top tier evidence initiative in 2008 to identify and validate social interventions meeting the standard of evidence set out in these provisions .

in requesting this report , you expressed interest in knowing whether limiting the search for effective interventions to those that had been tested against these particular criteria might exclude from consideration other important interventions .

to learn whether the coalition's approach could be valuable in helping federal agencies implement such funding requirements , you asked gao to independently assess the coalition's approach .

gao's review focused on the following questions .

1 .

how valid and transparent is the process the coalition used — searching , selecting , reviewing , and synthesizing procedures and criteria — to identify social interventions that meet the standard of “well - designed randomized controlled trials showing sizable , sustained effects on important outcomes” ? .

2 .

how do the coalition's choices of procedures and criteria compare to ( a ) generally accepted design and analysis techniques for identifying effective interventions and ( b ) similar standards and processes other federal agencies use to evaluate similar efforts ? .

3 .

what types of interventions do randomized controlled experiments appear to be best suited to assessing effectiveness ? .

4 .

for intervention types for which randomized controlled experiments appear not to be well suited , what alternative forms of evaluation are used to successfully assess effectiveness ? .

to assess the coalition's top tier initiative , we reviewed documents , conducted interviews , and observed the deliberations of its advisory panel , who determined which interventions met the “top tier” evidence standard — well - designed , randomized controlled trials showing sizable , sustained benefits to program participants or society .

we evaluated the transparency of the initiative's process against its own publicly stated procedures and criteria , including the top tier evidence standard .

to assess the validity of the coalition's approach , we compared its procedures and criteria to those recommended in program evaluation textbooks and related publications , as well as to the processes actually used by six federally supported initiatives with a similar purpose to the coalition .

through interviews and database searches , we identified six initiatives supported by the u.s. department of education , department of health and human services ( hhs ) , and department of justice that also conduct systematic reviews of evaluation evidence to identify effective interventions .

we ascertained the procedures and criteria these federally supported efforts used from interviews and document reviews .

we identified the types of interventions for which randomized controlled experiments — the coalition's primary evidence criterion — are best suited and alternative methods for assessing effectiveness by reviewing the program evaluation methodology literature and by having our summaries of that literature reviewed by a diverse set of experts in the field .

we obtained reviews from seven experts who had published on evaluation methodology , held leadership positions in the field , and had experience in diverse subject areas and methodologies .

we conducted this performance audit from may 2008 through november 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

over the past two decades , several efforts have been launched to improve federal government accountability and results , such as the strategic plans and annual performance reports required under the government performance and results act of 1993 ( gpra ) .

the act was designed to provide executive and congressional decision makers with objective information on the relative effectiveness and efficiency of federal programs and spending .

in 2002 , the office of management and budget ( omb ) introduced the program assessment rating tool ( part ) as a key element of the budget and performance integration initiative under president george w. bush's governmentwide management agenda .

part is a standard set of questions meant to serve as a diagnostic tool , drawing on available program performance and evaluation information to form conclusions about program benefits and recommend adjustments that may improve results .

the success of these efforts has been constrained by lack of access to credible evidence on program results .

we previously reported that the part review process has stimulated agencies to increase their evaluation capacity and available information on program results .

after 4 years of part reviews , however , omb rated 17 percent of 1,015 programs “results not demonstrated” — that is , did not have acceptable performance goals or performance data .

many federal programs , while tending to have limited evaluation resources , require program evaluation studies , rather than performance measures , in order to distinguish a program's effects from those of other influences on outcomes .

program evaluations are systematic studies that assess how well a program is working , and they are individually tailored to address the client's research question .

process ( or implementation ) evaluations assess the extent to which a program is operating as intended .

outcome evaluations assess the extent to which a program is achieving its outcome - oriented objectives but may also examine program processes to understand how outcomes are produced .

when external factors such as economic or environmental conditions are known to influence a program's outcomes , an impact evaluation may be used in an attempt to measure a program's net effect by comparing outcomes with an estimate of what would have occurred in the absence of the program intervention .

a number of methodologies are available to estimate program impact , including experimental and nonexperimental designs .

concern about the quality of social program evaluation has led to calls for greater use of randomized experiments — a method used more widely in evaluations of medical than social science interventions .

randomized controlled trials ( or randomized experiments ) compare the outcomes for groups that were randomly assigned either to the treatment or to a nonparticipating control group before the intervention , in an effort to control for any systematic difference between the groups that could account for a difference in their outcomes .

a difference in these groups' outcomes is believed to represent the program's impact .

while random assignment is considered a highly rigorous approach in assessing program effectiveness , it is not the only rigorous research design available and is not always feasible .

the coalition for evidence - based policy is a private , nonprofit organization that was sponsored by the council for excellence in government from 2001 until the council closed in 2009 .

the coalition aims to improve the effectiveness of social programs by encouraging federal agencies to fund rigorous studies — particularly randomized controlled trials — to identify effective interventions and to provide strong incentives and assistance for federal funding recipients to adopt such interventions .

coalition staff have advised omb and federal agencies on how to identify rigorous evaluations of program effectiveness , and they manage a web site called “social programs that work” that provides examples of evidence - based programs to “provide policymakers and practitioners with clear , actionable information on what works , as demonstrated in scientifically - valid studies .

 .

 .

.” in 2008 , the coalition launched a similar but more formal effort , the top tier evidence initiative , to identify only interventions that have been shown in “well - designed and implemented randomized controlled trials , preferably conducted in typical community settings , to produce sizeable , sustained benefits to participants and / or society.” at the same time , it introduced an advisory panel of evaluation researchers and former government officials to make the final determination .

the coalition has promoted the adoption of this criterion in legislation to direct federal funds toward strategies supported by rigorous evidence .

by identifying interventions meeting this criterion , the top tier evidence initiative aims to assist agencies , grantees , and others in implementing such provisions effectively .

because of the flexibility provided to recipients of many federal grants , achieving these federal programs' goals relies heavily on agencies' ability to influence their state and local program partners' choice of activities .

in the past decade , several public and private efforts have been patterned after the evidence - based practice model in medicine to summarize available effectiveness research on social interventions to help managers and policymakers identify and adopt effective practices .

the department of education , hhs , and department of justice support six initiatives similar to the coalition's to identify effective social interventions .

these initiatives conduct systematic searches for and review the quality of evaluations of intervention effectiveness in a given field and have been operating for several years .

we examined the processes used by these six ongoing federally supported efforts to identify effective interventions in order to provide insight into the choices of procedures and criteria that other independent organizations made in attempting to achieve a similar outcome as the top tier initiative: to identify interventions with rigorous evidence of effectiveness .

the top tier initiative , however , aims to identify not all effective interventions but only those supported by the most definitive evidence of effectiveness .

the processes each of these initiatives ( including top tier ) takes to identify effective interventions are summarized in appendix i .

in 1997 , the agency for healthcare research and quality ( ahrq ) established the evidence - based practice centers ( epc ) ( there are currently 14 ) to provide evidence on the relative benefits and risks of a wide variety of health care interventions to inform health care decisions .

epcs perform comprehensive reviews and synthesize scientific evidence to compare health treatments , including pharmaceuticals , devices , and other types of interventions .

the reviews , with a priority on topics that impose high costs on the medicare , medicaid , or state children's health insurance ( schip ) programs , provide evidence about effectiveness and harms and point out gaps in research .

the reviews are intended to help clinicians and patients choose the best tests and treatments and to help policy makers make informed decisions about health care services and quality improvement .

hhs established the guide to community preventive services ( the community guide ) in 1996 to provide evidence - based recommendations and findings about public health interventions and policies to improve health and promote safety .

with the support of the centers for disease control and prevention ( cdc ) , the community guide synthesizes the scientific literature to identify the effectiveness , economic efficiency , and feasibility of program and policy interventions to promote community health and prevent disease .

the task force on community preventive services , an independent , nonfederal , volunteer body of public health and prevention experts , guides the selection of review topics and uses the evidence gathered to develop recommendations to change risk behaviors , address environmental and ecosystem challenges , and reduce disease , injury , and impairment .

intended users include public health professionals , legislators and policy makers , community - based organizations , health care service providers , researchers , employers , and others who purchase health care services .

cdc established the hiv / aids prevention research synthesis ( prs ) in 1996 to review and summarize hiv behavioral prevention research literature .

prs conducts systematic reviews to identify evidence - based hiv behavioral interventions with proven efficacy in preventing the acquisition or transmission of hiv infection ( reducing hiv - related risk behaviors , sexually transmitted diseases , hiv incidence , or promoting protective behaviors ) .

these reviews are intended to translate scientific research into practice by providing a compendium of evidence - based interventions to hiv prevention planners and providers and state and local health departments for help with selecting interventions best suited to the needs of the community .

the office of juvenile justice and delinquency prevention established the model programs guide ( mpg ) in 2000 to identify effective programs to prevent and reduce juvenile delinquency and related risk factors such as substance abuse .

mpg conducts reviews to identify effective intervention and prevention programs on the following topics: delinquency ; violence ; youth gang involvement ; alcohol , tobacco , and drug use ; academic difficulties ; family functioning ; trauma exposure or sexual activity and exploitation ; and accompanying mental health issues .

mpg produces a database of intervention and prevention programs intended for juvenile justice practitioners , program administrators , and researchers .

the substance abuse and mental health services administration ( samhsa ) established the national registry of evidence - based programs and practices ( nrepp ) in 1997 and provides the public with information about the scientific basis and practicality of interventions that prevent or treat mental health and substance abuse disorders .

nrepp reviews interventions to identify those that promote mental health and prevent or treat mental illness , substance use , or co - occurring disorders among individuals , communities , or populations .

nrepp produces a database of interventions that can help practitioners and community - based organizations identify and select interventions that may address their particular needs and match their specific capacities and resources .

the institute of education sciences established the what works clearinghouse ( wwc ) in 2002 to provide educators , policymakers , researchers , and the public with a central source of scientific evidence on what improves student outcomes .

wwc reviews research on the effectiveness of replicable educational interventions ( programs , products , practices , and policies ) to improve student achievement in areas such as mathematics , reading , early childhood education , english language , and dropout prevention .

the wwc web site reports information on the effectiveness of interventions through a searchable database and summary reports on the scientific evidence .

the coalition provides a clear public description on its web site of the first two phases of its process — search and selection to identify candidate interventions .

it primarily searches other evidence - based practice web sites and solicits nominations from experts and the public .

staff post their selection criteria and a list of the interventions and studies reviewed on their web site .

however , their public materials have not been as transparent about the criteria and process used in the second two phases of its process — review and synthesize study results to determine whether an intervention met the top tier criteria .

although the coalition provides brief examples of the panel's reasoning in making top tier selections , it has not fully reported the panel's discussion of how to define sizable and sustained effects in the absence of detailed guidance or the variation in members' overall assessments of the interventions .

through its web site and e - mailed announcements , the coalition has clearly described how it identified interventions by searching the strongest evidence category of 15 federal , state , and private web sites profiling evidence - based practices and by soliciting nominations from federal agencies , researchers , and the general public .

its web site posting clearly indicated the initiative's search and selection criteria: ( 1 ) early childhood interventions ( for ages 0 – 6 ) in the first phase of the initiative and interventions for children and youths ( ages 7 – 18 ) in the second phase ( starting in february 2009 ) and ( 2 ) interventions showing positive results in well - designed and implemented randomized experiments .

coalition staff then searched electronic databases and consulted with researchers to identify any additional randomized studies of the interventions selected for review .

the july 2008 announcement of the initiative included its august 2007 “checklist for reviewing a randomized controlled trial of a social program or project , to assess whether it produced valid evidence.” the checklist describes the defining features of a well - designed and implemented randomized experiment: equivalence of treatment and control groups throughout the study , valid measurement and analysis , and full reporting of outcomes .

it also defines a strong body of evidence as consisting of two or more randomized experiments or one large multisite study .

in the initial phase ( july 2008 through february 2009 ) , coalition staff screened studies of 46 early childhood interventions for design or implementation flaws and provided the advisory panel with brief summaries of the interventions and their results and reasons why they screened out candidates they believed clearly did not meet the top tier standard .

reasons for exclusion included small sample sizes , high sample attrition ( both during and after the intervention ) , follow - up periods of less than 1 year , questionable outcome measures ( for example , teachers' reports of their students' behavior ) , and positive effects that faded in later follow - up .

staff also excluded interventions that lacked confirmation of effects in a well - implemented randomized study .

coalition staff recommended three candidate interventions from their screening review ; advisory panel members added two more for consideration after reviewing the staff summaries ( neither of which was accepted as top tier by the full panel ) .

while the top tier initiative explains each of its screening decisions to program developers privately , on its web site it simply posts a list of the interventions and studies reviewed , along with full descriptions of interventions accepted as top tier and a brief discussion of a few examples of the panel's reasoning .

the top tier initiative's public materials are less transparent about the process and criteria used to determine whether an intervention met the top tier standard than about candidate selection .

one panel member , the lead reviewer , explicitly rates the quality of the evidence on each candidate intervention using the checklist and rating form .

coalition staff members also use the checklist to review the available evidence and prepare detailed study reviews that identify any significant limitations .

the full advisory panel then discusses the available evidence on the recommended candidates and holds a secret ballot on whether an intervention meets the top tier standard , drawing on the published research articles , the staff review , and the lead reviewer's quality rating and top tier recommendation .

the advisory panel discussions did not generally dispute the lead reviewer's study quality ratings ( on quality of overall design , group equivalence , outcome measures , and analysis reporting ) but , instead , focused on whether the body of evidence met the top tier standard ( for sizable , sustained effects on important outcomes in typical community settings ) .

the checklist also includes two criteria or issues that were not explicit in the initial statement of the top tier standard — whether the body of evidence showed evidence of effects in more than one site ( replication ) and provided no strong countervailing evidence .

because neither the checklist nor the rating form provides definitions of how large a sizable effect should be , how long a sustained effect should last , or what constituted an important outcome , the panel had to rely on its professional judgment in making these assessments .

although a sizable effect was usually defined as one passing tests of statistical significance at the 0.05 level , panel members raised questions about whether particular effects were sufficiently large to have practical importance .

the panel often turned to members with subject matter expertise for advice on these matters .

one member cautioned against relying too heavily on the reported results of statistical tests , because some studies , by conducting a very large number of comparisons , appeared to violate the assumptions of those tests and , thus , probably identified some differences between experimental groups as statistically significant simply by chance .

the checklist originally indicated a preference for data on long - term outcomes obtained a year after the intervention ended , preferably longer , noting that “longer - term effects .

 .

 .

are of greatest policy and practical importance.” panel members disagreed over whether effects measured no later than the end of the second grade — at the end of the intervention — were sufficiently sustained and important to qualify as top tier , especially in the context of other studies that tracked outcomes to age 15 or older .

one panel member questioned whether it was realistic to expect the effects of early childhood programs to persist through high school , especially for low - cost interventions ; others noted that the study design did not meet the standard because it did not collect data a year after the intervention ended .

in the end , a majority ( but not all ) of the panel accepted this intervention as top tier because the study found that effects persisted over all 3 program years , and they agreed to revise the language in the checklist accordingly .

panel members disagreed on what constituted an important outcome .

two noted a pattern of effects in one study on cognitive and academic tests across ages 3 , 5 , 8 , and 18 .

another member did not consider cognitive tests an important enough outcome and pointed out that the effects diminished over time and did not lead to effects on other school - related behavioral outcomes such as special education placement or school drop - out .

another member thought it was unreasonable to expect programs for very young children ( ages 1 – 3 ) to show an effect on a child at age 18 , given all their other experiences in the intervening years .

a concern related to judging importance was whether and how to incorporate the cost of the intervention into the intervention assessment .

on one hand , there was no mention of cost in the checklist or intervention rating form .

on the other hand , panel members frequently raised the issue when considering whether they were comfortable recommending the intervention to others .

one aspect of this was proportionality: they might accept an outcome of less policy importance if the intervention was relatively inexpensive but would not if it was expensive .

additionally , one panel member feared that an expensive intervention that required a lot of training and monitoring to produce results might be too difficult to successfully replicate in more ordinary settings .

in the february 2009 meeting , it was decided that program cost should not be a criterion for top tier status but should be considered and reported with the recommendation , if deemed relevant .

the panel discussed whether a large multisite experiment should qualify as evidence meeting the replication standard .

one classroom - based intervention was tested by randomly assigning 41 schools nationwide .

because the unit of analysis was the school , results at individual schools were not analyzed or reported separately but were aggregated to form one experimental – control group comparison per outcome measure .

some panel members considered this study a single randomized experiment ; others accepted it as serving the purpose of a replication , because effects were observed over a large number of different settings .

in this case , limitations in the original study report added to their uncertainty .

some panel members stated that if they had learned that positive effects had been found in several schools rather than in only a few odd cases , they would have been more comfortable ruling this multisite experiment a replication .

because detailed guidance was lacking , panel members , relying on individual judgment , arrived at split decisions ( 4 – 3 and 3 – 5 ) on two of the first four early childhood interventions reviewed , and only one intervention received a unanimous vote .

panel members expressed concern that because some criteria were not specifically defined , they had to use their professional judgment yet found that they interpreted the terms somewhat differently .

this problem may have been aggravated by the fact that , as one member noted , they had not had a “perfect winner” that met all the top tier criteria .

indeed , a couple of members expressed their desire for a second category , like “promising,” to allow them to communicate their belief in an intervention's high quality , despite the fact that its evidence did not meet all their criteria .

in a discussion of their narrow ( 4 – 3 ) vote at their next meeting ( february 2009 ) , members suggested that they take more time to discuss their decisions , set a requirement for a two - thirds majority agreement , or ask for votes from members who did not attend the meeting .

the latter suggestion was countered with concern that absent members would not be aware of their discussion , and the issue was deferred to see whether these differences might be resolved with time and discussion of other interventions .

disagreement over top tier status was less a problem with later reviews , held in february and july 2009 , when none of the votes on top tier status were split decisions and three of seven votes were unanimous .

the coalition reports that it plans to supplement guidance over time by accumulating case decisions rather than developing more detailed guidance on what constitutes sizable and sustained effects .

the december 2008 and may 2009 public releases of the results of the top tier evidence review of early childhood interventions provided brief discussion of examples of the panel's reasoning for accepting or not accepting specific interventions .

in may 2009 , the coalition also published a revised version of the checklist that removed the preference for outcomes measured a year after the intervention ended , replacing it with a less specific reference: “over a long enough period to determine whether the intervention's effects lasted at least a year , hopefully longer.” at the february 2009 meeting , coalition staff stated that they had received a suggestion from external parties to consider introducing a second category of “promising” interventions that did not meet the top tier standard .

panel members agreed to discuss the idea further but noted the need to provide clear criteria for this category as well .

for example , they said it was important to distinguish interventions that lacked good quality evaluations ( and thus had unknown effectiveness ) from those that simply lacked replication of sizable effects in a second randomized study .

it was noted that broadening the criteria to include studies ( and interventions ) that the staff had previously screened out may require additional staff effort and , thus , resources beyond those of the current project .

the top tier initiative's criteria for assessing evaluation quality conform to general social science research standards , but other features of the overall process differ from common practice for drawing conclusions about intervention effectiveness from a body of research .

the initiative's choice of a broad topic fails to focus the review on how to achieve a specific outcome .

its narrow evidence criteria yield few recommendations and limited information on what works to inform policy and practice decisions .

the top tier and all six of the agency - supported review initiatives we examined assess evaluation quality on standard dimensions to determine whether a study provides credible evidence on effectiveness .

these dimensions include the quality of research design and execution , the equivalence of treatment and comparison groups ( as appropriate ) , adequacy of samples , the validity and reliability of outcome measures , and appropriateness of statistical analyses and reporting .

some initiatives included additional criteria or gave greater emphasis to some issues than others .

the six agency - supported initiatives also employed several features to ensure the reliability of their quality assessments .

in general , assessing the quality of an impact evaluation's study design and execution involves considering how well the selected comparison protects against the risk of bias in estimating the intervention's impact .

for random assignment designs , this primarily consists of examining whether the assignment process was truly random , the experimental groups were equivalent before the intervention , and the groups remained separate and otherwise equivalent throughout the study .

for other designs , the reviewer must examine the assignment process even more closely to detect whether a potential source of bias ( such as higher motivation among volunteers ) may have been introduced that could account for any differences observed in outcomes between the treatment and comparison groups .

in addition to confirming the equivalence of the experimental groups at baseline , several review initiatives examine the extent of crossover or “contamination” between experimental groups throughout the study because this could blur the study's view of the intervention's true effects .

all seven review initiatives we examined assess whether a study's sample size was large enough to detect effects of a meaningful size .

they also assess whether any sample attrition ( or loss ) over the course of the study was severe enough to question how well the remaining members represented the original sample or whether differential attrition may have created significant new differences between the experimental groups .

most review forms ask whether tests for statistical significance of group differences accounted for key study design features ( for example , random assignment of groups rather than individuals ) , as well as for any deviations from initial group assignment ( intention - to - treat analysis ) .

the rating forms vary in structure and detail across the initiatives .

for example , “appropriateness of statistical analyses” can be found under the category “reporting of the intervention's effects” on one form and in a category by itself on another form .

in the model programs guide rating form , “internal validity” — or the degree to which observed changes can be attributed to the intervention — is assessed through how well both the research design and the measurement of program activities and outcomes controlled for nine specific threats to validity .

the epc rating form notes whether study participants were blind to the experimental groups they belonged to — standard practice in studies for medical treatments but not as common in studies of social interventions , while the prs form does not directly address study blinding in assessing extent of bias in forming study groups .

the major difference in rating study quality between the top tier initiative and the six other initiatives is a product of the top tier standard as set out in certain legislative provisions: the other initiatives accept well - designed , well - conducted quasi - experimental studies as credible evidence .

most of the federally supported initiatives recognize well - conducted randomized experiments as providing the most credible evidence of effectiveness by assigning them their highest rating for quality of research design , but three do not require them for interventions to receive their highest evidence rating: epc , the community guide , and national registry of evidence - based programs and practices ( nrepp ) .

the coalition has , since its inception , promoted randomized experiments as the highest - quality , unbiased method for assessing an intervention's true impact .

federal officials provided a number of reasons for including well - conducted quasi - experimental studies: ( 1 ) random assignment is not feasible for many of the interventions they studied , ( 2 ) study credibility is determined not by a particular research design but by its execution , ( 3 ) evidence from carefully controlled experimental settings may not reflect the benefits and harms observed in everyday practice , and ( 4 ) too few high - quality , relevant random assignment studies were available .

the top tier initiative states a preference for studies that test interventions in typical community settings over those run under ideal conditions but does not explicitly assess the quality ( or fidelity ) of program implementation .

the requirement that results be shown in two or more randomized studies is an effort to demonstrate the applicability of intervention effects to other settings .

however , four other review initiatives do explicitly assess intervention fidelity — the community guide , mpg , nrepp , and prs — through either describing in detail the intervention's components or measuring participants' level of exposure .

poor implementation fidelity can weaken a study's ability to detect an intervention's potential effect and thus lessen confidence in the study as a true test of the intervention model .

epc and the community guide assess how well a study's selection of population and setting matched those in which it is likely to be applied ; any notable differences in conditions would undermine the relevance or generalizability of study results to what can be expected in future applications .

all seven initiatives have experienced researchers with methodological and subject matter expertise rate the studies and use written guidance or codebooks to help ensure ratings consistency .

codebooks varied but most were more detailed than the top tier checklist .

most of the initiatives also provided training to ensure consistency of ratings across reviewers .

in each initiative , two or more reviewers rate the studies independently and then reach consensus on their ratings in consultation with other experts ( such as consultants to or supervisors of the review ) .

after the top tier initiative's staff screening review , staff and one advisory panel member independently review the quality of experimental evidence available on an intervention , before the panel as a group discussed and voted on whether it met the top tier standard .

however , because the panel members did not independently rate study quality or the body of evidence , it is unknown how much of the variation in their overall assessment of the interventions reflected differences in their application of the criteria making up the top tier standard .

the top tier initiative's topic selection , emphasis on long - term effects , and narrow evidence criteria combine to provide limited information on the effectiveness of approaches for achieving specific outcomes .

it is standard practice in research and evaluation syntheses to pose a clearly defined research question — such as , which interventions have been found effective in achieving specific outcomes of interest for a specific population ? — and then assemble and summarize the credible , relevant studies available to answer that question .

a well - specified research question clarifies the objective of the research and guides the selection of eligibility criteria for including studies in a systematic evidence review .

in addition , some critics of systematic reviews in health care recommend using the intervention's theoretical framework or logic model to guide analyses toward answering questions about how and why an intervention works when it does .

evaluators often construct a logic model — a diagram showing the links between key intervention components and desired results — to explain the strategy or logic by which it is expected to achieve its goals .

the top tier initiative's approach focuses on critically appraising and summarizing the evidence without having first formulated a precise , unambiguous research question and the chain of logic underlying the interventions' hypothesized effects on the outcomes of interest .

neither of the top tier initiative's topic selections — interventions for children ages 0 – 6 or youths ages 7 – 18 — identify either a particular type of intervention , such as preschool or parent education , or a desired outcome , such as healthy cognitive and social development or prevention of substance abuse , that can frame and focus a review as in the other effectiveness reviews .

the other initiatives have a clear purpose and focus: learning what has been effective in achieving a specific outcome or set of outcomes ( for example , reducing youth involvement in criminal activity ) .

moreover , recognizing that an intervention might be successful on one outcome but not another , epc , nrepp , and wwc rate the effectiveness of an intervention by each outcome .

even epc , whose scope is the broadest of the initiatives we reviewed , focuses individual reviews by selecting a specific healthcare topic through a formal process of soliciting and reviewing nominations from key stakeholders , program partners , and the public .

their criteria for selecting review topics include disease burden for the general population or a priority population ( such as children ) , controversy or uncertainty over the topic , costs associated with the condition , potential impact for improving health outcomes or reducing costs , relevance to federal health care programs , and availability of evidence and reasonably well - defined patient populations , interventions , and outcome measures .

the top tier initiative's emphasis on identifying interventions with long - term effects — up to 15 years later for some early childhood interventions — also leads away from focusing on how to achieve a specific outcome and could lead to capitalizing on chance results .

a search for interventions with “sustained effects on important life outcomes,” regardless of the content area , means assembling results on whatever outcomes — special education placement , high school graduation , teenage pregnancy , employment , or criminal arrest — the studies happen to have measured .

this is of concern because it is often not clear why some long - term outcomes were studied for some interventions and not others .

moreover , focusing on the achievement of long - term outcomes , without regard to the achievement of logically related short - term outcomes , raises questions about the meaning and reliability of those purported long - term program effects .

for example , without a logic model or hypothesis linking preschool activities to improving children's self - control or some other intermediate outcome , it is unclear why one would expect to see effects on their delinquent behavior as adolescents .

indeed , one advisory panel member raised questions about the mechanism behind long - term effects measured on involvement in crime when effects on more conventional ( for example , academic ) outcomes disappeared after a few years .

later , he suggested that the panel should consider only outcomes the researcher identified as primary .

coalition staff said that reporting chance results is unlikely because the top tier criteria require the replication of results in multiple ( or multi - site ) studies , and they report any nonreplicated findings as needing confirmation in another study .

unlike efforts to synthesize evaluation results in some systematic evidence reviews , the top tier initiative examines evidence on each intervention independently , without reference to similar interventions or , alternatively , to different interventions aimed at the same goal .

indeed , of the initiatives we reviewed , only epc and the community guide directly compare the results of several similar interventions to gain insight into the conditions under which an approach may be successful .

 ( wwc topic reports display effectiveness ratings by outcome for all interventions they reviewed in a given content area , such as early reading , but do not directly compare their approaches. ) .

these two initiatives explicitly aim to build knowledge about what works in an area by developing logic models in advance to structure their evaluation review by defining the specific populations and outcome measures of interest .

a third , mpg , considers the availability of a logic model and the quality of an intervention's research base in rating the quality of its evidence .

where appropriate evidence is available , epcs conduct comparative effectiveness studies that directly compare the effectiveness , appropriateness , and safety of alternative approaches ( such as drugs or medical procedures ) to achieving the same health outcome .

officials at the other initiatives explained that they did not compare or combine results from different interventions because they did not find them similar enough to treat as replications of the same approach .

however , most initiatives post the results of their reviews on their web sites by key characteristics of the intervention ( for example , activities or setting ) , outcomes measured , and population , so that viewers can search for particular types of interventions or compare their results .

the top tier initiative's narrow primary criterion for study design quality — randomized experiments only — diverges from the other initiatives and limits the types of interventions they considered .

in addition , the exclusivity of its top tier standard also diverges from the more common approach of rating the credibility of study findings along a continuum and resulted in the panel's recommending only 6 of 63 interventions for ages 0 – 18 reviewed as providing “sizable , sustained effects on important life outcomes.” thus , although they are not their primary audience , the top tier initiative provides practitioners with limited guidance on what works .

two basic dimensions are assessed in effectiveness reviews: ( 1 ) the credibility of the evidence on program impact provided by an individual study or body of evidence , based on research quality and risk of bias in the individual studies , and ( 2 ) the size and consistency of effects observed in those studies .

the six other evidence reviews report the credibility of the evidence on the interventions' effectiveness in terms of their level of confidence in the findings — either with a numerical score ( 0 to 4 , nrepp ) or on a scale ( high , moderate , low , or insufficient , epc ) .

scales permit an initiative to communicate intermediate levels of confidence in an intervention's results and to distinguish approaches with “promising” evidence from those with clearly inadequate evidence .

federal officials from initiatives using this more inclusive approach indicated that they believed that it provides more useful information and a broader range of choices for practitioners and policy makers who must decide which intervention is most appropriate and feasible for their local setting and available resources .

to provide additional guidance to practitioners looking for an intervention to adopt , nrepp explicitly rates the interventions' readiness for dissemination by assessing the quality and availability of implementation materials , resources for training and ongoing support , and the quality assurance procedures the program developer provides .

some initiatives , like top tier , provide a single rating of the effectiveness of an intervention by combining ratings of the credibility and size ( and consistency , if available ) of intervention effects .

however , combining scores creates ambiguity in an intermediate strength of evidence rating — it could mean that reviewers found strong evidence of modest effects or weak evidence of strong effects .

other initiatives report on the credibility of results and the effect sizes separately .

for example , wwc reports three summary ratings for an intervention's result on each outcome measured: an improvement index , providing a measure of the size of the intervention's effect ; a rating of effectiveness , summarizing both study quality and the size and consistency of effects ; and an extent of evidence rating , reflecting the number and size of effectiveness studies reviewed .

thus , the viewer can scan and compare ratings on all three indexes in a list of interventions rank - ordered by the improvement index before examining more detailed information about each intervention and its evidence of effectiveness .

in our review of the literature on program evaluation methods , we found general agreement that well - conducted randomized experiments are best suited for assessing intervention effectiveness where multiple causal influences lead to uncertainty about what has caused observed results but , also , that they are often difficult to carry out .

randomized experiments are considered best suited for interventions in which exposure to the intervention can be controlled and the treatment and control groups' experiences remain separate , intact , and distinct throughout the study .

the evaluation methods literature also describes a variety of issues to consider in planning an evaluation of a program or of an intervention's effectiveness , including the expected use of the evaluation , the nature and implementation of program activities , and the resources available for the evaluation .

selecting a methodology follows , first , a determination that an effectiveness evaluation is warranted .

it then requires balancing the need for sufficient rigor to draw firm conclusions with practical considerations of resources and the cooperation and protection of participants .

several other research designs are generally considered good alternatives to randomized experiments , especially when accompanied by specific features that help strengthen conclusions by ruling out plausible alternative explanations .

in reviewing the literature on evaluation research methods , we found that randomized experiments are considered appropriate for assessing intervention effectiveness only after an intervention has met minimal requirements for an effectiveness evaluation — that the intervention is important , clearly defined , and well - implemented and the evaluation itself is adequately resourced .

conducting an impact evaluation of a social intervention often requires the expenditure of significant resources to both collect and analyze data on program results and estimate what would have happened in the absence of the program .

thus , impact evaluations need not be conducted for all interventions but reserved for when the effort and cost appear warranted .

there may be more interest in an impact evaluation when the intervention addresses an important problem , there is interest in adopting the intervention elsewhere , and preliminary evidence suggests its effects may be positive , if uncertain .

of course , if the intervention's effectiveness were known , then there would be no need for an evaluation .

and if the intervention was known or believed to be ineffective or harmful , then it would seem wasteful as well as perhaps unethical to subject people to such a test .

in addition to federal regulations concerning the protection of human research subjects , the ethical principles of relevant professional organizations require evaluators to try to avoid subjecting study participants to unreasonable risk , harm , or burden .

this includes obtaining their fully informed consent .

an impact evaluation is more likely to provide useful information about what works when the intervention consists of clearly defined activities and goals and has been well implemented .

having clarity about the nature of intended activities and evidence that critical intervention components were delivered to the intended targets helps strengthen confidence that those activities caused the observed results ; it also improves the ability to replicate the results in another study .

confirming that the intervention was carried out as designed helps rule out a common explanation for why programs do not achieve their goals ; when done before collecting expensive outcome data , it can also avoid wasting resources .

obtaining agreement with stakeholders on which outcomes to consider in defining success also helps ensure that the evaluation's results will be credible and useful to its intended audience .

while not required , having a well - articulated logic model can help ensure shared expectations among stakeholders and define measures of a program's progress toward its ultimate goals .

regardless of the evaluation approach , an impact evaluation may not be worth the effort unless the study is adequately staffed and funded to ensure the study is carried out rigorously .

if , for example , an intervention's desired outcome consists of participants' actions back on the job after receiving training , then it is critical that all reasonable efforts are made to ensure that high - quality data on those actions are collected from as many participants as possible .

significant amounts of missing data raises the possibility that the persons reached are different from those who were not reached ( perhaps more cooperative ) and thus weakens confidence that the observed results reflect the true effect of the intervention .

similarly , it is important to invest in valid and reliable measures of desired outcomes to avoid introducing error and imprecision that could blur the view of the intervention's effect .

we found in our review of the literature on evaluation research methods that randomized experiments are considered best suited for assessing intervention effectiveness where multiple causal influences lead to uncertainty about program effects and it is possible , ethical , and practical to conduct and maintain random assignment to minimize the effect of those influences .

as noted earlier , when factors other than the intervention are expected to influence change in the desired outcome , the evaluator cannot be certain how much of any observed change reflects the effect of the intervention , as opposed to what would have occurred anyway without it .

in contrast , controlled experiments are usually not needed to assess the effects of simple , comparatively self - contained processes like processing income tax returns .

the volume and accuracy of tax returns processed simply reflect the characteristics of the returns filed and the agency's application of its rules and procedures .

thus , any change in the accuracy of processed returns is likely to result from change in the characteristics of either the returns or the agency's processes .

in contrast , an evaluation assessing the impact of job training on participants' employment and earnings would need to control for other major influences on those outcomes — features of the local job market and the applicant pool .

in this case , randomly assigning job training applicants ( within a local job market ) to either participate in the program ( forming the treatment group ) or not participate ( forming the control group ) helps ensure that the treatment and control groups will be equally affected .

random assignment is , of course , suited only to interventions in which the evaluator or program manager can control whether a person , group , or other entity is enrolled in or exposed to the intervention .

control over program exposure rules out the possibility that the process by which experimental groups are formed ( especially , self - selection ) may reflect preexisting differences between them that might also affect the outcome variable and , thus , obscure the treatment effect .

for example , tobacco smokers who volunteer for a program to quit smoking are likely to be more highly motivated than tobacco smokers who do not volunteer .

thus , smoking cessation programs should randomly assign volunteers to receive services and compare them to other volunteers who do not receive services to avoid confounding the effects of the services with the effects of volunteers' greater motivation .

random assignment is well suited for programs that are not universally available to the entire eligible population , so that some people will be denied access to the intervention in any case .

this addresses one concern about whether a control group experiment is ethical .

in fact , in many field settings , assignment by lottery has often been considered the most equitable way to assign individuals to participate in programs with limits on enrollment .

randomized experiments are especially well suited to demonstration programs for which a new approach is tested in a limited way before committing to apply it more broadly .

another ethical concern is that the control group should not be harmed by withholding needed services , but this can be averted by providing the control group with whatever services are considered standard practice .

in this case , however , the evaluation will no longer be testing whether a new approach is effective at all ; it will test whether it is more effective than standard practice .

random assignment is also best suited for interventions in which the treatment and control groups' experiences remain separate , intact , and distinct throughout the life of the study so that any differences in outcomes can be confidently attributed to the intervention .

it is important that control group participants not access comparable treatment in the community on their own ( referred to as contamination ) .

their doing so could blur the distinction between the two groups' experiences .

it is also preferred that control group and treatment group members not communicate , because knowing that they are being treated differently might influence their perceptions of their experience and , thus , their behavior .

sometimes people selected for an experimental treatment are motivated by the extra attention they receive ; sometimes those not selected are motivated to work harder to compete with their peers .

thus , random assignment works best when participants have no strong beliefs about the advantage of the intervention being tested and information about their experimental status is not publicly known .

for example , in comparing alternative reading curriculums in kindergarten classrooms , an evaluator needs to ensure that the teachers are equally well trained and do not have preexisting conceptions about the “better” curriculum .

sometimes this is best achieved by assigning whole schools — rather than individuals or classes — to the treatment and control groups , but this can become very expensive , since appropriate statistical analyses now require about as many schools to participate in a study as the number of classes participating in the simpler design .

interventions are well suited for random assignment if the desired outcomes occur often enough to be observed with a reasonable sample size or study length .

studies of infrequent but not rare outcomes — for example , those occurring about 5 percent of the time — may require moderately large samples ( several hundred ) to allow the detection of a difference between the experimental and control groups .

because of the practical difficulties of maintaining intact experimental groups over time , randomized experiments are also best suited for assessing outcomes that occur within 1 to 2 years after the intervention , depending on the circumstances .

although an intervention's key desired outcome may be a social , health , or environmental benefit that takes 10 or more years to fully develop , it may be prohibitively costly to follow a large enough proportion of both experimental groups over that time to ensure reliable results .

evaluators may then rely on intermediate outcomes , such as high - school graduation , as an adequate outcome measure rather than accepting the costs of directly measuring long - term effects on adult employment and earnings .

random assignment is not appropriate for a range of programs in which one cannot meet the requirements that make this strategy effective .

they include entitlement programs or policies that apply to everyone , interventions that involve exposure to negative events , or interventions for which the evaluator cannot be sure about the nature of differences between the treatment and control groups' experiences .

for a few types of programs , random assignment to the intervention is not possible .

one is when all eligible individuals are exposed to the intervention and legal restrictions do not permit excluding some people in order to form a comparison group .

this includes entitlement programs such as veterans' benefits , social security , and medicare , as well as programs operating under laws and regulations that explicitly prohibit ( or require ) a particular practice .

a second type of intervention for which random assignment is precluded is broadcast media communication where the individual — rather than the researcher — controls his or her exposure ( consciously or not ) .

this is true of radio , television , billboard , and internet programming , in which the individual chooses whether and how long to hear or view a message or communication .

to evaluate the effect of advertising or public service announcements in broadcast media , the evaluator is often limited to simply measuring the audience's exposure to it .

however , sometimes it is possible to randomly assign advertisements to distinct local media markets and then compare their effects to other similar but distinct local markets .

a third type of program for which random assignment is generally not possible is comprehensive social reforms consisting of collective , coordinated actions by various parties in a community — whether school , organization , or neighborhood .

in these highly interactive initiatives , it can be difficult to distinguish the activities and changes from the settings in which they take place .

for example , some community development partnerships rely on increasing citizen involvement or changing the relationships between public and private organizations in order to foster conditions that are expected to improve services .

although one might randomly assign communities to receive community development support or not , the evaluator does not control who becomes involved or what activities take place , so it is difficult to trace the process that led to any observed effects .

random assignment is often not accepted for testing interventions that prevent or mitigate harm because it is considered unethical to impose negative events or elevated risks of harm to test a remedy's effectiveness .

thus , one must wait for a hurricane or flood , for example , to learn if efforts to strengthen buildings prevented serious damage .

whether the evaluator is able to randomly apply different approaches to strengthening buildings may depend on whether the approaches appear to be equally likely to be successful in advance of a test .

in some cases , the possibility that the intervention may fail may be considered an unacceptable risk .

when evaluating alternative treatments for criminal offenders , local law enforcement officers may be unwilling to assign the offenders they consider to be the most dangerous to the less restrictive treatments .

as implied by the previous discussion of when random assignment is well suited , it may simply not be practical in a variety of circumstances .

it may not be possible to convince program staff to form control groups by simple random assignment if it would deny services to some of the neediest individuals while providing service to some of the less needy .

for example , individual tutoring in reading would usually be provided only to students with the lowest reading scores .

in other cases , the desired outcome may be so rare or take so long to develop that the required sample sizes or prospective tracking of cases over time would be prohibitively expensive .

finally , the evaluation literature cautions that as social interventions become more complex , representing a diverse set of local applications of a broad policy rather than a common set of activities , randomized experiments may become less informative .

when how much of the intervention is actually delivered , or how it is expected to work , is influenced by characteristics of the population or setting , one cannot be sure about the nature of the difference between the treatment and control group experiences or which factors influenced their outcomes .

diversity in the nature of the intervention can occur at the individual level , as when counselors draw on their experience to select the approach they believe is most appropriate for each patient .

or it can occur at a group level , as when grantees of federal flexible grant programs focus on different subpopulations as they address the needs of their local communities .

in these cases , aggregating results over substantial variability in what the intervention entails may end up providing little guidance on what , exactly , works .

in our review of the literature on evaluation research methods , we identified several alternative methods for assessing intervention effectiveness when random assignment is not considered appropriate — quasi - experimental comparison group studies , statistical analyses of observational data , and in - depth case studies .

although experts differed in their opinion of how useful case studies are for estimating program impacts , several other research designs are generally considered good alternatives to randomized experiments , especially when accompanied by specific features that help strengthen conclusions by ruling out plausible alternative explanations .

quasi - experimental comparison group designs resemble randomized experiments in comparing the outcomes for treatment and control groups , except that individuals are not assigned to those groups randomly .

instead , unserved members of the targeted population are selected to serve as a control group that resembles the treatment group as much as possible on variables related to the desired outcome .

this evaluation design is used with partial coverage programs for which random assignment is not possible , ethical , or practical .

it is most successful in providing credible estimates of program effectiveness when the groups are formed in parallel ways and not based on self - selection — for example , by having been turned away from an oversubscribed service or living in a similar neighborhood where the intervention is not available .

this approach requires statistical analyses to establish groups' equivalence at baseline .

regression discontinuity analysis compares outcomes for a treatment and control group that are formed by having scores above or below a cut - point on a quantitative selection variable rather than through random assignment .

when experimental groups are formed strictly on a cut - point and group outcomes are analyzed for individuals close to the cut - point , the groups are left otherwise comparable except for the intervention .

this technique is used where those considered most “deserving” are assigned to treatment , in order to address ethical concerns about denying services to those in need — for example , when additional tutoring is provided only to children with the lowest reading scores .

the technique requires a quantitative assignment variable that users believe is a credible selection criterion , careful control over assignment to ensure that a strict cut - point is achieved , large sample sizes , and sophisticated statistical analysis .

interrupted time - series analysis compares trends in repeated measures of an outcome for a group before and after an intervention or policy is introduced , to learn if the desired change in outcome has occurred .

long data series are used to smooth out the effects of random fluctuations over time .

statistical modeling of simultaneous changes in important external factors helps control for their influence on the outcome and , thus , helps isolate the impact of the intervention .

this approach is used for full - coverage programs in which it may not be possible to form or find an untreated comparison group , such as for change in state laws defining alcohol impairment of motor vehicle drivers ( “blood alcohol concentration” laws ) .

but because the technique relies on the availability of comparable information about the past — before a policy changed — it may be limited to use near the time of the policy change .

the need for lengthy data series means it is typically used where the evaluator has access to long - term , detailed government statistical series or institutional records .

observational or cross - sectional studies first measure the target population's level of exposure to the intervention rather than controlling its exposure and then comparing the outcomes of individuals receiving different levels of the intervention .

statistical analysis is used to control for other plausible influences .

level of exposure to the intervention can be measured by whether one was enrolled or how often one participated or heard the program message .

this approach is used with full - coverage programs , for which it is impossible to directly form treatment and control groups ; nonuniform programs , in which individuals receive different levels of exposure ( such as to broadcast media ) ; and interventions in which outcomes are observed too infrequently to make a prospective study practical .

for example , an individual's annual risk of being in a car crash is so low that it would be impractical to randomly assign ( and monitor ) thousands of individuals to use ( or not use ) their seat belts in order to assess belts' effectiveness in preventing injuries during car crashes .

because there is no evaluator control over assignment to the intervention , this approach requires sophisticated statistical analyses to limit the influence of any concurrent events or preexisting differences that may be associated with why people had different exposure to the intervention .

case studies have been recommended for assessing the effectiveness of complex interventions in limited circumstances when other designs are not available .

in program evaluation , in - depth case studies are typically used to provide descriptive information on how an intervention operates and produces outcomes and , thus , may help generate hypotheses about program effects .

case studies may also be used to test a theory of change , as when the evaluator specifies in advance the expected processes and outcomes , based on the program theory or logic model , and then collects detailed observations carefully designed to confirm or refute that model .

this approach has been recommended for assessing comprehensive reforms that are so deeply integrated with the context ( for example , the community ) that no truly adequate comparison case can be found .

to support credible conclusions about program effects , the evaluator must make specific , refutable predictions of program effects and introduce controls for , or provide strong arguments against , other plausible explanations for observed effects .

however , because a single case study most likely cannot provide credible information on what would have happened in the absence of the program , our experts noted that the evaluator cannot use this design to reliably estimate the magnitude of a program's effect .

reviewing the literature and consulting with evaluation experts , we identified additional measurement and design features that can help strengthen conclusions about an intervention's impact from both randomized and nonrandomized designs .

in general , they involve collecting additional data and targeting comparisons to help rule out plausible alternative explanations of the observed results .

since all evaluation methods have limitations , our confidence in concluding that an intervention is effective is strengthened when the conclusion is supported by multiple forms of evidence .

although collecting baseline data is an integral component of the statistical approaches to assessing effectiveness discussed above , both experiments and quasi - experiments would benefit from including pretest measures on program outcomes as well as other key variables .

first , by chance , random assignment may not produce groups that are equivalent on several important variables known to correlate with program outcomes , so their baseline equivalence should always be checked .

second , in the absence of random assignment , ensuring the equivalence of the treatment and control groups on measures related to the desired outcome is critical .

the effects of potential self - selection bias or other preexisting differences between the treatment and control groups can be minimized through selection modeling or “propensity score analysis.” essentially , one first develops a statistical model of the baseline differences between the individuals in the treatment and comparison groups on a number of important variables and then adjusts the observed outcomes for the initial differences between the groups to identify the net effect of the intervention .

extending data collection either before or after the intervention can help rule out the influence of unrelated historical trends on the outcomes of interest .

this is in principle similar to interrupted time - series analysis , yielding more observations to allow analysis of trends in outcomes over time in relation to the timing of program activities .

for example , one could examine whether the outcome measure began to change before the intervention could plausibly have affected it , in which case the change was probably influenced by some other factor .

another way to attempt to rule out plausible alternative explanations for observed results is to measure additional outcomes that are or are not expected to be influenced by the treatment , based on program theory .

if one can predict a relatively unique pattern of expected outcomes for the intervention , in contrast to an alternative explanation , and if the study confirms that pattern , then the alternative explanation becomes less plausible .

in comparison group studies , the nature of the effect one detects is defined by the nature of the differences between the experiences of the treatment and control groups .

for example , if the comparison group receives no assistance at all in gaining employment , then the evaluation can detect the full effect of all the employment assistance ( including child care ) the treatment group receives .

but if the comparison group also receives child care , then the evaluation can detect only the effect , or value added , of employment assistance above and beyond the effect of child care .

thus , one can carefully design comparisons to target specific questions or hypotheses about what is responsible for the observed results and control for specific threats to validity .

for example , in evaluating the effects of providing new parents of infants with health consultation and parent training at home , the evaluator might compare them to another group of parents receiving only routine health check - ups to control for the level of attention the first group received and test the value added by the parent training .

sometimes the evaluator can capitalize on natural variations in exposure to the intervention and analyze the patterns of effects to learn more about what is producing change .

for example , little or no change in outcomes for dropouts — participants who left the program — might reflect either the dropouts' lower levels of motivation compared to other participants or their reduced exposure to the intervention .

but if differences in outcomes are associated with different levels of exposure for administrative reasons ( such as scheduling difficulties at one site ) , then those differences may be more likely to result from the intervention itself .

as reflected in all the review initiatives we identified for this report , conclusions drawn from findings across multiple studies are generally considered more convincing than those based on a single study .

the two basic reasons for this are that ( 1 ) each study is just one example of many potential experiences with an intervention , which may or may not represent that broader experience , and ( 2 ) each study employs one particular set of methods to measure an intervention's effect , which may be more or less likely than other methods to detect an effect .

thus , an analysis that carefully considers the results of diverse studies of an intervention is more likely to accurately identify when and for whom an intervention is effective .

a recurring theme in the evaluation literature is the tradeoffs made in constructing studies to rigorously identify program impact by reducing the influence of external factors .

studies of interventions tested in carefully controlled settings , a homogenous group of volunteer participants , and a comparison group that receives no services at all may not accurately portray the results that can be expected in more typical operations .

to obtain a comprehensive , realistic picture of intervention effectiveness , reviewing the results of several studies conducted in different settings and populations , or large multisite studies , may help ensure that the results observed are likely to be found , or replicated , elsewhere .

this is particularly important when the characteristics of settings , such as different state laws , are expected to influence the effectiveness of a policy or practice applied nationally .

for example , states set limits on how much income a family may have while receiving financial assistance , and these limits — which vary considerably from state to state — strongly influence the proportion of a state's assistance recipients who are currently employed .

thus , any federal policy regarding the employment of recipients is likely to affect one state's caseload quite differently from that of another .

because every research method has inherent limitations , it is often advantageous to combine multiple measures or two or more designs in a study or group of studies to obtain a more comprehensive picture of an intervention .

in addition to choosing whether to measure intermediate or long - term outcomes , evaluators may choose to collect , for example , student self - reports of violent behavior , teacher ratings of student disruptive behavior , or records of school disciplinary actions or referrals to the criminal justice system , which might yield different results .

while randomized experiments are considered best - suited for assessing intervention impact , blended study designs can provide supplemental information on other important considerations of policy makers .

for example , an in - depth case study of an intervention could be added to develop a deeper understanding of its costs and implementation requirements or to track participants' experiences to better understand the intervention's logic model .

alternatively , a cross - sectional survey of an intervention's participants and activities can help in assessing the extent of its reach to important subpopulations .

the coalition provides a valuable service in encouraging government adoption of interventions with evidence of effectiveness and in drawing attention to the importance of evaluation quality in assessing that evidence .

reliable assessments of the credibility of evaluation results require expertise in research design and measurement , but their reliability can be improved by providing detailed guidance and training .

the top tier initiative provides another useful model in that it engages experienced evaluation experts to make these quality assessments .

requiring evidence from randomized experiments as sole proof of an intervention's effectiveness is likely to exclude many potentially effective and worthwhile practices for which random assignment is not practical .

the broad range of studies assessed by the six federally supported initiatives we examined demonstrates that other research designs can provide rigorous evidence of effectiveness if designed well and implemented with a thorough understanding of their vulnerability to potential sources of bias .

assessing the importance of an intervention's outcomes entails drawing a judgment from subject matter expertise — the evaluator must understand the nature of the intervention , its expected effects , and the context in which it operates .

defining the outcome measures of interest in advance , in consultation with program stakeholders and other interested audiences , may help ensure the credibility and usefulness of a review's results .

deciding to adopt an intervention involves additional considerations — cost , ease of use , suitability to the local community , and available resources .

thus , practitioners will probably want information on these factors and on effectiveness when choosing an approach .

a comprehensive understanding of which practices or interventions are most effective for achieving specific outcomes requires a synthesis of credible evaluations that compares the costs and benefits of alternative practices across populations and settings .

the ability to identify effective interventions would benefit from ( 1 ) better designed and implemented evaluations , ( 2 ) more detailed reporting on both the interventions and their evaluations , and ( 3 ) more evaluations that directly compare alternative interventions .

the coalition for evidence - based policy provided written comments on a draft of this report , reprinted in appendix ii .

the coalition stated it was pleased with the report's key findings on the transparency of its process and its adherence to rigorous standards in assessing research quality .

while acknowledging the complementary value of well - conducted nonrandomized studies as part of a research agenda , the coalition believes the report somewhat overstates the confidence one can place in such studies alone .

the coalition and the departments of education and health and human services provided technical comments that were incorporated as appropriate throughout the text .

the department of justice had no comments .

we are sending copies of this report to the secretaries of education , justice , and health and human services ; the director of the office of management and budget ; and appropriate congressional committees .

the report is also available at no charge on the gao web site at http: / / www.gao.gov .

if you have questions about this report , please contact me at ( 202 ) 512- 2700 or kingsburyn@gao.gov .

contacts for our offices of congressional relations and public affairs are on the last page .

key contributors are listed in appendix iii .

nancy kingsbury , ph.d managing director applied research and methods .

select randomized and quasi - observational studies ( eg , cohort , case control ) body of evidence on each outcome is scored on four domains: risk of bias , consistency , directness , and precision of effects .

strength of evidence for each outcome is classified as high moderate 2 .

guide to community preventive services at the centers for disease control and prevention select randomized and quasi - observational studies ( eg , time series , case control ) validity and reliability of outcome measures data analysis and reporting assessment of harm 4 .

model programs guide at the office of juvenile justice and delinquency prevention select randomized and quasi - experimental studies with one or more positive outcomes and documentation of program implementation ( fidelity ) summary research quality ratings ( 0 – 4 ) are provided for statistically significant outcomes .

interventions themselves are not rated .

in addition to the person named above , stephanie shipman , assistant director , and valerie caracelli made significant contributions to this report .

agency for healthcare research and quality .

systems to rate the strength of scientific evidence: summary .

evidence report / technology assessment no .

47 .

rockville , md .

: u.s. department of health and human services , march 2002. www.ahrq.gov / clinic / epcsums / strengthsum.htm auspos , patricia , and anne c. kubisch .

building knowledge about community change: moving beyond evaluations .

new york: the aspen institute , 2004 .

berk , richard a. randomized experiments as the bronze standard .

california center for population research on - line working paper series ccpr - 030-05 .

los angeles: august 2005. http: / / repositories.cdlib.org / uclastat / papers / 2005080201 boruch , robert .

“encouraging the flight of error: ethical standards , evidence standards , and randomized trials.” new directions for evaluation no .

113 ( spring 2007 ) : 55 – 73 .

boruch , robert f. , and ellen foley .

“the honestly experimental society: sites and other entities as the units of allocation and analysis in randomized trials.” in validity and social experimentation: donald campbell's legacy , leonard bickman , ed .

thousand oaks , calif.: sage , 2000 .

campbell , donald t. , and julian c. stanley .

experimental and quasi - experimental designs for research .

chicago: rand mcnally , 1966 .

chalmers , iain .

“trying to do more good than harm in policy and practice: the role of rigorous , transparent , up - to - date evaluations.” the annals of the american academy of political and social science 589 ( 2003 ) : 22 – 40 .

cook , thomas d. “randomized experiments in educational policy research: a critical examination of the reasons the educational evaluation community has offered for not doing them.” educational evaluation and policy analysis 24:3 ( 2002 ) : 175 – 99 .

european evaluation society .

ees statement: the importance of a methodologically diverse approach to impact evaluation — specifically with respect to development aid and development interventions .

nijkerk , the netherlands: december 2007. www.europeanevaluation.org flay , brian r. , and others .

“standards of evidence: criteria for efficacy , effectiveness , and dissemination.” prevention science 6:3 ( 2005 ) : 151 – 75 .

fulbright - anderson , anne c. kibisch , and james p. connell , eds .

new approaches to evaluating community initiatives .

vol .

2 .

theory , measurement , and analysis .

washington , d.c.: the aspen institute , 1998 .

glazerman , steven , dan m. levy , and david myers .

“nonexperimental versus experimental estimates of earnings impacts.” the annals of the american academy of political and social science 589 ( 2003 ) : 63 – 93 .

institute of medicine .

knowing what works in health care: a roadmap for the nation .

washington , d.c.: the national academies press , 2008 .

jackson , n. , and e. waters .

“criteria for the systematic review of health promotion and public health interventions.” for the guidelines for systematic reviews in health promotion and public health task force .

health promotion international 20:4 ( 2005 ) : 367 – 74 .

julnes , george , and debra j. rog .

“pragmatic support for policies on methodology.” new directions for evaluation no .

113 ( spring 2007 ) : 129 – 47 .

mark , melvin m. , and charles s. reichardt .

“quasi - experimental and correlational designs: methods for the real world when random assignment isn't feasible.” in the sage handbook of methods in social psychology , carol sansone , carolyn c. morf , and a. t. panter , eds .

thousand oaks , calif.: sage , 2004 .

moffitt , robert a .

“the role of randomized field trials in social science research: a perspective from evaluations of reforms of social welfare programs.” the american behavioral scientist 47:5 ( 2004 ) : 506 – 40 .

national research council , center for education .

scientific research in education .

washington , d.c.: national academies press , 2002 .

national research council , committee on law and justice .

improving evaluation of anticrime programs .

washington , d.c.: national academies press , 2005 .

orr , larry l. social experiments: evaluating public programs with experimental methods .

thousand oaks , calif.: sage , 1999 .

posavac , emil j. , and raymond g. carey .

program evaluation: methods and case studies , 6th ed .

upper saddle river , n.j.: prentice hall , 2003 .

rossi , peter h. , mark w. lipsey , and howard e. freeman .

evaluation: a systematic approach , 7th ed .

thousand oaks , calif.: sage , 2004 .

trochim , william m. k. president , american evaluation association , chair , aea evaluation policy task force .

letter to robert shea , associate director for administration and government performance , office of management and budget , washington , d.c. , march 7 , 2008 , and attachment , “comments on ‘what constitutes strong evidence of a program's effectiveness ? '” www.eval.org / eptf.asp victora , cesar g. , jean - pierre habicht , and jennifer bryce .

“evidence - based public health: moving beyond randomized trials.” american journal of public health 94:3 ( march 2004 ) : 400 – 05 .

west , stephen , and others .

“alternatives to the randomized controlled trial.” american journal of public health 98:8 ( august 2008 ) : 1359 – 66 .

juvenile justice: technical assistance and better defined evaluation plans will help to improve girls' delinquency programs .

gao - 09-721r .

washington , d.c.: july 24 , 2009 .

health - care - associated infections in hospitals: leadership needed from hhs to prioritize prevention practices and improve data on these infections .

gao - 08-283 .

washington , d.c.: march 31 , 2008 .

school mental health: role of the substance abuse and mental health services administration and factors affecting service provision .

gao - 08-19r .

washington , d.c.: october 5 , 2007 .

abstinence education: efforts to assess the accuracy and effectiveness of federally funded programs .

gao - 07-87 .

washington , d.c.: october 3 , 2006 .

program evaluation: omb's part reviews increased agencies' attention to improving evidence of program results .

gao - 06-67 .

washington , d.c.: october 28 , 2005 .

program evaluation: strategies for assessing how information dissemination contributes to agency goals .

gao - 02-923 .

washington , d.c.: september 30 , 2002 .

the evaluation synthesis .

gao / pemd - 10.1.2 .

washington , d.c.: march 1992 .

designing evaluations .

gao / pemd - 10.1.4 .

washington , d.c.: march 1991 .

case study evaluations .

gao / pemd - 10.1.9 .

washington , d.c.: november 1990 .

