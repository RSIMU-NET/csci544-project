many information technology ( it ) projects undertaken by the federal government have cost hundreds of millions of dollars more than anticipated and taken years longer to deploy than was originally expected .

to help address these issues , the office of management and budget ( omb ) launched techstat accountability sessions ( techstats ) in january 2010 .

a techstat is a face - to - face , evidence - based accountability review of an it investment that enables the federal government to intervene to turnaround , halt , or terminate projects that are failing or are not producing results .

omb began leading techstat sessions on agency it projects in 2010 , and subsequently required federal agencies to start holding them too .

you asked us to evaluate how omb and selected federal agencies are implementing techstats .

our objectives were to ( 1 ) identify key characteristics of techstats conducted by omb and selected agencies , ( 2 ) evaluate whether selected agencies are conducting techstats in accordance with omb guidance , and ( 3 ) analyze the extent to which reported results from techstat review sessions are documented , tracked , and validated .

to do so , we selected four agencies — the department of agriculture ( agriculture ) , the department of commerce ( commerce ) , the department of health and human services ( hhs ) , and the department of homeland security ( dhs ) — based on two factors: the number of at - risk it investments shown on omb's it dashboard as of august 2012 and a determination on whether the agency had led techstat sessions during 2011 .

we analyzed omb's list of the techstat sessions it had led through february 2013 as well as the latest budget data for each of these investments to determine key characteristics of the investments that underwent omb - led techstats .

we also analyzed the four selected agencies' lists of the agency - led techstat reviews held as of march 2013 as well as budgetary and other supporting documentation for each investment and review to determine key characteristics of the investments that underwent agency - led techstats .

we compared the omb and agency techstat documentation against omb guidance to identify strengths or weaknesses in the agencies' approaches .

we also compared omb's and the agencies' reported techstat results to omb's guidance in on reporting agency - led outcomes and other relevant best practices.addition , we interviewed officials at omb and our four selected agencies to better understand the techstat process and associated results from these reviews .

while we were able to corroborate information about the it investments , we were unable to determine the reliability of the reported outcomes and cost savings for the omb - led techstats because omb did not provide evidence that would allow us to verify its reported data .

we conducted this performance audit from september 2012 to june 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

appendix i provides additional details on our objectives , scope , and methodology .

if done correctly , investments in it have the potential to make organizations more efficient in fulfilling their missions .

for example , department of defense ( dod ) officials recently reported that an it system supporting military logistics has improved the organization's performance by providing real - time information about road conditions , construction , incidents , and weather to facilitate rapid deployment of military assets .

also , federal aviation administration officials reported that an it system supporting weather data processing has improved aviation operations by integrating terminal and aircraft sensor data with forecast data from the national weather service , and providing it to air traffic controllers .

these officials estimated that the system allows them to increase airspace capacity by 25 percent in certain weather conditions .

however , as we have previously reported , federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

for example , in may 2010 , we reported that after spending $127 million over 9 years on an outpatient scheduling system , the department of veterans affairs has not implemented any of the system's capabilities and was essentially starting over .

further , in may 2012 , we reported that while it should enable government to better serve the american people , the federal government had not achieved expected productivity improvements — despite spending more than $600 billion on it over the past decade .

omb plays a key role in overseeing how federal agencies manage their it investments by working with them to better plan , justify , and determine how to manage them .

each year , omb and federal agencies work together to determine how much the government plans to spend on it projects and how these funds are to be allocated .

omb also guides agencies in developing sound business cases for it investments and establishing management processes for overseeing these investments throughout their life cycles .

the scope of this undertaking is quite large: in planning for fiscal year 2014 , 27 federal agencies reported plans to spend about $76.5 billion on 8,142 it investments .

over the last three decades , congress has enacted several laws to assist agencies and the federal government in managing it investments .

for example , the paperwork reduction act of 1995 specifies omb and agency responsibilities for managing it .

among its provisions , this law establishes agency responsibility for maximizing the value and assessing it also and managing the risks of major information systems initiatives.requires that omb develop and oversee policies , principles , standards , and guidelines for federal agency it functions , including periodic evaluations of major information systems .

in addition , to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 .

this law requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies .

it also requires that omb report to congress on the net program performance benefits achieved as a result of these investments .

as set out in these laws , omb is to play a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

within omb , the office of e - government and information technology , headed by the federal chief information officer ( cio ) , directs the policy and strategic planning of federal it investments and is responsible for oversight of federal technology spending .

agency cios are also expected to have a key role in it management .

federal law , specifically the clinger - cohen act , has defined the role of the cio as the focal point for it management , requiring agency heads to designate cios to lead reforms that would help control system development risks ; better manage technology spending ; and achieve real , measurable improvements in agency performance .

in september 2011 , we reported that federal cios are not consistently responsible for all of the areas assigned by law or identified as critical to effective it management .

for example , although most of the cios were responsible for capital planning and investment management , we found that cios are less frequently responsible for information management duties such as records management and privacy requirements .

in an august 2011 memo , omb reiterated the primary areas of responsibility for agency cios .

this memo detailed four areas in which the cio should have a lead role: it governance , program management , commodity services , and information security .

it emphasized the role of the cio in driving the investment review process , including techstats , and the cio's responsibility over the entire it portfolio for an agency .

to help carry out its oversight role and further improve the transparency into and oversight of agencies' it investments , in june 2009 omb publicly deployed a website , known as the it dashboard .

the dashboard displays federal agencies' cost , schedule , and performance data for over 700 major federal it investments at 27 federal agencies that comprise about $40 billion of the federal budget .

according to omb , these data are intended to provide both a historical and a near - real - time perspective on the performance of these investments .

omb analysts are expected to use the dashboard to identify it investments that are experiencing performance problems .

using data drawn from federal agency budget submissions , the it dashboard provides information on an it investment's primary function , as defined by the federal enterprise architecture .

for fiscal year 2012 submissions , agencies were required to select a primary function from categories within the federal enterprise architecture business reference models .

the primary functions available for it investments in fiscal year 2012 submissions were: administrative management community and social services controls and oversight correctional activities defense and national security disaster management economic development education energy environmental management financial management general government general science and innovation health homeland security human resource management information and technology management intelligence operations internal risk management and mitigation international affairs and commerce law enforcement legislative relations litigation and judicial activities natural resources planning and budgeting public affairs workforce management regulatory development revenue collection supply chain management transportation the dashboard visually presents performance ratings for individual investments using metrics that omb has defined — cost , schedule , and the cio's evaluation of risk .

to develop the cio's risk evaluation , omb instructed agency cios to assess their it investments against a set of six evaluation factors , including risk management , requirements management , contractor oversight , historical performance , and human capital .

the cio assigns a rating of 1 to 5 based on his or her best judgment of the level of risk facing the investment .

omb then translates the agency cio's numerical assignment for an investment into a color for depiction on the dashboard , with green signifying low or moderately low risk , yellow signifying medium risk , and red signifying moderately high or high risk ( see table 1 ) .

in january 2010 , omb began conducting techstats to enable the federal government to intervene to turnaround , halt , or terminate it projects that are failing or are not producing results .

techstats are face - to - face , evidence - based reviews of an at - risk it investment .

omb used cio ratings from the it dashboard , among other sources , to select at - risk investments for the techstats it conducted from 2010 through 2011 .

subsequently , as part of the federal cio's 25-point it reform plan,omb empowered agency cios to hold their own techstat sessions within their respective agencies , and required federal agencies to hold at least one techstat by march 2011 .

the it reform plan also required agencies to roll the techstat model out to its component - level agencies and bureaus ( bureaus ) by june 2012 .

to do this , agencies were required to make agency cios responsible for deploying the necessary tools and training on how to conduct techstat reviews and have at least one bureau conduct techstat reviews by june 2012 .

in august 2011 , omb required agency cios to continue holding techstat sessions .

in establishing and rolling out the techstat sessions , omb stated that it expects that the sessions will help strengthen it governance , improve line - of - sight between project teams and senior executives , increase the precision of ongoing measurement of it program health , and boost the quality and timing of interventions to keep projects on track .

we have found that the techstat model is consistent with government and industry best practices for overseeing it investments , including our own guidance on it investment management processes .

by focusing management attention on troubled projects and establishing clear action items to turn the projects around or terminate them .

gao , information technology investment management: a framework for assessing and improving process maturity , gao - 04-394g ( washington , d.c.: mar .

2004 ) .

deficiencies and use that list to report to congress on progress made in correcting high - risk problems .

as a result , omb started publicly releasing aggregate data on its internal list of mission - critical projects that needed to improve ( called its management watch list ) and disclosing the projects' deficiencies .

the agency also established a high - risk list , which consisted of projects identified as requiring special attention from oversight authorities and the highest levels of agency management .

in june 2009 , omb replaced the management watch list and the high - risk list when it deployed a public website — the it dashboard — to further improve the transparency and oversight of agencies' it investments .

in 2010 and 2011 , we reported that while the dashboard was an important tool for monitoring major it projects , the cost and schedule ratings were not always accurate for selected agencies.recommendations to improve the accuracy of the data and , more recently , found that the accuracy had improved .

in april 2012 , we reported on the progress of omb and selected federal agencies on action items in omb's it reform plan .

we found that , of 10 selected action items , agencies had completed 3 and made progress on the other 7 .

however , we found that agencies lacked time frames for completing 5 of those 7 in - progress action items , and only had performance measures for 4 of the 10 selected action items .

thus , we recommended , among other things , that omb ensure that action items are completed prior to the it reform plan's june 2012 deadline and establish time frames and performance measures for the action items .

omb agreed with our recommendations to complete action items and provide deadlines , but disagreed with our recommendation to establish performance measures .

omb has made progress with completing incomplete action items , but more remains to be done .

for example , one action item involved issuing contracting guidance to support modular development and omb completed this action in june 2012 .

however , another action item involved ensuring agency data center consolidation plans were in place so agencies could close 800 data centers by 2015 , and we recently reported that all but 1 of the 24 agencies' plans were incomplete .

most recently , in october 2012 , we reported specifically on the cio rating portion of the it dashboard at six selected agencies .

cios at six federal agencies rated the majority of their it investments as low risk , and that many ratings remained constant over time .

for ratings that did change , we found two agencies reported more investments with reduced risk compared to earlier risk ratings ; the other four agencies reported more investments with increased risk .

in addition , we reported instances where the cio ratings did not appropriately reflect significant cost , schedule , and performance issues reported by gao and others .

we recommended that the federal cio analyze agency trends reflected in dashboard cio ratings , and report the results of this analysis in future budget submissions .

omb agreed with the recommendation and recently released some information on trends as part of the 2014 budget .

we have work under way to evaluate this information .

gao , information technology dashboard: opportunities exist to improve transparency and oversight of investment risk at select agencies , gao - 13-98 ( washington , d.c.: oct. 16 , 2012 ) .

omb and the four selected agencies have held multiple techstats on it investments that varied in terms of function , significance , amount spent to date , and risk level .

specifically , from january 2010 through april 2013 , omb reported leading 79 techstat sessions , which focused on 55 it investments at 23 federal agencies .

these investments covered 21 functional areas ( such as information and technology management , law enforcement , and health ) , and consist of 45 major , 8 non - major , and 2 unrated investments .

for the 45 major investments that omb reviewed , almost 70 percent had a cio rating of medium to high risk at the time of the techstat review .

the four agencies selected for our review — agriculture , commerce , dhs , and hhs — held a total of 37 techstat sessions covering 28 investments from january 2011 through march 2013 .

most of the investments underwent a techstat session at the agency level , but 8 underwent a techstat review at the bureau level , and 2 underwent techstat reviews at both .

the agency - led techstats covered 14 functional areas , and consist of 21 major and 7 non - major investments .

for the 21 major investments , about 76 percent of the agency - led techstats were on investments that had a cio rating of medium - to high - risk .

while both omb and agencies have made progress in holding techstat sessions , there is more that could be done .

specifically , the number of techstats held to date is relatively small compared to the total number of medium - and high - risk it investments .

further , there are multiple high - risk it investments spending millions of dollars that have not yet been assessed .

two of the selected agencies — agriculture and commerce — had reviewed all of their high - risk investments , and dhs has plans to review its remaining high - risk investments .

however , hhs has not yet established plans to review all of its high - risk investments .

also , omb does not have plans or schedules for assessing the other high - risk it investments .

until omb and agencies intervene to turn around these at - risk projects , the government will continue to spend limited it investment dollars on underperforming projects .

as of april 2013 , omb reported conducting 79 techstat reviews , with 59 reviews occurring in 2010 , 8 in 2011 , 11 in 2012 , and 1 so far in 2013 .

omb conducted fewer techstats in recent years because it expected the agencies to increase the number of agency - led techstats .

most of the omb - led techstats were at dhs and dod .

omb has conducted at least one techstat at 23 agencies ( including the office of the director of national intelligence ) .

it has not held a techstat at the department of labor , the national aeronautics and space administration , the national science foundation , the nuclear regulatory commission , or the smithsonian institution .

figure 1 shows the total number of reported omb - led techstats at each agency as of april 2013 .

these 79 techstat reviews included 55 it investments from 23 federal agencies .

omb also reported leading follow - up sessions for many of these investments .

for example , omb led 4 techstats on a dhs investment called the federal emergency management agency — national flood insurance program information technology systems and services .

table 2 identifies the investments for which omb led techstats , the date of the first session , and the total number of sessions that omb has held on that investment .

these 55 investments span 21 primary functional areas.common functional area was information and technology management , followed by homeland security and human resource management .

figure 2 provides a numerical depiction of the functions of those investments that were subject to a techstat .

omb held most of its techstats on major it investments , with 45 of its 55 investments rated as major investments .

omb also held 8 techstats on non - major investments and 2 on investments lacking a major / non - major designation .

figure 3 shows the breakdown of major and non - major investments .

thirty - eight of the 45 major investments that underwent an omb - led techstat have reportedly cost the federal government about $16 billion through fiscal year 2012 .

dhs's investments that were the subject of a techstat session accounted for most of the cost , with a reported $4.9 billion spent .

dot follows with investments that have reportedly cost $2.8 billion through september 2012 .

the reported cost of individual investments ranged from $4.18 million to $3.2 billion .

table 3 details how much each agency has spent on its major investment .

of the 45 major it investments that were subject to a techstat , 67 percent were considered medium - to high - risk investments at the time they were chosen for a techstat .

specifically , 9 had high - or moderately high - risk ( red ) cio ratings on the it dashboard , 21 had medium - risk ( yellow ) ratings , and 9 had low - or moderately low - risk ( green ) ratings .

in addition , 6 investments' risk levels were not identified .

table 4 shows the dashboard ratings for the major it investments that underwent a techstat .

from january 2011 through march 2013 , the four selected agencies — agriculture , commerce , dhs , and hhs — held 37 techstats that covered 28 different investments .

commerce held the most , with 15 techstat sessions , followed by dhs , hhs , and agriculture , with 8 , 7 , and 7 sessions , respectively .

table 5 lists the investments , the date of the first techstat session , and the total number of techstats by agency .

although most of the agency - led techstat reviews were held at the agency level , three of the agencies in our review have also been holding them at the bureau or component agency level .

specifically , of the agencies in our review , 18 of the 28 investments underwent at least one techstat review at the agency level , 8 underwent a techstat review at the bureau level , and 2 underwent techstat reviews at both .

of the selected agencies , agriculture had not held a bureau - level techstat ( see fig .

4 ) .

like the omb - led reviews , the investments selected by agencies for techstat reviews had diverse primary functional areas .

specifically , 25 of the investments that underwent agency - led techstats at the four selected agencies cover 14 different functional areas .

the most common area was information and technology management ( 6 investments ) , followed by environmental management ( 4 investments ) ( see fig .

5 ) .

most of the investments that underwent a techstat review at the four agencies were major investments , with a quarter of the sessions focusing on non - major investments ( see fig .

6 ) .

the four agencies reported spending $7.8 billion through the end of fiscal year 2012 on 20 of the 21 major investments that underwent an agency - led review.for a new commerce investment to $3.7 billion for a dhs investment .

table 6 provides a summary of the amount spent on major investments through september 2012 .

the four selected agencies are generally conducting techstats in accordance with omb guidance .

in 2011 , when omb decided to move beyond conducting its own techstats and to have agencies conduct them too , omb provided agencies guidance through the it reform plan , a memorandum , and the techstat toolkit , which is available on the cio council's website .

these documents include 15 key requirements , including when techstats should be implemented by the agencies , what participants should be included , how at - risk investments should be chosen , and how outcomes should be tracked and reported .

the requirements can be grouped into four broad categories: scope , governance , process , and outcomes .

table 10 shows key requirements for techstat reviews .

the four selected agencies implemented most of omb's 15 key requirements .

specifically , dhs implemented all 15 requirements , commerce fully implemented 14 requirements and partially implemented 1 requirement , hhs fully implemented 11 of the requirements and partially implemented 4 requirements , and agriculture fully implemented 10 of the requirements , partially implemented 3 requirements , and did not implement 2 requirements .

all four agencies fully implemented eight of omb's requirements: ( 1 ) the appropriate people are invited to the techstat meetings , ( 2 ) sessions are led by the cio , ( 3 ) the preparation for the techstat is performed by the techstat team and the cio , ( 4 ) investments selected by techstats are done so by the prescribed criteria , ( 5 ) all action items are tracked in a consolidated repository , ( 6 ) each investment is tracked into an outcome , ( 7 ) outcomes are shared with omb , and ( 8 ) results are published in an collaborative tool .

the requirement with the least implementation involves documenting action items , deadlines , and responsible parties in a memorandum following each techstat .

table 11 provides details regarding the agencies' implementation of omb guidance on techstats .

while the four selected agencies have largely implemented omb's guidance on conducting techstats , three agencies have selected areas where they can improve .

for example , agriculture created memorandums following a techstat , but did not consistently include responsible parties ; commerce created memorandums , but did not include deadlines ; and hhs did not always create memorandums or monitor all of its action items to closure .

agency officials noted several reasons for not fully implementing omb's guidance .

specifically , agriculture officials noted that they are updating their capital planning guidance to include techstat reviews , but that this guidance is in the process of being reviewed and approved .

also , commerce and hhs officials noted that omb gave agencies flexibility in exactly how to implement their guidance .

while omb did provide agencies flexibility in selecting investments for techstat reviews and conducting those reviews , the requirements are clearly delineated in omb instructions and training .

fully implementing omb's techstat guidance could better position the agencies to realize the benefits of the techstat initiative — including strengthening overall it governance and oversight , and proactively identifying and resolving problems before investments experience delays or cost overruns .

the it reform plan and related omb guidance instructed agencies to track and report on the outcomes of their techstat sessions ( including improved governance , accelerated deliveries , and terminated projects ) , and on any associated cost implications ( such as cost savings or cost avoidances ) .

omb and the four agencies we reviewed have tracked and reported positive results from techstats , with most resulting in improved governance or accelerated deliveries .

omb also reported that federal agencies achieved over $3 billion in cost savings or avoidances as a result of the omb - led techstats in 2010 and $900 million from agency - led techstats in 2011 .

using a different calculation formula , omb also reported that techstats resulted in $63.5 million in cost implications in 2012 .

however , we were unable to validate the reported outcomes and associated savings because omb did not provide supporting artifacts or demonstrate the steps that omb analysts took to verify the agencies' data .

without documentation or an explanation of its method in validating agencies' reported results and cost savings , it will be difficult for omb to provide a sufficient level of confidence to congress and the public that the information it has presented is credible .

both omb and agencies reported achieving positive results from their respective techstat sessions .

specifically , in 2011 omb staff reported achieving a variety of positive outcomes from the omb - led techstat sessions , including 11 investments being reduced in scope , four investments that were cancelled , and multiple investments with accelerated program delivery .

we also identified four investments that were the focus of an omb - led techstat that were subsequently terminated and two other investments that were split into multiple smaller investments to improve governance and accelerate the delivery of discrete capabilities .

further , we previously found that the june 2010 techstat on the national archives and records administration's electronic records archives investment resulted in six corrective actions , including halting fiscal year 2012 development funding pending the completion of a strategic plan.likely experience cost overruns of between $205 and $405 million if the agency completed the program as originally designed .

we estimated that the program would seeking to improve the reporting of outcomes , omb instructed federal agencies to track the results of each agency - led techstat session into one of six outcomes: accelerated delivery , improved governance , reduced scope , eliminated duplication , halted , or terminated .

in december 2011 , omb reported a summary of the outcomes of 294 agency - led techstats across the federal government .

specifically , omb reported the following outcomes: 49 percent resulted in accelerated delivery , 42 percent resulted in improved governance , 3 percent did not report results , 2 percent resulted in terminations , 1 percent resulted in reducing the scope , 1 percent eliminated duplication , and 1 percent halted the investment .

in addition , the four agencies in our review reported on the results of their agency - led techstats , with the majority resulting in improved governance .

specifically , out of 36 techstat reviews , 28 resulted in improved governance , 4 in accelerated delivery , 2 in terminations , and 2 in a reduced scope .

table 12 provides a summary of reported results , by agency .

in conjunction with the reported outcomes of agency - led techstats , omb instructed agencies to provide information on the cost implications of the outcomes .

these implications could include cost savings ( a reduction in actual expenditures ) or cost avoidances ( an action taken immediately that will reduce costs in the future ) .

in addition , omb's guidance on performance reporting notes that performance data should be appropriately accurate and reliable for their intended use .

this guidance further describes verification and validation techniques that omb encourages agencies to use in internal assessments , including ensuring that supporting documentation is maintained and readily available , data are verified as appropriate to the needed level of accuracy , and data limitations are explained and documented .

omb has reported cost savings and avoidances from omb - and agency - led techstat reviews .

specifically , omb has reported that federal agencies achieved about $3 billion in cost savings or avoidances as a result of the omb - led techstats held in 2010 and 2011 , and $900 million from agency - led techstats in 2011 .

see figure 7 for the reported cost implications for both omb - led and agency - led techstats as of november 2011 .

in addition , omb reported that the cost savings and avoidances for the agency - led techstat reviews , as of november 2011 , came from 10 of the 27 agencies under their purview .

department of transportation had the most cost implications , with $510 million .

see figure 8 for a depiction of the cost implications by agency .

two of the agencies in our review — dhs and hhs — reported $23 million in cost implications from three agency - led techstats: dhs reported a $14 million cost avoidance after deciding to decommission one part of the federal protective service's risk assessment and management program ; hhs reported saving $8.2 million by reengineering redundant business processes supporting the food and drug administration's mission accomplishment and regulatory compliance service project ; and hhs reported $800,000 in cost implications associated with improving governance and reducing the scope of the one stop service solution project ( now called govzone ) .

more recently , omb has been reporting cost savings from agency - led techstat reviews in quarterly reports to congress .

from december 2011 through december 2012 , omb identified a total of $63.5 million in cost implications from agency - led techstats .

omb staff stated that they are calculating the cost savings in these quarterly reports differently than their prior reports on cost savings , and thus the costs should not be compared .

when collecting data , it is important to have assurance that the data are accurate .

best practices in implementing the government performance and results act of 1993 emphasize the need for agencies , when providing information , to explain the procedures used to verify or validate their data .

specifically , agencies should ensure that reported data are sufficiently complete , accurate , and consistent , and also identify any significant data limitations .

explaining the limitations of the information can provide a context for understanding and assessing the challenges agencies face in gathering , processing , and analyzing needed data .

such a presentation of data limitation can also help identify the actions needed to improve the agency's ability to measure its performance .

more recently , we have reiterated the importance of providing omb with complete and accurate data .

dhs and hhs , the two agencies in our review that reported cost savings and avoidances , generally perform analyses to establish these estimates .

dhs provided supporting documentation for how the cost implications for its investment were calculated and hhs provided supporting documentation for its larger investment .

we were unable to validate the cost savings for hhs's smaller investment because the agency was unable to provide supporting documentation .

from a governmentwide perspective , omb staff explained that they review agencies' cost implication data for completeness and quality .

however , we were unable to validate omb's reported outcomes and almost $4 billion in cost implications because omb did not provide documentation on any steps that it or the agencies took to ensure the validity of the agencies' outcome and cost data .

for example , omb staff did not provide memorandums documenting action items from the omb - led techstats , the outcomes identified for omb - and agency - led techstats , documentation identifying which investments resulted in cost savings , documentation demonstrating the methodology used to calculate the cost savings , or a summary of steps the agencies took to validate reported cost savings .

moreover , omb does not require agencies to report on what steps they took to verify their reported outcomes and cost savings .

by not requiring agencies to report on their efforts to validate reported outcomes and cost savings , it is not evident that omb is following its own guidance for ensuring that performance data are reliable and accurate .

moreover , omb is not providing reasonable assurance to congress and the public that the information it has presented is credible .

one entity that was formed to assist omb in its oversight of the techstat results was the cio council's subcommittee on it governance and techstats , but according to omb and agency officials this subcommittee has already been dissolved .

the cio council's management best practices committee formed the subcommittee to report on the outcomes and lessons learned from implementing the techstat process .

this subcommittee was to play a key role in continuing to mature the techstat process .

however , in 2012 , the cio council dissolved the subcommittee because the committee chairs determined that it was no longer needed .

consistent with government and industry best practices for overseeing it investments , techstat sessions hold value by focusing management attention on troubled projects and establishing clear action items to turn the projects around or terminate them .

while omb and agencies are reporting positive results from holding techstat sessions , neither are doing enough to ensure that at - risk investments are undergoing review , sound processes are in place , and reported results are valid .

specifically , agencies are reviewing only about a third of their at - risk it investments .

until omb and agencies develop plans and schedules for addressing these at - risk investments , the investments will likely remain at risk .

the four agencies we reviewed had implemented most of the omb - required techstat processes , but three had shortfalls in selected processes .

for example , agriculture had not yet incorporated techstats in its investment management processes , commerce had not consistently included deadlines for action items in its techstat memoranda , and hhs had not consistently created action item memoranda following techstats or tracked its action items to completion .

addressing these shortfalls could better position these agencies to realize the full benefits that techstats offer .

omb regularly reports on cost savings associated with techstats , but it has not taken basic steps to provide reasonable assurance to congress and the public that these data are valid .

until omb requires agencies to report on what they did to validate cost savings data and shares this information , neither congress nor the public can be assured that techstats are as effective as reported .

while the cio council recently dissolved its subcommittee responsible for reviewing techstats , the council is one entity that is uniquely positioned to assist omb in its oversight of the techstat results .

to ensure that techstat sessions are having the appropriate impact in the oversight of underperforming projects , we are making three recommendations to omb .

specifically , we recommend that the director of the office of management and budget direct the federal chief information officer to require agencies to conduct techstats for each it investment rated with a moderately high - or high - risk cio rating on the it dashboard , unless there is a clear reason for not doing so ; require agencies to report to omb on efforts to validate the outcomes , cost savings , and cost avoidances resulting from techstat sessions ; this information should be summarized when omb reports on governmentwide outcomes ; and direct the federal cio council to track the outcome of techstat sessions and to support omb's efforts to validate the resulting cost savings it reports to congress .

in addition , we are making a recommendation to the secretaries of agriculture and commerce to address the weaknesses in agency - and bureau - led techstat processes and management outlined in this report .

we are also making two recommendations to the secretary of health and human services to establish a plan and schedule for addressing each it investment rated with a moderately high - or high - risk cio rating on the it dashboard ; such a plan could include conducting a techstat session , and address the weaknesses in agency - and bureau - led techstat processes and management outlined in this report .

we requested comments from omb , agriculture , commerce , dhs , and hhs on a draft of our report .

in that draft report , we had made recommendations to the secretaries of agriculture , commerce , and dhs to establish a plan and schedule for addressing their high - risk investments that had not yet undergone techstat reviews .

however , these agencies completed these steps prior to the issuance of this report .

therefore , we removed the applicable recommendations .

omb and three agencies provided comments on our draft report .

omb generally agreed with our recommendations ; commerce agreed with our recommendation ; and agriculture and hhs did not agree or disagree with our recommendations .

dhs declined to provide comments .

each agency's comments are discussed in more detail below: in comments provided via e - mail , staff from omb's office of e - government and information technology generally concurred with our findings and recommendations and stated that omb and the agencies are currently taking appropriate steps to meet the recommendations .

omb provided additional details as follows: in commenting on our finding that it did not ensure the validity of the outcomes and savings it reported , omb noted that it is confident in the agency validation of cost - savings and avoidances through the techstat sessions , and that it was speculative to conclude that the reported cost savings data were not valid because we could not actually assess these figures .

omb did not provide supporting documentation for the cost savings calculations and explained that the process used to validate the cost figures was deliberative .

however , part of a sound process for ensuring validity involves documenting the procedures used to verify or validate the data ; as such , we were unable to validate omb's reported outcomes and almost $4 billion in cost implications from 2010 and 2011 because omb did not provide documentation on any steps that it or the agencies took to ensure the validity of the agencies' outcome and cost data .

for example , omb staff did not provide memorandums documenting action items from the omb - led techstats , the outcomes identified for omb - and agency - led techstats , documentation identifying which investments resulted in cost savings , documentation demonstrating the methodology used to calculate the cost savings , or a summary of steps the agencies took to validate reported cost savings .

moreover , omb did not require agencies to report on what steps they took to verify their reported outcomes and cost savings .

without disclosing the steps it took to validate the outcomes and cost savings , omb has not provided reasonable assurance to congress and the public that the information it has presented is credible .

omb also noted that since june 2012 , omb and agencies have validated and transmitted cost - savings and avoidance data associated with techstat sessions to congress on a quarterly basis through the integrated , efficient , and effective use of it report .

these reports summarize the validation methods omb and agencies undertake .

we acknowledge that these reports summarize methods through which the costs savings and avoidances were documented ; however , due to a lack of supporting documentation , we could only validate $22.2 million of the $63.5 million cost savings and avoidances omb recently reported .

in written comments , the department of agriculture's chief information officer partially agreed with our assessment of the agency's techstat process but did not specify the part of the assessment with which she disagreed .

the department stated that it would continue to conduct periodic reviews of its it investments to ensure a review of all major it investments .

in addition , the department stated that it dashboard ratings alone are not true indicators that an investment is poorly performing or underperforming and that it takes into consideration other factors , such as earned value management data , when determining an investment's performance .

we agree that the cio rating should reflect the cio's assessment of the risk and should use many factors in determining the rating .

the department's written comments are provided in appendix ii .

in written comments , the department of commerce's acting secretary concurred with our recommendation and stated that the department is in full compliance with the recommendation .

the department plans to ensure that all techstat memos have a specific point of contact and a specific due date .

the department's written comments are provided in appendix iii .

in comments provided via e - mail , a management analyst within the department of health and human services' office of the assistant secretary for legislation stated that the department had no general comments .

the department provided technical comments , which we have incorporated as appropriate .

in its technical comments , hhs commented on our finding that the agency did not satisfy omb's requirement to hold its first techstat session by march 2011 .

the hhs officials stated that the agency had appropriately initiated the process for holding a techstat prior to omb's march 2011 deadline .

while we acknowledge that the agency initiated the process prior to the deadline , the actual session was held after the deadline .

if you or your staff have any questions on the matters discussed in this report , please contact me at ( 202 ) 512-9286 or at pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

our objectives were to ( 1 ) identify key characteristics of techstat accountability sessions ( techstats ) conducted by the office of management and budget ( omb ) and selected agencies , ( 2 ) evaluate whether selected agencies are conducting techstats in accordance with omb guidance , and ( 3 ) analyze the extent to which reported results from techstat review sessions are documented , tracked , and validated .

in conducting our review , we selected four agencies based on two factors: the number of investments with medium - , moderately high - , and high - risk chief information officer ( cio ) ratings on the information technology ( it ) dashboard as of august 2012 , and a determination on whether the agency had led its own techstats in 2011 .

the four agencies we selected — the departments of agriculture ( agriculture ) , commerce ( commerce ) , health and human services ( hhs ) , and homeland security ( dhs ) — had the highest number of medium - , moderately high - , and high - risk investments .

in addition , in december 2011 , the cio council reported we that all four agencies had held their own techstat sessions in 2011.assessed the reliability of the it dashboard data and the data on agency - led techstats by comparing them to agency documents , and found that the data were sufficient for our purpose of selecting agencies for further review .

we reviewed exhibit 300 and 53 data linked on the it dashboard as of the last update in august 2012. data for each of these investments .

we also interviewed omb , agriculture , commerce , dhs , and hhs officials regarding the techstat sessions .

we assessed the reliability of omb's and the four agencies' lists of reported techstat sessions by seeking corroboration for the sessions held at our selected agencies .

we determined that omb's and the four agencies' lists were sufficient for our purposes .

to address the second objective , we analyzed omb's guidance to agencies on conducting techstat reviews and identified15 key requirements .

we then compared these requirements to the selected agencies' documentation , including capital planning and investment control plans and guidance , agency - specific techstat guidance and training materials , meeting minutes from investment review board meetings , and electronic submissions from the agency to omb regarding the techstat sessions .

to identify whether agencies were documenting , tracking , and monitoring action items — one of the 15 key requirements — we conducted a random sample of 10 percent of each agency's action items and sought documentation from each of the agencies to support the status of the selected action items .

this sample is not generalizable to the entire population of action items from each agency , but we determined it was sufficiently reliable for our purposes of determining whether the agencies were tracking and monitoring action items .

we also interviewed officials from omb and our four selected agencies about their respective techstat processes .

to address the third objective , we identified omb's guidance to agencies on reporting the results of agency - led techstat sessions .

we analyzed the selected agencies' and omb's reported results in each of omb's outcome categories and attempted to validate omb's and the agencies' reported cost implications .

we also reviewed quarterly reports from omb to congress to identify additional cost implication information .

as noted in the body of this report , we were unable to determine the reliability of the reported cost implications because omb did not provide artifacts demonstrating how it validated the data or evidence that it obtained agency validation of the data .

we conducted this performance audit from september 2012 to june 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , individuals making contributions to this report included colleen phillips ( assistant director ) , rebecca eyler , kate feild , and jessica waselkow .

