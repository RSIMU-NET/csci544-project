large commercial trucks and buses are vital for the movement of goods and people across america .

according to the american trucking associations , the trucking industry moved 9.4 billion tons of freight in 2012 , and according to the american bus association , the “motor - coach” industry provided about 694 million passenger trips in 2010 .

however , this activity comes with a cost .

from 2009 to 2012 , crashes involving large commercial trucks and buses averaged around 125,000 per year , resulting in about 78,000 injuries and about 4,100 fatalities .

the primary mission of the u.s. department of transportation's ( usdot ) federal motor carrier safety administration ( fmcsa ) is to reduce crashes , injuries , and fatalities involving large trucks and buses .

fmcsa partners with states to conduct roadside inspections and uses inspection or crash information to assess and prioritize the riskiest motor carriers for further intervention .

from 1997 through 2010 , fmcsa used a program known as safestat to track how well motor carriers — the companies that own commercial trucks and buses — complied with safety standards .

under safestat , fmcsa reviewed only a small percentage of the more than 500,000 motor carriers operating in the united states in a given year .

in an attempt to increase the number of motor carriers that fmcsa can evaluate each year and , ultimately , to improve large commercial truck and bus safety , fmcsa began to develop the compliance , safety , accountability ( csa ) program in 2004 .

one component of the csa program is the safety measurement system ( sms ) , a data - driven approach for identifying motor carriers at risk of presenting a safety hazard or causing a crash .

sms uses information collected during roadside inspections and from reported crashes to calculate scores across seven categories that quantify a carrier's safety performance relative to other carriers .

since 2008 , when csa was first piloted , law enforcement and industry stakeholders have been generally supportive of fmcsa's overall csa approach .

nonetheless , several evaluations of csa conducted by a range of outside groups concluded that some sms safety scores inaccurately assess a carrier's relative crash risk .

the precision and accuracy of these scores is vital because fmcsa investigators and their state partners use sms results to focus their resources to help reduce the number of motor carrier crashes , injuries , and fatalities .

in addition , fmcsa currently posts most of the scores publicly on its website for use by industry stakeholders and the public and has indicated that a future rulemaking will include similar information to help determine whether a carrier is fit to operate motor vehicles .

we were directed in a senate appropriations committee report to continue monitoring fmcsa's implementation of the csa program.report examines the effectiveness of the csa program in assessing safety risk for motor carriers .

fmcsa provided us historical carrier data for several time periods , including december 2008 , december 2010 , june 2012 , and december 2012. with varying levels of carrier exposure — measured by fmcsa as either inspections or an adjusted number of vehicles .

we assessed changes in fmcsa's requirements for carriers to receive sms scores , changes in sms score calculation , and adjustments to the scoring weights .

we also evaluated the potential of fmcsa's general approach to predict future crashes by using data on violations of fmcsa regulations and crashes to examine the relationships , if any , between violations of specific regulations and subsequent crashes .

due to ongoing litigation related to csa and the publication of sms scores , we did not assess the potential effects or tradeoffs resulting from the display or any public use of these scores .

our analysis included nearly 315,000 u.s. - based carriers that were under fmcsa's jurisdiction and , with reasonable certainty , were active during the period from december 2007 through june 2011 .

we considered a carrier active during this period if it received a state or federal inspection , was involved in a crash , or reported the number of vehicles it operates to fmcsa .

information on inspections , violations , and crashes from december 2007 through december 2009 , our observation period , was used to calculate sms scores .

we used crash information from the remaining 18 month period — from december 2009 through june 2011 — referred to as our evaluation period , to determine these carriers' subsequent crash rates and involvement in crashes .

carriers in our analysis population accounted for approximately 120,000 reported crashes during this 18-month period .

throughout this report , our analysis is based on this population , during this time frame , unless otherwise specified .

to identify any modifications to fmcsa's method that could improve effectiveness , we compared the results from our changes to fmcsa's existing methodology and identified an illustrative combination of changes that better distinguished between carriers that later crashed and those that did not .

these illustrative changes included a change to the data sufficiency standards for a carrier to receive an sms score and changes to the calculation method .

we also spoke with 1 ) fmcsa officials in its headquarters office , western service center in colorado , and colorado division office about the implementation of csa and 2 ) representatives from the colorado state patrol and industry and safety interest groups .

we selected colorado because it was one of the initial pilot states for csa and has been implementing the program since early 2008 .

we reviewed existing studies and literature on csa and congressional testimony from industry and safety interest representatives from a september 2012 hearing for the house transportation and infrastructure committee .

appendix i contains a more detailed explanation of our scope and methodology .

appendix ii contains details about estimating rates of regulatory violations in the sms component of csa .

appendix iii contains details about the statistical validity of the sms component of csa .

appendix iv describes prior evaluations of sms scores as measures of safety .

appendix v describes our analysis of regulatory violations and crash risk .

appendix vi describes the carriers we analyzed and provides the results from our analysis of fmcsa's methodology and our illustrative alternative .

we conducted this performance audit from august 2012 through february 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on the audit objectives .

the commercial motor carrier industry represents a range of businesses , including private and for - hire freight transportation , passenger carriers , and specialized transporters of hazardous materials .

as of 2012 , fmcsa estimates that there were more than 531,000 active motor carriers , a number that fluctuates over time due to the approximately 75,000 new applications that enter the industry each year combined with thousands of carriers annually leaving the market .

among carriers we assessed for this report , most that operate in the united states are small firms ; 93 percent of carriers own or operate 20 or fewer motor vehicles .

nonetheless , a large percentage of vehicles on the road are operated by large carriers .

approximately 270 carriers have more than 1,000 vehicles each and account for about 29 percent of all vehicles that fmcsa oversees .

fmcsa is responsible for overseeing this large and diverse industry .

fmcsa establishes safety standards for interstate motor carriers as well as intrastate hazardous material carriers operating in the united states .

to enforce compliance with these standards , fmcsa partners with state agencies to perform roadside inspections of vehicles and investigations of carriers .

in fiscal year 2012 , fmcsa had a budget of approximately $550 million and more than 1,000 fmcsa staff members located at headquarters , four regional service centers , and 52 division offices .

in 2008 , fmcsa launched an operational model test of csa in four states and began implementing the csa program nationwide in 2010 .

csa is intended to improve safety beyond the prior safestat program by identifying safety deficiencies through better use of roadside inspection data , assessing the safety fitness of more motor carriers and drivers , and using less resource - intensive interventions to improve investigative and enforcement actions .

from fiscal year 2007 through fiscal year 2013 , fmcsa obligated $59 million to its csa program , including csa development and technical support , information technology upgrades , and training .

for fiscal year 2014 , fmcsa requested $7.5 million for csa .

csa has three main components: safety measurement system .

sms uses data obtained from federal or state roadside inspections and from crash investigations to identify the highest risk carriers .

sms was designed to improve on safestat by incorporating all of the safety - related violations recorded during roadside inspections .

carriers potentially receive an sms score in seven categories based on this information .

intervention .

a set of enforcement tools , such as warning letters , additional investigations , or fines are used to encourage the highest risk carriers to correct safety deficiencies , or place carriers out - of - service .

safety fitness determination rule .

this future rulemaking will amend regulations to allow a determination — based in part on some of the same information used to calculate sms — as to whether a motor carrier is fit to operate on the nation's roads .

sms , the measurement system component of csa , uses the data collected from roadside inspections and crash reports to quantify a carrier's safety performance relative to other carriers .

specific carrier violations recorded during roadside inspections are assigned to one of six behavioral analysis and safety improvement categories ( basic ) .

according to fmcsa , these basics were developed under the premise that motor carrier crashes can be traced to the behavior of motor carriers and their drivers .

a seventh category , called the crash indicator , measures a carrier's crash involvement history ( see table 1 ) .

each sms score is designed to be a quantitative determination of a carrier's safety performance .

for each of the approximately 800 violations that fall under the various basics , fmcsa assigns a severity weight that is meant to reflect the violation's association with crash occurrence and crash consequence when compared with other violations within the same basic .

for example , reckless driving violations , categorized in the unsafe driving basic , are assigned a severity weight of 10 out of a possible 10 because fmcsa determined that these violations have a stronger relationship to safety risk than some other types of violations .

unlawfully parking , by comparison , is also categorized in the unsafe driving basic , but is assigned a severity weight of 1 out of 10 .

fmcsa calculates sms scores for carriers every month through a process that has three main steps , each of which is made up of several calculations .

relevant inspections are either a driver inspection , in which the inspection focuses on driver - related requirements , such as the driver's record of duty or medical certificate , or a vehicle inspection , which focuses on the condition of the motor vehicle .

driver inspections are the relevant inspection for the unsafe driving , hours - of - service compliance , driver fitness , and controlled substances and alcohol basics .

vehicle inspections are considered relevant inspections for the vehicle maintenance basic .

for the hazardous materials basic , carriers that transport placardable quantities of hazardous materials are also subject to vehicle inspections as the relevant inspections .

throughout the report , we will refer to relevant inspections as simply inspections .

another calculation — the number of vehicles a carrier operates adjusted by the number of vehicle miles .

fmcsa accounts for exposure in order to make the scores comparable across carriers .

this approach has tradeoffs ; while carriers can be compared without penalizing some for having had more inspections or road activity , exposure itself can be considered an element of risk .

all else being equal , carriers with more road activity are involved in more crashes and potentially pose more risk to safety .

step 2: data sufficiency .

depending on the basic , carriers generally receive sms scores if they meet minimum thresholds of exposure ( i.e. , number of vehicles or inspections ) , or a minimum number of inspections with violations ( i.e. , “critical mass” ) .

for purposes of display on fmcsa's public website and identifying the highest risk carriers for directing enforcement resources , fmcsa does not include scores for carriers that do not meet a so - called critical mass of violations .

for each basic , this typically requires a minimum number of inspections that include violations in that basic , a violation in that basic in the last 12 months , and , for some basics , a violation during the most recent inspection .

step 3: dividing carriers into peer groups .

after calculating violation rates , fmcsa assigns carriers it determines have sufficient exposure to peer groups with similar levels of on - road activity , or what the agency refers to as safety event groups .

according to fmcsa , safety event groups are designed to account for the inherent greater variability in violation rates based on limited levels of exposure and the stronger level of confidence in violation rates based on carriers with higher exposure .

fmcsa assigns carriers to safety event groups based on their number of inspections , the number of inspections with violations , or crashes the carriers have accrued in the previous 2 years .

within each safety event group , fmcsa calculates sms scores by ranking carriers' violation rates ( obtained in step 1 above ) and assigning each carrier a percentile score ranging from 0 to 100 , where 100 indicates the highest violation rate and the highest estimated risk for future crashes .

fmcsa displays scores for five of the basics on its public website .

once sms scores are calculated , fmcsa begins a safety evaluation that uses sms scores to identify carriers with safety performance problems requiring intervention .

fmcsa has defined a fixed percentage threshold for each basic that identifies those carriers that pose the greatest safety risk .

 ( for example , the threshold for the unsafe driving basic is 65 for most carriers. ) .

these carriers are then subject to one or more fmcsa actions from a suite of intervention tools that were expanded as part of csa .

tools such as warning letters and on - and off - site investigations allow fmcsa and state investigators to focus on specific safety behaviors .

fmcsa can also use enforcement strategies such as fines or placing a carrier out - of - service .

the range of available enforcement options gives fmcsa investigators flexibility to apply interventions commensurate with a carrier's safety performance ( see table 2 ) .

seven of the nine interventions are currently implemented nationwide .

prior to csa , fmcsa investigators' only tool was a labor intensive , comprehensive on - site investigation .

with the additional set of interventions , fmcsa aims to reach more carriers with its existing resources .

according to fmcsa and state safety officials , an investigation or other intervention can also be initiated based on the results of a crash investigation , a complaint against a carrier , or a consistent pattern of unsafe behavior by a carrier .

fmcsa further designates some carriers that exceed multiple basic thresholds as “high risk.” according to fmcsa , many of these carriers are assigned a safety investigator , who must complete a comprehensive review within a year regardless of any changes in the carrier's score .

a carrier is considered high risk if it either: has an sms score of 85 or higher in the unsafe driving basic or hours - of - service compliance basic or the crash indicator , and one other basic at or above the intervention threshold , or exceeds the intervention threshold for any four or more basics .

currently , fmcsa can only declare a carrier as unfit to operate upon a final unsatisfactory rating following an on - site inspection .

in addition , fmcsa can order a carrier to cease interstate operations if it determines that the carrier is an imminent hazard .

fmcsa can make this determination for several reasons including: fmcsa determining the carrier to be an imminent hazard .

receiving an “unsatisfactory” safety rating during an on - site comprehensive investigation and failing to improve the rating within 45 or 60 days ; failing to pay a fine after 90 days ; failing to meet the standards required for a new entrant audit ; or according to fmcsa , during fiscal year 2012 , the agency issued 855 out - of - service orders due to an unsatisfactory rating , 1,557 for failing to pay a fine , and 47 because a carrier was determined to be an imminent hazard .

fmcsa has indicated its plans to propose using the same performance data that inform sms scores to determine whether a carrier is fit to continue to operate .

according to fmcsa , the safety fitness determination rulemaking would seek to allow fmcsa to determine if a motor carrier is not fit to operate based on a carrier's performance in five of the basics , an investigation , or a combination of roadside and investigative information .

fmcsa proposes doing this through a public rulemaking process ; it currently estimates that it will issue a proposed rule in may 2014 .

csa has been successful in raising the profile of safety in the motor carrier industry and providing fmcsa with more tools to increase interventions with carriers .

however , fmcsa faces two major challenges in reliably assessing safety risk for the majority of carriers in the industry and prioritizing the riskiest carriers for intervention .

first , we found that the majority of regulations used to calculate sms scores are not violated often enough to strongly associate them with crash risk for individual carriers .

second , for most carriers , fmcsa lacks sufficient safety performance information to ensure that fmcsa can reliably compare them with other carriers .

fmcsa mitigates this issue by — among other things — establishing data sufficiency standards .

however , we found that these standards are set too low , and by strengthening data sufficiency standards sms would better identify risky carriers and better prioritize intervention resources to more effectively reduce crashes .

setting a data sufficiency standard involves tradeoffs between scoring more carriers and ensuring that the scores calculated are reliable for the purposes for which they are used .

csa has helped fmcsa reach more carriers and provided benefits to a range of stakeholders .

since csa was implemented nationwide in 2010 , fmcsa has intervened with more carriers annually than under safestat .

from fiscal year 2007 to fiscal year 2012 , fmcsa increased its number of annual interventions from about 16,000 to about 44,000 , largely by sending warning letters to carriers deemed to be above the intervention threshold in one or more basics ( see table 3 ) .

fmcsa and state partners also took advantage of new ways to investigate carriers , such as off - site investigations and on - site focused investigations , to complete 23 percent more investigations in fiscal year 2012 compared to fiscal year 2007 when only compliance reviews were used .

in addition , csa provides data for law enforcement and industry stakeholders about the safety record of individual carriers .

for example , as part of the csa program , fmcsa publicly provides historical individual carrier data on inspections , violations , crashes , and investigations on its website .

according to law enforcement and industry stakeholders we spoke with , csa organizes violation information for law enforcement and carrier data related to the basics help guide the work of state inspectors during inspections .

law enforcement officials and industry stakeholders generally supported the structure of the csa program .

these stakeholders told us that csa's greater reach and provision of data have helped raise the profile of safety issues across the industry .

according to industry stakeholders , carriers are now more engaged and more frequently consulting with law enforcement for safety briefings .

in colorado , law enforcement officials told us that csa has improved awareness and engagement within the motor carrier industry there .

a state industry representative told us that csa has improved safety because carriers are in a competitive business and can feel pressure to improve safety scores to gain an advantage over the competition .

the relationship between violation of most regulations fmcsa included in the sms methodology and crash risk is unclear , potentially limiting the effectiveness of sms in identifying carriers that are likely to crash .

according to fmcsa , sms was designed to improve on its previous approach to identify unsafe motor carriers by incorporating into the basics all of the safety - related violations recorded during roadside inspections .

for sms to be effective in identifying carriers that crash , the violation information that is used to calculate sms scores should have a relationship with crash risk .

carriers that violate a given regulation more often should have a higher chance of a crash or a higher crash rate than carriers that violate the regulation less often .

however , we found that fmcsa's safety data do not allow for validations of whether many regulatory violations are associated with higher crash risk for individual carriers .

our analysis found that most of the regulations used in sms were violated too infrequently over a 2-year period to reliably assess whether they were accurate predictors of an individual carrier's likelihood to crash in the future .

we found that 593 of the approximately 750 regulations we examined were violated by less than one percent of carriers .

of the remaining regulations with sufficient violation data , we found 13 regulations for which violations consistently had some association with crash risk in at least half the tests we performed , and only two violations had sufficient data to consistently establish a substantial and statistically reliable relationship with crash risk across all of our tests .

 ( for more information , see app .

v. ) fmcsa attempted to compensate for the infrequency of violations by , among other things , evaluating aggregate data to establish a broader relationship between a however , evaluations completed by group of violations and crash risk.outside groups have found weaker relationships between sms scores and the crash risk of individual carriers than fmcsa's evaluations of aggregate data ( for more information , see app .

iv ) .

sms is intended to provide a safety measure for individual carriers , and fmcsa has not demonstrated relationships between groups of violations and the risk that an individual motor carrier will crash .

therefore , this approach of aggregating data does not eliminate the limitations we identified .

most carriers lack sufficient safety performance information to ensure that fmcsa can reliably compare them with other carriers .

as mentioned , sms is designed to compare violation rates across carriers for the purposes of prioritizing intervention resources .

these violation rates are calculated by summing a carrier's weighted violations relative to each carrier's exposure to committing violations , which for the majority of the industry is very low .

about two - thirds of carriers we evaluated operate fewer than four vehicles and more than 93 percent operate fewer than 20 vehicles .

moreover , many of these carriers' vehicles are inspected infrequently .

 ( see table 14 in app .

vi ) generally , statisticians have shown that estimations of any sort of rate — such as the violation rates that are the basis for sms scores — become more reliable when they are calculated from more observations .

in other words , as observations increase , there is less variation and thus more confidence in the precision of the estimated rate .

given that sms calculates violation rates for carriers having a very low exposure to violations , such as operating one or two vehicles or subject to a few inspections , many of the sms scores carriers with based on these violation rates are likely to be imprecise.few inspections or vehicles will potentially have estimated violation rates that are artificially high or low and thus not sufficiently precise for comparison across carriers .

further , because sms scores are calculated by ranking carriers in relation to one another , imprecise rate estimates for some carriers can cause other carriers' sms scores to be higher or lower than they would be if they were ranked against only carriers with more reliable violation rates .

this creates the likelihood that many sms scores do not represent an accurate or precise safety assessment for a carrier .

as a result , there is less confidence that sms scores are effectively determining which carriers are riskier than others .

 ( app .

ii provides a more technical discussion of these issues. ) .

for the five sms basics for which fmcsa uses relevant inspections as a measure of exposure — hours - of - service compliance , driver fitness , controlled substances and alcohol , vehicle maintenance , and hazardous materials — estimated violation rates can change by a large amount for carriers with few inspections even when the number of their violations changes by a small amount .

for example , for a carrier with 5 inspections , a single additional violation could increase that carrier's violation rate 20 times more than it would for a carrier with 100 inspections .

this sensitivity can result in artificially high or low estimated violation rates that are potentially imprecise for carriers with few inspections .

as an example , our analysis of fmcsa's method shows that among carriers for which we calculated a violation rate for the hours - of - service compliance basic , violation rate estimates are more variable for carriers with fewer inspections .

as shown in figure 1 , violation rates tend to vary by a larger amount across carriers with few inspections than across carriers with more inspections .

as a consequence , a high estimated violation rate for a carrier with few inspections may reflect greater safety risk , an imprecise estimate , or both .

further , comparisons among carriers are meaningful only to the extent they involve carriers with sufficient inspections and thus more precise estimated violation rates .

similar to carriers with few inspections , carriers with few vehicles are also subject to potentially large changes in their estimated violation rates , which can affect a carrier's sms scores .

for the unsafe driving basic and the crash indicator , fmcsa measures exposure using a hybrid approach that considers a carrier's number of vehicles and its vehicle miles traveled — when the latter information is available .

figure 2 shows that among carriers for which we calculated a violation rate using fmcsa's method for the unsafe driving basic , carriers that operate fewer vehicles , for example fewer than 5 , experience a greater range in violation rates per vehicle than carriers operating more vehicles , for example , greater than 100 .

 ( for similar results on other basics , see figures 10 to 16 in app .

vi. ) .

researchers have raised additional concerns about the quality and accuracy of the data fmcsa uses to calculate sms scores that could potentially compound the problems with the precision of violation rate estimates .

these issues further limit the precision of carriers' estimated violation rates , and consequently their sms scores .

for example: the frequency of an individual carrier's inspections varies depending on where the carrier operates .

states vary on inspection and enforcement practices .

some studies have shown that inspectors or law enforcement officers in some states cite vehicles for certain violations more frequently than in other states .

delays in reporting crash data to fmcsa , as well as missing or inaccurate data , can affect a carrier's crash indicator sms scores .

these delays can vary by state .

data elements used to calculate violation rates for the unsafe driving basic and crash indicator are based on information that is self reported by the carrier .

inaccurate , missing , or misleading reports by a carrier could directly influence their sms scores .

additionally , among carrier data we evaluated , more than 50 percent did not report their vehicle miles traveled to fmcsa .

fmcsa acknowledges that violation rates for carriers with low exposure can be less precise and they attempt to address this limitation in two main ways , but the methods incorporated do not solve the underlying problems .

as a result , sms scores for these carriers are less reliable as relative safety performance indicators , which may limit fmcsa's ability to more effectively prioritize carriers for intervention .

fmcsa established minimum data sufficiency standards to eliminate carriers that lack what it has determined to be a minimum number of inspections , inspections with violations , or crashes to produce a reliable sms score .

for example , in the hours - of - service compliance basic , fmcsa does not calculate sms scores for a carrier unless it has at least three inspections and at least one violation within the preceding two years .

in addition , as previously mentioned fmcsa applies another data sufficiency standard requiring a carrier to have a “critical mass” of inspections with violations in order for an sms score to be a basis for potential intervention , or to be publicly displayed .

while this approach helps address the problems for carriers with low exposure , it is not sufficient to ensure that sms scores effectively prioritize the riskiest carriers for intervention .

for most basics , we found fmcsa's data sufficiency standards too low to ensure reliable comparisons across carriers .

in other words , many carriers' violations rates are based on an insufficient number of observations to be comparable to other carriers in calculating an accurate safety score .

our analysis shows that rate estimates generally become more precise around 10 to 20 observations , higher than the numbers that fmcsa uses for data sufficiency standards .

however , the determination of the exact data sufficiency standard needs to based on a quantitative measure of confidence to fully consider how precise the scores need to be for the ( for more information , see purposes for which the scores are used.app .

ii. ) .

fmcsa groups the carriers meeting fmcsa's data sufficiency standards for each basic into safety event groups in order to , according to fmcsa , “account for the inherent greater variability in violation rates based on limited levels of exposure and the stronger level of confidence in violation rates based on higher exposure.” based on inspections or inspections with violations depending on the basic or on crashes for the crash indicator .

for example , the first safety event group in the hours - of - service compliance basic includes carriers that received from 3 to 10 inspections ; the second group includes carriers that received from 11 to 20 inspections , and so forth .

within each safety event group , fmcsa rank orders carriers by violation rate and assigns a percentile as an sms score .

csa , csms methodology , version 3.0.1 , revised august 2013. exceed fmcsa's intervention thresholds at disproportionately higher rates than carriers with more exposure .

for example , fmcsa's hours - of - service compliance basic has five safety event groups .

the group of carriers with the fewest number of inspections in each safety event group tends to have a higher percentage of carriers identified as above the intervention threshold than the group of carriers with a greater number of inspections ( see fig .

3 ) .

this suggests that fmcsa's methodology is not adequately accounting for differences in exposure , as it is intended to do , but rather is systematically assigning higher scores for carriers with fewer inspections .

 ( see figs .

17 to 25 in app .

vi for other basics. ) .

fmcsa's method of categorizing the carriers into safety event groups for the remaining basics also demonstrates how imprecision disproportionately affects small carriers .

for the unsafe driving and controlled substances basics , fmcsa forms safety event groups based on the number of inspections with violations .

similarly , for the crash indicator , safety event groups are based on a carriers' number of crashes .

by using infractions or crashes to categorize carriers , fmcsa is not addressing its stated intent of having safety event groups account for differences in variability due to exposure .

as a result , fmcsa derives sms scores for the unsafe driving basic and the crash indicator by directly comparing small carriers with greater variability in their violation rates — including many carriers with a violation rate based on one vehicle — to larger carriers for which violations rates can be calculated with greater confidence .

we found that among carriers that received an sms score in unsafe driving , carriers with fewer than 20 vehicles are more than 3 times as likely to be identified as above the intervention threshold than carriers with 20 or more vehicles ( see fig .

4 ) .

of the carriers operating one vehicle , nearly all were identified as above the intervention threshold .

 ( see figs .

26 to 32 in app .

vi for other basics. ) .

fmcsa contends that these results are expected because only small carriers that exceed critical mass standards receive an sms score , and small carriers that exceed this threshold have demonstrated several occurrences of risky behavior despite their limited exposure .

however , this illustrates the volatility of rates and the disproportionate effect a single violation can have given how fmcsa has structured sms .

for example , using fmcsa's data sufficiency standards , a carrier with one vehicle ( forty percent of the carriers in our analysis population have one vehicle ) and two inspections with unsafe driving violations does not have sufficient information to be displayed or considered for intervention .

however , a single additional violation , regardless of the severity of the violation , would likely mean that the carrier would be scored above threshold and prioritized for intervention .

a relatively small difference in the number of violations could change a carrier's status from “insufficient information” , to “prioritized for intervention” with potentially no interim steps .

conversely , a carrier such as this will have a very difficult time improving its sms score to be below threshold .

our analysis shows that fmcsa could improve its ability to identify carriers at higher risk of crashing by applying a more stringent data sufficiency standard .

as previously discussed , fmcsa uses sms scores to identify carriers with safety performance problems — those above the threshold in any basic — for prioritization for intervention , and considers carriers with sms scores above the intervention threshold in multiple basics as high risk .

overall , sms is successful at identifying a group of high risk carriers that have a higher group crash rate than the average crash rate of all carriers that we evaluated .

however , further analysis shows that a majority of these high risk carriers did not crash at all , meaning that a minority of carriers in this group were responsible for all the crashes .

as a result , fmcsa may devote significant intervention resources to carriers that do not pose as great a safety risk as other carriers , to which fmcsa could direct these resources .

given the issues with precision discussed above , we developed and tested an alternative to fmcsa's method that sets a single data sufficiency standard , based on the relevant measure of exposure — either at least 20 inspections or at least 20 vehicles ( depending on the basic ) , and eliminates the use of safety event groups .

this approach is designed to illustrate how a stronger data sufficiency standard can affect the identification of higher risk carriers and is not meant to be a prescriptive design to replace current sms methods.effect that including carriers with low levels of exposure and highly variable violation rates can have on fmcsa's prioritization of carriers for intervention .

using this illustrative alternative , we found that fmcsa would have more reliably identified a higher percentage of carriers that actually had crashed than when compared to its existing methods .

 ( apps .

i and vi provide more detail on this approach. ) .

specifically: the result of this analysis demonstrates the this illustrative alternative identified about 6,000 carriers as high risk .

during the evaluation period of our analysis , these carriers' group crash rate was approximately the same as the rate for fmcsa's high risk group ( about 8.3 crashes per 100 vehicles ) .

however , a much greater percentage of carriers ( 67% ) identified as high risk using alternative higher data sufficiency standards crashed , and these carriers were associated with nearly twice as many crashes ( see table 4 ) .

for five out of six basics , the crash indicator , and the high - risk designation , the illustrative alternative identified a higher percentage of individual carriers above the intervention threshold that actually crashed compared with fmcsa's existing method .

 ( see fig .

5. ) .

using both fmcsa's method and the illustrative alternative , for most of the basics and the crash indicator the carriers identified above the intervention threshold had a higher crash rate ( crashes per 100 vehicles ) than those below the intervention threshold ( see table 5 ) .

however , using fmcsa's method , crash rates for the controlled substances and alcohol basic have the opposite , negative association ( 3.2 crashes per 100 vehicles for carriers above threshold versus 5.2 crashes per 100 vehicles for carriers below threshold ) , whereas the illustrative alternative produces a positive association ( 4.7 crashes per 100 vehicles for carriers above threshold versus 3.8 crashes per 100 vehicles for carriers below threshold ) .

overall , these results raise concerns about the effectiveness of the existing sms as a tool to help fmcsa prioritize intervention resources to most effectively reduce crashes .

fmcsa's existing sms method successfully identified as high risk more than 2,800 carriers whose vehicles were involved in 12,624 crashes .

however , fmcsa would have potentially prioritized limited resources to investigate more than 4,000 carriers that did not crash at all .

prioritizing resources to these carriers would limit fmcsa's ability to reduce the number of overall crashes , resulting in lost opportunities to intervene with the carriers associated with many crashes .

implementing a stronger data sufficiency standard as presented involves tradeoffs between the number of carriers fmcsa can score , and the reliability of those scores .

our analysis found that by increasing the data sufficiency standards , fewer carriers would receive at least one sms score ( approximately 44,000 carriers in the illustrative alternative versus approximately 89,000 using fmcsa's method ) .

the carriers assigned an sms score under the illustrative alternative accounted for 78.2 percent of all crashes during our evaluation period .

fmcsa's existing method scores carriers responsible for about 85.9 percent of all crashes ( see table 6 ) .

on the other hand , by setting a higher standard for data sufficiency , the illustrative alternative focuses on carriers that have a higher level of road activity , or exposure , to more reliably calculate a rate that tracks violations and crashes over the 2-year observation period .

in addition , exposure itself is a large determinant of overall risk , when defined as a combination of threat and consequence , and could be used as a factor to identify carriers that analysis suggest present a higher future crash risk .

this is consistent with the results in table 4 above , which show that a larger proportion of the higher risk carriers in the illustrative alternative crashed and were associated with a larger number and proportion of crashes .

regardless of where the data sufficiency standard is set , using only sms scores limits risk assessment for carriers that do not have sufficient performance information .

our analysis shows that using fmcsa's existing method , about 28% of carriers have at least one sms score , leaving approximately 72% of carriers without any sms scores — largely due to insufficient information .

the illustrative alternative scores fewer carriers — 14% , leaving 86% of carriers without any sms scores .

however , according to an fmcsa official , there are other enforcement mechanisms to assess and place unsafe carriers out - of - service , including when a carrier fails to improve from an unsatisfactory safety rating during a comprehensive review , fails to pay a fine , or fmcsa determines a carrier is an imminent hazard .

further , the fmcsa official said carriers that do not receive an sms score can still be monitored because the officials can initiate investigations and remove carriers based on complaints and other initiatives .

for example , fmcsa conducts inspection strike forces targeting unsafe drivers and carriers in a particular safety aspect , such as drug and alcohol safety records .

these tools used in conjunction with the performance data , including roadside inspection and crash data , could provide fmcsa with complementary means to assess and target carriers that do not otherwise have sufficient data to reliably calculate sms scores .

the safety scores generated by sms are used for many purposes , thus the appropriate level of precision required depends on the nature of these applications .

according to fmcsa's methodology , sms is intended to prioritize intervention resources , identify and monitor carrier safety problems , and support the safety fitness determination process .

in setting a data sufficiency standard , fmcsa needs to consider how precise the scores need to be , and a score's required precision depends on the purposes for which the scores are used .

fmcsa officials told us the primary purpose of sms is to serve as a general radar screen for prioritizing interventions .

however , as discussed above , due to insufficient data , sms is not as effective as it could be for this purpose .

further , if the same safety performance data used to inform sms scores are intended to help determine a carrier's fitness to operate , most of these same limitations will apply .

according to fmcsa , the safety fitness determination rulemaking would seek to allow fmcsa to determine if a motor carrier is not fit to operate based on a carrier's performance in five of the basics , an investigation , or a combination of roadside and investigative information .

fmcsa has postponed the planned rulemaking until may 2014 .

however , basing a carrier's safety fitness determination on limited performance data may misrepresent the safety status of carriers , particularly those without sufficient data from which to reliably draw such a conclusion .

in addition to using sms for internal purposes , fmcsa has also stated that sms provides stakeholders with valuable safety information , which can “empower motor carriers and other stakeholders…to make safety - based business decisions.” publicly released sms scores stating that the data are intended for agency and law enforcement purposes , and readers should not draw safety conclusions about a carrier's safety condition based on the sms score , but rather the carrier's official safety rating .

nonetheless , entities outside of fmcsa are also using sms scores to assess and compare the safety of carriers .

for example: fmcsa includes a disclaimer with the the department of defense has written sms scores into its minimum safety criteria for selecting carriers of hazardous munitions .

fmcsa has released a mobile phone application — saferbus — that is designed to provide safety information , including sms scores , for consumers to use in selecting a bus company .

multiple stakeholders have reported that entities such as insurers , freight shippers and brokers , and others use sms scores .

given such uses , it is important that any information about sms scoresmake clear to users , including fmcsa , the purpose of the scores , their precision , and the context around how they are calculated .

stakeholders have said that there is a lot of confusion in the industry about what the sms scores mean and that the public , unlike law enforcement , may not understand the relative nature of the system and its limitations .

csa , csms methodology , version 3.0.1 motor carrier preview , revised august 2013 .

with the establishment of its csa program , fmcsa has implemented a data - driven approach to identify and intervene with the highest risk motor carriers .

csa helps fmcsa to reach more carriers through interventions and provides the agency , state safety authorities , and the industry with valuable information regarding carriers' performance on the road and problems detected during roadside inspections .

gao continues to believe a data - driven , risk - based approach holds promise and can help fmcsa effectively identify carriers exhibiting compliance or safety issues — such as violations or involvement in crashes .

however , assessing risk for a diverse population of motor carriers — many of which are small and inspected infrequently — presents several significant challenges for fmcsa .

as a result , the precision and confidence of many sms scores is limited , a limitation that raises questions about whether sms is effectively identifying carriers at highest risk for crashing in the future .

as presented in the report , strengthening data sufficiency standards is one of several potential reforms that might improve the precision and confidence of sms scores .

however , strengthening data sufficiency standards involves a trade - off between assigning scores to more carriers and ensuring that those scores are reliable .

our analysis shows how improving the reliability of sms scores by strengthening data sufficiency standards could better account for limitations in available safety performance information and help fmcsa better focus intervention resources where they can have the greatest impact on reducing crashes .

in addition , if these same safety performance data are going to be used to determine whether a carrier is fit to operate , fmcsa needs to consider and address all identified data limitations , or these determinations will also be at risk .

to improve the csa program , the secretary of transportation should direct the fmcsa administrator to take the following two actions: revise the sms methodology to better account for limitations in drawing comparisons of safety performance information across carriers ; in doing so , conduct a formal analysis that specifically identifies: limitations in the data used to calculate sms scores including variability in the carrier population and the quality and quantity of data available for carrier safety performance assessments , and limitations in the resulting sms scores including their precision , confidence , and reliability for the purposes for which they are used .

ensure that any determination of a carrier's fitness to operate properly accounts for limitations we have identified regarding safety performance information .

we provided a draft of this report to the usdot for review and comment .

usdot agreed to consider our recommendations , but expressed what it described as significant and substantive disagreements with some aspects of our analysis and conclusions .

usdot's concerns were discussed during a meeting on january 8 , 2014 , with senior usdot officials , including the fmcsa administrator .

following this meeting , we made several clarifications in our report .

in particular , fmcsa understood our draft recommendation to be calling for specific changes to its sms methodology .

it was not our intent to be prescriptive , so we revised our first recommendation to state that fmcsa should conduct a formal analysis to inform potential changes to the sms methodology .

in addition , we clarified in the analysis and conclusions our meaning of reliability in context of the purpose for which sms is used .

we are sending copies of this report to relevant congressional committees and the secretary of transportation .

in addition , the report is available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-2834 or flemings@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vii .

this report addresses the effectiveness of the compliance , safety , accountability ( csa ) program in assessing safety risk for motor carriers .

to assess how effectively csa assesses the safety risk of motor carriers , we reconstructed the models the federal motor carrier safety administration ( fmcsa ) uses to compute the sms scores for all six behavior analysis and safety improvement categories ( basics ) and the crash indicator .

we then assessed the effect of changes to key assumptions made by the models .

using data collected by the u.s. department of transportation's motor carrier management information system ( mcmis ) and historical sms scores , and referencing the sms algorithm and methodological documentation , we replicated the algorithm for calculating the sms basic scores for the sms 3.0 methodology .

reconstructing fmcsa's models and replicating the sms scores fmcsa produced for carriers was a necessary step to ensure that we understood the complexities of the models , the data used in the calculation of the sms scores , and that the results we present in this report are comparable to fmcsa's outcomes .

to corroborate our models with fmcsa's , we compared the sms violation rates ( measure scores ) to fmcsa's results for december 2012 .

we assessed the reliability of data used , for our purposes , by reviewing documentation on fmcsa's data collection efforts and quality assurance processes , talking with fmcsa and volpe national transportation systems center officials about these data , and checking the data for completeness and reasonableness .

we determined that the data were sufficiently reliable for the purpose of our data analysis .

we established a population of about 315,000 carriers for analysis that were under fmcsa's jurisdiction and showed indicators of activity over a 3 and a half year analysis period from december 2007 through june 2011 .

the criteria used to identify these carriers were: u.s. - based carriers ; interstate or intrastate hazardous materials carriers ; carriers with at least one inspection or crash during the 2-year analysis observation period ( december 18 , 2007 to december 17 , 2009 ) ; and carriers with a positive average number of vehicle count at any point during the analysis observation period ( december 18 , 2007 , to december 17 , 2009 ) and at any point during the evaluation period ( december 17 , 2009 , to june 17 , 2011 ) .

during the first 2 years of this period , december 2007 through december 2009 , we used each carrier's inspection , crash , and violation history to calculate sms scores .

this period is referred to as the observation period .

the remaining 18 months , december 2009 through june 2011 , were classified as the evaluation period .

we used data from this period to identify carriers involved in a crash and estimate crash rates for these carriers .

for the approximately 315,000 carriers in our analysis , there were approximately 120,000 crashes during the evaluation period .

we chose the lengths of time for observation and evaluation , in part , to match fmcsa's effectiveness testing methods .

we tested the effectiveness of sms by identifying and making changes to key assumptions of the model .

given fmcsa's use of these scores as quantitative determinations of a carrier's safety performance , we assessed the reliability of sms scores as defined by the precision , accuracy , and confidence of these scores when calculated for carriers with varying levels of carrier exposure — measured by fmcsa as either inspections or an adjusted number of vehicles .

we tested changes to the following characteristics of the model: the sms measures of exposure , the method used to calculate time weights , the organization of the violations to the six basics , and the data sufficiency standards .

to evaluate the results produced by each model , including fmcsa's , we examined the sms scores and classifications of carriers into the high risk group .

we compared the results from our revised models to the results from a baseline model , sms 3.0 .

for each model , we measured whether carriers were involved in a crash , calculated group crash rates , and calculated total crashes in the evaluation period for carriers that were and were not classified as high risk in the observation period .

due to ongoing litigation related to csa and the publication of sms scores , we did not assess the potential effects or tradeoffs resulting from any public use of these scores .

to determine the extent to which csa identifies and intervenes with the highest risk carriers , we examined how our changes to fmcsa's key assumptions affected the safety scores and identification of high risk carriers .

specifically , we identified the carriers with sms scores above fmcsa's intervention threshold in each basic and the carriers considered high risk according to fmcsa's high risk criteria .

using this analysis , we designed an illustrative alternative method that incorporates the following changes: including only carriers with at least 20 observations in the following measures of exposure: driver inspections when calculating scores for the hours - of - service compliance , driver fitness , and controlled substances basics ; vehicle related inspections for the vehicle maintenance basic ; vehicle related inspections where placardable quantities of hazardous materials are being transported for hazardous materials basic ; and average power units for the unsafe driving and crash indicator assigning an sms score to any carrier meeting these data sufficiency standards ( eg , 20 inspections ) , even if that carrier does not have any violations , was free of violations for 12 months , or had a clean last inspection ; eliminating safety event groups because of the stricter data sufficiency using only the average number of vehicles as the measure of exposure for carrier's assessed in the unsafe driving and crash indicator basics .

appendix vi provides the complete results of our replication of fmcsa's existing sms and our illustrative revision to it .

we also examined the extent to which the regulatory violations that largely determine sms scores can predict future crashes .

we developed eight model groups to test the relationship between violations and violation rates , and crashes .

we tested only the violations that had non - zero variance and observations for at least 1 percent of the test population .

to control for small exposure measures when estimating rates , we estimated models comparing carriers' observed crash status to bayesian crash rates ; used observed violation rates versus bayesian violation rates ; and compared a full model sample to a restricted model sample of carriers with at least 20 vehicles.sensitivity analysis to validate the predictive power of the models we developed .

we ran multiple variations of these models to determine the number and types of violations that were predictive versus unstable .

for we also conducted a more information on this specific analysis and model results , please see appendix v. in addition , we spoke with fmcsa officials in washington , d.c. , and at the western service center and the colorado division office in lakewood , colorado , and reviewed existing studies and stakeholder concerns about the sms model and its outcomes .

to understand the impact of csa on law enforcement , we spoke with law enforcement officials at the colorado state patrol .

we selected colorado because it was one of the initial pilot states for csa , and has been implementing the program since early 2008 .

we also interviewed representatives from industry and safety interest groups from the colorado motor carriers association , the commercial vehicle safety alliance , and the american trucking associations .

additionally , we attended meetings of the motor carrier safety advisory committee's csa subcommittee and reviewed the minutes and related documentation from other meetings we did not attend .

we also reviewed congressional testimony from industry and safety interest representatives from a september 2012 hearing for the house transportation and infrastructure committee .

we reviewed stakeholder comments submitted between march 2012 and july 2012 in response to fmcsa's planned improvements to sms .

we conducted this performance audit from august 2012 to february 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the fmcsa safety measurement system ( sms ) methodology involves the calculation of weighted violation rates for regulations within each of six behavioral analysis and safety improvement categories ( basics ) and a given time period .

 ( a seventh indicator measures weighted crash rates in previous time periods , or “crash history.” ) carriers are assigned to safety event groups based on measures of their exposure to committing violations , such as the number of driver or vehicle inspections , depending on the basic , and the weighted violation rates are transformed into percentiles for carriers within the same group .

these percentiles ultimately determine carriers' alert or high - risk statuses .

because regulatory violation rates strongly influence sms scores , the precision with which these rates can be calculated becomes important for developing reliable measures of safety , as we discuss in the body of this report .

in this appendix , we summarize statistical methods for estimating rates and assessing their precision , or sampling error .

we use these methods to estimate crash rates and their sampling error for a population of motor carriers that were active from december 2007 through december 2009 .

carriers may vary widely in their level of activity , known as “exposure.” both statistical theory and our analysis show that the precision of estimated rates for carriers with low exposure , measured by vehicles or inspections , is lower than for carriers with more exposure , and that rate estimates can become distorted to artificially low or high values for these low - exposure carriers .

these results support our findings in the body of this report on the precision of fmcsa's current approach to calculating safety risk scores and setting data sufficiency standards .

estimating rates of regulatory violations requires data on the number of violations that carriers incur within a given time period .

if one makes the assumption that the number of violations is proportional to some measure of exposure ( activity ) and also assumes that the probability of observing violations within a large number of small independent exposure periods is small , the sampling error of a rate estimate decreases as exposure increases .

specifically , assume that each carrier in a population of interest has a unique violation rate , λ .

for a fixed time period and known exposure , t , the number of violations , v , is distributed as v ~ poisson ( λ t ) , with e ( v ) = var ( v ) = λ t. since λ is unknown , it must be estimated from data on regulatory violations and exposure .

the maximum likelihood ( ml ) estimator for a single carrier's λ , given the model above , is λı� = v / t , with var ( λı� ) = λı� / t = v / t.the rate estimate increases exponentially as exposure decreases .

accordingly , an estimated rate for a specific carrier and time period can vary substantially from λ , particularly when exposure is low .

sms is primarily concerned with measuring how regulatory violation rates vary over a population of active motor carriers .

even though ordinary methods of estimating these rates are unbiased and consistent , the collection of estimated rates for the population , 𝛌� = {λestimates , such as the percentiles that sms uses to place carriers into alert and high - risk status , may be similarly prone to error .

empirical bayesian methods correct for this problem by estimating λı� for each carrier to better estimate the distribution of rates across a population .

bayesian methods prevent estimates from converging to artificially extreme values for carriers whose raw rate estimates are based on small samples ( low exposure ) .

the estimator does this by effectively “borrowing information” from other , larger carriers whose rates can be estimated more precisely .

in the evaluation of the csa pilot test for fmcsa , the university of michigan transportation research institute used empirical bayesian rate estimation methods to evaluate the association between sms scores and crash risk , and cited similar benefits to those we discuss here .

for example , see roger j. marshall , “mapping disease and mortality rates using empirical bayes estimators,” journal of the royal statistical society , series c ( applied statistics ) 40 , no .

2 ( 1991 ) : 284 , or j. n. k. rao , small area estimation ( hoboken , nj , 2003 ) , 206 .

specifically , assume that regulatory violation rates over a population of carriers are distributed as λı� ~ gamma ( α , β ) , the prior distribution of the parameter of interest .

parameter values for the prior distribution can be assumed , based on historical data on the population of interest , or estimated using a particular sample .

conditional on these rates , the data on regulatory violations are distributed as v | λ , t ~ poisson ( λ t ) , and the posterior distribution for a specific carrier is given by λ | v , t ~ gamma ( α + v , β + t ) ( 1 ) since the mean of a gamma variate is α / β and the variance is α / β , the posterior mean and variance of the rate for a given carrier are given by e ( λ | v , t ) = ( α + v ) / ( β + t ) ( 2 ) var ( λ | v , t ) = ( α + v ) / ( β + t ) , and the mean of the prior distribution , α / β .

when enough data are available , as indicated by a large exposure term relative to the violation term , the estimate converges to the ordinary , carrier - specific rate estimate .

when exposure is low , however , the method combines data from the specific carrier with the mean rate for all carriers .

the variance of bayesian rate estimates decreases with increased exposure , similar to the variance of ordinary rate estimates .

figure 6 shows how hypothetical rate estimates and 90% posterior intervals for a carrier that experienced 5 crashes vary with the carrier's exposure , as measured by the number of vehicles .

 ( although we illustrate rate estimation issues using crash rates , we likely would have obtained similar results if we had estimated regulatory violation rates. ) .

as expected , the precision of the estimates decreases exponentially as the number of vehicles increases .

the variance is high in the range of 1 to 5 vehicles and begins to decrease less quickly at approximately 20 vehicles , consistent with our discussion in the body of this report and prior evaluations of sms .

thresholds in this approximate range are consistent with criteria used by the centers for disease control and prevention ( cdc ) to suppress or caveat rate estimates for the purpose of public display .

for example , in its compendium of health statistics in the united states , cdc cautions that “hen the number of events is small and the probability of such an event is small , considerable caution must be observed in interpreting the conditions described by the figures.” even though the bayesian estimates do not converge to extremely low or high values when exposure is low , the uncertainty around the estimates remains high .

as figure 6 shows , statistical methods for modeling and estimating rates can quantify this uncertainty explicitly , in order to reflect the varying precision of estimates for motor carriers with more or less observed data .

although the amount of uncertainty that is acceptable in practice depends on the purpose of the estimates , both statistical theory and government agencies estimating rates similar to those involved in the calculation of sms scores have recognized the need to express the uncertainty of these estimates , particularly when the derived from small samples .

this contrasts with fmcsa's approach , which reports sms scores as safety risk estimates with no quantitative measures of precision .

to illustrate the rate estimation issues discussed above in the context of motor carrier safety , we estimated individual crash rates for a population of motor carriers that were actively operating in each of two time periods , december 2007 through december 2009 , and december 2009 through june 2011 , as measured in fmcsa's motor carrier management information system ( mcmis ) .

an “active” carrier was one that , in each time period , had at least one inspection or crash and had been recorded as a us - based interstate or intrastate hazmat carrier .

this definition resembled the one we used in replicating sms , as described in the body of this report and appendix i .

we obtained these data from the december 2010 and december 2012 mcmis “snapshot” data files , as well as a historical file of carrier - specific information that covered all snapshots .

we estimated the raw and empirical bayesian crash rates for each carrier in the first time period , using data on the number of crashes and vehicles for these carriers and the formulas above .

we used the “empirical bayes” version of the rate estimator , in which the parameters of the prior distribution were estimated from the data .

specifically , we fit the observed rate data for all carriers in the first time period to the negative binomial distribution , parameterized with exposure measured by number of vehicles , and estimated α and β using standard methods of maximum likelihood estimation .

the final rate estimates for each carrier were a combination of these parameter estimates and carrier - specific data , according to equation 2 above .

as theory would predict , bayesian methods prevented crash rates from converging to zero or extremely high values for carriers with low exposure .

the left half of figure 7 presents the raw crash rates for our analysis carriers , while the right half presents the empirical bayesian estimates .

the raw estimates for carriers with about 1 to 10 vehicles can be 10 to 20 times higher than for carriers with more than 10 vehicles .

in addition , the raw rates cluster at zero for a large number of carriers , particularly for those with low exposure .

an underlying crash rate of zero is implausible for active carriers .

in contrast , the bayesian rate estimates are more stable , with no inflation or deflation to extreme values .

since the body of this report finds that 93 percent of carriers in our replication of sms had fewer than 20 vehicles , bayesian methods may provide more stable estimates for many specific carriers and may better approximate the distribution of rates across carriers .

in addition to stabilizing rates for small carriers , bayesian rate estimation methods provide an explicit measure of precision for each carrier's rate , regardless of size .

in figure 8 , we show the bayesian rate estimates for a random sample of 109 carriers in the first period of our analysis population , along with 90 percent bayesian posterior intervals .

 ( we present these results for a sample to make the intervals readable. ) .

the posterior interval expresses the range over which the true rate exists with a 90 percent probability .

consistent with theory , the precision of the rate estimates increases with exposure — in this case , the number of vehicles .

these results apply to actual carriers in the sample , but the results are consistent with those expected by theory .

the width of the posterior intervals does not decrease monotonically , however , because the relative number of crashes also affects the variance and is not held constant in the plot .

in this appendix , we express the safety management system ( sms ) as a statistical measurement model , in order to make its assumptions explicit , and describe how estimating the model could validate those assumptions .

we find that fcmsa's sms makes a number of strong assumptions about motor carrier safety that empirical data cannot easily validate .

the sms uses administrative data on inspections of commercial motor carriers , violations of regulations , and crashes to measure carrier safety .

statisticians and other researchers have developed methods to validate measures of such broad concepts as safety , referred to as “latent variables,” using empirical data .

these methods are known as “measurement models.” for example , mental health professionals have created scales to measure the existence of broad disorders , such as depression , by combining responses to multiple items on patient questionnaires .

sms has a similar goal: to create scales to measure motor carrier safety risk on several dimensions , such as “unsafe driving” or “vehicle maintenance,” by combining violation rate data across multiple regulations .

latent variable measurement methods can assess whether these broader measures are valid and reliable , and whether the empirical indicators that go into them actually measure the intended concepts .

estimating the degree to which various indicators measure a broader concept helps confirm and often improve the reliability and validity of the scales constructed .

much of the sms involves calculating weighted regulatory violation rates for motor carriers in a given time period .

fmcsa assigns weights that , in principle , reflect the violations' associations with one of six dimensions of safety , known as behavioral analysis and safety improvement categories ( basics ) , such as “unsafe driving” and “vehicle maintenance.” the weights represent what fmcsa considers to be the strength of each violation's association with safety , relative to other violations in the same basic .

all violations that are categorized in a basic get a positive weight ranging from 1 to 10 , which implies that they have some association with safety .

these weighted violation rates strongly influence the final sms measures of safety on these dimensions .

each basic is linked to a set of violations , which are all assumed to measure the same dimension of safety .

each violation maps to exactly one basic , though basics map to multiple violations in their associated groups .

 .

vij measures the number of times that carrier i violated regulation j in a given time period .

𝜆𝑗 is a weight for each violation .

it is the product of a “severity” weight , measuring what fmcsa considers the violation's “crash risk relative to the other violations comprising the basic measurement,” in addition to outcomes thought to be particularly severe ( eg , out - of - service violations ) , and a time weight , measuring what fmcsa considers the importance of violations from different time periods to estimating a carrier's current level of safety .

by defining vij for fixed time periods , such as 6 or 12 months prior to the measurement time , we collapse the separate weights used in sms into 𝜆𝑗 , in order to simplify the notation .

lastly , t measures exposure to committing violations in the time period , which is either a function of carrier's vehicles and vehicle miles traveled ( vmt ) or the time - weighted sum of relevant inspections , depending on the basic .

sms transforms the weighted violation rates for each carrier into percentile ranks , after applying a number of “data sufficiency standards” to exclude carriers with few violations , inspections , and / or vehicles .

carriers with percentiles that exceed established thresholds are “alerted” on the relevant basics and , if enough alerts or other conditions exist , are identified as “high risk.” as a result , the ultimate measures of safety risk are ordered groups , with cut - points defined by basic percentiles for carriers that meet fmcsa's standards for data sufficiency .

the sms can be viewed as an attempt to measure latent concepts of “safety,” such as “unsafe driving” or “vehicle maintenance,” using observed data on regulatory violations and the opportunity to commit them ( exposure ) .

consider the latent variable measurement model below , using notation from a prominent textbook: the weights describing the relationship between the latent and observed the model assumes that a vector of 𝑘=∑ 𝑛𝑔𝑝𝑔 observed variables , r , are determined by p latent variables , 𝜉 , and random measurement error , 𝛿. variables make up the block diagonal matrix λ , with p blocks of weights applied to the corresponding blocks of observed variables .

this structure latent variable .

in many applications , the model assumes that cov ( 𝜉 implies that each group of observed variables is related to exactly one , 𝛿 ) =0 and e ( 𝛿 ) = 0 but allows other variances and covariances to be estimated from the data as parameters or fixed to known values .

the sms is a particular form of the model above .

specifically , sms defines r as violation rates for k = 826 regulations , where r may include variables measured at different times .

it sets p = 6 and relates the violation rates to the basics , or latent variables 𝜉 measuring safety , through the weighting matrix λ. fmcsa created fixed time and severity assumes that 𝛿=0 .

a graphical version of sms as a measurement weights for each regulation through a combination of statistical analysis and the opinions of stakeholders .

since sms is not a stochastic model , it model appears in figure 9 below .

when expressed as a measurement model , the strong assumptions of sms — and their potential detrimental effect on its usefulness — become clear .

fmcsa's assumption of zero measurement error is unusual for statistical approaches to measurement , given that any particular violation is likely to represent variation in latent variables ( in this case , safety ) as well as unmeasured variables summarized by the error term .

sms makes specific assumptions about the number of safety dimensions — the latent variables assumed by the model above — as well as their relationships to violation rates .

exactly six dimensions of safety exist ( involving regulations ) , and each violation rate measures only one of them .

in other efforts to measure broad concepts using numerous indicators , inference about the existence and relationships among observed and latent variables are endogenous parameters ( determined by the model ) to be estimated , rather than exogenous parameters ( determined outside the model ) that are fixed ex ante , ahead of time , as they are here .

finally , sms takes the unusual step of fixing the values of the weights relating the latent variables measuring safety to violation rates at values other than 0 .

this assumes a high degree of prior knowledge about the relationships between latent and observed variables .

although fmcsa has conducted several studies of how regulatory violation rates are associated with crash risk , these studies do not directly estimate the degree to which each type of violation reflects one of several dimensions of safety .

one approach to validating the assumptions of sms is to estimate the parameters of the measurement model above using empirical data on regulatory violation rates .

this approach is known as confirmatory factor analysis , which is a special type of measurement model .

because sms makes specific assumptions about the number of basics and the violations that go into them , we can express the system as a measurement model , as discussed above , and estimate the degree to which its assumptions are consistent with reality .

for example , sms assumes that six dimensions of safety exist — labeled basics in sms — and that each violation reflects only one dimension .

however , a model that assumes three basics and allows violations to reflect multiple dimensions of safety might be a plausible alternative .

high violation rates for brake maintenance regulations may indicate worse performance on both the vehicle maintenance and unsafe driving dimensions of safety .

measurement modeling can identify which of these approaches better fits empirical patterns of regulatory violations .

more generally , analyzing sms as a measurement model can validate its assumptions , such as the values of the severity and time weights , and suggest improvements to better measure safety .

we can extend the sms measurement model to predict empirical data on crash risk , in order to further validate its ability to identify high - risk carriers .

this structural equation modeling ( sem ) approach combines the measurement model above with a model that describes how the latent dimensions of safety predict crash risk , generically known as “endogenous observed variables.” to incorporate outcomes , we extend the measurement model above to assume that the six basics are directly related to an empirical measure of crash risk: c measures crash risk ; 𝛾 are parameters describing how the latent safety dimensions are related to crash risk ; 𝜉 are the safety dimensions ; and 𝜀𝑖 parameters describing how the sms scores relate to crash risk , 𝛾 .

strong is a random error term .

estimating this larger model would yield the original parameters of the measurement model , in addition to the correlations between sms scores and crash risk would further support their ability to identify higher - risk carriers .

this is known as “criterion validity” in statistics and social research .

a key strength of this validation approach is that it accounts for the error in measuring broad dimensions of safety when predicting crash risk .

because empirical data on violation rates and sms scores are indicators of latent concepts of safety , measurement error can distort the underlying relationships between these broader concepts and crash risk .

for example , poor vehicle maintenance may be positively associated with higher crash risk , but empirical data on violations of vehicle maintenance regulations may measure both the concept of interest and the enforcement efforts of state and local governments .

as a result , the violation rates may be uncorrelated with crash risk simply due to error in measuring the concept of interest .

sem models estimate the relationships among latent variables more precisely by accounting for this measurement error .

this contrasts with simpler regression models of crash risk as a function of observed violation rates , which assume that violation rates measure the dimensions of safety without error .

previous evaluations of sms have focused on estimating the correlations between crash risk and regulatory violation rates and safety measurement system ( sms ) scores .

these evaluations have found mixed evidence that sms scores predict crash risk with a high degree of precision for specific carriers or groups of carriers .

this appendix synthesizes the results of these prior evaluations .

several prior evaluations of sms have analyzed grouped data , rather than directly analyzing how a carrier's individual regulatory violation rates and sms scores predict its own future crash risk .

for example , in a pilot evaluation conducted for fmcsa , the university of michigan transportation research institute ( umtri ) estimated group crash rates within percentiles of sms scores for each behavioral analysis and safety improvement category ( basic ) , pooling several hundred carriers in each percentile , to trace out the aggregate relationship between sms scores and crash risk .

similarly , fmcsa's violation severity assessment study analyzed grouped violation data from roadside inspections conducted from 2003 through 2006 , in order to compare rates cited in post - crash reports to rates in the general population of carriers .

ibid. , 4-2 , 4-6. that did and did not exceed the sms thresholds to be placed in “alert” or “high risk” statuses .

aggregate approaches , such as those used in several prior evaluations , do not directly assess the ability of sms and regulatory violations to predict future crash risk for specific carriers .

well - known findings in statistics on “ecological fallacies” show that associations at higher levels of analysis are not guaranteed to exist at lower levels of analysis .

in this application , carriers that crash may have higher violation rates or sms scores as a group than carriers that do not crash , but this pattern does not necessarily apply to specific carriers within the groups .

because less variation exists at the carrier level , aggregation can overstate the strength and precision of these correlations for individual carriers .

even when similar correlations exist at the carrier level , comparing average crash rates for sms percentiles or risk groups does not assess the prediction error for any particular carrier .

the average crash rate may be higher for groups of carriers with increasingly high sms percentiles , but crash rates may vary significantly around these means .

this residual variation , not differences in means or other aggregate statistics , is more directly relevant for assessing the quality of predicted crash rates for a particular carrier .

in statistical terms , the prediction error summarized by the residual variance of a linear regression model or the classification matrix of a categorical model is what matters for assessing predictive power for individual carriers , not the models' coefficients , which estimate mean crash rates conditional on these percentiles .

thus , it is not surprising that previous evaluations of carrier - level data have found weaker relationships between crash risk and sms scores and regulatory violations than have the evaluations of aggregated data .

umtri estimated the relationship between exceeding thresholds in the six non - crash basics and mean crash rates , using an empirical bayesian negative binomial model estimated on carrier - level data .

the results showed that carriers exceeding the thresholds for the unsafe driving and vehicle maintenance basics had average crash rates that were 1.1 to 1.8 times higher than carriers not exceeding the thresholds — usually lower than the rate ratios of 1.0 to 5.4 reported by umtri's aggregate analysis and fmcsa's december 2012 effectiveness testing .

however , this relationship was negative for the driver fitness and loading / cargo ( currently hazardous materials ) basics , with mean crash rates for alerted carriers that were 0.85 and 0.91 times the rates of non - alerted carriers , respectively .

the ratios were not significantly greater than 1 for the fatigued driving and substance abuse / alcohol basics .

similarly , the american transportation research institute ( atri ) found that alerted carriers in the unsafe driving , vehicle maintenance , hours - of - service , and controlled substances / alcohol basics had mean crash rates that were 1.3 to 1.7 times larger than scored carriers not in alert status , but carriers exceeding the driver fitness thresholds had mean crash rates that were 0.87 times those of non - alert scored carriers .

although umtri and atri analyzed carrier - level data , they validated sms measures using regression coefficients and similar statistics that describe aggregate correlations .

as we discuss above , this approach does not directly quantify predictive power for specific carriers .

two studies that have directly estimated prediction error for specific carriers , conducted by wells fargo securities and james gimpel of the university of maryland , found weaker evidence of the model's predictive effectiveness .

gimpel found that mean crash rates increased by small amounts as sms scores increased on the unsafe driving , hours - of - service , and vehicle maintenance basics increased.found a similarly positive association for the unsafe driving basic , but a wells fargo negative association for the hours - of - service basic , in its analysis of 4,600 carriers with at least 25 vehicles and 50 inspections .

more critically , the authors showed that scores on these basics predict crash rates with a large amount of error , with most r - squared fit statistics ranging from nearly zero to 0.07 for reasonably large analysis samples .

although these studies do not report critical estimates of the residual variance , the r - squared statistics likely imply confidence intervals around predicted crash rates for individual carriers with widths that are several times larger than the predictions themselves .

this implies that sms scores predict future crash risk for specific carriers with substantial error , even though mean crash rates can be higher among carriers with higher sms scores .

fmcsa used aggregate data to dispute the findings of the wells fargo evaluation .

specifically , the agency cited the umtri findings that aggregate crash rates were 3.0 to 3.6 times higher for carriers exceeding thresholds for the unsafe driving and hours - of - service basics than for carriers that did not exceed thresholds for any basic .

in addition , fmcsa highlighted analyses by umtri and the volpe center of aggregate crash rates across percentiles of sms scores in the unsafe and fatigued driving basics , respectively , which they claimed to show a stronger correlation to crash risk .

fmcsa's approach to evaluating the predictive power of sms scores resembles its effectiveness testing , which compares aggregate crash rates for carriers above and below thresholds for various basics .

however , as we discuss above and wells fargo discussed in its response to fmcsa , the fact that sms scores predict aggregate crash rates more strongly at the alert - group or percentile level does not necessarily imply that the scores will predict the crash risk of individual carriers .

recognizing this , the umtri evaluation analyzes the data at both the aggregate and carrier levels , and finds that mean crash rate ratios are far smaller at the carrier level than at the alert - group or percentile levels .

it should be intuitive that aggregate evidence of effectiveness , stressed in some fmcsa evaluations , shows stronger predictive power than the carrier - level analyses of atri , gimpel , umtri , and wells fargo .

aggregating violation and crash rates within larger groups effectively increases the sample size used to calculate rates , which reduces their sampling error when compared to the equivalent carrier - level measures .

the reduction of sampling error can strengthen the correlations between violation rates and sms scores and crash risk .

evaluations of sms that focus on carrier - level prediction error provide the most appropriate evidence of effectiveness for assessing the safety of individual carriers .

fmcsa has stated that one purpose for sms scores is to predict the future crash risk of individual motor carriers , in order to prioritize resources for intervention and enforcement .

in addition , fmcsa reports sms scores as measures of safety on a public website and the saferbus mobile app .

to assess the validity of sms scores for this purpose , evaluations should focus on the system's ability to predict the crash risk at the carrier level , not its ability to identify groups of carriers with larger crash rates on average or collectively .

measures of predictive accuracy — such as the residual error made when predicting crash rates or the classification error made when assigning carriers to risk groups — are the critical metrics of success , not aggregated crash rate ratios and regression coefficients .

when evaluated on these criteria , prior studies show that sms predicts future crash risk for individual carriers with substantial imprecision .

none of the prior studies has explicitly incorporated measurement error into evaluations of sms .

since sms is ultimately a method of creating measures of latent variables , as we discuss in appendix iii , the regulations used to calculate scores and the scores themselves have some degree of measurement error .

because existing studies have used statistical methods that assume zero measurement error , more comprehensive attempts to model the measurement structure of sms and validate its assumptions and predictive power , such as those we discuss in appendix iii , may produce different results .

the correlations among sms scores , violation rates , and crash risk may reflect measurement error as much as the underlying relationships among the variables of interest .

this more complex analysis is critical for future evaluations of sms and its ability to measure safety risk .

as a more basic approach to validating sms , which focuses on the ability of data on regulatory violations in one time period to predict crash risk in a subsequent period , we analyzed the relationship between violation rates and crash risk using a series of statistical models .

these models predicted the probability of a crash and crash rates as a function of regulatory violation rates for a population of motor carriers that were actively operating over a recent 3.5-year time period ( described below ) .

we find that a substantial portion of regulatory violations in sms cannot be empirically linked to crash risk for individual carriers .

consistent with prior research , about 160 of the 754 regulations with data available in this time period had sufficient variation across carriers for analysis .

of the approximately 160 regulations with sufficient violation data , less than 14 were consistently associated with crash risk , across statistical models .

these results suggest that the specific weights that sms assigns to many regulations when calculating safety risk cannot be directly validated with empirical data , and many of the remaining regulations do not have meaningful associations with crash risk at the carrier level .

we assembled data for a population of motor carriers using the mcmis snapshot files dated december 2010 and 2012 .

specifically , we identified carriers that were actively operating in each of two time periods: from december 2007 through december 2009 ( the “pre - period” ) and from december 2009 through june 2011 ( the “post - period” ) .

we defined an active carrier as one that is as outlined in appendix i , consistent with fmcsa's definition of active carriers for its effectiveness testing and other analyses .

for each of the approximately 315,000 carriers that met these criteria , we extracted data on the number of regulatory violations and crashes incurred in each time period , along with the number of inspections , vehicles , and use of straight versus combo trucks , among other variables , from the crash and inspection tables in mcmis .

the goal of our analysis was to predict crash risk in the post - period , using data on regulatory violations , crash data , and carrier characteristics measured in the pre - period .

we developed a series of linear and generalized linear regression models to predict two measures of crash risk for individual carriers: a binary indicator for having crashed in the post - period and the ratio of crashes to vehicles .

estimating and evaluating all potential models and model types was not the goal of these analyses .

rather , we sought to estimate the associations between regulatory violation rates and crash risk at the carrier level , in order to validate the violations' severity weights in sms .

we reduced the list of 754 regulations whose violations are tracked in mcmis to those that had enough variation across carriers for analysis .

after excluding 593 violations that had zero variance or zero counts for more than 99 percent of the analysis carriers , we retained data on the violation of approximately 160 regulations for use in predicting crash risk .

as we discuss in appendix ii and the body of this report , crash and violation rates based on small exposure measures , generally resulting from carriers with few vehicles , may be estimated with less precision than rates based on larger exposure measures .

to better understand and attempt to overcome these rate estimation issues and assess the sensitivity of our results , we used both ordinary and empirical bayesian estimators of crash and violation rates .

in addition , we estimated separate models limited to carriers that had more than 20 vehicles .

these methodological choices produced 8 groups of models , as described in table 7 .

the groups were defined by the combined categories of crash measure ( binary crash status versus bayesian crash rate ) , methods of violation rate estimation ( ordinary versus bayesian ) , and carrier size ( full data or restricted to more than 20 vehicles ) .

these parallel analyses allowed us to assess the sensitivity of our results to different assumptions .

for each of the eight model groups , we include three sets of covariates to predict crash risk in the post - period: “simple model:” indicator ( binary ) for crashing in the pre - period , carrier size , and carrier type ( percent straight versus combo ) .

“full model:” predictors in the simple model , plus all violation rates with viable data in the pre - period .

“stepwise full model:” we applied a stepwise selection algorithm applied to all predictors in the “full model,” in order to select the most predictive covariates .

the algorithm's constraints required a p - value of 0.30 for a covariate to enter the model and 0.35 to remain in the model .

to avoid over - fitting our models to any particular sample of data , we divided our data using a random method to form a model - building sample and a validation sample .

we used the model - building sample to estimate the models described above and the validation sample to assess the accuracy of the model's predictions of crash probability against new data .

when seeking to develop statistical methods for predictive purposes , this type of out - of - sample validation is extremely useful to ensure that any method identified can consistently predict well on all samples of data , not just the sample that was used to develop the method .

this is an important limitation of prior evaluations of sms , which , to our knowledge , have not used replication samples to avoid over - fitting when identifying predictive violation types or methods of identifying higher - risk carriers .

model selection required addressing statistical estimation issues , such as instability of the parameter estimates caused by co - linearity of predictors or lack of variability in the predictors , and other model fitting concerns .

for the linear crash rate models , the dependent variable required a log transformation to remove non - constant error variance , which would invalidate results if left untreated .

these statistical issues resulted in sub - models within the major model groups that were explored until a stable model resulted .

therefore , the results within each model group focus on three sub models , when applicable: simple , stepwise and full , where stepwise is the model that eliminated independent variables until a stabilized model with estimable coefficients resulted .

see table 8 for the final list of 30 models and subsamples .

models that use the sms violation information do not fit well according to various measures discussed below .

in addition , the violation rates , as measured in sms , do not have a strong predictive relationship with crashes , regardless of whether the observed or the bayesian violation rates are used as inputs .

models for crash status ( yes / no ) were examined for stability of parameter estimates , fit statistics , number and types of violations that were predictive and that were stable , and future predictive performance according to these measures .

models for bayesian crash rates were examined for stability of parameter estimates , fit statistics , number and types of violations that were predictive , predictive power and future predictive power .

some of the diagnostics cannot be compared in absolute terms , but rather should be compared across models fit to the same data .

for example , the aic must be compared across competing models fit on the same data .

the crash status ( yes / no ) model was evaluated in the out - of - sample validation data , where each model was re - fit on the validation sample , and the diagnostics were examined and compared to those from the model - building sample .

as an additional sensitivity analysis , the same set of inputs for each of the model groups one through four were also fit using a bayesian crash rate outcome , via a linear regression fit to the model - building sample .

results were compared .

since diagnostics will differ according to the outcome measure , crash status ( yes / no ) versus crash rate , information for these outcome types is displayed separately .

for results of models for the crash status ( yes / no ) , see tables 9 and 10 .

for results for the bayesian crash rates , see table 11 .

given that a high value of the h - l p - value ( close to 1 ) indicates good model fit , according to this measure , most of the models fail to fit acceptably , and none of the models fit well .

within the same data , a lower value of the aic indicates better fit ; therefore , the stepwise models perform best , and do nearly as well regarding the roc and generalized r - squared when compared to the more complicated full model .

but even for the stepwise models , the roc and r - squared do not indicate a strong predictive relationship .

this finding is echoed by the number of effects in the model , relative to the number of potential violations ( about 160 ) and the number of stable effects .

one aspect of predictive power is the ability for a model to discriminate the observed outcomes based on model predictions .

classification tables describe a model's classification accuracy with correct and incorrect classifications , as measured by sensitivity ( correctly predict an event ) and specificity ( correctly predict a non - event ) , and false positive ( incorrectly predict a non - event ) and negative rates ( incorrectly predict an event ) .

classification tables for the simple , full , and stepwise model within a model group are presented in table 10 .

the observed proportion of crashes , approximately 0.2 for the unrestricted data and 0.66 for the data restricted to carriers with more than 20 vehicles , is used as the cut - point to classify predicted probabilities for a carrier into a predicted event ( crash ) versus non - event ( no crash ) .

the predicted crash status for a particular model is compared to the actual post - crash status , resulting in a series of table rows , one for each model , that examine the false positives , false negatives , and other quantities that help evaluate the predictive quality of a model .

for unrestricted data , the false negative rate ( or the rate that results from incorrectly classifying a carrier to a non - alert status ) , is relatively low ( around 11 percent ) compared to the false positive rate ( ranges from about 56 to 58 percent ) .

this is a desired result if it is considered more appropriate to be conservative and put a carrier in alert status , even if that alert status is incorrect ( false positive ) , compared to misclassifying a carrier into non - alert when an alert would be called for ( false negative ) .

the restricted data have a higher false negative rate ( from 42 to 44 percent ) than false positive rate ( around 14 to 19 percent ) , and this false negative rate is also higher than the full data false negative rate .

for the restricted data with higher false negative rates , this means a higher percentage of carriers are being classified in non - alert when they have crashed than the percent classified as alert , but that did not crash , and such a scenario is not desirable under a conservative preference toward low false negative rates .

in addition , the sensitivity and specificity are both moderate at best within data ( restricted versus full ) , further evidence of the inability for models to discriminate .

to address whether crash status ( yes / no ) has a different relationship with violations than the crash rate , we compare conclusions of crash status ( yes / no ) versus crash rate models .

examining sensitivity to the prediction of crash status ( yes / no ) versus crash rate , the stepwise selected model will be compared to logistic regression results for the model - building and the validation sample ( see table 11 ) .model indicates that the numbers of effects that are related to crash rate are small , and that the better fitting models tend to have only a few predictors included .

specifically , mallow's cp statistic indicates a model is preferable when cp is around or smaller than the number of effects ( p ) , and the model is more parsimonious than competing models .

the model fit to the restricted data , where carriers have greater than 20 vehicles , ( stepwise model number 22 ) , includes only 34 stable effects , and 72 effects altogether , but the model fit is more stable ( i.e. , relatively fewer unstable effects ) and has the best ( lowest ) cp , while also having similar explained variance and low aic .

however , it is interesting to note that the simple model , model 21 , performs similarly according to some measures , such as root mse and r - squared , though this model does not contain violation rate information .

comparing how well the models perform when applied to the validation sample that consists of new observations — — which are not included in the model - building sample — informs the precision of sms with respect to predicting crashes .

we examine the number of violations and the violation types that are included across the model groups ( logistic and linear ) and sub - models ( stepwise and full ) .

we compare this to the number of models within which each violation was found to be a significant and a stable predictor of crash outcomes .

importantly , of the reduced set of approximately 160 violations considered , only 13 violations were significant in at least half of the 24 models that incorporate violations ( i.e. , stepwise and full models ) .

there were 10 different possible models for the logistic model - building sample , and these were also evaluated on the validation sample and on the model - building sample , but with a linear regression setting , resulting in 30 possible models .

however , we regarded only 24 of these 30 models as informative since we exclude the 6 simple models that ignore the pre - violation information .

of the violations considered , only speeding ( violation 3922s ) and failure to use a seatbelt while operating cmv ( 39216 ) were significant and stable in all 24 models .

a similar picture arises for some other violations , though many of the models did not result in a significant relationship between the violation in question and the crash outcome , as indicated in table 12 .

only 41 violations were significant in 5 or more models out of 24 .

however , even for the top 13 violations with respect to frequency of significance and stability across the 24 models , predictive power is still affected by poor model diagnostics .

this is echoed in the results from the predictive relationship when compared to the linear regression model for bayesian crash rates ( results in table 11 ) , where the model that excluded all violations performed similarly to models that included some significant violations .

whether modeling crash status ( yes / no ) or a crash rate , the predictive power of sms violations is weak .

when comparing the predictive power of the models that result from the model - building sample , once applied to the validation sample , there is a consistent picture regarding the model fit ( see table 13 ) .

in particular , the model fit is generally poor according to the h - l value ; the stepwise model tends to perform better according to the aic , but the roc , adjusted r2 , and percent discordant do not indicate the models have a strong ability to discriminate and predict future crashes .

classification tables that result from evaluating the model - building sample models , but estimated from the validation sample , generally resulted in similar results to those presented in table 10 .

the predictive power observed in these modeling and sensitivity analyses indicates that sms may be less precise than what is reported and that the available information on violations is limited for the purpose of scoring carriers or predicting their crash risk .

regardless of which type of model we fit , we see that the predictive power of our models is low , and the use of the sms violations in predicting future crashes is not very precise .

the number of stable and significant effects across the various model - fitting scenarios that include violations is small .

for the about 800 violations in sms , only around 160 met the basic criteria of non - zero variance and non - zero counts for at least 1 percent of the sample .

of these , only two violations ( speeding and failure to wear a seatbelt while operating a cmv ) consistently appeared as a stable predictor of crashes , regardless of data and model .

while some other violations appeared in models , only 13 were significant and stable in at least half of the models , most were significant in no more than half the models examined , and most often in fewer than 5 of the models .

the results did not vary substantially according to whether observed versus bayesian violation rates , crash versus bayesian crash rates , or restricted data ( carriers with more than 20 vehicles ) versus full data were used to estimate crashes .

therefore the modeling attempts did not overcome the issues that result from small exposures .

the results were generally confirmed when evaluated on a validation sample , indicating the future prediction is stable , yet not strong .

ultimately , much of the variance in crash predictions remains unexplained , regardless of the model and model - building data , so that the sms might be less precise when the objective is to predict crashes .

this appendix provides additional information and illustrations of the distribution of motor carrier population included in our analysis such as carrier size , number of crashes , inspections , and high risk status ( see table 14 ) .

it also provides results of our analysis on the number and percentage of carriers above or below intervention thresholds , as well as the frequency and rate of crashes for each of those groups of carriers within each basic using fmcsa's methodology and the illustrative alternative methodology ( i.e. , using a stronger data sufficiency standard ) demonstrated earlier in the report .

in addition , this appendix provides summary statistics of the various motor carrier populations used in fmcsa and gao analysis .

these statistics include , among other things , the numbers of carriers with an sms score ( i.e. , “measure” ) and the number of carriers above an intervention threshold in at least one basic .

finally , this appendix provides the complete graphical results of our analysis of fmcsa's violation rates , safety event groups , and distribution of sms scores for carriers above fmcsa's intervention threshold using fmcsa's methodology .

table 15 contains the results of our analysis using fmcsa's sms 3.0 methodology .

this analysis calculated the number and percentage of carriers above and below intervention thresholds for each basic using carrier data from december 2007 through december 2009 , and determined which carriers subsequently crashed during the 18-month evaluation period , december 2009 through june 2011 .

the analysis also presents aggregate crash rates for comparison purposes .

table 16 contains the results of our analysis using an illustrative alternative incorporating a stronger data sufficiency standard , among other things , as described elsewhere in this report ( e.g .

carriers with 20 or more inspections or 20 or more vehicles , depending upon the basic ) .

as in the previous table , this analysis calculated the number of carriers above and below intervention thresholds for each basic using carrier data from december 2007 through december 2009 , and determined which carriers subsequently crashed during the subsequent 18-month period , december 2009 through june 2011 .

the analysis also presents aggregate crash rates for comparison purposes .

table 17 contains selected sms outcomes based on results reported by fmcsa's and from gao's analysis .

the following figures are graphical results of our analysis of the average and range of violation rates for carriers , percentage of carriers above fmcsa's intervention thresholds for various safety event group categories , and distribution of sms scores for carriers above fmcsa's intervention thresholds using fmcsa's methodology as discussed in the body of this report above .

figures 10 through 16 contain the average and range of violation rates for all carriers ( where a violation rate could be calculated ) by carrier size , for all the basics .

figures 17 through 25 contain the percentage of carriers above intervention thresholds within safety event groups for each basic .

finally , figures 26 through 32 show the distribution of carriers above intervention thresholds for each basic by carrier size .

in addition to the individual named above , h. brandon haller , assistant director , russell burnett , melinda cordero , jennifer dubord , colin fallon , david hooper , matthew latour , grant mallie , jeff tessin , sonya vartivarian , and joshua ormond made key contributions to this report .

