federal agencies rely extensively on computerized information systems and electronic data to carry out their missions .

the security of these systems and data is essential to prevent data tampering , disruptions in critical operations , fraud , and inappropriate disclosure of sensitive information .

concerned with accounts of attacks on systems through the internet and reports of significant weaknesses in federal computer systems that make them vulnerable to attack , congress passed the federal information security management act ( fisma ) in 2002 .

among other things , fisma requires federal agencies to periodically test and evaluate the effectiveness of their information security policies , procedures , and practices as part of developing and implementing an agencywide information security program .

in addition , agencies and their inspectors general are required to annually report to congress and the office of management and budget ( omb ) on the adequacy and effectiveness of information security policies and practices and compliance with the act .

the act also assigns specific responsibilities to omb and the national institute of standards and technology ( nist ) .

omb's responsibilities include ( 1 ) developing and overseeing the implementation of policies , principles , standards , and guidelines on information security and ( 2 ) reporting to congress on the agencies' compliance with fisma requirements .

omb also provides instructions to agencies and inspectors general to assist them in meeting fisma reporting requirements .

these instructions have a strong focus on performance measures , which are the basis of agencies' annual reports and inspectors general independent annual evaluations .

the act requires nist to develop , for systems other than national security systems , standards and guidelines to assist agencies in implementing their information security programs .

as agreed with your office , our objective was to determine whether agencies have adequately designed and effectively implemented policies for periodically testing and evaluating information security controls .

to accomplish this objective , we conducted a survey of 24 major federal agencies and their inspectors general , analyzed information security policies , and selected 6 of the 24 agencies to use as case studies for conducting in - depth evaluations of their periodic testing and evaluation methods and practices .

specifically , to determine whether the 24 agencies adequately designed policies for periodic testing , we obtained and analyzed their policies to determine whether they included elements important for conducting effective tests and evaluations .

to determine whether the 6 agencies had effectively implemented policies and procedures , we assessed methods and practices used to test and evaluate controls for 30 of their systems .

we examined instructions , standards , and guidelines issued by omb and nist as a framework for assessing the adequacy of the 24 agencies' policies and for determining the effectiveness of the 6 agencies' testing and evaluation methods and practices .

details of our objective , scope , and methodology are included in appendix i .

we conducted our work from november 2005 through july 2006 in accordance with generally accepted government auditing standards .

increasing computer interconnectivity — most notably growth in the use of the internet — has revolutionized the way that our government , our nation , and much of the world communicate and conduct business .

while this interconnectivity offers us huge benefits , without proper safeguards , it also poses significant risks to the government's computer systems and , more importantly , to the critical operations and infrastructures they support .

we reported in 2005 that while federal agencies showed improvement in addressing information security , they have also continued to have significant control weaknesses in federal computer systems , which puts federal assets at risk of inadvertent or deliberate misuse , financial information at risk of unauthorized modification or destruction , sensitive information at risk of inappropriate disclosure , and critical operations at risk of disruption .

the federal information security management act of 2002 requires each agency to develop , document , and implement an agencywide information security program .

this program should provide security for the information and information systems that support the operations and assets of the agency , including those provided or managed by another agency , contractor , or other source .

among other things , the program is to include periodic testing and evaluation of the effectiveness of information security policies , procedures , and practices , to be performed with a frequency depending on risk , but no less than annually .

the testing is to include management , operational , and technical controls for every system identified in the agency's required inventory of major information systems .

the act also assigns specific responsibilities to omb and nist .

omb's responsibilities include the following: overseeing agency information security policies and practices , including developing and overseeing the implementation of policies , principles , standards , and guidelines on information security .

reviewing agency information security programs , at least annually .

reporting to congress annually on agency compliance with fisma requirements .

as part of the reporting process , omb provides instructions to agencies and their inspectors general on the annual fisma reporting requirements .

these instructions include performance measures for such things as the number of systems for which security controls have been tested and evaluated in the past year .

omb also uses performance measures to assist in its oversight responsibilities and to annually report to congress on agencies' compliance with the requirements of the act .

fisma also directs nist to develop standards and guidelines for systems other than national security systems .

these standards and guidelines instruct agencies on providing an acceptable level of information security for all agency operations and assets and contribute to the testing and evaluation of information security controls within an agencywide information security program .

recognizing the importance of documenting standards and guidelines as part of an agencywide information security program , nist emphasizes that agencies must develop and promulgate formal , documented policies and procedures in order to ensure the effective implementation of security requirements .

nist standards and guidelines that contain elements applicable to periodic testing and evaluation include the following: special publication 800-26 , security self - assessment guide for information technology systems , november 2001 .

this publication is a self - assessment guide for agencies to use in determining the current status of their information security program .

the guide includes a standardized form for reporting the results of system - level assessments and a method for evaluating the effectiveness of the agency's information security program .

the guide also emphasizes the importance of establishing levels of implementation , referred to as the it security assessment framework .

nist special publication 800-26 is effective through the 2006 fisma reporting period and will be rescinded when special publications 800-53a and 800-100 are finalized .

special publication 800-37 , guide for the security certification and accreditation of federal information systems , may 2004 .

this guide is to be used for certifying and accrediting nonnational security systems .

developed as part of nist's project to promote the development of standards and guidelines to support fisma , this guide specifies the need for ongoing activities to continuously monitor the effectiveness of security controls .

special publication 800-53 , recommended security controls for federal information systems , february 2005 .

this publication provides instructions on selecting and specifying security controls for information systems .

it also provides the set of security controls that satisfy the depth and breadth of security requirements levied on information systems and provides the fundamental concepts associated with security controls selection and specification , including the identification and use of common security controls .

in conducting security assessments , nist states that assessment results can be used and shared to enhance the efficiency of evaluations and reduce security program costs .

special publication 800-53a , guide for assessing the security controls in federal information systems , april 2006 .

the publication is a second public draft to be used by agencies to assess the effectiveness of security controls employed in federal information systems .

nist establishes methods and procedures to assess the security controls in federal information systems , specifically those controls listed in nist special publication 800-53 , recommended security controls for federal information systems .

these methods and procedures are designed for agencies to use in determining if the controls are implemented correctly , operating as intended , and producing the desired outcome with respect to meeting the security requirements of the agency .

nist closed acceptance of public comments on this draft on july 31 , 2006 , and plans to issue a final publication in december 2006 .

having well - designed policies is critical for performing effective testing and evaluation of security controls .

to assist agencies , omb and nist developed instructions , standards , and guidelines for testing and evaluating the controls over information systems .

we used the following six elements to evaluate agencies' policies for periodically testing security controls: 1 .

identifying the frequency of periodic testing .

2 .

defining roles and responsibilities of personnel performing the testing .

3 .

selecting a minimum set of security controls evaluated during periodic tests .

4 .

identifying and testing common security controls .

5 .

determining the depth and breadth of periodic testing .

6 .

including assessment results in remediation plans .

the related federal and nist references are shown in table 1 .

agencies' policies for periodically testing and evaluating security controls have not been adequately designed and effectively implemented .

specifically , none of the federal agencies' policies fully addressed six important elements included in omb and nist guidelines and standards for performing effective security testing and evaluations .

in addition , there were weaknesses in the security control assessments for the 30 systems reviewed at the six case study agencies .

as a result , agencies have limited assurance that controls are implemented correctly , operating as intended , and producing the desired outcome .

in addition , agencies may not be fully aware of security control weaknesses in their systems , thereby leaving the agencies' operations and systems at risk .

agencies did not fully address six elements important for testing and evaluating security controls in their policies .

specifically , the ( 1 ) frequency of periodic testing was not always identified , ( 2 ) roles and responsibilities of personnel performing tests often were not clearly defined , ( 3 ) selection of a minimum set of security controls evaluated during periodic tests was not always fully addressed , ( 4 ) instructions on identification and testing of common security controls were not addressed , ( 5 ) instructions on determining the depth and breadth of testing were not included , and ( 6 ) descriptions of a process for documenting remedial actions to address deficiencies were not always addressed .

table 2 indicates weaknesses in developing and promulgating formal , documented policies to address the security elements needed for effective testing .

fisma requires agencies to perform — for all major information systems in their inventory — periodic testing and evaluation of the effectiveness of information security policies , procedures , and practices , to be performed with a frequency depending on risk , but no less than annually .

of the 23 agencies' policies we reviewed , 7 agencies did not require that their security controls ( management , operational , and technical ) be tested and evaluated at least annually .

for example , policies for 3 of the 7 agencies did not specify the frequency of periodic testing .

the other 4 agencies identified the frequency of some testing activities — reviewing the overall security program annually , testing standard user account procedures annually , and certifying and accrediting systems at least every 3 years — but did not specify the frequency of periodic testing for other management , operational , and technical security controls .

unless agencies specify the frequency for conducting periodic testing and evaluations at least annually per fisma , they may not have assurance that controls are being sufficiently evaluated and producing the desired outcome with respect to meeting the security requirements of the agency .

nist 800-37 identifies the roles and associated responsibilities with regard to testing and evaluating information security controls .

these roles include the chief information officer , authorizing official , senior agency information security officer , information system owner , and information system security officer .

in addition , nist special publication 800-26 specifies that agencies should have procedures in place that identify who is conducting the security testing .

roles and responsibilities of personnel performing testing were not clearly defined in policies for 15 of the 23 agencies .

ten of the 15 agencies did not define roles and responsibilities for personnel performing tests in their policies and the other 5 agencies defined them only partially .

for example , one agency defined roles and responsibilities for the system owner but not for other key security personnel such as the chief information security officer and information system security officer .

as a result , agency officials may not clearly understand their expected responsibilities and consequently , may not be able to carry out their duties correctly and effectively .

baseline controls are the minimum security controls recommended for an information system based on the system's security categorization .

nist special publication 800-53 provides guidance to agencies for selecting these security controls , which serve as a starting point in determining and designing methods for testing the security controls .

nist specifies that agency security personnel must develop , document , and implement policies for consistent identification , testing , and evaluation of baseline controls .

policies for selecting the minimum security controls evaluated during periodic tests for 11 of the 23 agencies were not always adequate .

to illustrate , 7 of the 11 agencies reported having no specific policies or procedures for selecting the minimum baseline security controls , and the other 4 agencies' policies partially addressed the selection of these controls .

for example , one agency's policy referenced nist guidance for identifying controls , but it did not first specify the use of the nist standard when determining the system's impact level .

in another example , an agency referenced nist 800-53 guidance for selecting baseline controls , but it provided a checklist of controls to be tested that did not include the baseline controls as identified in nist guidance .

without adequate instruction , security personnel may not consistently identify , test , and evaluate the baseline controls used to secure their systems .

identifying common security controls can increase efficiency in agencies' periodic testing .

nist 800-37 guidance defines a common security control as one that can be applied to one or more of an agency's information systems .

this guidance suggests that many of the management and operational controls — contingency planning , incident response , security training and awareness , personnel security , and physical security — needed to protect an information system may be excellent candidates for common security control status .

by identifying common controls , agencies can achieve efficiencies by testing common controls and using the results for multiple systems .

for example , nist states that an organizationwide approach to reusing and sharing test results can greatly enhance efficiencies and significantly reduce security program costs .

policies for 22 of the 23 agencies we reviewed did not specify how to identify and test common security controls .

for example , the security policies for 15 of the 22 agencies did not address the identification and testing of common security controls and policies and the other 7 agencies only partially addressed them .

specifically , the 7 agencies identified and tested some elements of common controls , but their policies did not describe how to identify , test , or share testing results with others .

for example , one agency encouraged the use of common controls , but it did not specify how common controls were to be identified , how to test them , or how test results should be shared with others .

in addition , another agency made reference to common controls as part of a pilot program , but no other discussion or reference was made regarding identifying and testing common security controls .

without policies and procedures that address or provide guidance for identifying and testing common controls , agencies may needlessly test common controls multiple times , thereby reducing efficiency and increasing costs for their periodic testing .

an important element of efficient and effective testing is the consideration of the depth and breadth of agency testing .

fisma requires testing of the management , operational , and technical controls for every system at least annually .

moreover , special publication 800-37 states that it is not feasible or cost effective to monitor all of the security controls in an information system on a continuous basis and that the information system owner should select an appropriate subset of those controls for periodic assessment .

in addition , omb memoranda m - 05-15 and m - 06-20 have identified three criteria for agency officials to consider when determining the depth and breadth of a review: the potential risk and magnitude of harm to the system or data .

the relative comprehensiveness of the past year's review .

the adequacy and successful implementation of a remediation plan to address weaknesses in the information system .

none of the 23 agencies' policies provided adequate instruction for determining the depth and breadth of periodic tests .

moreover , agencies did not incorporate the three omb criteria into their policies as consideration for determining the depth and breadth of periodic testing .

security personnel reported that they do not fully understand how to apply the current guidance on determining the depth and breadth of controls testing and need further clarification .

until additional guidance clarifies how to determine the depth and breadth of testing , increased risk exists that agencies may not sufficiently test security controls in a cost - effective manner .

fisma directs agencies to establish a process for remediating identified weaknesses in their information security policies and procedures .

key to an effective remediation plan is the accurate and complete inclusion of weaknesses identified during periodic testing .

remediation plans , also referred to as plans of action and milestones , should list all identified weaknesses and show estimated resource needs or other challenges to resolving them , key milestones and completion dates , and the status of corrective actions .

nist 800-37 states that remediation plans need to be updated to address weaknesses identified as a result of periodic testing .

policies for 10 of the 23 agencies did not fully describe a process for documenting identified control weaknesses .

for example , 7 of the 10 agencies did not have policies that described a process for incorporating weaknesses identified during periodic security testing into remediation plans .

the remaining 3 agencies had policies on remediation plans , but these were in draft form only and provided no further description of the process for addressing weaknesses .

without adequate guidance for ensuring that identified weaknesses are incorporated into remediation plans , there is increased risk that weaknesses identified through security controls testing are not being properly addressed .

thus , agencies may not realize the full benefits of such testing and have limited assurance that the controls for their systems are functioning effectively .

none of the six case study agencies fully implemented their policies for periodic information security testing .

during our review of 30 systems , we found implementation weaknesses at all six agencies .

these weaknesses consisted of insufficient testing documentation , inadequately defined assessment methods , inadequate security testing , and lack of remedial actions included in testing plans , as shown in table 3 .

testing documentation and supporting material serves as the basis for verifying that the security controls in the information system are implemented correctly , operating as intended , and producing the desired outcome with respect to meeting the security requirements for the information system .

test documents may include risk assessments , testing plans , the controls being tested , the results of the testing ( security weaknesses and vulnerabilities ) , including results from previous security assessments , security reviews , or audits .

support materials may include procedures , reports , logs , and records showing evidence of security controls implementation .

agencies did not sufficiently document periodic testing activities and results for 28 of the 30 systems reviewed .

these examples ranged from no documentation to documentation that omitted key elements , such as risk assessments , testing plans , and test results .

for example , testing plans did not provide enough detail to determine which tests were to be conducted or the scope of test coverage .

in addition , one security manager reported that maintaining supporting documentation was not a common practice and that no supporting documentation or test records had been maintained until recently .

unless agencies develop and maintain sufficient testing documentation , they will have limited evidence for making judgments about the security of their systems .

nist 800-37 identifies a variety of assessment methods such as interviewing , inspecting , studying , testing , demonstrating , and analyzing that agencies can use when evaluating their security controls .

nist guidelines describe these methods as interview , examine , and test .

the interview method of assessment is the process of conducting focused discussions with individuals or groups of individuals within an organization to facilitate assessor understanding , achieve clarification , or obtain evidence .

the examine method of assessment is the process of reviewing , inspecting , observing , studying , or analyzing one or more assessment objects ( specifications , mechanisms , or activities ) .

similar to the interview method , the primary purpose of the examine method is to facilitate assessor understanding , achieve clarification , or obtain evidence .

the test method of assessment is the process of exercising one or more assessment objects ( limited to mechanisms or activities ) under specified conditions to compare actual with expected behavior .

nist states that the results of assessments using these methods are to support the determination of overall security controls effectiveness .

agencies did not fully define the assessment methods used to evaluate their system controls for 7 of the 30 systems reviewed .

we found that the test plans , procedures , and testing results for 4 of the 7 systems did not identify how agencies evaluated system controls or whether they used interviews , examinations , or tests to determine the effectiveness of those controls .

for the 3 remaining systems , agencies did not provide documentation to show what assessment methods were used .

if agencies do not define assessment methods , they may not have information describing how that control was assessed .

without that information , agencies have limited assurance that those controls are being effectively tested or implemented .

once employed within an information system , security controls should be tested to determine the extent to which the controls are correctly implemented , operating as intended , and producing the desired outcome with respect to meeting the security requirements for the system .

nist states that assessments should be based on an examination of relevant documentation and a rigorous examination and testing of the controls .

the results of security testing contribute to the knowledge base of organization officials with regard to the security status of the information system and the overall risk to the operations and assets of the organization incurred by the operation of the system .

agencies did not adequately test security controls for 24 of the 30 systems reviewed .

the testing documentation showed no evidence of how testers assessed the security controls , whether they had tested the control as planned , or if they had conducted the test in accordance with the plan .

in one example , testers reviewed management control policies ; however , the testing guidelines required that the control be tested to determine if it had been effectively implemented .

unless agencies adequately test controls and document the results , they may not be able to measure the security status of their information systems , thereby limiting their ability to know whether controls are protecting their operations and assets .

fisma requires that agencies document remedial actions that address deficiencies in the information security policies , procedures , and practices .

nist 800-37 states that the plan of action and milestones should describe the measures that have been implemented or planned to correct any deficiencies or weaknesses noted during the assessment of the security controls .

nist also states that remedial actions should be evaluated to determine if they effectively mitigate previously identified weaknesses or vulnerabilities in the information system .

for 18 of the 30 systems , agencies did not consistently test or evaluate the effectiveness of remedial actions for weaknesses identified through security control assessments .

for example , testing documentation for some systems did not address the remedial actions that agencies had identified from prior assessments in their test plans .

unless agencies document and include remedial actions for previously identified control weaknesses in testing plans , agencies will have limited assurance that weaknesses have been corrected .

agencies have not adequately designed and effectively implemented policies for periodically testing information security controls .

while almost all agencies had documented policies for security testing , the policies did not always adequately address elements important for effective testing .

ensuring that agencies' policies' are sufficient to address federal standards and guidelines helps to ensure their effective implementation in meeting fisma requirements .

while nist has issued guidance on how agencies should apply the depth and breadth method for testing security controls , agencies have not been documenting or implementing this approach in their testing .

also , agency officials reported that they did not understand this method .

our review of 30 systems at six major federal agencies found weaknesses in testing practices and methods: documentation , testing methods , controls testing , and remedial actions in testing plans .

conducting effective periodic testing and evaluations of information security controls is a serious , pervasive , and crosscutting challenge to federal agencies , warranting increased attention from omb .

if these challenges are not addressed , federal agencies' information and operations may be at increased risk .

because of the governmentwide weaknesses in the design and implementation of agencies' policies for periodically testing and evaluating security controls , we recommend that the director of the office of management and budget take the following two actions: instruct federal agencies to develop and implement policies on periodic testing and evaluation .

revise instructions for future fisma reporting by requesting inspectors general to report on the quality of agencies' periodic testing processes .

we also recommend that the secretary of commerce direct the director , national institute of standards and technology , to strengthen guidance on determining the depth and breadth of testing security controls .

we received oral comments on a draft of this report from representatives of the office of management and budget's offices of information and regulatory affairs and general counsel .

the representatives agreed to consider our recommendations as part of their oversight responsibilities for information security at federal agencies .

the deputy secretary of the department of commerce provided written comments in response to our draft report ( see app .

ii ) .

he stated that the department agreed with our characterization of the national institute of standards and technology's fisma responsibilities and activities and also said that nist is currently reviewing its guidance , including that for the depth and breadth of testing security controls .

as agreed with your office , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the date of this letter .

at that time , we will send copies of the report to other interested congressional committees ; the director , office of management and budget ; and the deputy secretary of the department of commerce .

we will make copies available to others on request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you have any questions about this report , please contact me at ( 202 ) 512- 6244 or wilshuseng@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are acknowledged in appendix v .

the objective of our review was to determine the extent to which federal agencies have adequately designed and effectively implemented policies for periodically testing and evaluating security controls .

the scope of our review included ( 1 ) the 24 federal agencies , focusing on reviewing their policies and procedures and responses to our survey and ( 2 ) a selection of 30 systems at 6 of these agencies , focusing on in - depth evaluations of their periodic controls testing and evaluation practices and methods .

to determine the adequacy and effectiveness of federal agencies' policies and procedures for testing and evaluating security controls for their information systems , we conducted a survey of the 24 major agencies , which included 21 questions for the agencies and 4 questions for the agencies' inspectors general .

we also reviewed the agencies' policies that were submitted in response to the surveys and compared them against six policy elements from the office of management and budget ( omb ) and the national institute of standards and technology ( nist ) standards and guidelines that we considered to be important for performing effective testing .

the survey instruments were pretested with two federal information technology organizations — the department of defense and gao's office of the chief information officer .

to assess the implementation of federal information security management act of 2002 ( fisma ) requirements , we reviewed 30 systems at the six case study agencies to determine whether policies for testing and evaluating security controls were effectively implemented .

we selected for review the six agencies that reported the largest number of systems in their inventories of major systems , excluding agencies that had been recently reviewed by gao .

we relied on fisma standards and guidelines from omb and nist as criteria for evaluating agency testing and evaluation methods , policies , and procedures .

these criteria were used to evaluate agency system documentation on the results of security controls testing , such as system security plans , testing results , testing plans and schedules , remedial action plans , memoranda , and other artifacts used for information security testing .

we collected fiscal year 2005 self assessment and testing artifacts and fiscal years 2004 and 2005 remediation plans in order to standardize the data for analysis .

to augment our work , we considered the responses to our survey by the agencies and the inspectors general .

we selected and examined 5 systems comprised of low , medium , and high impact general support systems and major applications for a total of 30 systems across the six agencies .

because we were evaluating the extent to which agencies periodically test and evaluate the effectiveness of security controls , we avoided selecting systems that had recently undergone certification and accreditation where more rigorous ( independent ) testing is conducted .

in cases where an agency had recently certified and accredited the majority of its systems , we selected those having the oldest accreditation date within the selected time period .

we evaluated government - owned and operated systems , and government - owned , contractor - operated systems ; all were operational and none were under development .

we did not select systems that were recently or currently under review by an inspector general or those classified as national security or financial .

we performed our work in the washington , d.c. , metropolitan area and in three agency field offices in pennsylvania , texas , and georgia , from november 2005 to july 2006 , in accordance with generally accepted government auditing standards .

in addition to the individual named above , suzanne lightman ( assistant director ) , ayannah buford , larry crosland , neil doherty , nicole garofalo , nancy glover , joel grossman , david hong , john ortiz , jerome sandau , donald sebers , jenniffer wilson , and charles youman made key contributions to this report .

