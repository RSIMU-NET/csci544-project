the federal government is one of the world's largest and most complex entities , with about $3.5 trillion in outlays in fiscal year 2013 , funding a vast array of programs and operations .

it faces a number of significant fiscal , management , and performance challenges in responding to the diverse and increasingly complex issues it seeks to address .

addressing these challenges will require actions on multiple fronts .

our prior work on results - oriented management has found that data - driven decision making if agencies do not effectively use performance leads to better results.measures and performance information to track progress and achieve their goals , they increase the risk of failing to achieve them .

in that regard , we have previously reported that the performance planning and reporting framework originally put into place by the government performance and results act of 1993 ( gpra ) , and significantly enhanced by the gpra modernization act of 2010 ( gprama ) , provides important tools that can help inform congressional and executive branch decision making to address challenges the federal government faces .

this report is part of a series of reports under our mandate to examine the implementation of gprama .

this report compares the agency - level results from our 2013 survey of federal managers at the 24 agencies covered by the chief financial officers ( cfo ) act of 1990 , as amended , with our 2007 managers survey .

the 2007 survey is the most recent survey conducted before gprama was enacted in 2011 and includes results from the department of homeland security ( dhs ) .objective for this report was to assess agencies' use of performance information from responses to our federal managers surveys .

to address this objective , we analyzed survey data that we had previously collected and publicly reported in 2007 and 2013 .

in 2007 , we surveyed a stratified random sample of mid - level and upper - level managers and supervisors ( general schedule levels comparable to 13 through 15 and career senior executive service ( ses ) or equivalent ) at the 24 cfo act agencies ( 4,412 persons from a population of approximately 107,326 mid - level and upper - level civilian managers and supervisors ) .

for the 2007 survey results , the average response rate across all agencies was about 70 percent .

in 2013 , we again surveyed a stratified random sample of mid - level and upper - level managers and supervisors at the 24 cfo act agencies ( 4,391 persons from a population of approximately 148,300 mid - level and upper - level civilian managers and supervisors ) .

for the 2013 survey , the average response rate across all agencies was 69 percent .

similar to the previous surveys , the sample was stratified by agency and by whether the manager or supervisor was a member of the ses or non - ses .

the overall survey results are generalizable to the population of managers as described above at each of the 24 agencies and government - wide .

the responses of each eligible sample member who provided a usable questionnaire were weighted in the analyses to account statistically for all members of the population .

for additional information on survey development and implementation , please see our prior reports .

we did not interview officials at the agencies that participated in our 2013 managers survey to obtain additional information .

the use of performance information index was based primarily on an index that we developed and reported on the 2007 managers survey .

in both the 2007 and 2013 surveys , we defined the terms “performance information” and “performance measures” in the broadest sense .

in our 2013 survey , we defined performance information as the data collected to measure progress toward achieving an agency's established mission or program - related goals or objectives .

we further stated that performance information can focus on performance measures , such as quality , timeliness , customer satisfaction , or efficiency .

it can inform key management decisions such as setting program priorities , allocating resources , or identifying program problems and taking corrective actions to solve those problems .

after identifying a core set of items from the original index , we tested the impact of including and excluding several additional questions related to performance management use to ensure the cohesiveness and strength of our revised index that we used for the 2013 survey .

figure 1 shows the set of 2013 managers survey questions that comprise the use index .

after developing the use indices for 2007 and 2013 , we then analyzed the managers' responses , grouped the managers' responses according to their agencies , and compared the agencies' scores on each index .

we then reviewed our prior work where we identified leading practices that can help agencies enhance or facilitate the use of performance information for management decision making , as shown in figure 2 below .

to determine if there any additional factors related to these leading practices that could influence how an agency scored on our use index , we looked at the remaining questions from the 2013 managers survey and identified those additional questions that were associated with these leading practices .

we used statistical testing to determine if the relationship between these additional questions and an agency's use of performance information was statistically significant .

see figure 3 below for the survey questions we tested related to the five leading practices .

we conducted regression analyses to assess the relationship between these additional questions related to leading practices to enhance and facilitate the use of performance information and an agency's score and ranking on the use index .

regression analyses allowed us to assess the unique association between our outcome variable and a given predictor using variable , while controlling for multiple other predictor variables.our survey data , these analyses were intended to reflect the strength of the relationship between our previously identified practices with our 2013 use index .

our analyses did not seek to identify or assess other survey items addressing additional practices that we had not previously identified for improving the use of performance information for management decision making .

more information on our regression analysis can be found in appendix i .

to ensure reliability of the specific survey questions we used in our analyses , we conducted electronic testing of the data and reviewed prior information on the design and implementation of our 2007 and 2013 managers surveys .

purpose of this report .

we believe the data are sufficiently reliable for the we conducted this performance audit from august 2013 to september 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

gao - 08-1036sp and gao - 13-519sp .

use of performance information has not changed significantly over time government - wide .

these survey results are consistent with trends identified in other federal employee surveys government - wide .

for example , the office of personnel management ( opm ) surveys federal workers with the federal employee viewpoint survey ( fevs ) .

fevs is a tool that measures employees' perceptions of whether , and to what extent , conditions characterizing successful organizations are present in their agencies .

opm creates an index using a smaller subset selected from the fevs survey responses that are related to agencies' results - oriented performance culture .

opm also creates additional indices using different subsets of fevs survey questions related to: ( 1 ) leadership and knowledge management ; ( 2 ) talent management ; and ( 3 ) job satisfaction .

on the results - oriented performance culture index , 27 of the 37 agencies opm surveyed experienced a decline between 2008 and 2013 .

only seven agencies improved during this time period — opm , the u.s .

departments of education and transportation , the federal communications commission , national labor relations board , railroad retirement board , and the broadcasting board of governors .

the office of management and budget and the performance improvement council ( pic ) , work with federal agencies to improve performance across the federal government .

among the pic's responsibilities is the charge to facilitate the exchange of useful performance improvement practices and work among the federal agencies to resolve government - wide or crosscutting performance issues .

few federal agencies showed improvement in managers' use of performance information for decision making between 2007 and 2013 , as measured by our use index .

specifically , our analysis of the average use index score at each agency found that most agencies showed no statistically significant change in use during this period .

only two agencies — opm and the department of labor — experienced a statistically significant improvement in managers' use of performance information .

during the same time period , four agencies — the departments of energy and veterans affairs ( va ) , the national aeronautics and space administration , and the nuclear regulatory commission — experienced a statistically significant decline in managers' use of performance information as measured by our index .

see table 1 below for agency scores on the use of performance information index .

in addition , figure 4 illustrates that ses managers used performance information , as measured by our index , more than non - ses managers both government - wide and within each agency .

ses managers government - wide and at nine agencies scored statistically significantly higher than the non - ses managers at those agencies .

as shown in figure 4 below , ses and non - ses managers from dhs and va had the largest gaps in use of performance information between their ses and non - ses managers .

in one agency — the national science foundation — the trend was reversed , with non - ses managers reporting more favorably than ses managers .

however , this difference was not statistically significant .

using the data from our 2013 survey of federal managers , we found that specific practices identified in our previous work on the use of performance information to enhance or facilitate the use of performance information for decision making were significantly related to the use of performance information as measured by our use index .

figure 5 shows the questions that we tested based on each of the practices .

we have highlighted those questions and responses that we found to have a statistically significant and positive relationship with the use of performance information index.performance information index for agencies increased when managers reported that their agencies engaged to a greater extent in these practices as reflected in the survey questions .

for example , in 2013 , opm managers responded more favorably than the government - wide average on several of the survey questions related to these practices .

opm was one of the two agencies that experienced an increase in use of performance information from 2007 to 2013 , as measured by our index .

leading practices state that aligning an agency's goals , objectives , and measures increases the usefulness of the performance information collected to decision makers at each level , and reinforces the connection between strategic goals and the day - to - day activities of managers and staff .

in analyzing the 2013 survey results , we found that managers' responses to a related survey question were significantly related to the use of performance information controlling for other factors .

specifically , increase in the extent to which individuals agreed that managers aligned performance measures with agency - wide goals and objectives were associated with increase on the five - point scale we used for our use index .

government - wide , an estimated 46 percent of managers at federal agencies reported that managers at their levels took steps to align program performance measures with agency - wide goals and objectives .

the social security administration ( ssa ) and opm led the 24 agencies with approximately 65 percent of managers reporting that they aligned program performance measures with agency - wide goals and objectives .

dhs trailed the other agencies with only 34 percent of their managers reporting similarly .

leading practices state that to facilitate the use of performance information , agencies should ensure that information meets various users' needs for completeness , accuracy , consistency , timeliness , validity , and ease of use .

when analyzing the results of our 2013 survey , we found that managers' responses to the statement , “i have sufficient information on the validity of the performance data i use to make decisions,” related to their use of performance information .

specifically , individuals who rated their agencies as providing a higher extent of sufficient information on the validity of performance data for decision making , tended to rate their agencies higher on the performance use scale than individuals who rated their agencies lower , controlling for other factors .

having sufficient information on the validity of performance data for decision making had the largest potential effect of the questions included in our model .

this question was the strongest predictor in our regression analysis .

government - wide , the percentage of managers responding favorably about having sufficient information on the validity of performance data was particularly low , at about 36 percent .

the national aeronautics and space administration ( nasa ) and opm led the agencies with more than 50 percent of managers from nasa and opm responding that they have sufficient information about the validity of performance data for decision - making ( 58 percent and 54 percent , respectively ) .

the u.s. department of agriculture ( usda ) and dhs trailed the other agencies with less than 30 percent of their managers responding similarly ( 28 percent and 21 percent , respectively ) .

leading practices state that building the capacity for managers to use performance information is critical to using performance information in a meaningful fashion , and that inadequate staff expertise , among other when factors , can hinder agencies from using performance information.we analyzed the results of our 2013 survey , we found that managers who said that their agencies have provided training that would help them to use performance information to make decisions , rated their agencies more positively on our use index .

compared to managers who said their agencies had not trained them on using performance information in decision making , those who said their agencies did rated them higher on the use scale , controlling for other factors .

government - wide , an estimated 44 percent of the managers who responded to our survey reported that their agencies have provided training that would help them to use performance information in decision making .

the u.s. agency for international development ( usaid ) led the agencies in this area , with 62 percent of usaid managers responding that their agencies have provided training that would help them use of performance information in decision making in the last 3 years .

the u.s. department of the treasury ( treasury ) , dhs , the nuclear regulatory commission ( nrc ) , and the environmental protection agency ( epa ) trailed the other agencies with less than 35 percent of their managers responding similarly ( treasury and dhs with 34 percent , nrc with 33 percent , and epa with 32 percent ) that they had received training on use of performance information in the last 3 years .

other types of training did not appear to be positively related to use of performance information .

specifically , training on developing performance measures was significantly — but negatively — related to use of performance information .

training on ( 1 ) setting program performance goals ; ( 2 ) assessing the quality of performance data ; and ( 3 ) linking program performance to agency strategic plans was not found to relate to managers' use of performance information after controlling for other information .

leading practices state that the demonstrated commitment of leadership and management to achieving results and using performance information can encourage others to embrace using a model that uses performance information to make decisions .

when we analyzed the results of our 2013 survey , we found that managers' responses to the statement , “my agency's top leadership demonstrates a strong commitment to achieving results,” were significantly and positively related to the use of performance information .

specifically , on average , increases in a manager's rating of the strength of their agency's top leadership's commitment to achieving results were associated with increased ratings of their agencies on the use scale , controlling for other factors .

government - wide , the percentage of federal managers responding favorably about their agencies' top leadership demonstrating a strong commitment to achieving results was an estimated 60 percent .

managers from nrc ( 78 percent ) and ssa ( 74 percent ) had significantly higher scores on this question than the government - wide average , while managers from dhs ( 44 percent ) and usda ( 42 percent ) had lower scores than the government - wide average .

leading practices state that communicating performance information frequently and effectively throughout an agency can help managers to inform staff and other stakeholders of their commitment to achieve agency goals and to keep these goals in mind as they pursue their day - to - day activities .

when analyzing the results of our 2013 survey , we found that two related questions were significantly and positively related to an agency's use of performance information: agency managers / supervisors at my level effectively communicate performance information routinely .

employees in my agency receive positive recognition for helping the agency accomplish its strategic goals .

specifically , those who reported favorably that agency managers / supervisors at their levels effectively communicated performance information routinely tended to rate their agencies somewhat higher on the use index , controlling for other factors .

similarly , those who reported favorably that employees in their agency receive positive recognition for helping the agency accomplish its strategic goals rated their agencies somewhat higher on the use scale , controlling for other factors .

an estimate 41 percentage of managers government - wide who responded to our survey reported that agency managers / supervisors at their level effectively communicated performance information routinely .

about 60 percent of managers at the small business administration , department of labor , and opm , responded positively when asked about effectively communicating performance information routinely ( 62 percent , 61 percent , and 60 percent respectively ) .

dhs trailed the other agencies with only 34 percent of its managers reporting similarly .

government - wide , an estimated 42 percent of the managers responded favorably when asked about employees in their respective agencies receiving positive recognition for helping the agencies accomplish their strategic goals .

while the managers at nrc and the u.s. department of commerce scored at or higher than 50 percent when asked about positive recognition ( 58 percent and 50 percent , respectively ) , dhs trailed federal agencies with only 34 percent of its managers reporting similarly .

our analyses of agency - level results from our periodic surveys of federal managers in 2007 and 2013 reinforce that there are several leading practices and related survey questions that significantly influenced agencies' use of performance information for management decision making .

however , our surveys show that such usage generally has not improved over time .

this information can be helpful to the office of management and budget ( omb ) and the performance improvement council as they work with federal agencies to identify and implement stronger performance management practices to help improve agency use of performance information .

moreover , the use of performance information will remain a challenge unless agencies can narrow the gap in use between senior executive service ( ses ) and non - ses managers .

we provided a draft of this report to the director of omb and to the 24 agencies that responded to our 2007 and 2013 federal managers surveys .

on september 4 , 2014 , omb staff provided us with oral comments and generally agreed with our report .

omb staff also stated that they would continue to work with agencies to address the use of performance information through agencies' annual strategic reviews of progress toward agencies' strategic objectives , which began in 2014 .

we also received comments from the u.s .

departments of commerce ( commerce ) and the treasury ( treasury ) , the general services administration ( gsa ) , and the national aeronautics and space administration ( nasa ) .

on august 27 , 2014 , the liaison from nasa e - mailed us a summary of nasa officials' comments .

on august 28 , 2014 , the liaison from gsa e - mailed us a summary of gsa officials' comments .

on august 29 , 2014 , the liaisons from commerce and treasury e - mailed us summaries of their respective agency officials' comments .

commerce and gsa generally agreed with our report , and provided technical comments , which we incorporated as appropriate .

nasa and treasury raised concerns about the findings and conclusions in our report , including the design of the surveys .

we discuss their comments and our evaluation of them below , which generally fell into the following four categories: nasa and treasury raised concerns about the underlying methodology for the 2007 and 2013 federal managers surveys .

they said that it did not adequately provide agency - wide perspectives that fully represented the agencies' use of performance information .

specifically , nasa and treasury expressed concerns about the lack of demographic information about the survey respondents ( e.g .

survey respondents by agency component and geographic location ) .

treasury also expressed concern as to whether we had included senior leadership in our report .

to address this comment , we added some additional information to our report that discusses our survey design and administration , specifically that we did not collect demographic information beyond whether a federal manager' was a member of the ses or not ( non - ses ) .

moreover , our stratified random sample of federal managers ensured that we had a representative sample of federal managers both government - wide and within each of the 24 agencies we surveyed .

it was not our objective to design the survey and draw a sample of managers that would allow us to report in a generalizable way at the geographic location or organizational level within an agency .

designing a sample to produce estimates at the geographic location and / or organizational level within an agency would result in a much larger sample than the approximately 107,326 managers selected in our 2007 survey and the approximately 148,300 managers selected in our 2013 survey .

nevertheless , as previously discussed , our sample was sufficient for the purposes of this report .

nasa and treasury also expressed concern that despite all the efforts their respective agencies have undertaken to implement the gpra modernization act of 2010 , our draft report did not provide information on the root causes for the lack of progress in the use of performance information in their agencies .

for example , nasa cited some of its agency initiatives , including the development of an automated performance management data repository to assist in the agency's decision - making process .

treasury cited its quarterly performance review process as an example of the agency's commitment to using performance information in decision making .

we recognize the activities that the agencies have underway to improve staff engagement on the use of performance information for decision making , and have previously reported on some of these initiatives .

however , despite the efforts discussed above , our survey results showed that the use of performance information , as reported by managers at the agencies , has not improved within agencies between 2007 and 2013 .

our report analyzed the results from specific questions in both the 2007 and 2013 surveys .

we agree that our report does not provide information on the root causes for the trends we found in the use of performance information .

however , the results of the regression analysis in this report point to some specific practices that can enhance the use of performance information , areas where federal agencies may want to focus further analysis and efforts .

both nasa and treasury requested their respective agencies' 2007 and 2013 survey data sets to perform additional analyses that might provide further insights into root causes underlying the trends in the use of performance information within their agencies .

treasury also commented that the rankings we report based on the average scores on the 2013 use of performance information index might imply that agencies with a higher ranking are theoretically better at using performance information , and therefore , have superior performance management practices .

treasury also raised concerns about our use of the index to score agencies .

it asked if it should view the higher - ranking agencies as examples of what agencies should do to improve the use of performance information .

there is not a huge difference in scores between those agencies that scored higher on the use index than others at the lower end .

but , we believe our methodology is useful for generally distinguishing between agencies' levels of use of performance information , and for assessing change in use of performance information over time .

however , we revised our report to focus on agencies' scores rather than on rank ordering .

we also did additional statistical testing to determine whether or not the changes between the 2007 and 2013 use indexes were statistically different among agencies .

as for the implication of the rankings on the quality of management practices in particular agencies , in 2007 , we did employ a use index to identify agencies for further case study analysis .

we selected an agency that had significantly improved on the use index along with agencies that scored lower on the index to assess whether there were any promising practices or challenges facing those agencies .

nasa , treasury , and commerce all commented that it was difficult to tell how managers may have interpreted the term “performance information” when responding to our surveys .

treasury further commented that it was unclear what information managers were using to make management decisions if they were not using performance information .

in both the 2007 and 2013 surveys , we defined the terms “performance information” and “performance measures” in the broadest sense .

to clarify this point , we added the definition of performance information from the 2013 managers survey in the report .

moreover , as discussed above , additional agency analysis of the root causes underlying the use of performance information could provide some additional context to the types of information agencies are using for decision making .

the following 20 agencies had no comments on the draft report: the u.s .

departments of agriculture , defense , education , energy , health and human services , homeland security , housing and urban development , the interior , justice , labor , state , transportation , and veterans affairs , the environmental protection agency , nuclear regulatory commission , office of personnel management , national science foundation , small business administration , social security administration , and the united states agency for international development .

the written response from the social security administration is reproduced in appendix ii .

we are sending copies of this report to the agencies that participated in our 2013 managers survey , the director of omb , as well as appropriate congressional committees and other interested parties .

in addition , this report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff members have any questions about this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix ii .

in analyzing the results of our 2013 survey , we explored whether federal managers' responses to certain survey questions could help explain differences in how managers in agencies reported using performance information .

to examine which factors related to agency use of performance information , as measured by the use of performance information index , we conducted regression analysis .

the regression analysis allowed us to assess the unique association between our outcome variable — the performance information index — and a given predictor variable , while controlling for multiple other predictor variables .

to create the use of performance index , we identified survey questions that reflected managers' use of performance information for key management activities and decision making .

the 2013 use of performance index included most of the questions included in our 2007 index , and additional questions from the 2013 managers survey that we determined reflected the concept of use of performance information ( see figure 1 for specific questions included in our index ) .

core set of items from the original index , we tested the impact of including and excluding several additional questions related to performance management use to ensure the cohesiveness and strength of our revised index .

our revised index is an average of the questions used for the index and runs from 1 to 5 , where a 1 reflects that managers feel the agency engages “to no extent” and a 5 reflecting that managers feel the agency engages “to a very great extent” in the use of performance information activities .

we found the index met generally accepted standards for scale reliability .

for more information on the original index we created for the 2007 ffederal managers survey , see gao - 08-1026t .

to develop our regression model examining predictors of performance use as measured by our index , we first identified a series of variables that were related to one of the five practices we have previously found to enhance or facilitate use of performance information .

these practices include: aligning agencywide goals , objectives , and measures ; improving the usefulness of performance information ; developing the capacity to use performance information ; demonstrating management commitment ; and communicating performance information frequently and effectively .

see figure 3 for the specific questions related to these five practices that we included in the regression .

although we identified other questions also related to the five elements of effective performance management , many of these questions were already accounted for in our use index of performance information , and we excluded them from consideration in our regression .

overall , our results demonstrate that some types of management practices and training are more positively correlated than others , with manager perceptions of performance information use as measured by the use index , even when controlling for other factors .

further , these results suggest that certain specific efforts to increase agency use of performance information — such as increasing timeliness of performance information and providing information on the validity of performance measures — may have a higher return than others .

to execute our analysis , we began with a base model that treated differences in managers' views of agency performance management use as a function of the agency where they worked .

we found that despite statistically significant differences on average among managers at different agencies , a regression model based on agency alone had very poor predictive power ( r - squared of .03 ) .

we next examined whether managers' responses to other items reflecting the practices of effective performance management related to their perceptions of agency use of performance information , independent of agency .

we found that several items consistently predicted increases on individuals' ratings of their agencies use of performance management information , including whether managers align program performance measures with agency goals and objectives ; having information on the validity of performance measures ; and training on how to use performance management information in decision making .

we also tested this model controlling for whether a respondent was a member of the senior executive service ( ses ) , and found similar results .

we also tested our model with a variable to control for agency size in five categories .

we found that , relative to the largest agencies ( 100,000 or more employees ) , managers at smaller agencies tended to rate their agency's use of performance information slightly lower .

the significance and magnitude of other significant variables was similar whether we controlled for agency size or using intercepts to control for individual agencies .

our final model had an r - squared of .65 , suggesting that the independent variables in the model predicted approximately 65 percent of the variance in the use index .

specific results are presented in table 2 below .

each coefficient reflects the average increase in the dependent variable , our five - point use scale , associated with a one - unit increase in the value of the independent variables .

note that in our discussion , we highlight the maximum potential impact of each variable rather than the increase in the use score associated with each increase in a dependent variable .

as seen in table 2 , at least one question related to each of the five practices to enhance agencies' use of performance information was significant .

with respect to aligning agencywide goals , objectives , and measures , we found that each increase in terms of the extent to which individuals felt that managers aligned performance measures with agencywide goals and objectives was associated with a .13 increase in their score on the use scale , or approximately a .52 increase on the 5- point use scale when comparing individuals in the lowest to the highest categories .

in terms of improving the usefulness of performance information , we found that having information on the validity of performance data for decision making was the strongest predictor in our model .

compared to individuals who said that they did not have sufficient information on the validity of performance data for decision making , on average , individuals who said they had a very great extent of information rated their agencies approximately 0.64 points higher on the performance use scale , controlling for other factors .

in contrast , the potential effect of the timeliness of information , while significant , had a smaller potential impact on managers' perceptions of their agency's use of performance information .

on average , managers who responded “to a very great extent” on whether their agency's performance information was available in time to manage programs or projects rated their agency about .28 points higher on the performance use scale than those who responded “to no extent.” in terms of developing agency capacity to use performance information , we found that one type of training was positively related to use of performance information , though other types of training were either not related or were negatively related , after controlling for other factors .

compared to managers who said their agencies had not trained them with training on how to use performance information in decision making , those who said their agencies did provide such training rated their agencies an average of .14 points higher on the use scale , controlling for other factors .

the potential effect of this type of training was relatively small compared to the potential effect of some of the other predictors in our model .

in contrast , training in developing performance measures was negatively associated with managers' perceptions of performance information use .

with respect to demonstrating management commitment , managers that rated their agency's leadership highly in terms of demonstrating a strong commitment to achieving results tended to rate their agencies higher on performance information use , as measured by our use index .

each increase in the extent to which a manager felt their agency leadership was committed to results was associated with a .08 increase in the performance use index , or up to a .32 increase in the five - point performance use index when comparing managers who reported “no extent” of leadership commitment to those that reported “a very great extent.” two questions related to communicating performance information frequently and effectively were significantly and positively associated with manager's perceptions of an agency's use of performance information , controlling for other factors .

compared to those who rated their agencies the lowest in terms of whether managers and supervisors effectively communicated performance information routinely — those managers who rated their agencies most highly averaged .32 points higher on the five - point performance use index .

similarly , managers who reported that employees in their agency received “a very great extent” of positive recognition for helping the agency to accomplish strategic goals rated their agencies an average of .24 points higher on performance information use , as measured by our use index .

we did not find a statistically significant relationship between the accessibility of performance information ( to managers , employees or the public ) and managers' perceptions of use of performance information .

to conduct our analysis , we used stata software to generate regression estimates that incorporated variance calculations appropriate for the to ensure that large amounts of complex design of the survey data.missing data do not result from listwise deletion , we imputed values for individual questions if the individual is missing or indicated “no basis to judge” on three or fewer responses from the 23 variables initially tested in the regression , using the agency - level average to impute .

individuals missing data on more than 3 of the 23 potential variables were dropped from the analysis .

we conducted a variety of sensitivity checks to ensure that our results were robust across different specifications and assumptions .

for the most part , we found generally similar patterns across models in terms of the magnitude and significance of different variables related to the elements of effective performance management .

in general , our models assume that the relationship between the independent and dependent variables is linear , and that changes in the dependent variable associated with a change in the independent variable are similar across each ordinal category .

under this specification , the change in the use index associated with a shift from “to no extent” to “to a small extent” is assumed to be similar to the change associated with an increase from “to a great extent” to “a very great extent” .

to determine whether the linear specification was appropriate , or consistent with the observed data , we tested versions of our models that treated independent variables with a likert - scale response as categorical .

we found our results to be robust across a variety of specifications , including those that relaxed the assumption of linearity for responses based on a five - point scale .

in addition to the contact named above , sarah veale ( assistant director ) , margaret mckenna adams , tom beall , mallory barg bulman , chad clady , karin fangman , cynthia jackson , janice latimer , donna miller , anna maria ortiz , kathleen padulchick , mark ramage , joseph santiago , albert sim , and megan taylor made key contributions to this report .

