the u.s .

visitor and immigrant status indicator technology ( us - visit ) is a multibillion - dollar program of the department of homeland security ( dhs ) that is intended to record the entry into and exit from the united states of selected individuals , verify their identity , and confirm their compliance with the terms of their admission into and stay in the united states .

the goals of the program are to ( 1 ) enhance the security of our citizens and visitors , ( 2 ) facilitate legitimate travel and trade , ( 3 ) ensure the integrity of the u.s. immigration system , and ( 4 ) protect the privacy of our visitors .

since fiscal year 2002 , dhs has been legislatively directed to submit annual expenditure plans for the program , and we have been directed to review these plans and issue reports .

these reports have , among other things , identified risks that face the department in delivering promised program capabilities and benefits on time and within cost .

for example , we reported that the program office did not have the human capital and acquisition process discipline needed to effectively manage the program .

because of the number and severity of program management challenges that we identified , we concluded that the program was risky .

to address program risks , our reports have included 18 recommendations in such areas as system acquisition process controls , economic justification , human capital management , cost estimating , and test management , all of which dhs has agreed to implement .

because of your continued interest in ensuring that dhs is taking the necessary actions to successfully implement us - visit , you asked us to determine the progress being made in implementing these recommendations .

to achieve this objective , we analyzed program plans , reports , and system documentation relative to the intent of each of our recommendations , and we interviewed appropriate dhs and program officials .

 ( further details on our objective , scope , and methodology are provided in app .

i. ) .

our work was performed from august 2005 through december 2005 in accordance with generally accepted government auditing standards .

us - visit is a governmentwide program intended to enhance the security of u.s. citizens and visitors , facilitate legitimate travel and trade , ensure the integrity of the u.s. immigration system , and protect the privacy of our visitors .

its scope includes the pre - entry , entry , status , and exit of hundreds of millions of foreign national travelers who enter and leave the united states at over 300 air , sea , and land poes , and the provision of new analytical capabilities across the overall process .

to achieve its goals , us - visit uses biometric information ( digital fingerscans and photographs ) to verify identity .

in many cases , the us - visit process begins overseas at u.s. consular offices , which collect biometric information from applicants for visas and check this information against a database of known criminals and suspected terrorists .

when a visitor arrives at a poe , the biometric information is used to verify that the visitor is the person who was issued the visa .

in addition , at certain sites , visitors are required to confirm their departure by undergoing us - visit exit procedures — that is , having their visas or passports scanned and undergoing fingerscanning .

the exit confirmation is added to the visitor's travel records to demonstrate compliance with the terms of admission to the united states .

 ( app .

iii provides a detailed description of the pre - entry , entry , status , exit , and analysis processes. ) .

collecting , maintaining , and sharing information on certain foreign nationals who enter and exit the united states ; identifying foreign nationals who ( 1 ) have overstayed or violated the terms of their admission ; ( 2 ) may be eligible to receive , extend , or adjust their immigration status ; or ( 3 ) should be apprehended or detained by law enforcement officials ; detecting fraudulent travel documents , verifying traveler identity , and determining traveler admissibility through the use of biometrics ; and facilitating information sharing and coordination within the immigration and border management community .

in july 2003 , dhs established a program office with responsibility for managing the acquisition , deployment , operation , and sustainment of the us - visit system and its associated supporting people ( eg , customs and border protection ( cbp ) officers ) , processes ( eg , entry / exit policies and procedures ) , and facilities ( eg , inspection booths and lanes ) , in coordination with its stakeholders ( cbp and the department of state ) .

as of october 2005 , about $1.4 billion has been appropriated for the program , and , according to program officials , about $962 million has been obligated .

dhs plans to deliver us - visit capability in four increments , with increments 1 through 3 being interim , or temporary , solutions that fulfill legislative mandates to deploy an entry / exit system , and increment 4 being the implementation of a long - term vision that is to incorporate improved business processes , new technology , and information sharing to create an integrated border management system for the future .

in increments 1 through 3 , the program is building interfaces among existing ( “legacy” ) systems ; enhancing the capabilities of these systems ; and deploying these capabilities to air , sea , and land poes .

these increments are to be largely acquired and implemented through existing system contracts and task orders .

in may 2004 , dhs awarded an indefinite - delivery / indefinite - quantity prime contract to accenture and its partners .

according to the contract , the prime contractor will help support the integration and consolidation of processes , functionality , and data , and it will develop a strategy to build on the technology and capabilities already available to produce the strategic solution , while also assisting the program office in leveraging existing systems and contractors in deploying the interim solutions .

increment 1 concentrates on establishing capabilities at air and sea poes .

it is divided into two parts — 1 and 1b .

increment 1 ( air and sea entry ) includes the electronic capture and matching of biographic and biometric information ( two digital index fingerscans and a digital photograph ) for selected foreign nationals , including those from visa waiver countries .

increment 1 was deployed on january 5 , 2004 , for individuals requiring a nonimmigrant visa to enter the united states , through the modification of pre - existing systems .

these modifications accommodated the collection and maintenance of additional data fields and established interfaces required to share data among dhs systems in support of entry processing at 115 airports and 14 seaports .

increment 1b ( air and sea exit ) involves the testing of exit devices to collect biometric exit data for select foreign nationals at 11 airports and seaports .

three exit alternatives were pilot tested: kiosk — a self - service device ( which includes a touch - screen interface , document scanner , finger scanner , digital camera , and receipt printer ) that captures a digital photograph and fingerprint and prints out an encoded receipt .

mobile device — a hand - held device that is operated by a workstation attendant ; it includes a document scanner , finger scanner , digital camera , and receipt printer and is used to capture a digital photograph and fingerprint .

validator — a hand - held device that is used to capture a digital photograph and fingerprint , which are then matched to the photograph and fingerprint captured via the kiosk and encoded in the receipt .

increment 2 focuses primarily on extending us - visit to land poes .

it is divided into three parts — 2a , 2b , and 2c .

increment 2a ( air , sea , and land ) includes the capability to biometrically compare and authenticate valid machine - readable visas and other travel and entry documents issued by state and dhs to foreign nationals at all poes .

increment 2a was deployed on october 23 , 2005 , according to program officials .

it also includes the deployment by october 26 , 2006 , of technology to read biometrically enabled passports from visa waiver countries .

increment 2b ( land entry ) redesigns the increment 1 entry solution and expands it to the 50 busiest land poes .

the process for issuing form i - 94 was redesigned to enable the electronic capture of biographic , biometric ( unless the traveler is exempt ) , and related travel documentation for arriving travelers .

this increment was deployed to the busiest 50 u.s. land border poes as of december 29 , 2004 .

before increment 2b , all information on the form i - 94s was handwritten .

the redesigned systems electronically capture the biographic data included in the travel document .

in some cases , the form is completed by cbp officers , who enter the data electronically and then print the form .

increment 2c is to provide the capability to automatically , passively , and remotely record the entry and exit of covered individuals using radio frequency ( rf ) technology tags at primary inspection and exit lanes .

an rf tag that includes a unique id number is to be embedded in each form i - 94 , thus associating a unique number with a record in the us - visit system for the person holding that form i - 94 .

in august 2005 , the program office deployed the technology to five border crossings ( three poes ) to verify the feasibility of using passive rf technology to record traveler entries and exits via a unique id number embedded in the cbp form i - 94 .

the results of this demonstration are to be reported in february 2006 .

increment 3 extended increment 2b ( land entry ) capabilities to 104 land poes ; this increment was essentially completed as of december 19 , 2005 .

increment 4 is the strategic us - visit program capability , which program officials stated will likely consist of a further series of incremental releases or mission capability enhancements that will support business outcomes .

the program reports that it has worked with its prime contractor and partners to develop this overall vision for the immigration and border management enterprise .

increments 1 through 3 include the interfacing and integration of existing systems and , with increment 2c , the creation of a new system , the automated identification management system ( aidms ) .

the three main existing systems are as follows: the arrival departure information system ( adis ) stores noncitizen traveler arrival and departure data received from air and arrival data captured by cbp officers at air and sea poes , form i - 94 issuance data captured by cbp officers at increment 2b departure information captured at us - visit biometric departure pilot ( air and sea ) locations , pedestrian arrival information and pedestrian and vehicle departure information captured at increment 2c poe locations , and status update information provided by the student and exchange visitor information system ( sevis ) and the computer linked application information management system ( claims 3 ) ( described below ) .

adis provides record matching , query , and reporting functions .

the passenger processing component of the treasury enforcement communications system ( tecs ) includes two systems: advance passenger information system ( apis ) , a system that captures arrival and departure manifest information provided by air and sea carriers , and the interagency border inspection system , a system that maintains lookout data and interfaces with other agencies' databases .

cbp officers use these data as part of the admission process .

the results of the admission decision are recorded in tecs and adis .

the automated biometric identification system ( ident ) collects and stores biometric data on foreign visitors .

us - visit also exchanges biographic information with other dhs systems , including sevis and claims 3 .

these two systems contain information on foreign students and foreign nationals who request benefits , such as a change of status or extension of stay .

some of the systems previously described , such as ident and the new aidms , are managed by the program office , while some systems are managed by other organizational entities within dhs .

for example , tecs is managed by cbp , sevis is managed by immigration and customs enforcement , claims 3 is under united states citizenship and immigration services , and adis is jointly managed by cbp and us - visit .

us - visit also interfaces with other , non - dhs systems for relevant purposes , including watch list updates and checks to determine whether a visa applicant has previously applied for a visa or currently has a valid u.s. visa .

in particular , us - visit receives biographic and biometric information from state's consular consolidated database as part of the visa application process , and returns fingerscan information and watch list changes .

the us - visit program office structure includes nine component offices .

each of the program offices includes a director and subordinate organizational units , as established by the director .

the responsibilities for each office are stated below .

figure 1 shows the program office structure , including its nine offices .

the roles and responsibilities for each of the nine offices include the following: chief strategist is responsible for developing and maintaining the strategic vision , strategic documentation , transition plan , and business case .

budget and financial management is responsible for establishing the program's costs estimates ; analysis ; and expenditure management policies , processes , and procedures that are required to implement and support the program by ensuring proper fiscal planning and execution of the budget and expenditures .

mission operations management is responsible for developing business and operational requirements based on strategic direction provided by the office of the chief strategist .

outreach management is responsible for enhancing awareness of us - visit requirements among foreign nationals , key domestic audiences , and internal stakeholders by coordinating outreach to media , third parties , key influencers , members of congress , and the traveling public .

information technology management is responsible for developing technical requirements based on strategic direction provided by the office of the chief strategist and business requirements developed by the office of mission operations management .

implementation management is responsible for developing accurate , measurable schedules and cost estimates for the delivery of mission systems and capabilities .

acquisition and program management is responsible for establishing and managing the execution of program acquisition and management policies , plans , processes , and procedures .

administration and training is responsible for developing and administering a human capital plan that includes recruiting , hiring , training , and retaining a diverse workforce with the competencies necessary to accomplish the mission .

facilities and engineering management is responsible for establishing facilities and environmental policies , procedures , processes , and guidance required to implement and support the program office .

in response to legislative mandate , we have issued four reports on dhs's annual expenditure plans for us - visit .

our reports have , among other things , assessed whether the plans satisfied the legislative conditions and provided observations on the plans and dhs's program management .

as a result of our assessments , we made 24 recommendations aimed at improving both plans and program management , all of which dhs has agreed to implement .

of these 24 recommendations , 18 address risks stemming from program management .

the current status of dhs's implementation of our 18 recommendations on program risks is mixed , but progress in critical areas has been slow .

for example , over 2 years have passed , and the program office has yet to develop a security plan consistent with federal guidance or to economically justify its investment in system increments .

according to the program director , the pace of progress is attributable to competing demands on time and resources .

dhs agreed to implement all 18 recommendations .

of these 18 , dhs has completely implemented 2 , has partially implemented 11 , and is in the process of implementing another 5 .

of the 11 that are partially implemented , 7 are about 2 years old , and 4 are about 10 to 19 months old .

of the 5 that are in progress , 3 are about 10 months old .

these 18 recommendations are aimed at strengthening the program's management effectiveness .

the longer that the program takes to implement the recommendations , the greater the risk that the program will not meet its goals on time and within budget .

figure 2 provides an overview of the extent to which each recommendation has been implemented .

the figure is followed by sections providing details on each recommendation and our assessment of its implementation status .

in june 2003 , we reported that the immigration and naturalization service had not developed a security plan and performed a privacy impact assessment for the entry exit program ( as us - visit was then known ) .

a security plan and privacy impact assessment are important to understanding system requirements and ensuring that the proper safeguards are in place to protect system data and resources .

system acquisition best practices and federal guidance advocate understanding and defining security and privacy requirements both early and continuously in a system's life cycle , and effectively planning for their satisfaction .

accordingly , we recommended that dhs do the following: develop and begin implementing a system security plan , and perform a privacy impact assessment and use the results of the analysis in near - term and subsequent system acquisition decision making .

since we made the system security plan recommendation about 2½ years ago , its implementation has been slow .

for example , we reported in september 2003 and again in may 2004 that the program office had not developed a security plan .

in february 2005 , we reported that the program office had developed a security plan , dated september 2004 , and that this plan was generally consistent with federal guidance .

that is , the plan provided an overview of system security requirements , described the controls in place or planned for meeting those requirements , referred to the applicable documents that prescribe the roles and responsibilities for managing the us - visit component systems , and addressed security awareness and training .

however , the program office had not conducted a risk assessment or included in the plan when an assessment would be completed .

according to guidance from the office of management and budget ( omb ) , the security plan should describe the methodology that is used to identify system threats and vulnerabilities and to assess risks , and it should include the date the risk assessment was completed .

according to program officials , they completed a programwide risk assessment in december 2005 , but have yet to provide a copy of the assessment to us .

therefore , we cannot confirm that the assessment has been done , and done properly .

the absence of a risk assessment and a security plan that reflects this assessment is a significant program weakness .

risk assessments are critical to establishing effective security controls because they provide the basis for establishing appropriate policies and selecting cost - effective controls to implement these policies .

without such an assessment , us - visit does not have adequate assurance that it knows the risks associated with the program and thus whether it has implemented effective controls to address them .

notwithstanding these limitations in the security plan , the program office has begun to implement aspects of its september 2004 security plan .

for example , the information systems security manager told us that a security awareness program is established and key personnel have attended security training .

since june 2003 , us - visit has also developed and periodically updated a privacy impact assessment .

an initial impact assessment was issued in january 2004 , and a revised assessment was issued in september 2004 .

a more recent assessment , dated july 2005 , reflects changes related to increments 1b and 2c .

each of these assessments is generally consistent with omb guidance .

that is , each of the assessments addressed most omb requirements , including the impact that the system will have on individual privacy , the privacy consequences of collecting the information , and alternatives considered to collect and handle information .

the most recent impact assessment , for example , states that three alternatives were considered for increment 1b — the kiosk , the mobile device , and the validator ( a combination of the two ) — and discusses proposals to mitigate the privacy risks of all three , such as by limiting the duration of data retention on the exit devices and using encryption .

however , omb guidance also requires that privacy impact assessments developed for systems under development address privacy in relevant system documentation , including statements of need , functional requirements documents , and cost - benefit analyses .

as we reported about previous privacy impact assessments , privacy is only partially addressed in system documentation .

for example , the increment 1b cost - benefit analysis assesses the privacy risk associated with each exit alternative , and the increment 2c business requirements state that all solutions are to be compliant with privacy laws and regulations and adhere to us - visit privacy policy .

however , we did not find privacy in the increment 1b business requirements or the increment 2c functional requirements .

program officials , including the us - visit privacy officer , acknowledged that privacy is not included in the system documentation , but stated that privacy is considered in the development of the documentation and that the privacy office reviews key system documentation at relevant times during the system development life cycle .

nevertheless , we did not find evidence of privacy being addressed in the system documentation , and program officials acknowledged that it was not included .

until the program performs a risk assessment and fully implements a security plan that reflects this assessment , it cannot adequately ensure that us - visit is cost - effectively safeguarding assets and data .

moreover , without reflecting privacy in system documentation , it cannot adequately ensure that privacy needs are being fully addressed .

we reported in september 2003 that the program office had not defined key acquisition management controls to support the acquisition of us - visit , and therefore its efforts to acquire , deploy , operate , and maintain system capabilities were at risk of not satisfying system requirements and of not meeting benefit expectations on time and within budget .

the capability maturity model – integration® ( cmmi ) developed by carnegie mellon university's software engineering institute ( sei ) explicitly defines process management controls that are recognized hallmarks of successful organizations and that , if implemented effectively , can greatly increase the chances of successfully acquiring software - intensive systems .

sei's cmmi model uses capability levels to assess process maturity .

because establishing the basic acquisition process capabilities , according to sei , can take on average about 19 months , we recognized the importance of starting early to build effective acquisition management capabilities by recommending that dhs do the following: develop and implement a plan for satisfying key acquisition management controls , including acquisition planning , solicitation , requirements management , program management , contract tracking and oversight , evaluation , and transition to support , and implement the controls in accordance with sei guidance .

the program office has recently taken foundational steps to establish key acquisition management controls .

for example , it has developed a process improvement plan , dated may 16 , 2005 ( about 20 months after our recommendation ) , to define and implement these controls .

as part of its improvement program , the program office is implementing a governance structure for overseeing improvement activities , consisting of three groups: a management steering group , an enterprise process group , and process action teams .

specific roles for each of these groups are described below .

the management steering group is to provide policy and procedural guidance and to oversee the entire improvement program .

the steering group is chaired by the us - visit director , with the deputy director and the functional office directors serving as core members .

the enterprise process group is to provide planning , management , and operational guidance in day - to - day process improvement activities .

the group is chaired by the process improvement leader and is composed of individuals from each functional office .

process action teams are to provide specific process documentation and to provide implementation support and training services .

these teams are to be active as long as a particular process improvement initiative is under way .

to date , the program office has chartered five process teams — configuration management , cost analysis , process development , communications , and policy .

in addition , the program office has recently completed a self - assessment of its acquisition process maturity , and it plans to use the assessment results to establish a baseline of its acquisition process maturity for improvement .

according to program officials , the assessment included 13 key process areas that are generally consistent with the process areas cited in our recommendation .

the program has ranked these 13 process areas according to their priority , and , for initial implementation , it plans to focus on the following 6: configuration management .

establishing and maintaining the integrity of the products throughout their life cycle .

process and product quality assurance .

taking actions to provide management with objective insight into the quality of products and processes .

project monitoring and control .

tracking the project's progress so that appropriate corrective actions can be taken when performance deviates significantly from plans .

project planning .

establishing and maintaining plans for work activities .

requirements management .

managing the requirements and ensuring a common understanding of the requirements between the customer and the product developers .

risk management .

identifying potential problems before they occur so that they can be mitigated to minimize any adverse impact .

the improvement plan is currently being updated to reflect the results of the baseline assessment and to include a detailed work breakdown structure , process prioritization , and resource estimates .

according to the director , acquisition and program management office ( apmo ) , the goal is to conduct a formal sei appraisal to assess the capability level of some or all of the six processes by october 2006 .

notwithstanding the recent steps to begin addressing our recommendation , much work remains to fully implement key acquisition management controls .

moreover , effectively implementing these controls takes considerable time .

therefore , it is important that these improvement efforts stay on track .

until these processes are effectively implemented , us - visit will be at risk of not delivering promised capabilities on time and within budget .

in september 2003 , we reported that the program had not assessed the costs and benefits of increment 1 , which is extremely important because the decision to invest in any capability should be based on reliable analyses of return on investment .

further , according to omb guidance , individual increments of major systems are to be individually supported by analyses of benefits , cost , and risk .

without reliable analyses , an organization cannot adequately know that a proposed investment is a prudent and justified use of limited resources .

accordingly , we recommended that dhs do the following: determine whether proposed us - visit increments will produce mission value commensurate with cost and risks and disclose to the congress planned actions .

as we reported in september 2003 and again in february 2005 , the program office did not justify its planned investment in increments 1 and 2b , respectively , based on expected return on investment .

since then , the program has developed a cost - benefit analysis for increment 1b .

omb has issued guidance concerning the analysis needed to justify investments .

according to this guidance , such analyses should meet certain criteria to be considered reasonable .

these criteria include , among other things , comparing alternatives on the basis of net present value and conducting uncertainty analyses of costs and benefits .

dhs has also issued guidance on such economic analyses that is consistent with that of omb .

the latest cost - benefit analysis for increment 1b ( dated june 23 , 2005 ) identifies potential costs and benefits for three exit solutions at air and sea poes and provides a general rationale for the viability of the three alternatives described .

this latest analysis meets four of eight omb economic analysis criteria .

however , it does not , for example , include a complete uncertainty analysis ( i.e. , both a sensitivity analysis and a monte carlo simulation ) for the three exit alternatives evaluated .

that is , the cost - benefit analysis does include a monte carlo simulation , but it does not include a sensitivity analysis for the three alternatives .

an analysis of uncertainty is important because it provides decision makers with a perspective on the potential variability of the cost and benefit estimates should the facts , circumstances , and assumptions change .

table 1 summarizes our analysis of the extent to which us - visit's june 23 , 2005 , cost - benefit analysis for increment 1b satisfies eight omb criteria .

it is important that the program adhere to relevant guidance in developing its incremental cost - benefit analyses .

if this is not done , the reliability of the analyses is diminished , and an adequate basis for prudent investment decision making does not exist .

moreover , if the mission value of a proposed investment is not commensurate with costs , it is vital that this information be fully disclosed to dhs and congressional decision makers .

the underlying intent of our recommendation is that this information be available to inform such decisions .

in september 2003 , we reported that key aspects of the larger homeland security environment in which us - visit would need to operate had not been defined .

for example , we stated that certain policy and standards decisions had not been made ( eg , whether official travel documents will be required for all persons who enter and exit the country , including u.s. and canadian citizens , and how many fingerprints are to be collected ) .

in the absence of this operational context , program officials were making assumptions and decisions that , if they proved inconsistent with subsequent policy or standards decisions , would require us - visit rework .

to minimize the impact of these changes , we recommended that dhs do the following: clarify the operational context in which us - visit is to operate .

after about 27 months , defining this operational context remains a work in progress .

according to the chief strategist , an immigration and border management strategic plan was drafted in march 2005 that shows how us - visit is aligned with dhs's organizational mission and defines an overall vision for immigration and border management .

this official stated that this vision provides for an immigration and border management enterprise that unifies multiple internal departmental and other external stakeholders with common objectives , strategies , processes , and infrastructures .

since the plan was drafted , dhs has reported that other relevant initiatives have been undertaken , such as the security and prosperity partnership of north america and the secure border initiative .

the security and prosperity partnership is to , among other things , establish a common approach to securing the countries of north america — the united states , canada , and mexico — by , for example , implementing a border facilitation strategy to build capacity and improve the legitimate flow of people and cargo at our shared borders .

the secure border initiative is to implement a comprehensive approach to securing our borders and reducing illegal immigration .

according to the chief strategist , while portions of the strategic plan are being incorporated into these initiatives , these initiatives and their relationship with us - visit are still being defined .

we have yet to receive the us - visit strategic plan because , according to program officials , it had not yet been approved by dhs management .

until us - visit's operational context is fully defined , dhs is increasing its risk of defining , establishing , and implementing a program that is duplicative of other programs and not interoperable with them .

this in turn will require rework to address these areas .

while this issue was significant 27 months ago , when we made the recommendation , it is still more significant now .

we reported in september 2003 that the program had not fully staffed its program office .

our prior experience with major acquisitions like us - visit shows that to be successful , they need , among other things , to have adequate resources .

accordingly , we recommended that dhs do the following: ensure that human capital and financial resources are provided to establish a fully functional and effective program office .

about 2 years later , us - visit had filled 102 of its 115 planned government positions and all of its planned 117 contractor positions .

for the remaining 13 government positions , 5 positions had been selected ( pending completion of security clearances ) , and recruitment action was in process for filling the remaining 8 vacancies .

according to the office of administration and training manager , funding is available to complete the hiring of all 115 government employees .

notwithstanding this progress , in february 2005 , us - visit completed a workforce analysis and requested additional positions based on the results .

according to program officials , a revised analysis was submitted in the summer of 2005 , but the request has not yet been approved .

figure 3 shows the program office organization structure and functions and how many of the 115 positions needed have been filled .

securing necessary resources will be a continuing challenge and an essential ingredient to the program's ability to acquire , deploy , operate , and maintain system capabilities on time and within budget .

we reported in september 2003 that the program had not defined specific roles and responsibilities for its staff .

our prior experience and leading practices show that for major acquisitions like us - visit to be successful , program staff need , among other things , to understand what they are to do , how they relate to each other , and how they fit in their organization .

accordingly , we recommended that dhs do the following: define program office positions , roles , and responsibilities .

the program office has developed charters for its nine component offices that include roles and responsibilities for each .

for example , the acquisition and program management office is responsible , among other things , for establishing acquisition and program management policies ; coordinating development of configuration management plans and project schedules , including the integrated milestone schedule ; and developing policies and procedures for guidance and oversight of systems development and implementation activities .

the program has also defined a set of core competencies ( knowledge , skills , and abilities ) for each position .

for example , it has defined critical competencies for program and management analysts that include , among others , flexibility , interpersonal skills , organizational awareness , oral communication , problem solving , and teamwork .

these efforts to define position , roles , and responsibilities should help in managing the program effectively .

as previously stated , we reported in september 2003 that us - visit had not fully staffed its program office or defined roles and responsibilities for its program staff .

we observed that prior research and evaluations of organizations showed that effective human capital management can help agencies establish and maintain the workforce they need to accomplish their missions .

accordingly , we recommended that dhs do the following: develop and implement a human capital strategy for the program office that provides for staffing positions with individuals who have the appropriate knowledge , skills , and abilities .

in february 2005 , we reported that the program office , in conjunction with the office of personnel management ( opm ) , developed a draft human capital plan that employed widely accepted human capital planning tools and principles .

the draft plan included , for example , an action plan that identified activities , proposed completion dates , and the office ( opm or the program office ) responsible for the action .

we also reported that the program office had completed some of the activities , such as designating a liaison responsible for ensuring alignment between departmental and program human capital policies .

since then , the program office has finalized the human capital plan and completed more activities .

for example , program officials told us that they have analyzed the program office's workforce to determine diversity trends , retirement and attrition rates , and mission - critical and leadership competency gaps ; updated the program's core competency requirements to ensure alignment between the program's human capital and business needs ; developed an orientation program for new employees ; and administered competency assessments to incoming employees .

program officials also told us that they have plans to complete other activities , such as developing a staffing forecast to inform succession planning ; analyzing workforce data to maintain strategic focus on preserving the skills , knowledge , and leadership abilities required for the us - visit program's success ; and developing organizational leadership competency models for the program's senior executive , managerial , and supervisory levels .

in addition , the officials said that several activities in the plan have not been completed , such as assessing the extent of any current employees' competency gaps and developing a competency - based listing of training courses .

these officials said that the reason these activities have not been completed is that they are related to the department's new human capital initiative , maxhr , which is to provide greater flexibility and accountability in the way employees are paid , developed , evaluated , afforded due process , and represented by labor organizations .

maxhr is to include the development of departmentwide competencies .

because of this , the officials told us that it could potentially impact the program's ongoing competency - related activities .

as a result , these officials said that they are coordinating these activities closely with the department as it develops and implements this new initiative , which is currently being reviewed by the dhs deputy secretary for approval .

until us - visit fully implements a comprehensive human capital strategy , it will continue to risk not having staff with the right skills and abilities to successfully execute the program .

we reported in september 2003 that the operational performance of initial system increments was largely dependent on the performance of existing systems that were to be interfaced to create these increments .

for example , we said that the performance of an increment will be constrained by the availability and downtime of the existing systems that it includes .

accordingly , we recommended that dhs do the following: define performance standards for each increment that are measurable and reflect the limitations imposed by relying on existing systems .

in february 2005 ( 17 months later ) , we reported that several technical performance standards for increments 1 and 2b had been defined , but that it was not clear that these standards reflected the limitations imposed by the reliance on existing systems .

since then , for the increment 2c proof of concept ( phase 1 ) , the program office has defined certain other performance standards .

for example , the functional requirements document for increment 2c ( phase 1 ) defines several technical performance standards , including reliability , recoverability , and availability .

for each , the document states that the performance standard is largely dependent on those of increment 2b .

more specifically , the document states that phase 1 system availability is largely dependent upon the individual and collective availability of the current systems .

the document also states that the increment 2c components shall have an aggregated availability greater than or equal to 97.5 percent .

however , the document does not contain sufficient information to determine whether these performance standards actually reflect the limitations imposed by reliance on existing systems .

to further develop performance standards , the program office has prepared a performance engineering plan , dated march 31 , 2005 , that links us - visit performance engineering activities to its system development life cycle .

further , the plan ( 1 ) provides a framework to be used to align its business , application , and infrastructure performance goals and measures ; ( 2 ) describes an approach to translate business goals into operational measures , and then to quantitative metrics ; and ( 3 ) identifies system performance measurement areas ( effectiveness , efficiency , reliability , and availability ) .

according to program officials , they intend to establish a group to develop action plans for implementing the engineering plan , but did not have a time frame for doing so .

without defining performance standards that reflect the limitations of the existing systems upon which us - visit relies , the program lacks the ability to identify and effectively address performance shortfalls .

in september 2003 , we reported that us - visit was a risky undertaking because of several factors inherent to the program , such as its large scope and complexity , as well as because of various program management weaknesses .

we concluded that these risks , if not effectively managed , would likely cause program cost , schedule , and performance problems .

risk management is a continuous , forward - looking process that is intended either to prevent such problems from occurring or to minimize their impact if they occur by proactively identifying risks , implementing risk mitigation strategies , and measuring and disclosing progress in doing so .

because of the importance of effectively managing program risks , we recommended that dhs do the following: develop and implement a risk management plan and ensure that all high risks and their status are reported regularly to the executive body .

about 2 years later , the program office has developed and has begun implementing a risk management plan .

the plan , which was approved in september 2005 , includes , among other things , a process for identifying , analyzing , handling , and monitoring risk .

it also defines the governance structure to be used in overseeing and managing the process .

the program also maintains a risk database , which includes , among other things , a description of the risk , its priority ( eg , high , medium , or low ) , and its mitigation strategy .

according to program officials , the database is currently available to program management and staff .

the program has also begun implementing its risk management plan .

for example , it has established a risk review board , risk review council , and risk owners to govern its risk activities .

the roles and responsibilities are described below .

the risk review board directs all risk governance within the program and provides the mechanism to escalate / transfer the consideration of risks to program governing boards and to organizations external to the program .

the risk review council oversees and manages program - related risks that are significant , controversial , or cross - project or that may require escalation to the risk review board .

risk owners analyze , handle , and monitor risks .

however , full implementation of the risk management plan has yet to occur .

as part of its cmmi process maturity baseline self - assessment ( previously discussed ) , the program office found that the risk management process detailed in its plan was not being consistently applied across the program .

in response , according to program officials , they have developed risk management training and began conducting training sessions in november 2005 .

these officials also stated that the risk review board , where risks are reviewed with program executives , has been meeting monthly since september 2005 .

with respect to regular risk reports to program executives , the plan includes thresholds for escalating risks within the risk governance structure and to dhs governance entities .

for example , risks are to be elevated to the risk review board when the cost of the project exceeds more than 5 percent of the project baseline cost , the schedule slippage exceeds more than 5 percent of the baseline schedule , major areas of scope are affected , or quality reduction requires approval .

however , program officials stated that these thresholds are not currently being applied .

they further stated that although the plan allows for escalation of risks to officials outside the program office , doing so is at the discretion of the program director ; in addition , according to these officials , although high risks are not routinely escalated outside the program , selected high risks have been disclosed to the assistant secretary for policy in weekly program status reports .

as of december 5 , 2005 , the program director proposed submitting monthly reports of high - priority risks and issues through the assistant secretary for policy to the deputy secretary .

until us - visit fully implements its risk management plan and process , it cannot be assured that all program risks are being identified and managed in order to effectively mitigate any negative impact on the program's ability to deliver promised capabilities on time and within budget .

we reported in may 2004 , and again in february 2005 , that system testing was not based on well - defined test plans , and thus the quality of testing being performed was at risk .

the purpose of system testing is to identify and correct system defects ( i.e. , unmet system functional , performance , and interface requirements ) and thereby obtain reasonable assurance that the system performs as specified before it is deployed and operationally used .

to be effective , testing activities should be planned and implemented in a structured and disciplined fashion .

among other things , this includes developing effective test plans to guide the testing activities and ensuring that test plans are developed and approved before test execution .

according to relevant systems development guidance , an effective test plan ( 1 ) specifies the test environment ; ( 2 ) describes each test to be performed , including test controls , inputs , and expected outputs ; ( 3 ) defines the test procedures to be followed in conducting the tests ; and ( 4 ) provides traceability between the test cases and the requirements to be verified by the testing .

because these criteria were not being met , we recommended that dhs do the following: develop and approve test plans before testing begins that ( 1 ) specify the test environment ; ( 2 ) describe each test to be performed , including test controls , inputs , and expected outputs ; ( 3 ) define the test procedures to be followed in conducting the tests ; and ( 4 ) provide traceability between test cases and the requirements to be verified by the testing .

about 19 months later , the quality of the system test plans , and thus system testing , is still problematic .

to the program's credit , the test plans for the increment 2c proof of concept ( phase 1 ) , dated june 28 , 2005 , satisfied part of our recommendation .

specifically , the test plan for this increment was approved on june 30 , 2005 , and , according to program officials , testing began on july 5 , 2005 .

further , the test plan described , for example , the scope , complexity , and completeness of the test environment , and it described the tests to be performed , including a high - level description of controls , inputs , and outputs , and it identified test procedures to be performed .

however , the test plan did not adequately trace between test cases and the requirements to be verified by testing .

for example , 300 of the 438 functional requirements , or about 70 percent of the requirements that we analyzed , did not have specific references to test cases .

in addition , we identified traceability inconsistencies , including the following: one requirement was mapped to over 50 test cases , but none of the 50 cases referenced the requirement .

one requirement was mapped to a group of test cases in the traceability matrix , but several of the test cases to which the requirement was mapped did not reference the requirement , and several test cases referenced the requirement and were not included in the traceability matrix .

one requirement was mapped to all but one of the test cases within a particular group of test cases , but that test case did refer to the requirement .

time and resources were identified as the reasons that test plans have not been complete .

specifically , program officials stated that milestones do not permit existing testing / quality personnel the time required to adequately review testing documents .

according to these officials , even when the start of testing activities is delayed because , for example , requirements definition or product development takes longer than anticipated , testing milestones are not extended .

without complete test plans , the program does not have adequate assurance that the system is being fully tested , and thus unnecessarily assumes the risk that system defects will not be detected and addressed before the system is deployed .

this means that the system may not perform as intended when deployed , and defects will not be addressed until late in the systems development cycle , when they are more difficult and time - consuming to fix .

as we previously reported , this has happened: postdeployment system interface problems surfaced for increment 1 , and manual work - arounds had to be implemented after the system was deployed .

we reported in may 2004 that the program had not assessed its workforce and facility needs for increment 2b .

because of this , we questioned the validity of the program's workforce and facility assumptions used to develop its workforce and facility plans , noting that the program lacked a basis for determining whether its assumptions and thus its plans were adequate .

accordingly , we recommended that dhs do the following: assess the full impact of increment 2b on land poe workforce levels and facilities , including performing appropriate modeling exercises .

seven months later , the program office evaluated increment 2b operational performance .

the purpose of the evaluation was to determine the effectiveness of increment 2b performance at the 50 busiest land poes .

to assist in the evaluation , the program office established a baseline for comparing the average form i - 94 or form i - 94w issuance processing times at 3 of the 50 poes where processing times were to be evaluated .

the program office then conducted two evaluations of the processing times at the 3 poes following increment 2b deployment .

the first was in december 2004 , after increment 2b was deployed to these sites as a pilot , and the second was in february 2005 , after increment 2b was deployed to all 50 poes .

the evaluation results showed that the average processing times decreased for all 3 sites .

table 2 compares the results of the two evaluations and the baseline .

according to program officials , these evaluations supported the workforce and facilities planning assumption that no additional staff were required to support deployment of increment 2b , and that minimal modifications to interior workspace were required to accommodate biometric capture devices and printers and to install electrical circuits .

these officials stated that modifications to existing officer training and interior space were the only changes needed .

however , the scope of the evaluation was too limited to satisfy the evaluation's stated purpose or our recommendation for assessing the full impact of increment 2b .

specifically , program officials stated that the evaluation focused on the time to process form i - 94s and not on operational effectiveness , including workforce impacts and traveler waiting time .

second , the 3 sites were selected , according to program officials , on the basis of a number of factors , including whether the sites already had sufficient staff to support the pilot .

selecting sites on the basis of this factor could affect the results and presupposes that not all poes have the staff needed to support increment 2b .

third , evaluation conditions were not always held constant .

for example , fewer workstations were used to process travelers in establishing the baseline processing times at 2 of the poes — port huron ( 9 versus 14 ) and douglas ( 4 versus 6 ) — than were used during the pilot evaluations .

moreover , cbp officials from 1 poe , which was not an evaluation site , told us that us - visit has actually lengthened processing times .

 ( san ysidro processes the highest volume of travelers of all land poes. ) .

while these officials did not provide specific data to support this statement , it nevertheless raises questions about the potential impact of increment 2b on the 47 sites that were not evaluated .

it is important that the impact of increment 2b on workforce and facilities be fully assessed .

since we made our recommendation , increment 2b deployment and operational facts and circumstances have materially changed , making the implementation of our recommendation using predeployment baseline data for the other 47 sites impractical .

nevertheless , other alternatives , such as surveying officials at these sites to better understand the increment's impact on workforce levels and facilities , have yet to be explored .

until they are , the program may not be able to accurately project resource needs or make required modifications to achieve its goals of minimizing us - visit's impact on poe processing times .

we reported in may 2004 that us - visit had not established effective configuration management practices .

configuration management establishes and maintains the integrity of system components and items ( eg , hardware , software , and documentation ) .

a key ingredient is a change control board to evaluate and approve proposed configuration changes .

accordingly , we concluded that the program did not have adequate assurance that approved system changes were actually made , and that changes made to the component systems ( for non – us - visit purposes ) did not interfere with us - visit functionality .

accordingly , we recommended that dhs do the following: implement effective configuration management practices , including establishing a us - visit change control board to manage and oversee system changes .

after 19 months , us - visit has begun implementing configuration management practices .

to its credit , the program recently issued a configuration management policy ( september 2005 ) and prepared a draft configuration management plan ( august 2005 ) .

the policy contains guiding principles , direction , and expectations for planning and performing configuration management , and includes activities , authorities , and responsibilities .

the draft plan describes the configuration management governance structure , including organizational entities and their responsibilities , the processes and procedures to be applied , and how controls are to be applied to products .

the governance structure includes the executive configuration control board and the configuration management impact review team .

according to its charter , the configuration control board is responsible for determining the status of requested configuration changes and resolving any conflicts related to those changes for us - visit – managed systems ( i.e. , not for us - visit component systems managed by other dhs organizations ) .

the impact review team , which reports to the board , is responsible for reviewing requests for system changes and submitting a recommendation to the appropriate change review authority ( i.e. , either the us - visit control board or the control board in the dhs organization that manages the component system ) .

according to program officials , for us - visit – managed systems , the review authority is the executive configuration control board .

for other systems , such as tecs ( which cbp manages ) , the us - visit review team may submit a recommendation to the appropriate control board ( in this case , the cbp control board ) .

the apmo director stated that the planned configuration management program is intended to complement rather than replace the configuration management programs for the legacy systems .

that is , change requests approved by the us - visit executive configuration control board that require changes to a legacy system will be coordinated with the board having responsibility for that system .

this means , however , that changes to component systems ( eg , ident , adis , and tecs ) that are initiated and approved by another dhs organization , and that could affect us - visit performance , are not subject to us - visit configuration management processes and are not also being examined and approved by the us - visit control board .

this lack of us - visit control was the impetus for our recommendation .

although us - visit has recently taken steps to begin addressing our recommendation , the program still does not adequately control changes to the component systems upon which us - visit performance depends .

until programwide configuration management practices are implemented , the program does not have an effective means for ensuring that approved system changes are actually made and that changes made to the component systems for non – us - visit purposes do not compromise us - visit functionality and performance .

we reported in may 2004 that the program office's independent verification and validation ( iv&v ) contractor was not independent of the products and processes that it was verifying and validating .

the purpose of iv&v is to provide management with objective insight into the program's processes and associated work products .

its use is a recognized best practice for large and complex system development and acquisition projects like us - visit .

to be effective , the verification and validation function is to be performed by an entity that is independent of the processes and products that are being reviewed .

accordingly , we recommended that dhs do the following: ensure the independence of the iv&v contractor .

in july 2005 , the program office issued a new contract for iv&v services .

to ensure the contactor's independence , the program office ( 1 ) required that iv&v contract bidders be independent of the development and integration contractors ; ( 2 ) reviewed each of the bidder's affiliations with the prime contract ; ( 3 ) included provisions in the contract that prohibit the contractor from soliciting , proposing , or being awarded work ( other than iv&v services ) for the program ; ( 4 ) required all contractor personnel to certify that they do not have any conflicts of interest ; and ( 5 ) ensured that the contractor's management plan ( oct. 17 , 2005 ) describes how the contractor will ensure technical , managerial , and financial independence .

such steps , if effectively enforced , should adequately ensure that verification and validation activities are performed in an objective manner and , thus , should provide valuable assistance to program managers and decision makers .

we reported in may 2004 that us - visit's overall progress on implementing our recommendations had been slow , and considerable work remained to fully address them .

as we also noted , given that most of our recommendations focused on fundamental limitations in us - visit's ability to manage the program , it was important to implement the recommendations quickly and completely .

accordingly , we recommended that dhs do the following: develop a plan , including explicit tasks and milestones , for implementing all of our open recommendations and periodically report to the dhs secretary and under secretary on progress in implementing this plan ; and report this progress , including reasons for delays , in all future expenditure plans .

about 19 months after our recommendation , the program assigned responsibility to specific individuals for preparing a plan , including specific actions and milestones , to address each recommendation .

in addition , it developed a report that identifies the responsible person for each recommendation and summarizes progress made in implementing each .

the program office provided this report for the first time to the dhs deputy secretary on october 3 , 2005 , and plans to forward subsequent reports every 6 months .

however , the report's description of progress on 4 recommendations is inconsistent with our assessment , as discussed below: first , the report states that the program completed a privacy impact assessment that is in full compliance with omb guidance .

as previously discussed , an assessment has been developed , but omb guidance requires that these assessments for systems under development ( such as increment 2c ) address privacy in the system's documentation .

increment 2c systems documentation does not address privacy and therefore is not fully compliant with omb guidance .

second , the report states that a human capital strategy has been completed .

however , as previously discussed , several of the activities in the human capital plan have yet to be implemented .

for example , the program has not developed a staffing forecast to inform succession planning .

third , the report states that the impact of increment 2b on land poe workforce levels and facilities has been fully assessed .

however , as we previously stated , the scope of the evaluations was not sufficient to satisfy our recommendation .

for example , program officials stated that the evaluation focused on the time to process form i - 94s and not on operational effectiveness , including workforce impacts and traveler waiting time .

moreover , officials at the largest land poe told us that the effect of increment 2b was the opposite of that reported in the pilot results .

fourth , the report states that the program has partially completed implementing configuration management practices .

however , as previously discussed , the program office has yet to implement practices or establish a configuration control board with authority over all changes affecting us - visit functionality and performance , including those made to component systems for non – us - visit purposes , which was the intent of our recommendation .

in addition , the report does not specifically describe progress against 11 of our other recommendations , so that we could not determine whether the program's assessment is consistent with ours ( described in this report ) .

for example , we recommended that the program reassess plans for deploying an exit capability to ensure that the scope of the exit pilot provides for adequate evaluation of alternative solutions .

the report states that the program office has completed exit testing and has forwarded the exit evaluation report to the deputy secretary for a decision .

however , it does not state whether the program office had expanded the scope or time frames of the pilot .

fully understanding and disclosing progress against our recommendations are essential to building the capability needed to effectively manage the program , and to ensuring that key decision makers have the information needed to make well - informed choices among competing investment options .

we reported in february 2005 that us - visit had not followed effective practices to develop cost estimates for its system increments , and thus the reliability of its cost estimates was questionable .

such cost - estimating practices are embedded in the 13 criteria in sei's checklist for determining the reliability of cost estimates .

of these 13 criteria , we reported in february 2005 that the program's cost estimate met 2 , partially met 6 , and did not meet 5 .

accordingly , we recommended that dhs do the following: follow effective practices for estimating the costs of future increments .

the latest us - visit – related cost estimate is for increment 1b .

this estimate is in the june 2005 cost - benefit analysis for increment 1b and establishes the costs associated with three exit solutions for air and sea poes .

as was the case for the estimate described in our february 2005 report , this latest estimate also did not meet all 13 criteria , meeting 3 and partially meeting another 5 .

for example , these estimates did not include a detailed work breakdown structure and omitted important cost elements , such as system testing .

a work breakdown structure serves to organize and define the work to be performed , so that associated costs can be identified and estimated .

thus , it provides a reliable basis for ensuring that the estimates include all relevant costs .

in addition , the uncertainties associated with the increment 1b cost estimate were not identified .

an uncertainty analysis provides the basis for adjusting these estimates to reflect unknown facts and circumstances that could affect costs and identifies the risk associated with the cost estimate .

table 3 summarizes our analysis of the extent to which us - visit's increment 1b cost estimates satisfy sei's 13 criteria .

program officials stated that they recognize the importance of developing reliable cost estimates and have initiated actions to more reliably estimate the costs of future increments .

for example , as part of its process improvement program , the program has chartered a cost - analysis process action team , which is to develop , document , and implement a cost - analysis policy , process , and plan for the program .

program officials also stated that they have hired additional contracting staff with cost - estimating experience .

strengthening the program's cost - estimating capability is extremely important .

the absence of reliable cost estimates , among other things , prevents the development of reliable economic justification for program decisions and impedes effective performance measurement .

in february 2005 , we reported that us - visit had not adequately planned for evaluating the increment 1b exit alternative because its exit pilot evaluation's scope and timeline were compressed .

accordingly , we recommended that dhs do the following: reassess plans for deploying an exit capability to ensure that the scope of the exit pilot provides for adequate evaluation of alternative solutions and better ensures that the exit solution selected is in the best interest of the program .

over the last 10 months , the program office has taken actions to expand the scope and time frames of the pilot .

for example , it extended the pilot from 5 to 11 poes — 9 airports and 2 seaports .

it also extended the time frame for data collection and evaluation to april 2005 , which is about 7 months beyond the date for which all exit pilot evaluation tasks were to be completed .

further , according to program officials , they achieved the target sample sizes necessary to have a 95 percent confidence level .

notwithstanding the expanded scope of the pilot , questions remain about whether the exit alternatives have been evaluated sufficiently to permit selection of the best exit solution for national deployment .

for example , each of the three exit alternatives was evaluated against three criteria , including compliance with the us - visit exit process ( i.e. , foreign travelers providing information as they exit the united states ) .

however , across the three alternatives , the average compliance with this process was only 24 percent , which raises questions as to the effectiveness of the three alternatives .

the evaluation report cites several reasons for the low compliance rate , including that compliance during the pilot was voluntary .

the report further concludes that national deployment of the exit solution will not have the desired compliance rate unless the exit process incorporates an enforcement mechanism , such as not allowing persons to reenter the united states if they do not comply with the exit process .

although an enforcement mechanism might indeed improve compliance , program officials stated that no formal evaluation has been conducted of enforcement mechanisms or their effect on compliance .

the program director stated that he agrees that additional evaluation is needed to assess the impact of implementing potential enforcement mechanisms and plans to do so .

until the program office adequately evaluates the exit alternatives and knows whether the alternative to be selected will be effective , the program office will not be in a position to select the exit solution that is in the best interest of the program .

this is very important because without an effective exit capability , the benefits and the mission value of us - visit are greatly diminished .

we reported in february 2005 that the overall capacity of the system was not being effectively managed .

at that time , us - visit , which comprises several legacy systems , was relying on the capacity management activities of these systems .

it was not focused on the capacity requirements and performance of the collective systems that make up us - visit .

this approach increases the risk that the system may not be properly designed and configured for efficient performance , and that it has insufficient processing and storage capacity for current , future , and unpredictable workload requirements .

accordingly , we recommended that dhs do the following: develop and implement processes for managing the capacity of the us - visit system .

according to program officials , they have initiated efforts to develop a capacity management process , including a high - level description of the necessary steps , such as identifying tools needed to implement the process .

however , a plan , including specific tasks and milestones for developing and implementing capacity management processes , has not yet been developed .

until the program office develops a programwide capacity management program , it increases the risk that us - visit may not be able to adequately support program mission needs .

we reported in february 2005 that the program office recognized that us - visit and the automated commercial environment ( ace ) have related missions and operational environments .

in addition , us - visit and ace could potentially develop , deploy , and use common information technology infrastructures and services .

we also reported that managing this relationship has not been a priority .

accordingly , we recommended that dhs do the following: make understanding the relationships and dependencies between the us - visit and ace programs a priority matter , and report periodically to the under secretary on progress in doing so .

us - visit and ace managers met in february 2004 , to identify potential areas for collaboration between the two programs and to clarify how the programs could best support the dhs mission and provide officers with the information and tools they need .

according to program officials , they have established a us - visit / ace integrated project team to , among other things , ensure that the two programs are programmatically and technically aligned .

the team has discussed potential areas of focus and agreed to three areas: rf technology , program control , and data governance .

however , it does not have an approved charter , and it has not developed explicit plans or milestone dates for identifying the dependencies and relationships between the two programs .

program officials stated that the team has met three times and plans to meet on a quarterly basis going forward .

it is important that the relationships and dependencies between these two programs be managed effectively .

the longer it takes for the programs to understand and exploit their relationships , the more rework will be needed at a later date to do so .

over the last 3 years , we have made recommendations aimed at correcting fundamental limitations in us - visit's program management ability and thereby better ensuring the delivery of mission capability and value on time and commensurate with costs .

while progress on the implementation of the recommendations is mixed , progress in critical areas has been slow .

as with any program , introducing and institutionalizing the program management and accountability discipline at which our recommendations are aimed require investing time and resources while continuing to meet other program demands .

in making such investment choices , it is important to remember that institutionalizing such program discipline in the near term will produce long - term payback in a program's ability to meet these other demands .

accordingly , the longer that us - visit takes to implement our recommendations , the greater the risk that the program will not meet its stated goals and commitments .

our open recommendations are all aimed at strengthening us - visit program management and improving dhs's ability to make informed us - visit investment decisions .

with the exception of one , these recommendations are still relevant and applicable .

since we made our recommendation , facts and circumstances surrounding increment 2b deployment and operational status have materially changed , making the collection of increment 2b predeployment impractical .

nevertheless , the need remains to better understand the impact of us - visit entry capabilities on all land poes .

until this understanding exists , the department will be challenged in its ability to accurately estimate and provide facilities and staff resource needs .

to recognize both the need to fully assess the impact of us - visit entry capabilities on staffing levels and facilities at land poes , as well as the current operational status of increment 2b , we are closing our existing recommendation related to assessing the impact of increment 2b .

we recommend that the dhs secretary direct the us - visit program director to explore alternative means of obtaining an understanding of the full impact of us - visit at all land poes , including its impact on workforce levels and facilities ; these alternatives should include surveying the sites that were not part of the previous assessment .

in its written comments on a draft of this report , signed by the director , departmental gao / oig liaison office , and reprinted in appendix ii , dhs stated that it agreed with many areas of the report and that our recommendations had made us - visit a stronger program .

further , the department stated that while it disagreed with certain areas of the report , it nevertheless concurred with the need to implement our open recommendations with all due speed and diligence .

dhs commented specifically on 11 of the 18 recommendations discussed in the report .

the recommendations , the department's comments , and our responses follow: 1 .

recommendation: develop and begin implementing a system security plan , and perform a privacy impact assessment and use the results of the analysis in near - term and subsequent system acquisition decision making .

dhs stated that this recommendation has been fully implemented .

in support , it said that it has completed a us - visit security plan that is consistent with national institute of standards and technology ( nist ) guidance , and that it provided the plan to us in september 2004 .

it also stated that the security risk assessment aspect of this recommendation was established in february 2005 , 20 months after we made the recommendation , and thus the age of the recommendation should be shown as 10 months rather that the 30 months cited in the report .

the department also commented that there is no us - visit system , but rather a us - visit program with capabilities delivered by existing interconnected systems .

according to the department , these component systems have been certified and accredited , consistent with nist guidance , and as part of their certification and accreditation , security plans and risk assessments , as well as risk mitigation strategies , have been developed for each system .

the department stated that it provided us with these system - level risk assessments , as well as system - specific action plans and milestones for implementing the mitigation strategies .

in addition , the department noted that it completed a programwide risk assessment in december 2005 that specifically addresses information security issues that might not be captured in the system - specific documentation used to certify and accredit each system .

in light of its system - specific certification and accreditation efforts , existing system - level risk assessments , and the program - level risk management process ( see response 4 for discussion of the risk management process ) , dhs commented that it is inaccurate to state that us - visit officials are not in a position to know program risks , and the recommendation should be closed .

while we agree that we received a copy of the us - visit security plan , dated september 2004 , we do not agree that the plan satisfied all relevant federal guidance and that dhs has fully implemented our recommendation .

in particular , it has not provided us with evidence that a programwide risk assessment has been done and that a security plan reflective of such an assessment exists .

according to relevant guidance , a security plan should describe , among other things , the methodology that is to be used to identify system threats and vulnerabilities and to assess risks , and it should include the date the risk assessment was completed because the assessment is a necessary driver of the security controls described in the plan .

as we reported in february 2005 and state in this report , the us - visit security plan did not include this information ; further , although dhs stated in its comments that it completed this risk assessment in december 2005 , this statement is contradicted by a statement elsewhere in its comments that it is still in the process of doing the assessment .

in addition to this contradiction , dhs's comments did not include any evidence to demonstrate that it has developed a complete risk assessment , such as a copy of the assessment .

with regard to the age of the recommendation , we do not agree with dhs's position that we established a new finding regarding the lack of a programwide risk assessment in our february 2005 report .

rather , as part of our analysis of actions to implement our prior recommendation to develop a security plan , which is to include information about the related security risk assessment , we observed that the plan did not indicate a date for completing a risk assessment in accordance with federal guidelines .

therefore , our position that about 30 months had passed from the time of our initial recommendation ( june 2003 ) is accurate .

with regard to the individual system - level risk assessments , we agree that we have received them .

however , we do not agree that we have received the action plans and milestones cited in the comments .

regardless , we do not believe that system - level assessments are a sufficient substitute for a programwide assessment .

accordingly , our recommendation focused on the need for an integrated us - visit system risk assessment as part of security planning .

while the system - level plans and risk assessments are relevant and useful , they neither individually nor collectively address the threats and vulnerabilities imposed as a result of these systems' integration .

by stating in its comments its commitment to having a programwide risk assessment that identifies and proposes mitigations for security risks that arise as a result of the interface and integration of the legacy systems , dhs is agreeing with our position .

moreover , without evidence that the program has completely assessed its risks , we continue to find no basis for how program officials would know the full range and degree of us - visit security risks .

our position in this regard has been reinforced by a recent dhs inspector general report that identified a number of us - visit security risks .

to further support its position that this recommendation has been fully implemented , dhs also commented that it has completed numerous privacy impact assessments and continues to update them to reflect system changes .

in particular , it said that it updated the privacy impact assessment in december 2005 to reflect all increments and that it considers the assessment to be part of us - visit system documentation .

it further commented that we appear to be unaware of privacy staff activities to review system documents and perform privacy risk assessments throughout the system life cycle .

nevertheless , the department acknowledged that its privacy work was not always noted within us - visit system documentation .

accordingly , dhs stated that it plans to appropriately reference all privacy requirements and privacy risk assessments in the program's system documentation in the future .

we agree that us - visit has developed and updated its privacy impact assessment and would note that our report states this fact .

we do not agree , however , with the comment that we are not aware that the privacy staff review system documents and perform privacy risk assessments .

in fact , it is because we were aware of these facts that we were careful to ensure that they were reflected in our report .

the point that we are making is that privacy is not addressed in all relevant systems documentation , which dhs acknowledged in its comments .

with regard to this point of agreement , we support the department's stated plans to reference all privacy requirements and any privacy risk assessments in all relevant system documentation in the future .

2 .

recommendation: develop and implement a plan for satisfying key acquisition management controls , including acquisition planning , solicitation , requirements management , program management , contract tracking and oversight , evaluation , and transition to support , and implement the controls in accordance with sei guidance .

dhs commented that the report should reflect that us - visit had initially adopted carnegie mellon university's software engineering institute ( sei ) software acquisition capability maturity model® to guide its software - related process improvement efforts and that , in december 2004 , it transitioned to sei's capability maturity model – integration ( cmmi® ) .

as a result , it said that the program's process improvement strategy and plans , process development , and process appraisals are now aligned to the most applicable cmmi process areas .

we agree that us - visit has transitioned to cmmi .

we state in our report that us - visit has done so and that the key process areas it is addressing in its process improvement strategy and plan are consistent with those cited in our recommendation .

we do not believe that this transition materially affects our recommendation , however , because even though the names of the key processes in these two models may in some cases differ , the processes and respective practices are fundamentally consistent .

3 .

recommendation: clarify the operational context in which us - visit is to operate .

consistent with our report , dhs commented that the operational context in which us - visit operates is in progress , meaning that it has yet to be fully established .

for example , it said that the mission of dhs , and therefore the scope of us - visit activities to meet the mission , is continually expanding .

further , it acknowledged that more certainty in the operational context is desirable .

in mitigation of the risks associated with not having a more stable operational context , dhs made several statements .

for example , it said that the principal role of us - visit is to integrate information and immigration and border management systems across dhs and the state department , and to facilitate agencies working toward a common environment that will eliminate redundancies .

it also said that elements of its draft immigration and border management strategic plan are being used in current us - visit operations .

in addition , the department said that mechanisms to mitigate the risks that we cited have been developed and are being implemented .

we support dhs's acknowledgment of the importance of having a well - defined operational context within which to define and implement us - visit and related border security programs .

however , we do not believe that dhs's comments provided any evidence showing that sufficient steps and activities to mitigate the associated risks have been taken or are planned .

4 .

recommendation: determine whether proposed us - visit increments will produce mission value commensurate with cost and risks and disclose to the congress planned actions .

dhs commented that its cost - benefit analysis ( cba ) for increment 1b conforms to relevant federal guidance , and noted that our expectations as to the scope and level of detail of analysis that should be included in the cba document are inconsistent with its understanding of omb circular a - 94 and dhs's cba workbook , which were used to guide the development of the cba analysis .

as an example , the department took exception with our statement that year - by - year benefit estimates were not reported by noting that the net present value was based on an estimate of annual benefits and costs , and that net present value could not be estimated without a year - by - year benefit analysis .

the department further commented that a comprehensive uncertainty analysis was conducted because it completed a risk analysis , which is more comprehensive , rigorous , and appropriate than conducting a sensitivity analysis .

in this regard , it added that the results of the risk analysis provided an indication of increment 1b's worthiness in light of existing uncertainty , rather than information on a specific cba variable or another .

the department further noted that it had provided some of these supporting analyses to us .

dhs also stated that any investment that has a 5-year life cycle and is considered interim in nature will face considerable challenge in providing economic benefits commensurate with cost .

we do not agree that the cba fully conforms to relevant federal guidance .

as our report states , for example , the analysis does not explicitly state the numerical value of the discount rate used for calculating each alternative's net present value , and hence does not conform to omb guidance .

in addition , the cost estimates used in the analysis were not complete and reliably derived .

in deriving the estimate , for example , the department did not clearly define the project's life cycle to ensure that key factors were not overlooked and that the full cost of the program was included .

 ( see response 10 below for more information on this point. ) .

last , while we agree that a year - by - year benefit analysis is a necessary component of a net present value determination , omb nevertheless requires that the year - by - year benefit estimates be reported in the analysis to promote independent review of the estimates .

also , we do not agree that dhs performed a complete uncertainty analysis .

according to omb and dhs guidance , a complete uncertainty analysis should include both a risk analysis and a sensitivity analysis .

however , the latter was not done .

thus , our point is not , as dhs comments suggest , that us - visit should have performed a sensitivity analysis instead of a risk analysis , but rather , that both types of analyses are necessary to completely examine investment uncertainty .

5 .

recommendation: develop and implement a risk management plan and ensure that all high risks and their status are reported regularly to the executive body .

dhs commented that us - visit began the development and implementation of its risk management plan in 2004 immediately after we made our recommendation .

it further commented that , as part of a cmmi maturity internal appraisal that it completed in july 2005 , it found that the risk management process had not been consistently applied across the program .

to address this , the department cited actions that it has taken to fully implement risk management , such as approving the risk management plan in september 2005 ; defining a risk governance structure ; establishing and maintaining a risk database ; and developing risk management training and providing this training to program personnel and contractors beginning in november 2005 .

we support the recent actions that the program cited as having been taken to strengthen risk management .

however , the actions cited do not demonstrate that the risk management process is being consistently applied .

until us - visit fully implements its risk management plan and process , it cannot be assured that all program risks are being identified and managed in order to effectively mitigate any negative impact on the program's ability to deliver promised capabilities on time and within budget .

6 .

recommendation: develop and approve test plans before testing begins that ( 1 ) specify the test environment ; ( 2 ) describe each test to be performed , including test controls , inputs , and expected outputs ; ( 3 ) define the test procedures to be followed in conducting the tests ; and ( 4 ) provide traceability between test cases and the requirements to be verified by the testing .

dhs stated that our report does not accurately reflect the status of the increment 2c phase 1 testing .

in particular , it said that the issues associated with the traceability of requirements to test cases were minor and that the extent of the discrepancies is far less than what our report presents .

it further stated that the discrepancies in our report are based on old traceability documentation and do not reflect revised documentation provided to us on november 9 , 2005 .

we agree that dhs provided us with revised traceability matrixes after we had shared with them our analysis of the test plans and traceability matrixes , dated june 28 , 2005 , and june 27 , 2005 , respectively .

however , the revised documentation referenced in dhs's comments was provided in november 2005 , about 4 months after testing began .

this means that the test plans and traceability matrixes available at the time of testing — which are what we reviewed because they governed the scope and nature of actual testing performed — did not adequately trace between test cases and the requirements to be verified .

specifically , 300 of the 438 increment 2c requirements , or about 70 percent , did not have specific references to test cases .

7 .

recommendation: implement effective configuration management practices , including establishing a us - visit change control board to manage and oversee system changes .

dhs commented that a us - visit representative attends all configuration control board meetings for all applicable legacy component systems , and that any proposed change request from a legacy component control board that could affect us - visit functionality is brought to the attention of the us - visit executive configuration control board for consideration .

we do not question these statements .

however , we do not believe that they demonstrate that us - visit has adequate control over system changes that could affect the program .

that is , they do not ensure that changes to the component systems that are initiated and approved by another dhs organization and that could affect us - visit performance are subject to us - visit configuration management and approval processes .

us - visit could establish explicit and enforceable control over changes to the legacy systems through such mechanisms as defined and enforced memorandums of understanding among the affected dhs organizations .

it was the lack of such control that prompted our recommendation .

8 .

recommendation: assess the full impact of increment 2b on land poe workforce levels and facilities , including performing appropriate modeling exercises .

the department stated that , given the imperative to meet the legislatively mandated time frames , the scope of increment 2b was limited to only one part of poe operations — incorporating the collection of a biometric into the previously manual form i - 94 issuance process .

it also stated that wait times are affected by various factors , including traffic volume , staffing levels , and availability of officers .

therefore , dhs focused the increment 2b evaluation on just the change to this process .

the department further commented that given the events since the evaluation — namely , increment 2b full operations — it is not practical to collect and model baseline data for the 47 sites that were not part of the initial evaluation .

regarding the 3 pilot sites included in the assessment , the department stated that the sites were selected based on criteria developed from input from us - visit , as well as cbp operational constraints .

the department further commented that the 3 sites provided a reasonable mix of travelers and they did not have other constraints that directly impacted the collection of performance data specific to form i - 94 issuance .

dhs also stated that the i - 94 processing times vary by poe , and therefore they are not easily generalized from one port to another .

further , the department commented that the number of workstations and officers available to operate those workstations to process applicants for a form i - 94 do not impact the time it takes to issue a form i - 94 .

we agree that the scope of the increment 2b evaluation was limited to the i - 94 issuance process , and that it did not address the increment's impact on the poes' ability to meet other performance parameters .

our point is that the limited nature of the evaluation does not satisfy either the intent of our recommendation or dhs's own stated purpose for the evaluation , which was to determine the effectiveness of increment 2b performance at the 50 busiest land poes .

we also agree that the i - 94 processing times vary by poe and cannot be easily generalized .

it is for this reason , among others , that we questioned whether the 3 sites selected for the assessment were sufficiently representative to satisfy both our recommendation and the evaluation's stated purpose .

in addition , while we also agree that collecting pre - increment 2b baseline data is not practical at this time , the fact remains that the operational impact of increment 2b on workforce levels and facilities has not been adequately assessed , as evidenced by officials at 1 large poe telling us that processing times have increased and dhs's recognition that each poe is somewhat different .

in light of these new facts and circumstances , we are closing our existing recommendation and making a new recommendation to recognize the need for dhs to explore alternative means to assess the impact of us - visit entry capabilities at land poes .

this new recommendation will be shown as an open recommendation , and the original recommendation will be closed .

9 .

recommendation: develop a plan , including explicit tasks and milestones , for implementing all of our open recommendations and periodically report to the dhs secretary and under secretary on progress in implementing this plan ; and report this progress , including reasons for delays , in all future expenditure plans .

dhs stated that it is untrue that 19 months had elapsed from the time we made this recommendation to the time that it assigned responsibilities to program officials for addressing each of our recommendations .

in support , it commented that it issued its first plan to address our recommendations on august 18 , 2003 , and subsequent reports have been issued periodically that update progress in doing so .

we agree that dhs has assigned responsibilities to specific individuals for addressing each recommendation .

however , we have yet to be provided any evidence to support its statement that it issued the first report addressing our recommendations on august 18 , 2003 .

similarly , we have not received evidence showing that it has prepared a plan , including specific actions and milestones , for implementing all of our open recommendations , which is a focus of this recommendation .

we would also observe that we made this recommendation in may 2004 , and at that time the department stated that it agreed with the recommendation but did not indicate that it had taken any steps to address it , such as commenting that a report was issued on august 18 , 2003 .

10 .

recommendation: follow effective practices for estimating the costs of future increments .

dhs either tacitly or explicitly agreed with our findings relative to its satisfaction of 8 of the 13 cost - estimating criteria presented in table 4 ( now table 3 ) of our draft report .

for example , it agreed that it did not clearly define the life cycle to which the cost estimate applies .

it also agreed that it did not include a work breakdown structure , noting that it used the available project implementation schedule as a proxy for the activities related to the deployment of the exit alternatives .

regarding our five findings concerning its satisfaction of cost - estimating with which dhs disagreed , the department's primary area of disagreement was with the intended purpose of the increment 1b cba that used the cost estimate , which it said in its comments was to inform decision makers about the relative worthiness of each of the three exit alternatives considered for deployment .

hence , dhs stated that the purpose of the cba was to analyze only the costs associated with deploying an operational solution , not to analyze the costs and benefits of both developing and deploying alternative solutions .

dhs further stated that the cba thus includes only those costs to be incurred in deploying a selected alternative , and it does not include costs already incurred in developing system alternatives ( i.e. , sunk costs ) .

it further commented that dhs guidance states that sunk costs are not relevant to the current investment analysis because “only current decisions can affect the future consequences of investment alternatives.” dhs also disagreed that the cost estimate in the cba should have included nonrecurring development costs , and commented that it did appropriately size the task described in the cost estimates for each alternative exit solution , noting that sizing metrics related to software development were not relevant to deployment of the alternatives because development activities had already occurred and therefore are sunk costs .

the department added that those sizing metrics that are relevant to the cost estimate are discussed in the cba , as are the cost estimating parameters ( i.e. , those associated with deployment and not those associated with development and testing ) .

in addition , dhs disagreed that dhs's cost estimate excluded important cost categories , such as system testing , and stated that the estimate addresses labor , facilities , operations and maintenance , information technology , travel , and training costs .

once again , dhs emphasized that since the focus of the cba was on operational deployment and not system design and development , system testing costs were not included because they were not considered relevant .

dhs also reiterated its early point that the uncertainty analysis that it conducted was comprehensive .

we agree that actual sunk costs should not be included in a cba cost estimate .

however , we disagree that the cost categories that dhs cited as not relevant are only costs that are associated with predeployment activities .

testing , for example , is an activity that is normally performed before , during , and following deployment , and thus the associated costs would be relevant to the stated purpose of the increment 1b cba .

however , a testing cost category was missing from the cba cost estimate , as was a cost category for software maintenance .

regarding dhs's statement that it conducted a complete uncertainty analysis , we reiterate our previous point that a complete uncertainty analysis should include both a risk analysis and a sensitivity analysis , and the cba did not include the latter .

11 .

recommendation: reassess plans for deploying an exit capability to ensure that the scope of the exit pilot provides for adequate evaluation of alternative solutions and better ensures that the exit solution selected is in the best interest of the program .

concerning the questions we raised about the adequacy of the exit pilots in light of the 24 percent compliance rate , dhs commented that we failed to consider the compliance rate of the previous exit pilot program , the national security entry exit registration system ( nseers ) , which , according to dhs , had a 75 percent compliance rate .

dhs added that nseers achieved this compliance rate with a very limited number of exit locations , and therefore , any of the three us - visit exit alternatives would have at least a 75 percent compliance rate once national deployment was completed .

further , the department commented that immigration and customs enforcement ( ice ) had recently conducted enforcement operations at the denver international airport , and that the compliance rate during these operations increased from 30 percent to over 90 percent .

it then concluded that the combined results of the exit pilot evaluation , the nseers pilot , and the ice enforcement activities at the denver international airport lead it to believe that the us - visit exit alternatives have been adequately evaluated .

we do not agree with this conclusion because it is based on unsupported assumptions .

specifically , dhs did not provide any evidence to support its claim that that us - visit would achieve a comparable compliance rate to the nseers program .

moreover , even if dhs could achieve a 75 percent compliance rate for us - visit exit,that still means that 25 percent of eligible persons would not be complying with the us - visit exit process .

further , dhs did not provide any information about the recent enforcement actions conducted by ice , nor did it provide any evidence that this is a practical and viable option for the us - visit exit solution .

while we agree that enforcement actions may indeed increase the exit compliance rate , dhs has not yet assessed the impact of such a solution on the us - visit exit process .

further , the us - visit program director acknowledged the need to evaluate the impact of implementing potential enforcement actions on us - visit exit and planned to do so .

we are sending copies of this report to the chairmen and ranking minority members of the senate and house appropriations committees , as well as to the chairmen and ranking minority members of other senate and house committees that have authorization and oversight responsibilities for homeland security .

we are also sending copies to the secretary of homeland security , secretary of state , and the director of omb .

copies of this report will also be available at no charge on our web site at www.gao.gov .

should you or your offices have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iv .

our objective was to determine the progress of the department of homeland security ( dhs ) in implementing 18 of our recommendations pertaining to the u.s .

visitor and immigrant status indicator technology ( us - visit ) program .

to accomplish this objective , we reviewed and analyzed us - visit's most recent status reports on the implementation of our open recommendations and related key documents , augmented as appropriate by interviews with program officials .

more specifically , we analyzed relevant systems acquisition documentation , including the program's process improvement plan , risk management plan , and configuration management plan .

we also analyzed the us - visit security plan , privacy impact assessment , cost - benefit analysis , cost estimates , test plans , human capital plans , and related evaluations and assessments .

in performing our analyses , we compared available documentation and program officials' statements with relevant federal guidance and associated best practices .

a more detailed description of our scope and methodology relative to the cost - benefit analysis , cost estimates , and test plans follows: our analysis of the cost - benefit analysis focused on increment 1b because this was the latest cost - benefit analysis and cost estimate prepared .

in doing this analysis , we compared the us - visit cost - benefit analysis to eight criteria in office of management and budget ( omb ) guidance .

our analysis of the cost estimate also focused on increment 1b for the same reason previously cited .

in doing this analysis , we compared the estimate to 13 criteria from the software engineering institute that we have previously reported to be the minimum set of actions needed to develop a reliable cost estimate .

we then determined whether the criteria were satisfied , partially satisfied , or not satisfied using the definitions given below .

our analysis of the test plans focused on increment 2c because it is the most recently tested increment .

this analysis included determining the extent to which the test plans for this increment met 4 key criteria that we have previously reported as essential to effective test plans .

in doing this analysis , we examined increment 2c systems documentation , including business and functional requirements and traceability matrixes .

we also independently traced 58 business requirements and 438 functional requirements to the test cases in the test plan .

further , we independently traced all test cases to the requirements to determine consistency .

in performing our work , we used the following categories and definitions in deciding the extent to which each recommendation had been implemented .

specifically , we considered a recommendation completely implemented when documentation demonstrated that it had partially implemented when documentation indicated that actions were under way to implement it , and in progress when documentation indicated that action had been initiated to implement it .

these categories and definitions are consistent with those used in our prior us - visit reports .

in determining the amount of time it has taken to implement actions on our recommendations , we calculated the time from the date the report was issued through december 2005 .

we conducted our audit work at the us - visit program office in rosslyn , virginia , from august 2005 through december 2005 , in accordance with generally accepted government auditing standards .

us - visit involves complex processes governing the stages of a traveler's visit to the united states ( pre - entry , entry , status , and exit ) and analysis of hundreds of millions of foreign national travelers at over 300 air , sea , and land ports of entry ( poe ) .

a simplified depiction of these processes is shown in figure 4 .

pre - entry processing begins with initial petitions for visas , grants of visa status , or the issuance of travel documentation .

when a foreign national applies for a visa at a u.s. consulate , biographic and biometric data are collected and shared with border management agencies .

the biometric data are transmitted from the department of state to dhs , where the prints are run against the automated biometric identification system ( ident ) database to verify identity and to run a check against the biometric watch list .

the results of the biometric check are transmitted back to state .

a “hit” response prevents state's system from printing a visa for the applicant until the information is reviewed and cleared by a consular officer .

pre - entry also includes transmission by commercial air and sea carriers of crew and passenger manifests to appropriate immigration officers before these carriers arrive in the united states .

these manifests are transmitted through the advanced passenger information system ( apis ) .

the apis lists are run against the biographic lookout system to identify those arrivals for whom biometric data are available .

in addition , poes review the apis list in order to identify foreign nationals who need to be scrutinized more closely .

when a foreign national arrives at a poe's primary ( air and sea ) or secondary ( land ) inspection booth , the inspector , using a document reader , scans the machine - readable travel documents .

apis returns any existing records on the foreign national to the us - visit workstation screen , including manifest data matches and biographic lookout hits .

when a match is found in the manifest data , the foreign national's name is highlighted and outlined on the manifest data portion of the screen .

biographic information , such as name and date of birth , is displayed on the bottom half of the computer screen , along with a photograph obtained from state's consular consolidated database .

the inspector at the booth scans the foreign national's fingerprints ( left and right index fingers ) and takes a digital photograph .

this information is forwarded to the ident database , where it is checked against stored fingerprints in the ident lookout database .

if the foreign national's fingerprints are already in ident , the system performs a match ( a comparison of the fingerprint taken during the primary inspection to the one on file ) to confirm that the person submitting the fingerprints is the person on file .

if no prints are currently in ident , the foreign national is enrolled in us - visit ( i.e. , biographic and biometric data are entered into ident ) .

during this process , the inspector also questions the foreign national about the purpose of his or her travel and length of stay .

the inspector adds the class of admission and duration of stay information into the treasury enforcement communications systems , and stamps the “admit until” date on the form i - 94 .

if the foreign national is ultimately determined to be inadmissible , the person is detained , lookouts are posted in the databases , and appropriate actions are taken .

the status management process manages the foreign national's temporary presence in the united states , including the adjudication of benefits applications and investigations into possible violations of immigration regulations .

as part of this process , commercial air and sea carriers transmit departure manifests electronically for each departing passenger .

these manifests are transmitted through apis and shared with the arrival departure information system ( adis ) .

adis matches entry and exit manifest data ( i.e. , each record showing a foreign national entering the united states is matched with a record showing the foreign national exiting the united states ) .

adis also receives status information from the computer linked application information management system and the student exchange visitor information system on foreign nationals .

the exit process includes the carriers' submission of electronic manifest data to apis .

this biographic information is transmitted to adis , where it is matched against entry information .

at the 11 poes where the exit solution is being implemented , the departure is processed by one of three exit methods .

within each port , one or more of the exit methods may be used .

the three methods are as follows: kiosk: at the kiosk , the traveler , guided by a workstation attendant if needed , scans the machine - readable travel documents , provides electronic fingerprints , and has a digital photograph taken .

a receipt is printed to provide documentation of compliance with the exit process and to assist in compliance on the traveler's next attempted entry to the country .

after the receipt prints , the traveler proceeds to his or her departure gate .

at the conclusion of the transaction , the collected information is transmitted to ident .

mobile device: at the departure gate , and just before the traveler boards the departure craft , either a workstation attendant or law enforcement officer scans the machine - readable travel documents , scans the traveler's fingerprints ( right and left index fingers ) , and takes a digital photograph .

a receipt is printed to provide documentation of compliance with the exit process and to assist in compliance on the traveler's next attempted entry to the country .

the device wirelessly transmits the captured data in real time to ident via the transportation security administration's data operations center .

if the device is being operated by a workstation attendant , he or she provides a printed receipt to the traveler , and the traveler then boards the departure craft .

if the mobile device is being operated by a law enforcement officer , the captured biographic and biometric information is checked in near real time against watch lists .

any potential match is returned to the device and displayed visually for the officer .

if no match is found , the traveler is allowed to board the departure craft .

validator: using a kiosk , the traveler , guided by a workstation attendant if needed , scans the machine - readable travel documents , provides electronic fingerprints , and has a digital photograph taken .

as with the kiosk , a receipt is printed to provide documentation of compliance with the exit process and to assist in compliance on the traveler's next attempted entry to the country .

however , this receipt has biometrics ( i.e. , the traveler's fingerprints and photograph ) embedded on the receipt .

at the conclusion of the transaction , the collected information is transmitted to ident .

the traveler presents his or her receipt to the attendant or law enforcement officer at the gate or departure area , who scans the receipt using a mobile device .

the traveler's identity is verified against the biometric data embedded on the receipt .

once the traveler's identity is verified , he or she is allowed to board the departure craft .

the captured data are not transmitted in real time back to ident .

instead , the data are periodically uploaded through the kiosk to ident .

an analysis capability is to provide for the continuous screening against watch lists of individuals enrolled in us - visit for appropriate reporting and action .

as more entry and exit information becomes available , it is to be used for analysis of traffic volume and patterns as well as for risk assessments .

the analysis is also to be used to support resource and staffing projections across poes , strategic planning for integrated border management analysis performed by the intelligence community , and determination of travel use levels and expedited traveler programs .

in addition to the contact named above , the following people made key contributions to this report: deborah davis , assistant director ; hal brumm ; tonia brown ; joanna chan ; barbara collier ; neil doherty ; jennifer echard ; james houtz ; scott pettis ; karen richey ; and karl seifert .

