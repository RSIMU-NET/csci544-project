spending on information technology ( it ) represents a significant portion of the federal budget — estimated at $80 billion for fiscal year 2012 .

more than 700 major investments account for approximately $40 billion of this it spending .

the clinger - cohen act of 1996 charges the director of the office of management and budget ( omb ) with responsibility for analyzing , tracking , and evaluating the risks and results of all major it investments as part of the federal budget process , and reporting to congress on the performance benefits achieved by these investments .

the act also places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cios ) to advise and assist agency heads in carrying out this responsibility .

omb launched the federal it dashboard in june 2009 as a public website that reports performance and supporting data for the major it investments .

the dashboard is to provide transparency for these investments in order to facilitate public monitoring of government operations and accountability for investment performance by the federal cios who oversee them .

in january 2010 , omb began using the dashboard as one of several tools to identify troubled investments .

these investments became the focus of joint omb - agency techstat accountability sessions ( techstats ) — evidence - based reviews intended to improve investment performance through concrete actions .

in december 2010 , omb reported that these sessions resulted in $3 billion in reduced life - cycle costs and subsequently incorporated the techstat model into its 25-point plan for reforming federal it management .

with this plan , agency cios became responsible for leading techstat sessions at the department level , analyzing investments using data from the dashboard , and terminating or turning around at least one - third of underperforming it projects within 18 months .

omb reported its progress on the plan , improvements to the dashboard , and results of techstat sessions in the analytical perspectives it provided for the president's 2012 and 2013 budget submissions .

in response to your request , our objectives for this review were to ( 1 ) characterize the cio ratings for selected federal agencies' it investments as reported over time on the dashboard , ( 2 ) determine how agencies' approaches for assigning and updating cio ratings vary , and ( 3 ) describe the benefits and challenges associated with agencies' approaches to the cio rating .

to establish the scope of our review , we selected six agencies that spanned a range of it spending for fiscal year 2011 , including the three highest spending agencies , two of the lowest , and an agency in the middle .

collectively , these agencies accounted for approximately $51 billion , or 65 percent , of 2011 spending on it investments .

the six agencies are the department of defense ( dod ) , department of homeland security ( dhs ) , department of health and human services ( hhs ) , department of the interior ( doi ) , national science foundation ( nsf ) , and office of personnel management ( opm ) .

to address our objectives , we downloaded cio ratings and related data reported for investments on the dashboard and analyzed these data for the period june 2009 to march 2012 .

we did not independently evaluate the ratings as reported by the agencies , but determined that they were sufficiently complete and accurate for our analyses .

we interviewed agency officials , including cios where possible , and obtained written responses and supporting documents , related agency policies , procedures , reported data , artifacts , as well as agency views on the benefits and challenges associated with performing these ratings and reporting them to the dashboard .

we also utilized recent gao and dod inspector general reviews of dod's major it investments and compared findings in these reports to the cio ratings that the department submitted to the dashboard .

in addition , we analyzed omb documentation and interviewed omb staff to update our information on how the dashboard has evolved , identify the guidance agencies received about cio ratings , determine the efforts omb has under way to improve the dashboard , and describe the ways in which omb is using the data to improve it management .

we conducted this performance audit from january 2012 through september 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are provided in appendix i .

the clinger - cohen act of 1996 requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these it investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes cios to advise and assist agency heads in carrying out this responsibility .

omb established the management watch list in 2003 to help carry out its oversight role .

the management watch list included mission - critical projects that needed to improve performance measures , project management , it security , or overall justification for inclusion in the president's budget submission .

further , in august 2005 , omb established a high - risk list , which consisted of projects identified by federal agencies , with omb's input , as requiring special attention from oversight authorities and the highest levels of agency management .

between 2005 and 2009 , omb described its efforts to monitor and manage risky federal it investments in the annual budget submission .

over the past several years , we have reported and testified on omb's initiatives to highlight troubled it projects , justify investments , and use project management tools .

for instance , in 2006 we recommended that omb develop a single aggregated list of high - risk projects and their deficiencies and use that list to report to congress on progress made in correcting high - risk problems .

as a result , omb started publicly releasing aggregate data on its management watch list and disclosing the projects' deficiencies .

moreover , between 2007 and 2009 , the president's budget submission included an overview of investment performance over several budget years , including the number of federal it projects in need of management attention .

such information helped congress stay better with informed of high - risk projects and make related funding decisions.the advent of its it dashboard in 2009 , omb discontinued this type of reporting in the fiscal year 2010 budget submission .

to allow omb ; other oversight bodies , including congress ; and the general public to hold the government agencies accountable for progress and results .

omb reported on plans and implementation progress for this management tool in the “analytical perspectives” section of the president's budget submissions for fiscal years 2012 and 2013 , including planned updates to the dashboard during 2012 to support closer executive oversight and intervention to prevent schedule delays , cost overruns , and failures in delivering key functionality needed by federal programs .

for example , it reported using the dashboard to identify investments for techstat reviews .

the dashboard visually presents performance ratings for agencies overall and for individual investments using metrics that omb has defined — cost , schedule , and cio evaluation .

the website also provides the capability to download certain data .

figure 1 is an example of an agency's ( opm ) portfolio page as recently depicted on the dashboard .

the dashboard's data spans the period from june 2009 to the present , and is based , in part , on each agency's exhibit 53 and exhibit 300 to omb , as well as on agency assessments and submissionssupporting information on each investment .

over the life of the dashboard , omb has issued guidance to agencies on , among other things , what data to report , how those data need to be structured and formatted for upload to the dashboard , and procedures for using the dashboard's submission tools .

for instance , omb instructed agencies to update and submit investment cost and schedule data monthly .

omb has made various changes to the organization , available data , and features of the dashboard over time , including improvements to dashboard calculations to incorporate the variance of “in progress” milestones rather than just “completed” milestones ; web pages containing data on historical ratings and rebaselines of eliminated and downgraded investments ; added data on awarded contracts , with links to usaspending.gov ; release of it dashboard source code and documentation to an open source hosting provider ; enhancements to baseline history , which give users the ability to see field - by - field changes for each rebaseline ; a mechanism for omb analysts to provide feedback to agencies on mobile - friendly formatting of dashboard displays .

once omb has received agency - reported investment data , it converts these into investment performance ratings for display on the dashboard according to calculations and protocols described on its website .

omb assigns cost and schedule performance ratings by using data submitted by agencies to calculate variances between the planned cost or schedule targets and the actual or projected cost or schedule values .

omb converts these variances to percentages , and assigns the ratings to be presented on the dashboard within three ranges , red , yellow , and green , as shown in table 1 .

although the thresholds for assigning cost and schedule variance ratings has remained constant over the life of the dashboard , the cost and schedule data agencies are required to submit have changed in several ways , as have the variance calculations .

for example , in response to our recommendations ( further discussed in the next section ) , omb changed how the dashboard calculates the cost and schedule ratings in july 2010 , to include “in progress” milestones rather than just “completed” ones for more accurate reflection of current investment status .

we have previously reported that omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , and by improving the accuracy of investment ratings .

we also found issues with the accuracy and data reliability of cost and schedule data , and recommended steps that omb should take to improve these data .

in july 2010 , we reporteddashboard were not always accurate for the investments we reviewed , because these ratings did not take into consideration current performance .

as a result , the ratings were based on outdated information .

we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting .

omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and that the cost and schedule ratings on omb's completed activities .

similarly , in march 2011 , we reportedhad initiated several efforts to increase the dashboard's value as an oversight tool , and had used its data to improve federal it management .

we also reported , however , that agency practices and the dashboard's calculations contributed to inaccuracies in the reported investment performance data .

for instance , we found missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data .

as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance .

most agencies generally concurred with our recommendations ; omb agreed with our recommendation for improving ratings for schedule variance .

it disagreed with our recommendation to improve how it reflects current performance in cost and schedule ratings , but more recently made changes to dashboard calculations to address this while also noting challenges in comprehensively evaluating cost and schedule data for these investments .

more recently , in november 2011 , we reported investment cost and schedule ratings had improved since our july 2010 report because omb had refined the dashboard's cost and schedule calculations .

most of the ratings for the eight investments we reviewed were accurate , although we noted that more could be done to inform oversight and decision making by emphasizing recent performance in the ratings .

we recommended that the general services administration comply with omb's guidance for updating its ratings when new information becomes available ( including when investments are rebaselined ) and the agency concurred .

since we previously recommended that omb improve how it rates investments , we did not make any further recommendations .

gao - 12-210 .

investment into a color for depiction on the dashboard .

an omb staff member from the office of e - government and information technology noted that the cio rating should be a current assessment of future performance based on historical results and is the only dashboard performance indicator that has been defined and produced the same way since the dashboard's inception .

according to omb's instructions , a cio rating should reflect the level of risk facing an investment on a scale from 1 ( high risk ) to 5 ( low risk ) relative to that investment's ability to accomplish its goals .

each agency cio is to assess their it investments against a set of six preestablished evaluation factors identified by omb ( shown in table 2 ) and then assign a rating of 1 to 5 based on his or her best judgment of the level of risk facing the investment .

according to an omb staff member , agency cios are responsible for determining appropriate thresholds for the risk levels and for applying them to investments when assigning cio ratings .

omb recommends that cios consult with appropriate stakeholders in making their evaluation , including chief acquisition officers , program managers , and other interested parties .

ultimately , cio ratings are assigned colors for presentation on the dashboard , according to the five - point rating scale , as illustrated in table 3 .

omb has made the cio's evaluation and rating a key component of its larger it reform initiative and 25 point plan .

in its plan , omb reported that it used agencies' cio ratings to select investments for the techstat review sessions it conducted between 2010 and 2011 .

these sessions are data - driven assessments of it investments by agency leaders that are intended to result in concrete action to improve performance .

omb reported that the techstats it conducted on selected investments resulted in approximately $3 billion in reduced costs .

building on the results of those sessions , the plan articulates a strategy for strengthening it governance , in part , through the adoption of the techstat model by federal agencies .

in conducting techstats , agencies are to rely , in part , on cio ratings from the it dashboard .

the techstat toolkit , developed by omb and a task force of agency leads , provides sample questions regarding an investment's cio rating and associated risks for use in techstat sessions .

furthermore , omb issued guidance in august 2011other things , that agency cios shall be held accountable for the performance of it program managers based on their governance process and the data reported on the it dashboard , which includes the cio that stated , among rating .

according to omb , the addition of cio names and photos on dashboard investments is intended to highlight this accountability and link it to the dashboard's reporting on investment performance .

figure 2 illustrates the cio rating information presented on the dashboard for an example it investment .

as of march 2012 , cio ratings for most investments listed on the dashboard for the six agencies we reviewed indicated either low risk or moderately low risk ( 223 out of 313 investments across all the selected agencies ) .

high risk or moderately high risk ratings were assigned to fewer investments ( 12 out of 313 investments across all the selected agencies ) .

figure 3 presents the total number of it investments rated on the dashboard for each of the selected agencies according to their risk levels , as of march 2012 , and illustrates the predominance of low risk investments for the agencies in our review .

the figure also reports agencies' budgets for their major it investments for fiscal year 2012 , as presented on the dashboard .

historically , over the life of the dashboard from june 2009 to march 2012 , low or moderately low risk ratings accounted for at least 66 percent of all ratings at five of the six agencies ( the exception is dhs with 51 percent ) .

medium risk ratings accounted for between 0 to 38 percent of all reported ratings across agencies during this period .

the maximum percentage of ratings in the high risk or moderately high risk categories for any agency during this 34-month period was 12 percent , with two agencies — dod dod stated in written and nsf — reporting no high risk investments.comments that this was because they did not deem any of their investments to be high risk .

 ( dod's investment risks are further discussed in the next section. ) .

an nsf official from the division of information systems stated that there were no high risk investments because most of their investments were in the operations and maintenance phase .

table 4 presents the average composition of ratings for each agency during the reporting period of june 2009 to march 2012 .

appendix iii depicts each agency's cio ratings by risk level on a monthly basis during the reporting period .

overall , the cio rating remained constant for 147 of 313 investments that were active as of march 2012 ( about 47 percent of the investments we reviewed ) .

these investments were rated at the same risk level during every rating period ( see fig .

4 ) .

four of the six agencies did not change the cio rating for a majority of their investments ( excluding any investments that were downgraded or eliminated ) during the time frame we examined .

in contrast , the other two agencies — opm and hhs — changed the cio rating for more than 70 percent of their investments at least once between the investment's initial rating and the rating reported as of march 2012 .

table 5 lists the number of each agency's investments whose ratings were constant and changed over time .

agencies offered several reasons for why many investments had no changes in their cio ratings during their entire time on the dashboard .

five of the six selected agencies indicated that many investments were in a steady - state or operations and maintenance phase with no new development .

one agency reported that their investments' cio ratings remained constant because the investments consistently met all requirements and deadlines and were using project management best practices .

the agencies we reviewed showed mixed results in reducing the number for investments of higher risk investments during the rating period.whose rating changed at least once during the period , 40 percent ( 67 investments ) received a lower risk rating in march 2012 than they received initially , 41 percent of investments ( 68 investments ) received a higher risk rating , and the remaining 19 percent ( 31 investments ) received the same rating in march 2012 as they had initially received , despite whatever interim changes may have occurred ( i.e. , there was no “net” change to their reported risk levels ) .

 ( see fig .

5. ) .

two agencies — dhs and opm — reported more investments with reduced risk in march 2012 , as compared with initial ratings .

the other four agencies reported more investments with increased risk .

table 6 presents net changes in risk levels at each of the selected agencies ( among investments that were not downgraded or eliminated ) .

appendix iii graphically summarizes these data for all six agencies .

agencies most commonly cited additional oversight or program reviews as factors that contributed to decreased risk levels .

specifically , agencies commented that the cio ratings and dashboard reporting had spurred improved program management and risk mitigation .

for example , one agency's officials commented that the cio now closely monitors the monthly performance and risk data generated by their investments , and that the additional oversight has brought about strengthened processes and more focused attention to issues .

in contrast , several agencies cited generally poor risk management at the investment level , the introduction of new investment / programs risks , as well as instances of poor project management as factors contributing to increased risk for investments .

for example , one agency responded that internal review findings revealed new risks that caused an investment's risk level to increase .

another agency's officials reported that various technical issues caused one of their investments to fall behind schedule , thus increasing risk .

both omb and several agencies suggested caution in interpreting changing risk levels for investments .

they noted that an increase in an investment's risk level can sometimes indicate better management by the program or cio because previously unidentified risks have been assessed and included in the cio evaluation .

conversely , a decrease in an investment's risk level may not indicate improved management if the data and analysis on which the cio rating are based is incomplete , inconsistent , or outdated .

further analysis of the characteristics and causes of dashboard's cio ratings , and reporting on the patterns of risk within and among agencies , could provide congress and the public with additional perspectives on federal it investment risk over time .

however , for the past four budget submissions , omb has not summarized the extent of risk represented by major federal it investments in the analysis it prepares annually for the president's budget submission , as it did prior to the fiscal year 2010 submission .

as a result , omb is missing an opportunity to integrate such risk assessments into its evaluation of major capital investments in reporting to congress .

omb has provided agencies with instructions for assigning cio ratings for the major it investments reported on the dashboard .

specifically , omb's instructions state that agency cios should rate each investment based on his / her best judgment and should include input from stakeholders , such as chief acquisition officers , program managers , and others ;  update the rating as soon as new information becomes available that might affect the assessment of a given investment ; and  utilize omb's investment rating factors , including: risk management , requirements management , contractor oversight , historical performance , and human capital , as well as any other factors deemed relevant by the cio .

despite differences in the specific inputs and processes used , agencies generally followed omb's instructions for assigning cio ratings .

however , dod's ratings reflected additional considerations beyond omb's instructions and did not reflect available information about significant risks for certain investments .

the sections that follow describe how each agency addressed omb's instructions .

include input from stakeholders .

each of the six agencies we reviewed relied on stakeholder input , at least in part , when assigning cio ratings .

agencies also cited a variety of review boards , data from program and financial systems , and other investment assessments as inputs to the rating .

table 7 describes the data and processes that agencies reported using when they derived their cio ratings .

update cio ratings .

all six agencies established guidelines for periodically reviewing and updating their cio ratings .

specifically , hhs , nsf , doi , and opm reported that they update cio ratings on a monthly basis .

dod has adopted a quarterly update cycle , although an official noted that the actual process of collecting information and evaluating investments for the ratings takes slightly longer than 3 months .

dhs officials with the office of the cio stated that the frequency of its updates varies based on the risk level of an investment's previous rating: investments with a previous cio rating of green are to be reviewed semiannually ; yellow investments are to be reviewed quarterly ; and red investments are to be reviewed monthly .

utilize omb's investment rating factors .

most of the selected agencies use omb's investment rating factors when evaluating their investments .

only one agency ( hhs ) does not use all of them .

specifically , an hhs official from the office of the cio told us that human capital issues are not explicitly covered in their cio rating criteria because investment owners are to provide adequate it human capital , and that these owners will reflect any issues that arise when providing input for the cio rating .

among the agencies we reviewed , dod was unique in that its ratings reflected additional considerations beyond omb's instructions .

for example , briefing slides prepared for dod's 2011 cio rating exercise identified the need to “balance” cio ratings , and advised that yellow or red ratings could lead to an omb review .

in addition , dod officials explained that the department rated investments green ( or low risk ) if the risk of the investment not meeting its performance goals is low ; yellow ( or medium risk ) if the investment is facing difficulty ; and red ( high risk ) only if the department planned to restructure or cancel the investment , or had already done so .

dod officials further stated that their cio ratings provide a measured assessment of how dod believes an investment will perform in the future .

although the cio ratings submitted by dod to the dashboard are consistent with their ratings approach , they do not reflect other available information about the risk of these investments .

as we previously noted , none of dod's investments that were active in march 2012 were rated as high risk , and approximately 85 percent were rated as either low risk or moderately low risk throughout their time on the dashboard .

however , these ratings did not always reflect significant schedule delays , cost increases , and other weaknesses identified for certain investments in our recent reviews , or problems with those investments identified in a recent report by the dod inspector general .

based on the department's long - standing difficulties with such programs , we designated dod business systems modernization as a high - risk area in 1995 and it remains a high - risk area today .

more recently , we reported weaknesses in several of the department's business system investments.effectively ensured that these systems would deliver capabilities on time and within budget ; that acquisition delays required extended funding for duplicative legacy systems ; that delays and cost overruns were likely to erode the cost savings these systems were to provide ; and that , ultimately , dod's management of these investments was putting the department's transformation of business operations at risk .

specifically , we reported that the department had not although the following selected examples of dod investments experienced significant performance problems and were included with those considered to be high - risk business system investments in our recent reviews of those systems , they were all rated low risk or moderately low risk by the dod cio .

air force's defense enterprise accounting and management system ( deams ) : deams is the air force's target accounting system designed to provide accurate , reliable , and timely financial information .

in early 2012 , gao reported that deams faced a 2-year deployment delay , an estimated cost increase of about $500 million for an original life - cycle cost estimate of $1.1 billion ( an increase of approximately 45 percent ) , and that assessments by dod users had identified operational problems with the system , such as data accuracy issues , an inability to generate auditable financial reports , and the need for manual workarounds.inspector general reported that the deams' schedule delays were likely to diminish the cost savings it was to provide , and would jeopardize the department's goals for attaining an auditable financial statement .

dod's cio rated deams low risk or moderately low risk from july 2009 through march 2012 .

in july 2012 , the dod army's general fund enterprise business system ( gfebs ) : gfebs is an army financial management system intended to improve the timeliness and reliability of financial information and to support the department's auditability goals .

in early 2012 , we reported that gfebs faced a 10-month implementation delay , and that dod users reported operational problems , including deficiencies in data accuracy and an inability to generate auditable financial reports .

these concerns were reiterated by the dod inspector general in july 2012 .

dod's cio rated gfebs as moderately low risk from july 2009 through march 2012 .

army's global combat support system - army ( gcss - army ) : gcss - army is intended to improve the army's supply chain management capabilities and provide accurate equipment readiness status reports , among other things .

in march 2012 , we reported that gcss - army was experiencing a cost overrun of approximately $300 million on an original life - cycle cost estimate of $3.9 billion ( an increase of approximately 8 percent ) and a deployment delay of approximately 2 years .

dod rated gcss - army as low or moderately low risk from july 2009 through march 2012 .

explanations submitted by dod with the cio ratings for these investments did not provide meaningful insight for why they were rated at the lowest risk levels in the face of known issues .

dod officials told us that they rated these investments as low risk because , in their view , the cost and schedule variances listed above did not constitute significant risks .

officials explained that: ( 1 ) the cost variances were not that large compared to dod's overall size and large amount of it spending ; ( 2 ) the schedule variance needed to be understood in the context that the average dod large - scale it program takes 7 years ( or 84 months ) to implement ; and ( 3 ) that each of those programs had risk mitigation plans in place .

however , the first two reasons are inconsistent with dod's own which recommends that risks be assessed risk management guidance,against the program's own cost and schedule estimates , not other department investments .

in addition , completing risk mitigation plans does not necessarily lower investment risk .

dod's guidance calls for implementing the mitigation plan and then reassessing resulting changes to the risk .

even if the department adopts these elements of its own guidance , the cio's evaluation will be incomplete unless it also reflects the assessments of investment performance and risks identified by us and others .

until the department does so , cio ratings for dod's dashboard investments may not be sufficiently accurate or useful for its techstat sessions or omb's management and oversight .

selected agencies identified various benefits associated with performing cio ratings and dashboard reporting in general .

almost all of the agencies ( five of six ) reported the following three benefits .

increased quality of investment performance data .

for example , one agency also reported that the dashboard has made information about investments more understandable .

greater transparency and visibility for cios and their staff into investment - and program - level performance data .

one agency reported that its cio was better able to conduct reviews with actual investment numbers , as opposed to self - reported data presented by the investment's program managers .

agencies could also compare their investments' ratings to those of other agencies and departments .

increased focus on project management practices .

two agencies reported improved investment performance as a direct result of their dashboard rating and reporting activities ; another stated that dashboard reporting supported and reinforced their existing it governance , capital planning , and program management processes .

some of these benefits were interrelated .

several agencies viewed the improved data quality as a by - product of greater scrutiny brought about by having to report such data to the dashboard on a regular basis .

one agency response noted that their program managers were surprised to see the extent to which investment data were visible to the public , and that this visibility motivated their staff to provide accurate and timely data ( which has improved data quality ) .

another agency noted that the visibility of the it dashboard has increased awareness among investment and project managers about the need to improve the planning of project activities and the definition of operational performance metrics ( which support program management ) .

nevertheless , agencies also identified challenges associated with producing and reporting cio ratings .

first , three agencies reported a challenge associated with the time and effort required to gather , validate , and gain internal approval for cio ratings and other data reported to the dashboard .

for example , one agency reported that , due to the number of organizations involved and the number of investments being evaluated , it generally takes 90 to 120 days to develop and update its cio ratings .

the agency further reported that this effort was separate from ( and in addition to ) time it already spends on its own internal processes for managing and overseeing acquisition programs .

second , four of the six agencies identified challenges with the number of changes omb has made to the dashboard , as well as with the timeliness and clarity of omb's communication regarding those changes .

for example , officials at one agency commented that the frequency of changes has actually hindered their efforts to improve data quality , since errors sometimes resulted when it adapted to changes required by omb .

officials at another agency stated that omb allowed insufficient time for agencies to test their systems' interfaces with the dashboard when changes were made , which they said resulted in data errors and challenges for staff .

these officials also noted that omb's guidance for agency submissions has , at times , not matched the technical data schemas implemented by omb , impeding agencies' efforts to successfully upload their data .

an omb staff member commented that their office releases changes to the dashboard as early in the fiscal year as possible to give agencies time to adjust and that omb announces planned changes to agencies before they are implemented via the dashboard's interagency web portal .

omb has recently held meetings with agency officials to discuss these issues and determine ways to better communicate going forward .

finally , one agency responded that while monthly updates to the dashboard have increased investment and project managers' attention to the performance of their investments and projects , this regular scrutiny could encourage investment and project managers to “perform to the test” rather than concentrate on effective investment and project management .

however , based on the interrelationships of the benefits of cio ratings identified by some agencies , the process of generating and reporting cio ratings does not have to be just a grading exercise .

as previously noted , the benefit of improved investment performance data for the cio's investment evaluation can lead to more effective management , which could , in turn , improve investment performance .

executives and staff who can envision these results from the dashboard's cio evaluations may be less likely to view the additional time and effort required to generate the cio ratings as a challenge , but as an opportunity for more efficient and effective management .

since its inception in 2009 , the federal it dashboard has increased the transparency of the performance of major federal it investments .

its cio ratings , in particular , have improved visibility into changes in the risk levels of agencies' investments over time .

determining whether such changes represent improvements or deficiencies in management and oversight can be difficult without additional information on investment performance and the rating process , but analyzing and reporting the ratings for investments and agencies over time for the president's budget submission could help omb ensure that risk is accurately assessed and that patterns of risk deserving of special management attention are identified .

dod demonstrated one such pattern of interest in its cio ratings .

during the 34-month life of the dashboard , none of the 87 investments that were active as of march 2012 were rated high risk or moderately high risk , and approximately 85 percent of ratings were low risk or moderately low risk .

although dod implemented omb's broad instructions for producing cio ratings , it also considered how the ratings might increase the likelihood of an omb review of an investment and minimized the effects of significant schedule delays and cost increases , which were identified in our reviews and those of dod's inspector general .

as a result , dod is masking significant investment risks , has not employed its own risk management guidance , and has not delivered the transparency intended by the dashboard .

by incorporating the results of external reviews into its evaluations , dod can further improve the quality of the information on which investment risk ratings are based .

beyond the transparency they promote , cio ratings present an opportunity to improve the data and processes agencies use to assess investment risk .

some agencies have already experienced collateral benefits and management results from their risk evaluations .

continuing focus from omb and agencies on how to accurately portray and derive value from the ratings and the associated processes could enable agencies to experience such benefits .

to ensure that omb's preparation of the president's budget submission accurately reflects the risks associated with all major it investments , we are recommending that the federal cio analyze agency trends reflected in dashboard cio ratings , and present the results of this analysis with the president's annual budget submission .

to ensure that dod's cio evaluations of investment risk for its major it dashboard investments reflect all available performance assessments and are consistent with the department's own guidance for managing risk , we are recommending that the secretary of defense direct the department's cio to reassess the department's considerations for assigning cio risk levels for dashboard investments , including assessments of investment performance and risk from outside the programs , and apply the appropriate elements of the department's risk management guidance to omb's evaluation factors in determining cio ratings .

we provided a draft of our report to the six agencies selected for our review and to omb .

in oral comments , staff from omb's office of e - government & information technology stated that omb concurred with our recommendation that the federal cio analyze agency trends reflected in dashboard cio ratings and present the results of this analysis with the president's annual budget submission .

omb staff also provided technical comments , which we incorporated as appropriate .

in a written response , dod's deputy chief information officer for information enterprise agreed with our recommendation that the department's cio reassess considerations for assigning cio risk levels for dashboard investments , and committed to updating the department's cio ratings process to better report risk and improve the timeliness and transparency of reporting .

dod's written response is reprinted in appendix iv .

officials at doi provided technical comments , which we incorporated as appropriate .

the remaining agencies had no comment on the draft report .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to interested congressional committees ; the secretaries of defense , interior , homeland security , health and human services , the director of the national science foundation , the director of the office of personnel management , the director of the office of management and budget ; and other interested parties .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staffs have any questions on the matters discussed in this report , please contact david a. powner at ( 202 ) 512-9286 or by e - mail at pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

our objectives were to ( 1 ) characterize the chief information officer ( cio ) ratings for selected federal agencies' information technology ( it ) investments as reported over time on the federal it dashboard ; ( 2 ) determine how agencies' approaches for assigning and updating cio ratings vary ; and ( 3 ) describe the benefits and challenges associated with agencies' approaches to the cio rating .

to establish the scope of our review , we downloaded and examined data on total it spending for fiscal year 2011 for the 27 agencies reported on the it dashboard .

 ( the office of management and budget ( omb ) extracts these data based on exhibit 300 forms submitted by each agency. ) .

we then selected six agencies that spanned a range of it spending for fiscal year 2011 , including the three highest spending agencies , two of the lowest , and an agency in the middle .

collectively , these agencies accounted for approximately $51 billion , or 65 percent , of 2011 spending on it investments .

the six agencies are the department of defense , department of homeland security , department of health and human services , department of the interior , national science foundation , and office of personnel management .

the results in this report represent only these agencies .

to address the first objective , we downloaded and examined the dashboard's cio ratings for all investments at the six agencies we selected ( a total of approximately 308 investments reported by these agencies ) .

to characterize the numbers and percentages of major it investments at each risk level at each of our subject agencies , we analyzed , summarized , and — where appropriate — graphically depicted average cio ratings for investments by agencies over time during the period from june 2009 to march 2012 .

specifically , we compared the cio ratings in june 2009 ( or whenever an individual investment was first rated ) up through and including each investment's rating as of march 2012 and summarized the data by agency .

to describe whether cio ratings indicated higher or lower investment risk over time , we calculated the numbers and percentages of investments ( by agency and collectively for all the agencies ) that maintained a constant rating over the entire performance period , and those that experienced a change to their cio rating in at least one rating period .

then we analyzed the subset of investments that experienced at least one changed rating and compared the first cio rating with the latest cio rating ( no later than march 2012 ) to determine the numbers and percentages of investments ( by agency and collectively for all the agencies ) that experienced a net rating increase , a net rating decrease , or no net change .

we also examined the comments provided with the ratings to determine whether such comments were useful in understanding the ratings .

we presented our results to each agency and omb and solicited their input , explanations for the results , and additional corroborating documentation , where appropriate .

to address our second objective , we reviewed available documentation , obtained written responses to questions we posed to all agencies , and interviewed omb and agency officials to determine their policies and practices related to assigning and updating the cio ratings and related data for the dashboard .

specifically , we gathered descriptions about the data , participants , and processes used to generate cio ratings for investments ; when and under what circumstances each agency updates its ratings ; the specific factors agencies used in assigning their ratings ; and the reason ( s ) for their approaches to assigning and reporting the ratings .

we reviewed our results with agency officials to ensure that our presentation of their approach was accurate .

in addition , we utilized our prior work and a report by the department of defense's office of the inspector general related to the department's major it investments .

we compared the findings in these reports to the cio ratings the department submitted to the dashboard for investments that had been rated consistently low or moderately low risk , and discussed our results with department officials .

to address our third objective , we reviewed written and oral descriptions of the benefits and challenges that agencies and omb have experienced in developing , submitting , updating , and utilizing cio ratings .

we sought specific examples , corroborating documentation , and causal factors , where available .

after obtaining this information from individual agencies , we compared their responses to identify benefits and challenges common to multiple agencies and applied our judgment in determining whether any additional benefits or challenges were present .

we conducted this performance audit from january 2012 to september 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the table below lists the total number of major information technology ( it ) investments rated on the federal it dashboard as of march 2012 for each agency selected for this review , with the numbers of investments rated at each of the risk levels specified by the office of management and budget ( omb ) for the chief information officer ( cio ) rating .

the last line in the table reports each agency's total budget for fiscal year 2012 for their major it investments , as also reported on the dashboard in march 2012 .

this appendix provides additional information about chief information officer ( cio ) ratings for major it information technology ( it ) investments at each of the agencies selected for this review .

the first figure for each agency depicts the number of investments at each rating level for the end of each month , as reported on the federal it dashboard .

the second figure depicts the number of investments whose risk level demonstrated a net increase , net decrease , no net change , or remained constant during the investment's entire time on the dashboard .

in addition to the contact name above , the following staff also made key contributions to this report: paula moore ( assistant director ) , neil doherty , lynn espedido , rebecca eyler , kate feild , dan gordon , andrew stavisky , sonya vartivarian , shawn ward , kevin walsh , jessica waselkow , and monique williams .

