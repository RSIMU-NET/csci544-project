for decades , the department of the defense ( dod ) has been challenged in modernizing its timeworn business systems .

in 1995 , we designated dod's business systems modernization program as high - risk , and continue to do so today .

our reasons include the modernization's large size , complexity , and its critical role in addressing other high - risk areas , such as overall business transformation and financial management .

moreover , we continue to report on business system investments that fail to effectively employ acquisition management controls and deliver promised benefits and capabilities on time and within budget .

nevertheless , dod continues to invest billions of dollars in thousands of these business systems , 11 of which account for about two - thirds of the department's annual spending on business programs .

the navy enterprise resource planning ( erp ) program is one such program .

initiated in 2003 , navy erp is to standardize the navy's acquisition , financial , program management , plant and wholesale supply , and workforce management business processes across its dispersed organizational environment .

as envisioned , the program consists of a series of major increments , the first of which includes three releases and is expected to cost approximately $2.4 billion over its 20-year life cycle and to be fully operational in fiscal year 2013 .

we recently reported that navy erp program management weaknesses had contributed to a 2-year schedule delay and about $570 million in cost overruns .

as agreed , our objectives were to determine whether ( 1 ) system testing is being effectively managed , ( 2 ) system changes are being effectively controlled , and ( 3 ) independent verification and validation ( iv&v ) activities are being effectively managed .

to accomplish this , we analyzed relevant program documentation , such as test management documents , individual test plans and procedures and related test results and defect reports ; system change procedures and specific change requests and decisions ; change review board minutes ; and verification and validation plans and contract documents .

we also observed the use of tools for recording and tracking test defects and change requests , including tracing a statistically valid sample of transactions through these tools .

we conducted this performance audit from august 2008 to september 2009 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

additional details on our objectives , scope , and methodology are in appendix i .

the department of the navy's ( don ) primary mission is to organize , train , maintain , and equip combat - ready naval forces capable of winning wars , deterring aggression by would - be foes , preserving freedom of the seas , and promoting peace and security .

its operating forces , known as the fleet , are supported by four systems commands .

table 1 provides a brief description of each command's responsibilities .

to support the department's mission , these commands perform a variety of interrelated and interdependent business functions ( eg , acquisition and financial management ) , relying heavily on business systems to do so .

in fiscal year 2009 , don's budget for business systems and associated infrastructure was about $2.7 billion , of which about $2.2 billion was allocated to operations and maintenance of existing systems and about $500 million to systems in development and modernization .

of the approximately 2,480 business systems that dod reports having , don accounts for 569 , or about 23 percent , of the total .

navy erp is one such system investment .

in july 2003 , the assistant secretary of the navy for research , development , and acquisition established navy erp to converge the functionality of four pilot systems that were under way at the four commands into one system .

according to dod , navy erp is to address the navy's long - standing problems related to financial transparency and asset visibility .

specifically , the program is intended to standardize the navy's acquisition , financial , program management , plant and wholesale supply , and workforce management business processes across its dispersed organizational components , and support about 86,000 users when fully implemented .

navy erp is being developed in a series of increments using the systems applications and products ( sap ) commercial software package , augmented as needed by customized software .

sap consists of multiple , integrated functional modules that perform a variety of business - related tasks , such as finance and acquisition .

the first increment , called template 1 , is currently the only funded portion of the program and consists of three releases ( 1.0 , 1.1 , and 1.2 ) .

release1.0 , financial and acquisition , is the largest of the three releases in terms of template 1 functional requirements .

see table 2 for a description of these releases .

don estimates the life - cycle cost for template 1 to be about $2.4 billion , including about $1 billion for acquisition and $1.4 billion for operations and maintenance .

the program office reported that approximately $600 million was spent from fiscal year 2004 through fiscal year 2008 .

for fiscal year 2009 , about $190 million is planned to be spent .

to acquire and deploy navy erp , don established a program management office within the program executive office for executive information systems .

the program office manages the program's scope and funding and is responsible for ensuring that the program meets its key objectives .

to accomplish this , the program office performs program management functions , including testing , change control , and iv&v .

in addition , various dod and don organizations share program oversight and review activities .

a listing of key entities and their roles and responsibilities is provided in table 3 .

to deliver system and other program capabilities and to provide program management support services , navy erp relies on multiple contractors , as described in table 4 .

template 1 of navy erp was originally planned to reach full operational capability ( foc ) in fiscal year 2011 , and its original estimated life - cycle cost was about $1.87 billion .

the estimate was later baselined in august 2004 at about $2.0 billion .

in december 2006 and again in september 2007 , the program was rebaselined .

foc is now planned for fiscal year 2013 , and the estimated life - cycle cost is about $2.4 billion ( a 31 percent increase over the original estimate ) .

the program is currently in the production and deployment phase of the defense acquisition system , having completed the system development and demonstration phase in september 2007 .

this was 17 months later than the program's original schedule set in august 2004 , but on time according to the revised schedule set in december 2006 .

changes in the program's acquisition phase timeline are depicted in figure 1 , and life - cycle cost estimates are depicted in figure 2 .

release 1.0 was deployed at navair in october 2007 , after passing developmental testing and evaluation .

initial operational capability ( ioc ) was achieved in may 2008 , 22 months later than the baseline established in august 2004 , and 4 months later than the new baseline established in september 2007 .

according to program documentation , these delays were due , in part , to challenges experienced at navair in converting data from legacy systems to run on the new system and implementing new business procedures associated with the system .

in light of the delays at navair in achieving ioc , the deployment schedules for the other commands were revised in 2008 .

release 1.0 was deployed at navsup in october 2008 as scheduled , but deployment at spawar was rescheduled for october 2009 , 18 months later than planned , and at navsea general fund in october 2010 , and at navy working capital fund in october 2011 , each 12 months later than planned .

release 1.1 is currently being developed and tested , and is planned to be deployed at navsup in february 2010 , 7 months later than planned , and at the navy's fleet and industrial supply centers ( fisc ) starting in february 2011 .

changes in the deployment schedule are depicted in figure 3 .

we have previously reported that dod has not effectively managed key aspects of a number of business system investments , including navy erp .

among other things , our reviews have identified weaknesses in such areas as architectural alignment and informed investment decision making , which are the focus of the fiscal year 2005 defense authorization act business system provisions .

our reviews have also identified weaknesses in other system acquisition and investment management areas , such as earned value management , economic justification , risk management , requirements management , test management , and iv&v practices .

in september 2008 , we reported that dod had implemented key information technology ( it ) management controls on navy erp to varying degrees of effectiveness .

for example , the control associated with managing system requirements had been effectively implemented , and important aspects of other controls had been at least partially implemented , including those associated with economically justifying investment in the program and proactively managing program risks .

however , other aspects of these controls , as well as the bulk of what was needed to effectively implement earned value management , had not been effectively implemented .

as a result , the controls that were not effectively implemented had , in part , contributed to sizable cost and schedule shortfalls .

accordingly , we made recommendations aimed at improving cost and schedule estimating , earned value management , and risk management .

dod largely agreed with our recommendations .

in july 2008 , we reported that dod had not implemented key aspects of its it acquisition policies and related guidance on its global combat support system – marine corps ( gcss - mc ) program .

for example , we reported that it had not economically justified its investment in gcss - mc on the basis of reliable estimates of both benefits and costs and had not effectively implemented earned value management .

moreover , the program office had not adequately managed all program risks and had not used key system quality measures .

we concluded that by not effectively implementing these it management controls , the program was at risk of not delivering a system solution that optimally supports corporate mission needs , maximizes capability mission performance , and is delivered on time and within budget .

accordingly , we made recommendations aimed at strengthening cost estimating , schedule estimating , risk management , and system quality measurement .

the department largely agreed with our recommendations .

in july 2007 , we reported that the army's approach for investing about $5 billion in three related programs — the general fund enterprise business system , global combat support system - army field / tactical , and logistics modernization program — did not include alignment with the army enterprise architecture or use of a portfolio - based business system investment review process .

further , the logistics modernization program's testing was not adequate and had contributed to the army's inability to resolve operational problems .

in addition , the army had not established an iv&v function for any of the three programs .

accordingly , we recommended , among other things , use of an independent test team and establishment of an iv&v function .

dod agreed with the recommendations .

in december 2005 , we reported that don had not , among other things , economically justified its ongoing and planned investment in the naval tactical command support system ( ntcss ) and had not adequately conducted requirements management and testing activities .

specifically , requirements were not traceable and developmental testing had not identified problems that , subsequently , twice prevented the system from passing operational testing .

moreover , don had not effectively performed key measurement , reporting , budgeting , and oversight activities .

we concluded that don could not determine whether ntcss , as defined and as being developed , was the right solution to meet its strategic business and technological needs .

accordingly , we recommended developing the analytical basis necessary to know if continued investment in ntcss represented a prudent use of limited resources , and strengthening program management , conditional upon a decision to proceed with further investment in the program .

the department largely agreed with our recommendations .

in september 2005 , we reported that while navy erp had the potential to address some of don's financial management weaknesses , it faced significant challenges and risks , including developing and implementing system interfaces with other systems and converting data from legacy systems .

also , we reported that the program was not capturing quantitative data to assess effectiveness , and had not established an iv&v function .

we made recommendations to address these areas , including having the iv&v agent report directly to program oversight bodies , as well as the program manager .

dod generally agreed with our recommendations , including that an iv&v function should be established .

however , it stated that the iv&v team would report directly to program management who in turn would inform program oversight officials of any significant iv&v results .

in response , we reiterated the need for the iv&v to be independent of the program and stated that performing iv&v activities independently of the development and management functions helps to ensure that the results are unbiased and based on objective evidence .

we also reiterated our support for the recommendation that the iv&v reports be provided to the appropriate oversight body so that it can determine whether any of the iv&v results are significant .

we noted that doing so would give added assurance that the results were objective and that those responsible for authorizing future investments in navy erp have the information needed to make informed decisions .

to be effectively managed , testing should be planned and conducted in a structured and disciplined fashion .

according to dod and industry guidance , system testing should be progressive , meaning that it should consist of a series of test events that first focus on the performance of individual system components , then on the performance of integrated system components , followed by system - level tests that focus on whether the entire system ( or major system increments ) is acceptable , interoperable with related systems , and operationally suitable to users .

for this series of related test events to be conducted effectively , all test events need to be , among other things , governed by a well - defined test management structure and adequately planned .

further , the results of each test event need to be captured and used to ensure that problems discovered are disclosed and corrected .

key aspects of navy erp testing have been effectively managed .

specifically , the program has established an effective test management structure , key development events were based on well - defined plans , the results of all executed test events were documented , and problems found during testing ( i.e. , test defects ) were captured in a test management tool and subsequently analyzed , resolved , and disclosed to decision makers .

further , while we identified instances in which the tool did not contain key data about defects that are needed to ensure that unauthorized changes to the status of defects do not occur , the number of instances found are not sufficient to conclude that the controls were not operating effectively .

notwithstanding the missing data , this means that navy erp testing has been performed in a manner that increases the chances that the system will meet operational needs and perform as intended .

the program office has established a test management structure that satisfies key elements of dod and industry guidance .

for example , the program has developed a test and evaluation master plan ( temp ) that defines the program's test strategy .

as provided for in the guidance , this strategy consists of a sequence of tests in a simulated environment to verify first that individual system parts meet specified requirements ( i.e. , development testing ) and then verify that these combined parts perform as intended in an operational environment ( i.e. , operational testing ) .

as we have previously reported , such a sequencing of test events is an effective approach because it permits the source of defects to be isolated sooner , before it is more difficult and expensive to address .

more specifically , the strategy includes a sequence of developmental tests for each release consisting of three cycles of integrated system testing ( ist ) followed by user acceptance testing ( uat ) .

following development testing , the sequence of operational tests includes the navy's independent operational test agency conducting initial operational test and evaluation ( iot&e ) and then follow - on operational test and evaluation ( fot&e ) , as needed , to validate the resolution of deficiencies found during iot&e .

see table 5 for a brief description of the purpose of each test activity , and figure 4 for the schedule of release 1.0 and 1.1 test activities .

according to relevant guidance , test activities should be governed by well - defined and approved plans .

among other things , such plans are to include a defect triage process , metrics for measuring progress in resolving defects , test entrance and exit criteria , and test readiness reviews .

each developmental test event for release 1.0 ( i.e. , each cycle of integrated systems testing and user acceptance testing ) was based on a well - defined test plan .

for example , each plan provided for conducting daily triage meetings to ( 1 ) assign new defe documented criteria , ( 2 ) defects in the test management tool , and ( 3 ) address other de testing issues .

further , each plan included defect metrics , such as t number of plan specified that testing was not complete until all major defects found during the cycle were resolved , and all unresolved defects' impact on the olding next test event were understood .

further , the plans provided for h test readiness reviews to review test results as a condition for proceeding to the next activities in activities no increasing t and perform as inten cts a criticality level using record new defects and update the status of old defects found and corrected and their age .

in addition , each event .

by ensuring that plans for key clude these aspects of effective test planning , the risk of test t being effectively and efficiently p erformed is reduced , thus he chances that the system will ded .

according to industry guidance , effective system testing includes capturing , analyzing , resolving , and disclosing to decision makers the status of problems found during testing ( i.e. , test defects ) .

further , this guidance states that these results should be collected and stored accordin to defined procedures and placed under appropriate levels of control to ensure that any changes to the results are fully documented .

to the program's credit , the relevant testing organizations have documented test defects in accordance with defined plans .

for example , daily triage meetings involving the test team lead , testers , and functional experts were held to review each new defect , assign it a criticality level , and designate someone responsible for resolving it and for monitoring and updating its resolution in the test management tool .

further , test readines reviews were conducted at which entrance and exit criteria for each key test event were evaluated before proceeding to the next event .

as p art of these reviews , the program office and oversight officials , command representatives , and test officials reviewed the results of test events to ensure , among other things , that significant defects were closed and that there were no unresolved defects that could affect execution of the next test event .

however , the test management tool did not always contain key data recorded defects that are needed to ensure that unauthorized changes to the status of defects do not occur .

according to information systems auditing guidelines , audit tools should be in place to monitor user access to systems to detect possible errors or unauthorized changes .

for navy erp , this was not always the case .

specifically , while the tool capability to track changes to test defects in a history log , our analysis of 80 randomly selected defects in the tool disclosed two instances the tool did not record when a change in the defect's status was made or who made the change .

in addition , our analysis of 12 additional defects that were potential anomalies disclosed two additional instances where the tool did not record when a change was made and who made it .

while our sample size and results do not support any conclusions as to the overall effectiveness of the controls in place for recording and tracking test defect status changes , they do show that it is possible that changes es .

can be made without a complete audit trail surrounding those chang after we shared our results with program officials , they stated that they provided each instance for resolution to the vendor responsible for the tracking tool .

these officials attributed these instances to vendor updates to the tool t hat caused the history settings to default to “off.” to address this weakness , they added that they are now ensuring that the history log are set correctly after any update to the tool .

this addition is a positive step because without an effective information system access audit tool , the probability of test defect status errors or unauthorized changes is increased .

industry best practices and dod guidance recognize the importance of system change control when developing and maintaining a system .

once the composition of a system is sufficiently defined , a baseline configuration is normally established , and changes to that baseline are placed under a disciplined change control process to ensure that unjustified and unauthorized changes are not introduced .

elements of disciplined change control include ( 1 ) formally documenting a change control process , ( 2 ) rigorously adhering to the documented process , and ( 3 ) adopting objective criteria for considering a proposed change , including its estimated cost and schedule impact .

to its credit , the navy erp program has formally documented a change control process .

specifically , it has a plan and related procedures that include the made to the system are properly identified , developed , and implemented in a defined and controlled environment .

it also is using an automated tool to capture and track the disposition of each change request .

further , it has defined roles and responsibilities and a related decision - making structure for reviewing and approving system changes .

in this regard , the program has established a hierarchy of review and approval boards , including a configuration control board to review all changes and a configuration management board to further review changes estimated to require more than 100 hours or $25,000 to implement .

furthermore , a navy erp senior integration board was recently established to review and approve requests to add , delete , or change the program's requirements .

in addition , the change control process states that the decisions are to be based on , amon others , the system engineering and earned value management ( i.e. , co and schedule ) impacts the change w number of work hours that will be required to effect the change .

table 7 purpose and scope of the process — to ensure that any changes st ill introduce , such as the estimated provides a brief description of the decision - making authorities and boards and their respective roles and responsibilities .

navy erp is largely adhering to its documented change control process .

specifically , our review of a random sample of 60 change requests and minutes of related board meetings held between may 2006 and april 20 showed that the change requests were captured and tracked using an automated tool , and they were reviewed and approved by the designated decision - making authorities and boards , in accordance with the program documented process .

however , the program has not sufficiently or co cost and schedule impacts of proposed changes .

our analysis of the random sample of 60 change requests , including our review of related board meeting minutes , showed no evidence that cost and schedule impacts were identified or that they were considered .

specifically , we did not see evidence that the cost and schedule impacts of these change requests were assessed .

according to program officials , the cost and schedule impacts of each change were discussed at control board meetings .

in addition , they provided two change requests to demonstrate this .

however , while these change requests did include schedule impact , they did not include the anticipated cost impact of proposed changes .

rather , these two , as well as those in our random sample , included the estimated number of work hours required to implement the change .

nsistently considered the because the cost of any proposed change depends on other factors besides work hours , such as labor rates , the estimated number of work hours is not sufficient for considering the cost impact of a change .

i absence of verifiable evidence that cost and schedule impacts were consistently considered , approval authorities do not appear to have b provided key information needed to fully inform their decisions on whether or not to approve a change .

system changes that are approved without a full understanding of their cost and schedule impacts could result in unwarranted cost increases and schedule delays .

the purpose of iv&v is to independently ensure that program processes and products meet quality standards .

the use of an iv&v function is recognized as an effective practice for large and complex system development and acquisition programs , like navy erp , as it provides objective insight into the program's processes and associated work p to be effective , verification and validation activities should be roducts.performed by an entity that is managerially independent of the system development and management processes and products that are being reviewed .

among other things , such independence helps to ensure that the results are unbiased and based on objective evidence .

the navy has not effectively managed its iv&v function because it has not ensured that the contra ctor performing this function is independent of the products and processes that this contractor is reviewing and because it has not ensured that the contractor is meeting contractual requirements .

in june 2006 , don awarded a professional support services contract general dynamics information technology ( gdit ) , to include responsibilities for , among other things , iv&v , program management support , and delivery of releases according to cost and schedule constraints .

according to the program manager , the contractor's iv&vfunction is organizationally separate from , and thus independent of , th e contractor's navy erp system development function .

however , the subcontractor performing the iv&v function is also performing release management .

according to the gdit contract , the release manager is responsible for developing and deploying a system release that meets operational requirements within the program's cost and schedule constraints , but it also states that the iv&v function is resp supporting the government in its review , approval , and acceptance of navy rp products ( eg , releases ) .

the contract also states that gdit is eligible e for an optional award fee payment based on its performance in meeting , among other things , these cost and schedule constraints .

because performance of the system development and management role ma contractor potentially unable to render impartial assistance to the government in performing the iv&v function , the contractor has an inherent conflict of interest relative to meeting cost and schedule commitments and disclosing the results of verification and validation reviews that may affect its ability to do so .

the iv&v function's lack of independence is amplified by t reports directly and solely to the program manager .

as we have previously reported , the iv&v function should report the issues or weaknesses that increase the risks associated with the project to program oversight officials , as well as to program management , to better ensure that the verification and validation results are objective and that the officials responsible for making program investment decisions are fully informed .

furthermore , these officials , once informed , can ensure that the issues or weaknesses reported are promptly addressed .

without ensuring sufficient managerial independence , valuable information may not reach decision makers , potentially leading to the release of a system that does not adequately meet users' needs and o as intended .

beyond the iv&v function's lack of independence , the program office h not ensured that the subcontractor has produced the range of deliv that were contractually required and defined in the iv&v plan .

for example , the contract and plan call for weekly and monthly reports identifying weaknesses in program processes and recommendations for improvement , a work plan for accomplishing iv&v tasks , and associated assessment reports that follow the system engineering plan and program schedule .

however , the iv&v contractor has largely not delivered these products .

specifically , until recently , it did not produce a work plan and erables only monthly reports were delivered , and these reports only list meetings that the iv&v contactor attended and documents that it reviewed .

they d not , for example , identify program weaknesses or provide recommendations for improvement .

according to program officials , they have relied on oral reports from the subcontractor at weekly meetings , and these lessons learned have been incorporated into program guid according to the contractor , the navy has expended about $1.8 million between june 2006 and september 2008 for iv&v activities , with an additional $249,000 planned to be spent in fiscal year 2009. ance .

following our inquiries about an iv&v work plan , the iv&v contractor developed such a plan in october 2008 , more than 2 years after the contract was awarded , that lists program activities and processes to be assessed , such as configuration management and testing .

while this does not include time frames for starting and completing these assessments , meeting minutes show that the status of assessments ha been discussed with the program manager during iv&v review meetings .

the first planned assessment was delivered to the program in march 2009 he program's configuration and provides recommendations for improving t management process , such as using the automated tool to produce certain reports and enhancing training to understand how the tool is use further , program officials stated that they have also recently begun requiring the contractor to provide formal quarterly reports , the first of which was de of this quarterly report shows that it provides recommendations for improving the program's risk management process and organizational change management strategy .

d. livered to the program manager in january 2009 .

our review notwithstanding the recent steps that the program office has taken , nevertheless lacks an independent perspective on the program's products and management processes .

dod's successes in delivering large - scale business systems , such as navy erp , are in large part determined by the extent to which it employs the kind of rigorous and disciplined it management controls that are reflected in department policies and related guidance .

while implementing these controls does not guarantee a successful program , it does minimize a program's exposure to risk and thus the likelihood that it will fall short of expectations .

in the case of navy erp , living up to expectations is important because the program is large , complex , and critical to addressing the department's long - standing problems related to financial transparency and asset visibility .

the navy erp program office has largely implemented a range of effective controls associated with system testing and change control , including acting quickly to address issues with the audit log for its test manage tool , but more can be done to ensure that the cost and sched proposed changes are explicitly documented and considered when decisions are reached .

moreover , while the program office has contract for iv&v activities , it has not ensured that the contractor is indepen the products and processes that it is to review and has not held the contractor accountable for producing the full range of iv&v deliverables required under the contract .

moreover , it has not ensured that it s iv&v contractor is accountable to a level of management above the program office , as we previously recommended .

notwithstanding the program office's considerable effectiveness in how it has managed both system testing and change control , these weaknesses increase the risk of investing in system changes that are not economically justified and unnecessarily limit the value that an iv&v agent can bring to a program like navy e by addressing these weaknesses , the department can better ensure t taxpayer dollars are wisely and prudently invested .

rp .

to strengthen the management of navy erp's change control process , recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ( 1 ) revise the navy erp procedures for controlling system changes to explicitly require that a proposed change's life - cycle cost impact be estimated and considered in making change request decisions and ( 2 ) capture the cost and schedule impacts of each proposed change in the navy erp automated change c ontrol tracking tool .

to increase the value of navy erp iv&v , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ( 1 ) stop performance of the iv&v function under the existing contract and ( 2 ) engage the services of a n iv&v agent that is independent of all navy erp management , development , testing , and deployment activities that it may review .

in addition , we reiterate our prior recommendation relative to ensur the navy erp iv&v agent report directly to program oversight officials , while concurrently sharing iv&v results with the program office .

in written comments on a draft of this report , signed by the assistan deputy chief management officer and reprinted in appendix ii , the department concurred with our recommendations , and stated that it will take the appropriate corrective actions within the next 7 months .

we are sending copies of this report to interested congressional committees ; the director , office of management and budget ; the congressional budget office ; and the secretary of defense .

the report also is available at no charge on our web site at http: / / www.gao.gov .

if you or your staffs have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or hiter@gao.gov .

contact points for our offices of congressional relations and be found on the last page of this report .

gao staff who made major c ontributions to this report are listed in appendix iii .

our objectives were to determine whether ( 1 ) system testing is being effectively managed , ( 2 ) system changes are being effectively con and ( 3 ) independent verification and validation ( iv&v ) activities a effectively managed for the navy enterprise resource planning ( erp ) program .

to determine if navy erp testing is being effectively managed , we reviewed relevant documentation , such as the test and evaluation mas plan and test reports and compared them with relevant federal and related res guidance .

further , we reviewed development test plans and procedu for each test event and compared them with best practices to determine whether well - defined plans were developed .

we also examined test results and reports , including test readiness review documentation and compared them against plans to determine whether they had been executed in accordance with the plans .

moreover , to determine the extent to which test defect data were being captured , analyzed , and reported , we inspected 80 randomly selected defects from a sample of 2,258 def program's test management system .

in addition , we re logs associated with each appropriate levels the results were fully documented .

th percent tolerable error rate at the 95 pe we found 0 problems in the error rate was less than 4 percent .

in addition , we interviewed cognizant officials , including the program's test lead and the navy's independent operational testers , about their roles and responsibilities for test management .

of these 80 defects to determine whether of control were in place to ensure that any changes to is sample was designed with a 5 rcent level of confidence , so that , if our sample , we could conclude statistically that to determine if navy erp changes are being effectively controlled , we reviewed relevant program documentation , such as the change control policies , plans , and procedures , and compared them with relevant federal and industry guidance .

further , to determine the extent to which the program is reviewing and approving change requests according to its documented plans and procedures , we inspected 60 randomly selected change requests in the program's configuration management system .

in addition , we reviewed the change request forms associated with these 60 change requests and related control board meeting minutes to determine whether objective criteria for considering a proposed change , including estimated cost or schedule impacts , were adopted .

in addition , we interviewed cognizant officials , including the program manager and systems engineer , about their roles and responsibilities for reviewing , approving , and tracking change requests .

to determine if iv&v activities are being effectively managed we revie navy erp's iv&v contract , strategy , and plans and compared them with relevant industry guidance .

we also analyzed the contractual relationships relative to legal standards that govern organizational conflict of interest .

in addition , w assessment report , and a quarterly report , to determine the extent to which contract requirements were met .

we interviewed contractor a nd program officials about their roles and responsibilities for iv&v and to determine the extent to which the program's iv&v function is independent .

e examined iv&v monthly status reports , work plans , an we conducted this performance audit at department of defense offices in the washington , d.c. , metropolitan area ; annapolis , maryland ; and norfolk , virginia ; from august 2008 to september 2009 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the individual named above , key contributors to this report were neelaxi lakhmani , assistant director ; monica anatalio ; carl barden ; neil doherty ; cheryl dottermusch ; lee mccracken ; karl seifert ; adam vodraska ; shaunyce wallace ; and jeffrey woodward .

