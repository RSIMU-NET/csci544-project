medicaid section 1115 demonstrations , which allow states to test and evaluate new approaches for delivering medicaid services , have become a significant feature of the medicaid program , increasing both in number and cost over the years and affecting millions of beneficiaries .

in november 2016 , nearly three - quarters of states operated at least part of their medicaid program under section 1115 demonstrations , and , in fiscal year 2015 , federal demonstration expenditures amounted to $109 billion or about one - third of medicaid program expenditures that year .

under section 1115 of the social security act , the secretary of health and human services may waive certain federal medicaid requirements and approve new types of expenditures that would not otherwise be eligible for federal medicaid matching funds for experimental , pilot , or demonstration projects that , in the secretary's judgment , are likely to promote medicaid objectives .

for example , the centers for medicare & medicaid services ( cms ) , the agency within the department of health and human services ( hhs ) that oversees the medicaid program , has approved states' proposals to extend medicaid coverage under demonstrations to populations or for services that would not otherwise be covered under medicaid .

cms has also allowed states to use medicaid funds to finance costs that would not otherwise be eligible for federal funds , such as incentive payments to providers to improve access to and quality of care .

because medicaid section 1115 demonstrations ( hereafter referred to as demonstrations ) are intended to test new approaches to providing coverage and delivering medicaid services , evaluations of the demonstrations are essential to determining whether the new approaches are having their intended effect .

evaluations are also critical to ensuring that information on the effects of demonstrations , such as on beneficiary access to care , quality of care , and costs of care is available to inform federal and state policy decisions about new approaches to coverage and care .

further , because demonstrations allow states to use medicaid funds for costs that would not otherwise be covered under the program , evaluations serve as an important check of whether such funds are achieving federal medicaid objectives .

cms has long required states to conduct evaluations of demonstrations .

in addition , cms has initiated its own federal evaluations of selected medicaid demonstrations .

given continued state interest in undertaking medicaid section 1115 demonstrations and their budgetary significance and programmatic scope , you asked us to examine evaluations of demonstrations , including how the results have been used to inform medicaid policy .

this report examines: 1. state - led evaluations of demonstrations ; and 2. federal evaluations of demonstrations led by cms .

to examine state - led evaluations of demonstrations , we reviewed documentation for demonstrations in eight states — arizona , arkansas , california , indiana , kansas , maryland , massachusetts , and new york .

we selected these states by first identifying the 15 states with the highest average demonstration expenditures for fiscal years 2013 through 2015 — the most current , complete years of data available at the time we began our work .

from those , we selected eight states to achieve variation with regard to ( 1 ) total spending on the demonstrations , including as a percent of the state's total medicaid spending , ( 2 ) the number of years the state's most comprehensive demonstration had been in place , and ( 3 ) geography .

together , demonstration spending in the eight states accounted for about 47 percent of total demonstration spending for fiscal year 2015 .

 ( see appendix i for more information on the characteristics of the demonstrations in our selected states. ) .

for each state - led demonstration , we reviewed the following ( 1 ) evaluation requirements delineated in the contract negotiated between cms and the state — referred to as the special terms and conditions ( stc ) , ( 2 ) evaluation design plans submitted by the state , and ( 3 ) evaluation reports submitted by the state , including any stated limitations or gaps in evaluation findings .

for seven of our eight states — those which had completed more than one demonstration cycle — we reviewed the documentation for the most recently completed and current demonstration cycles as of the time of our review .

for kansas , which was in its first demonstration cycle at the time of our review , we reviewed the evaluation documentation for this cycle .

we also reviewed , when available , documentation of cms's review of design plans and reports .

we supplemented the documentation review by interviewing cms officials about the agency's policies and procedures for overseeing state - led evaluations , including recent and planned changes in the agency's policies and procedures and the agency's use of evaluation findings in decision making .

we also interviewed state medicaid officials ( in five of our eight selected states ) to gain an understanding of the design and implementation of their evaluations and their interactions with cms during the evaluation process .

in evaluating this information , we compared cms's policies and procedures against standards for internal control in the federal government , including those related to control activities and communication , and the american evaluation association's recommendations for evaluations of federal programs , which include recommendations related to the scope , quality , and transparency of evaluations .

to examine federal evaluations of demonstrations led by cms , we reviewed documents in the contract files for the two contract task orders ( hereafter referred to as contracts ) that cms awarded in 2014 and 2015 to conduct the agency's ongoing federal evaluations of demonstrations .

the options for these contracts were exercised annually and work was ongoing as of november 2017 .

the documentation we reviewed included the contract scopes of work that define the purposes of the contract , the timeframes for execution , and the expected products , or “deliverables ; ” monthly contractor progress reports ; evaluation design documents ; and other contract deliverables , including any reports of findings submitted as of october 2017 .

we reviewed the documents to assess the progress of the evaluations , including identifying any challenges encountered .

we also interviewed cms officials and one of cms's contractors about the progress and status of the federal evaluations and about the agency's policies and procedures for conducting federal evaluations , including policies for identifying demonstrations for federal evaluation and for making evaluation results public .

we compared cms's policies and procedures against the american evaluation association's recommendations for evaluations of federal programs .

we conducted this performance audit from november 2016 to january 2018 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

nearly three - quarters of states ( 37 as of november 2016 ) have cms - approved medicaid section 1115 demonstrations , which allow states to test new approaches to coverage and to improve quality and access or generate savings or efficiencies .

cms has approved demonstrations for a wide variety of purposes .

for example , under demonstrations , states have extended coverage to populations or for services not otherwise eligible for medicaid , made payments to providers to incentivize delivery system improvements , and , more recently , expanded medicaid to certain low - income adults by using medicaid funds to purchase private health insurance coverage .

while state demonstrations vary in size and scope , many are comprehensive in nature , affecting multiple aspects of states' medicaid programs simultaneously .

for example , kansas's demonstration , approved in 2012 , significantly expands the use of managed care to deliver physical , behavioral , and long - term care services to almost all the state's medicaid populations , care that for some populations was previously provided on a fee - for - service basis .

the demonstration also established a funding pool of up to $344 million to provide payments to hospitals to finance uncompensated care .

kansas's demonstration expenditures accounted for about 94 percent of the state's total medicaid expenditures in fiscal year 2015 .

in fiscal year 2015 , federal spending under demonstrations represented a third of all medicaid spending nationwide .

in 10 states , federal spending on demonstrations represented 75 percent or more of all federal spending on medicaid .

 ( see fig .

1. ) .

demonstrations are typically approved by cms for an initial 5-year period ( referred to as a demonstration cycle ) , but some states have operated portions of their medicaid programs under a demonstration for decades .

this can be achieved through a series of renewals approved by cms , generally occurring every 3 to 5 years .

what a state is testing and implementing under its demonstration can change from one cycle to the next .

states often make changes to their demonstrations , either through the renewal process or by requesting an amendment during the demonstration cycle .

these changes can be relatively small or can be significant and can represent testing of a new approach for the state .

for example , at renewal a state could request approval to expand coverage to a new population or add requirements that beneficiaries share in the cost of care by paying a monthly premium .

cms has long required states to conduct evaluations of section 1115 demonstrations .

cms oversees the evaluations and can influence them at several key points during the demonstration process .

application review and approval: when a state applies for a demonstration , cms reviews the state's application , which describes the goals and objectives of the demonstration and what the demonstration will test , among other things .

as part of the review and approval process , cms negotiates with the state on the stcs , including evaluation requirements .

these requirements might include , for example , reporting timeframes and broad standards for the evaluation , such as standards around the independence of the evaluator and acceptable evaluation methods .

evaluation design phase: after a demonstration is approved , states are required to submit an evaluation design to cms for review and approval .

the evaluation design must discuss , among other things , the hypotheses that will be tested , the data that will be used , and how the effects of the demonstration will be isolated from other changes occurring in the state .

during review of the design , cms can seek adjustments such as requiring the state to address certain objectives or using particular performance measures .

demonstration renewal: in the event that a state wishes to renew its demonstration , it must generally submit an application to cms at least 1 year before the demonstration is scheduled to expire .

the application must include , among other things , a report presenting the evaluation's findings to date , referred to as an interim evaluation report .

cms can use the information from the interim evaluation report to negotiate changes in the stcs for the evaluation of the next demonstration cycle .

if cms renews the demonstration , the evaluation process starts over with the state submitting a new evaluation design that reflects changes in what is being tested in the new cycle .

demonstration end: cms requires states to submit a final evaluation report for review and approval generally after the end of the demonstration , at which time the agency can work with the state to , for example , add clarity and disclose the limitations of the evaluation before the final evaluation report is made public .

within the framework that cms has established for state - led evaluations , states design evaluations to the specifics of their demonstrations .

as the size and scope of demonstrations varies considerably across states , so , too can evaluations vary in their breadth and complexity .

state - led evaluations may assess the effects of several different policies , each with its own set of hypotheses — predictions of the effects of the policy — and methods .

for example , a state could evaluate the effects of moving to a managed care delivery model for providing managed long - term services and supports ( referred to as mltss ) , implementing provider payment pools aimed at delivery system reform , and expanding coverage to a new population all within the same demonstration .

each of those three elements would have its own hypotheses and methods and may have varying timeframes for the number of years of experience needed to be able to effectively measure the effects of what is being tested .

cms has the authority to initiate its own federal evaluations of section 1115 demonstrations , and states must fully cooperate with any such evaluations .

between 2014 and 2016 , cms initiated three federal evaluations that were ongoing as of november 2017 .

the first evaluation , initiated in 2014 , is a large , multi - state evaluation examining four broad demonstration types in several states .

 ( see table 1. ) .

according to cms , it selected these demonstration types — which together account for tens of billions of dollars in federal and state medicaid spending — because they included policies that the agency considered priority areas for evaluation .

cms awarded a contract to an evaluation organization to implement the 5-year study .

according to cms , the estimated total cost of this evaluation for the 5-year life of the contract is $8.3 million .

the evaluation was designed to produce three sets of results: a series of reports providing contextual information about the demonstrations being evaluated , referred to as rapid cycle reports ; interim evaluation reports featuring early results of more in - depth analysis ; and final evaluation reports .

cms contracted with another evaluation organization to conduct two federal evaluations examining demonstrations in single states — indiana and montana — over 4 years .

as of september 2017 , the estimated cost of this contract , inclusive of all options , was $8.2 million .

in total , spending for indiana's and montana's demonstrations was about $2 billion in fiscal year 2015 , including $1.6 billion in federal spending .

indiana: cms initiated this evaluation in 2015 .

cms officials told us they started this evaluation to better understand how policies in indiana's demonstration , many of which were unprecedented , were affecting beneficiaries .

these policies included , for example , charging monthly contributions for most newly eligible adults with incomes from 0 to 138 percent of the federal poverty level ; imposing a lock - out period of 6 months for nonpayment of premiums for most people with incomes above the federal poverty level ; and charging co - payments above statutory levels for non - urgent use of emergency room services .

the federal evaluation is aimed at estimating the effects of indiana's demonstration on health insurance coverage and access to and use of care , and documenting beneficiary understanding of enrollment , disenrollment , and copayment policies , among other things .

montana: cms initiated this evaluation in 2016 .

cms officials told us they started this evaluation to provide a point of comparison to indiana's demonstration , as montana was implementing similar policies to indiana but with some variations .

for example , under montana's demonstration , the state charges premiums to most newly eligible adults with incomes between 51 and 138 percent of the federal poverty level ; and disenrolls beneficiaries with incomes above the federal poverty level for nonpayment of premiums , with reenrollment when overdue premiums are paid .

similar to the federal evaluation of indiana's demonstration , the evaluation of montana's demonstration is aimed at estimating the effects of the demonstration on insurance coverage , access to and use of care , and documenting beneficiary understanding of and experience with premiums , copayments , enrollment , and disenrollment , among other things .

state - led evaluations of demonstrations in selected states often had significant methodological weaknesses and gaps in results that affected their usefulness for federal decision - making .

though cms has been taking steps since 2014 to improve the quality of these evaluations , the agency has not established written procedures to help implement some of these improvements .

the state - led evaluations we reviewed in our selected states often had methodological limitations that affected what could be concluded about the demonstration's effects .

cms hired a contractor to review state evaluation designs and reports , and that contractor identified a number of methodological concerns with the evaluations in our selected states .

for example , cms's contractor raised concerns about the comparison groups , or lack thereof , used to isolate and measure the effects of the demonstrations in the arkansas , california , indiana , and maryland evaluations .

the contractor also raised concerns with the sufficiency of sample sizes and survey response rates for beneficiary surveys in indiana .

these surveys were key methods for assessing the effect of demonstrations on access , beneficiary understanding , and perceptions on affordability .

finally , the contractor raised concerns with the analysis of the effects of the demonstration on cost in arkansas , california , and maryland .

officials in several states told us that some of the methodological limitations in their evaluations were difficult to control .

for example , officials in two states told us that isolating the effects of the demonstration was difficult given other changes happening in the state's health care system at the same time .

some state officials also noted that state resources , including both funding and staff capacity , present challenges in completing robust evaluations .

program , with approved funding up to about $690 million .

under the demonstration stcs , the state was required to evaluate whether the seven hospitals participating in the dsrip were able to show improvements on certain outcome measures related to improving quality of care , improving population health and access to care , and reducing the per capita costs of health care .

however , the evaluation report , submitted by the state 5 years after approval of the dsrip program , provided only descriptive or summary information about the number and types of projects implemented by the hospitals receiving payments and did not provide any data to measure or conclusions on the effects of those payments .

arkansas: under its demonstration , the state was testing the effects of using medicaid funds to provide premium assistance for the more than 200,000 beneficiaries newly eligible under ppaca to purchase private insurance offered through the state's health insurance exchange .

the state's evaluation was designed to assess whether beneficiaries would have equal or better access to care and equal or better outcomes than they would have had in the medicaid fee - for - service system .

the evaluation was also aimed at examining continuity of coverage for beneficiaries , as the expansion population was anticipated to have frequent income fluctuations leading to changes in eligibility and gaps in coverage .

however , evaluation results submitted over two and a half years into the demonstration — the only results submitted for the state's first cycle — were limited to data only from the first year of the demonstration and did not provide data on continuity of coverage .

achieving continuity of coverage was part of the state's rationale for using an alternative approach to medicaid expansion .

arizona: among other things , arizona's demonstration includes mltss , including for the particularly complex populations of adults who have intellectual and developmental disabilities and for children with disabilities .

as part of its evaluation , the state was assessing whether the quality of and access to care , as well as quality of life , would improve during the demonstration period for long - term care beneficiaries enrolled in mltss .

however , evaluation results submitted in october 2016 — the only results submitted for the state's most recently completed demonstration cycle — lacked data on key measures of access , such as hospital readmission rates , and on quality of life , such as beneficiaries' satisfaction with their health plan , provider , and case manager .

a key contributor to the gaps in the information included in the state - led evaluations we reviewed was that cms historically had not required the states to submit final , comprehensive evaluation results at the end of each demonstration cycle .

as a result , for our selected states , including those discussed above , cms had received only interim evaluation reports that were generally based on more limited data from the early years of the demonstration cycle and did not include all of the analyses planned .

though cms had required final evaluation reports in the demonstration stcs , the due dates for those reports were tied to the expiration of the demonstrations or , in one case , cms did not enforce the specified due date .

under such conditions , due dates for final evaluation reports were effectively pushed out when the demonstrations were renewed .

evaluation due dates could be pushed out for multiple cycles .

cms officials acknowledged that the lack of data in the interim evaluation reports from the more mature years of the demonstration affected the conclusions that could be drawn from them .

we found that due dates for final evaluation reports were pushed out upon renewal in all seven of our states that had completed a demonstration cycle , leading to a gap in evaluation reporting of up to 6 or 7 years for several states .

in maryland , for example , cms approved the demonstration to run from 2013 to 2016 with a final evaluation report due 120 days after the expiration of the demonstration .

in 2016 , cms extended the demonstration , pushing the deadline for the final evaluation report to 18 months following the end of the new cycle , or june 2023 .

at that time , it will be 7 years since the interim evaluation report was submitted .

see figure 2 .

the limitations in state - led evaluations — including methodological weaknesses and gaps in results — have , in part , hindered cms's use of them to inform its policy decisions .

cms officials told us that , historically , state - led evaluations have generally provided descriptive information but lacked evidence on outcomes and impacts .

as a result , officials noted that they consider the data reported in the evaluations but , generally , state - led evaluations have not been particularly informative to their policy decisions .

cms officials told us that there have been cases where data , but not the conclusions , from state - led evaluations have informed their thinking on certain policy changes .

for example , cms officials said that data reported in early evaluations of dsrip programs helped them in considering whether and how the agency should modify the basic policy structure of these programs .

state officials had mixed perspectives on whether state - led evaluations influenced cms decision - making around renewing their demonstrations .

officials in one state told us that while cms reviewed their interim evaluation results , the results did not appear to influence the negotiations around the demonstration renewal .

in contrast , officials from another state told us that discussion of interim evaluation results and limitations was a significant part of negotiations in 2016 regarding whether cms would be willing to reauthorize funding for certain programs , including a new dsrip investment and broader delivery system reforms the state was trying to implement .

officials in several states told us that there was value to state - led evaluations and in the federal - state partnership in designing the evaluations .

cms has implemented several procedures since 2014 aimed at improving the quality of state - led evaluations .

cms officials told us that these changes were part of cms placing increased focus on monitoring and evaluation , which also resulted in cms establishing a new office in 2015 that is responsible for these activities .

one of the key changes cms began implementing in 2014 was to set more explicit requirements for evaluations in the stcs , including requirements to improve the evaluation methodologies .

according to cms officials , the agency realized that one reason why state - led evaluations had generally lacked rigor and been of limited usefulness was that cms had not been setting clear expectations for evaluations in the stcs .

the officials said that cms began strengthening evaluation requirements starting in 2014 with demonstrations implementing approaches in cms's high priority policy areas .

in our review of the stcs for current demonstration cycles in our seven selected states that had completed a demonstration cycle , all of which were approved in 2014 or later , we found evidence of cms's efforts .

specifically , we found an increased focus on the use of independent evaluators and more explicit expectations for rigor in the design and conduct of evaluations: consistent requirements for independent evaluators .

the stcs for the most recently approved cycle of demonstrations in all seven states required the state to use an independent evaluator to conduct the evaluation .

in some cases , the stcs also required that the evaluation design discuss the process to acquire the independent evaluator , including describing the contractor's qualifications and how the state will assure no conflict of interest .

these requirements were new in most states .

more explicit expectations for rigor .

in four of the seven states we reviewed , the stcs for the most recently approved cycle of states' demonstrations included new , explicit language requiring state evaluations to meet the prevailing standards of scientific and academic rigor .

these included standards for the evaluation design and conduct as well as the interpretation and reporting of findings .

some states' stcs further specified the characteristics of rigor that cms expected , including using the best available data , discussing the generalizability of results , and using controls and adjustments for and reporting the limitations of data and their effects on results .

according to cms , in the past , states have not always discussed methodological limitations in their evaluation reports .

in addition to strengthening evaluation requirements , cms has also taken steps since 2014 to enhance its oversight during the design and early stages of state - led evaluations , and , according to officials , some of these steps are likely to improve the usefulness of evaluations .

specifically , cms has provided technical assistance to help states design their evaluations , sometimes leveraging expertise from other parts of hhs , including the hhs office of the assistant secretary for planning and evaluation and the center for medicare & medicaid innovation as well as outside contractors .

for example , officials stated that the agency assists states in developing relevant and standardized measures and provides assistance to help address states' data limitations .

officials said this has resulted in more robust evaluation designs with increased potential to isolate outcomes and impacts .

cms has also used contractors to help in its review of state evaluation designs , including sampling designs , and evaluation reports .

since 2014 , one contractor has provided over 30 assessments of evaluation designs and findings in at least 11 states .

according to officials , this has increased cms's capacity to identify methodological weaknesses and negotiate changes with states to improve the usefulness of evaluations .

for example , cms's contractor reviewed four draft survey instruments that indiana planned to use in its evaluation , providing comments on the sampling frames and the structure and organization of survey questions .

in response to the contractor's feedback , indiana made changes to the surveys to gather more reliable information and improve their readability .

finally , cms has begun making changes to how it sets due dates for final evaluation reports .

cms officials told us that in spring 2017 , cms began requiring states to submit a comprehensive evaluation report for demonstrations in its high priority policy areas for evaluation at the end of each demonstration cycle , rather than after the expiration of the demonstration .

cms's recent demonstration renewals in florida and missouri — approved in august and september of 2017 , respectively — required a final , summative evaluation report at the end of the demonstration cycle , consistent with the policy .

in october 2017 , cms officials stated that the agency was expanding this policy and was now planning to require final reports at the end of each cycle for all demonstrations , as they are approved or renewed .

however , cms had not established written procedures for implementing this new policy .

it is too soon to assess the effectiveness of cms's recent efforts to strengthen state - led evaluations .

cms has been implementing the strategies on a rolling basis as states apply for demonstration renewals and new demonstrations .

if implemented and enforced consistently , cms's efforts to improve the quality of state - led evaluations have the potential to result in more conclusive evaluations .

further , cms's efforts to improve the quality of state - led evaluations and its plan to require final reports after each demonstration cycle are consistent with evaluation guidance from the american evaluation association that recommends that federal agencies conduct evaluations of public programs and policies throughout the programs' life cycles , not just at their end , and that agencies use evaluations to improve programs and assess their effectiveness .

federal internal control standards also state that management should implement control activities through policies .

however , cms does not have written procedures for implementing its planned policy , for example , for ensuring that the requirement is included in the stcs for all demonstrations , despite unique negotiations with each state , and that those requirements are consistently enforced .

as a result , some state - led evaluations could continue to produce only more limited , interim findings that leave critical questions about the effects of the these demonstrations on beneficiaries and costs unanswered .

cms oversight of state - led evaluations may see further changes , as cms officials told us that their oversight procedures are still evolving .

for example , cms officials told us that as of october 2017 the agency plans to begin to make distinctions in the level of evaluation required across demonstrations .

they said that they are considering , for example , whether longstanding and largely unchanged components of a demonstration , and approaches previously tested by a number of other states without concern , require the same level of evaluation as testing a new approach to medicaid expansion .

officials said that they plan to include language in demonstration stcs , as the agency did in the recent renewals for florida and missouri , instructing the state to consider those factors as the state designs its evaluation .

specifically , in the evaluation design submitted for cms approval , the state should include in the discussion of limitations whether the demonstration is long - standing , noncomplex , has previously been rigorously evaluated and found to be successful , or is also considered to be successful without issues or concerns .

cms officials said that the expected level of rigor for the evaluation could be balanced against such factors .

the implications of limiting evaluation requirements for certain types of demonstration approaches would depend on cms's definitions of what is , for example , noncomplex or has previously been rigorously evaluated .

as of october 2017 , cms had not established specific criteria for determining when a demonstration component would require less rigorous evaluation .

agency officials told us they were planning to develop such criteria after concluding a pilot of alternative criteria and expectations in certain demonstrations related to providing services for family planning and former foster care children .

they said that when these pilots have concluded they will evaluate the results .

it is unclear how these narrowly scoped demonstrations — scoped for a particular type of service or population — can be used to inform criteria for comprehensive demonstrations that can affect a state's entire medicaid population and all services .

further , though cms has begun indicating to states , including those with comprehensive demonstrations , that the agency may allow less rigorous evaluations for certain types of demonstration approaches , cms has not established timeframes for issuing the criteria defining those conditions .

federal standards for internal control stress that management should implement control activities through policy and should internally and externally communicate necessary information to achieve the agency's objectives .

if cms does not establish clear criteria for components of demonstrations that require limited evaluation , characteristics such as “long - standing” or “noncomplex” could be broadly interpreted .

this could result in demonstrations that receive significant amounts of federal funds and affect many beneficiaries not being thoroughly evaluated .

written criteria could also reduce the potential for inconsistencies in the level of evaluation required across demonstrations .

data and other challenges have significantly limited the scope and progress of cms's large , multi - state evaluation and the agency's evaluation of indiana's demonstration .

further , cms has not released available evaluation results from the multi - state evaluation nor set timeframes for making these and future federal evaluation findings public .

cms encountered numerous data challenges in its multi - state evaluation that significantly reduced the scope of the analyses planned .

these data challenges included limitations in the quality of cms data and delays obtaining data directly from states .

these limitations caused cms to narrow the evaluation's scope , often by reducing the number of state demonstrations evaluated or limiting what was being examined .

all four demonstration types targeted in the multi - state evaluation — which reflect cms's high priority policy areas — were affected by these challenges .

in the most extreme case , data limitations reduced the scope of the mltss evaluation to two states out of the more than 20 states operating such programs .

as a result , the evaluation findings will not be generalizable to all mltss programs .

 ( see table 2. ) .

the data challenges were in addition to other challenges that affected the evaluation .

for example , there were difficulties in trying to isolate demonstration effects in the context of rapidly changing health systems , or recent demonstrations had not been in operation long enough to allow cms to appropriately assess longer - term effects .

many of the data challenges cms encountered in the multi - state evaluation reflect long - standing concerns with the lack of accurate , complete , and timely medicaid data .

specifically , we and others have found that data states are required to submit to cms have , at times , been incomplete or have not been reported at all , particularly managed care encounter data .

complicating the availability of these data is cms's ongoing transition to a new data system , the transformed medicaid statistical information system ( t - msis ) , which is cms's primary effort to improve medicaid expenditure and utilization data .

states' transitions to t - msis , however , have introduced substantial delays in state data submissions .

for example , by 2015 , a large number of states had stopped submitting data through the legacy information system until they established t - msis submissions , which meant cms had to obtain data directly from individual states for the multi - state evaluation .

new data challenges have also emerged as states under demonstrations have enrolled newly eligible beneficiaries in health insurance exchange coverage .

lack of accessible data on beneficiaries enrolled in plans offered through the exchange resulted in the delays in obtaining data for arkansas for the multi - state evaluation .

in the past , we have made recommendations to cms to take action to improve the data available for medicaid program oversight , including to t - msis .

as with the multi - state evaluation , data challenges , particularly obtaining needed data from the state , also proved to be a significant hurdle in cms's evaluation of indiana's demonstration .

cms initiated its federal evaluation of indiana's demonstration in 2015 to understand how the approaches being tested in indiana's demonstration affected beneficiaries ( see sidebar ) .

however , in 2016 , indiana raised concerns about sharing enrollee data with cms's evaluation contractors .

specifically , in a letter to cms , the state cited concerns about the controls that cms had in place to ensure that its contractors would protect enrollee information consistent with state and federal privacy protections .

despite assurances by cms , cms's contractor and the state were not able to execute a data use agreement .

this effectively halted the evaluation's progress .

the data use agreement was necessary for the contractor to access state enrollment data that drove a number of planned evaluation activities , including a key beneficiary survey .

in october 2017 , cms officials told us that they were continuing to work with the state and anticipated that a data use agreement would be executed and the federal evaluation of indiana's demonstration would proceed .

they did not have timeframes for when the agreement would be reached .

despite the data challenges and delays , cms's evaluations of medicaid demonstrations , as planned , are likely to provide new information on the effects of demonstrations in different states to inform policy decisions .

the multi - state evaluation , for example , is expected to provide information on whether living in a state that collects monthly contributions from beneficiaries affects the likelihood of beneficiaries enrolling in medicaid and how per - beneficiary spending differs between premium assistance demonstration states and states that have implemented more traditional medicaid expansions .

cms officials emphasized that federal evaluations allow for cross - state evaluations that can be used to validate the findings of related studies and also to identify which findings are generalizable to other states and populations .

cms has yet to make initial reports from the multi - state evaluation publicly available , limiting the potential use of those findings by states and other federal policymakers .

as of october 2017 , cms's contractor had produced 15 rapid cycle reports on states' progress in implementing demonstrations in the high priority policy areas .

these reports provide information on states' implementation of their demonstrations and variations in design and provide details that can help with the interpretation of evaluation results , inform federal policymaking , and provide lessons learned to states and other stakeholders .

the reports also describe policy and other challenges states encountered in implementing their programs , which could be useful to other states interested in replicating these models .

 ( see table 3. ) .

however , despite having received some of these reports from its contractor in 2015 , cms had not released these findings as of october 2017 .

cms officials said that the reports were still under agency review and acknowledged that since some of the rapid cycle reports were almost 2 years old , cms's contractor was reviewing and updating the information in them .

cms officials noted that the rapid cycle reports had provided useful information and had influenced ongoing work with states designing related demonstrations .

for example , according to officials , findings from the rapid cycle reports played a part in how the agency structured the latest dsrip demonstrations .

they also said that rapid cycle reports on beneficiary engagement have shed light on the effectiveness of different beneficiary education strategies , such as what approaches are more successful in capturing beneficiaries' attention and what strategies are easiest for states to implement .

in october 2017 , cms officials stated that they had recently decided to make the rapid cycle reports public , although the agency's clearance process for the reports was still being decided and the officials did not have timeframes for the reports' release .

it is also uncertain when cms will make interim and final evaluation reports from the multi - state evaluation public .

by september 2017 , cms's contractor for the multi - state evaluation produced three interim evaluation reports covering the four demonstration types .

cms officials regard these as draft interim evaluation reports , and , as of october 2017 , said they were under agency review and would not be publicly released .

cms expects the contractor to submit final interim evaluation reports , which are anticipated to include some additional information beyond the draft reports , by september 2018 , about 1 year later than when the final interim evaluation reports were originally due .

cms officials said that the agency planned to release the final interim evaluation reports , although there was no specific timetable for this .

timeframes for the completion and release of final evaluation results are even more uncertain , both because of the delays in the evaluation progress and because cms has no standard policy for timeframes for releasing evaluation results .

it is also uncertain when evaluation results will be available and made public for cms's evaluations of the indiana and montana demonstrations .

two years after the approval of the contract for the indiana evaluation , cms's contractor has produced an evaluation design but no evaluation findings .

cms had not posted the evaluation design on its website until november 2017 , according to officials , about 1 year after it was originally submitted .

as discussed above , the lack of findings is due to the contractor and state not having negotiated a data use agreement .

to the extent that indiana's evaluation moves forward and evaluation reports are produced , cms officials said the agency plans to release the final evaluation report but did not indicate whether interim findings , available a year earlier , would be released .

with regard to the montana evaluation , cms expects to receive the interim evaluation report by september 2018 and the final evaluation report by september 2019 .

how soon these findings would be publicly available , however , is difficult to estimate , as cms officials told us the agency must review these before making them publically available and does not have timeframes for this review .

the lack of a standard policy for the public release of findings from federal evaluations of medicaid demonstrations is inconsistent with recommendations of the american evaluation association .

the association recommends that evaluation findings related to public accountability be disseminated to the public , and that evaluation results be made available in a timely manner and be easily accessible through the internet .

for state - led evaluations , cms must post on its website , or provide a link to the state's website , all evaluation materials , including research and data collection , for the purposes of sharing findings with the public within 30 days of receiving the materials .

cms has not established a comparable policy for the release of findings from federal evaluations of demonstrations .

cms officials stated that federal evaluations provide a unique cross - state perspective that states typically do not have the capacity to provide in their own state - led evaluations ; however , if these reports are not made public in a timely fashion , opportunities may be missed to inform federal and state policymakers and other stakeholders on the effects of medicaid demonstrations .

section 1115 demonstrations have long been an important tool for providing states with the flexibility to test new approaches to providing and financing medicaid coverage .

given the potential effects on millions of beneficiaries and significant federal investment in these demonstrations — over $100 billion in 2015 — it is critical that they be evaluated .

evaluating medicaid demonstrations is complex , both within a single state and across states .

these programs are dynamic , and there are many factors affecting outcomes , making it challenging to isolate the effects of policy changes implemented under a demonstration .

further , persistent challenges with medicaid data that we have highlighted over the years add to the complexity of evaluating demonstrations .

despite these challenges , targeted and well - designed evaluations offer the potential to identify policies that improve outcomes for beneficiaries and reduce costs to medicaid .

with the growing complexity of medicaid programs and limited resources , that information could prove key in helping to sustain the program .

cms's approach to overseeing state - led evaluations in the past has resulted in limited information about the effects of demonstrations , leaving gaps in evidence about policies that might improve state medicaid programs .

cms's efforts since 2014 to improve the usefulness of evaluations in informing state and federal medicaid policy decisions have promise .

if cms consistently sets and enforces clear expectations and provides support for rigorous and timely state - led evaluations for all demonstrations as planned , those evaluations could yield more useful information within the next several years .

however , cms has not established written procedures for requiring final , comprehensive evaluation reports at the end of each cycle for all demonstrations , a key step in improving the usefulness of state - led evaluations .

further , cms is planning to allow less rigorous evaluations for some demonstrations but has not yet established specific criteria for doing so .

federal evaluations led by cms also show promise .

the evaluations currently underway — despite challenges that caused delays and reduced scope — are likely to provide a cross - state look at the effects of policies that are of great interest to cms , congress , and other states .

however , cms has not yet made potentially useful rapid cycle reports public and has no established policy for making future evaluation reports public .

by not making the results of the federal evaluations public in a timely manner , cms is missing an opportunity to inform important policy discussions happening at the state and federal levels .

we are making the following three recommendations to cms: the administrator of cms should establish written procedures for implementing the agency's policy that requires all states to submit a final evaluation report after the end of each demonstration cycle , regardless of renewal status .

 ( recommendation 1 ) the administrator of cms should issue written criteria for when cms will allow limited evaluation of a demonstration or a portion of a demonstration , including defining conditions , such as what it means for a demonstration to be longstanding or noncomplex , as applicable .

 ( recommendation 2 ) the administrator of cms should establish and implement a policy for publicly releasing findings from federal evaluations of demonstrations , including findings from rapid cycle , interim , and final reports ; and this policy should include standards for timely release .

 ( recommendation 3 ) .

we provided a draft of this report to hhs for review and comment .

hhs concurred with all three recommendations .

regarding our first recommendation that cms establish written procedures for implementing its policy requiring states to submit final evaluation reports after the end of each demonstration cycle , hhs said that it is in the process of developing such written procedures .

hhs said that it is currently making this a requirement through the stcs for each demonstration as demonstrations are approved or renewed .

regarding our second recommendation that cms issue written criteria for when the agency will allow states to limit evaluations of their demonstrations , hhs said it is in the process of testing such criteria , and that once it has experience with the criteria , it will develop written guidance .

regarding our third recommendation that cms establish and implement a policy for publicly releasing findings from federal evaluations of demonstrations , hhs said that cms is in the process of establishing such a policy .

hhs added that cms plans to have all finalized federal rapid cycle reports and final interim evaluation reports publicly available in the near future .

hhs also provided technical comments , which we incorporated as appropriate .

hhs's comments are reproduced in appendix ii .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to the secretary of health and human services , appropriate congressional committees , and other interested parties .

the report will also be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff members have any questions about this report , please contact me at ( 202 ) 512-7114 or iritanik@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iii .

the medicaid section 1115 demonstrations ( referred to as demonstrations ) in our eight selected states varied in terms of the number of years the demonstrations had been in effect and cost , among other things .

for example , three of the more mature demonstrations — those in maryland , massachusetts , and new york — had been in place for two decades .

demonstrations in arkansas and kansas represented more recent approvals , both approved in 2013 .

 ( see table 4. ) .

with regard to cost , all of the selected states were among the top 15 states in terms of amount of spending under demonstrations .

together , spending under demonstrations in our selected states accounted for about 47 percent of all spending under demonstrations in fiscal year 2015 .

in addition to the contact named above , susan barnidge ( assistant director ) , linda mciver ( analyst - in - charge ) , john lalomio , hannah locke , and corissa kiyan - fukumoto made key contributions to this report .

also contributing were laurie pachter and emily wilson .

