in the wake of the 2000 and 2004 general elections , we issued a series of reports and testified on virtually every aspect of our nation's overall election system , including the many challenges and opportunities associated with various types of voting systems .

in this regard , we emphasized that voting systems alone were neither the sole contributor nor solution to the problems that were experienced during the 2000 and 2004 elections , and that the overall election system depended on the effective interplay of people , process , and technology and involved all levels of government .

among many things , we specifically reported in 2001 that no federal entity was responsible for accrediting the laboratories that tested voting systems , and we raised the establishment of such an entity as a matter for congressional consideration .

subsequently , congress passed the help america vote act ( hava ) , which created the election assistance commission ( eac ) and assigned both it and the national institute of standards and technology ( nist ) separate but related responsibilities for accrediting laboratories that test voting systems .

in general , nist is responsible for assessing a laboratory's technical qualifications and making an accreditation recommendation to eac , while eac is to use the assessment results and recommendation , along with its own review of related laboratory capabilities , to reach an accreditation decision .

in 2004 and 2007 , nist and eac established voting system testing laboratory accreditation programs , respectively .

to date , eac has accredited four laboratories .

in view of the continuing concerns about voting systems and the important roles that both nist and eac play in accrediting the laboratories that test these systems , you asked us to determine whether nist and eac have each defined an effective laboratory accreditation approach and whether each is following its defined approach .

to accomplish this , we reviewed nist and eac policies , guidelines , and procedures governing voting system testing laboratory accreditation , deaccreditation , and reaccreditation and compared them , as appropriate , to applicable statute , such as hava , and guidance published by nist , the international organization for standardization , and us .

we then compared nist and eac actions and artifacts that were used for accrediting four voting system testing laboratories to their respective policies , guidelines , and procedures .

we did not review a fifth laboratory because nist was in the process of assessing it when we started our review , and had yet to recommend the laboratory to eac for final accreditation .

in addition , we interviewed officials from nist , eac , and the four laboratories to understand and clarify approaches taken , documentation provided , and decisions reached .

we conducted this performance audit at eac and nist offices in washington , d.c. , and gaithersburg , maryland , respectively , from september 2007 to september 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objective , scope , and methodology are included in appendix i .

all levels of government share responsibility in the overall u.s. election system .

at the federal level , congress has authority under the constitution to regulate presidential and congressional elections and to enforce prohibitions against specific discriminatory practices in all federal , state , and local elections .

congress has passed legislation that addresses voter registration , absentee voting , accessibility provisions for the elderly and persons with disabilities , and prohibitions against discriminatory practices .

at the state level , individual states are responsible for the administration of both federal elections and their own elections .

states regulate the election process , including , for example , the adoption of voluntary voting system guidelines , the state certification and acceptance testing of voting systems , ballot access , registration procedures , absentee voting requirements , the establishment of voting places , the provision of election day workers , and the counting and certification of the vote .

in total , the overall u.s. election system can be seen as an assemblage of 55 distinct election systems — those of the 50 states , 4 u.s. territories , and the district of columbia .

further , although election policy and procedures are legislated primarily at the state level , states typically have decentralized election systems , so that the details of administering elections are carried out at the city or county levels , and voting is done at the local level .

as we reported in 2001 , local election jurisdictions number more than 10,000 , and their sizes vary enormously — from a rural county with about 200 voters to a large urban county , such as los angeles county , where the total number of registered voters for the 2000 elections exceeded the registered voter totals in 41 states .

further , these thousands of jurisdictions rely on many different types of voting methods that employ a wide range of voting system makes , models , and versions .

because of the prominent role played by electronic voting systems , testing these systems against national standards is critical to ensuring their security and reliability .

equally critical is ensuring that the laboratories that perform these tests are competent to carry out testing activities .

in the united states today , most votes are cast and counted by electronic voting systems , and many states require use of systems that have been certified nationally or by state authorities .

however , voting systems are but one facet of a multifaceted , continuous overall election system that involves the interplay of people , processes , and technology during the entire life of a system .

all levels of government , as well as commercial voting system manufacturers and system testing laboratories , play key roles in ensuring that voting systems perform as intended .

electronic voting systems are typically developed by manufacturers , then purchased as commercial , off - the - shelf products and operated by state and local election administrators .

viewed at a high level , these activities make up three phases of a system life cycle: product development , acquisition , and operations .

 ( see fig .

1. ) .

key processes that span these life cycle phases include managing the people , processes , and technologies within each phase and across phases , and testing the systems and components during and at the end of each phase .

additionally , voting system standards are important through all of the phases because they provide criteria for developing , testing , and acquiring voting systems , and they specify the necessary documentation for operating the systems .

the product development phase includes activities such as establishing requirements for the system , designing a system architecture , developing software , and integrating components .

activities in this phase are performed by the system vendor .

the acquisition phase includes activities such as publishing a solicitation , evaluating offers , choosing a voting technology and a vendor , and awarding and administering contracts .

for voting systems , activities in this phase are primarily the responsibility of state and local governments but entail some responsibilities that are shared with the system vendor ( eg , entering into the contract ) .

the operations phase consists of activities such as ballot design and programming , setup of systems before voting , pre - election testing , vote capture and counting during elections , recounts and system audits after elections , and storage of systems between elections .

responsibility for activities in this phase typically resides with local jurisdictions , whose officials may , in turn , rely on or obtain assistance from system vendors for aspects of these activities .

standards for voting systems , as will be discussed in a later section , were developed at the national level by the federal election commission in 1990 and 2002 and were updated by eac in 2005 .

in the product development phase , voting system standards serve as requirements to meet for developers to build systems .

in the acquisition phase , they also provide a framework that state and local governments can use to evaluate systems .

in the operations phase , they specify the necessary documentation for operating the systems .

testing processes are conducted throughout the life cycle of a voting system .

voting system vendors conduct product testing during development of the system and its components .

federal certification testing of products submitted by system vendors is conducted by national voting system testing laboratories ( vstl ) .

states may conduct evaluation testing before acquiring a system to determine how well products meet their state - specific specifications , or they may conduct certification testing to ensure that a system performs its functions as specified by state laws and requirements .

once a voting system is delivered by the system vendor , states and local jurisdictions may conduct acceptance testing to ensure that the system satisfies functional requirements .

finally , local jurisdictions typically conduct logic and accuracy tests related to each election and sometimes subject portions of the system to parallel testing during each election to ensure that the system components perform accurately .

management processes ensure that each life cycle phase produces a desirable outcome .

typical management activities that span the system life cycle include planning , configuration management , system performance review and evaluation , problem tracking and correction , human capital management , and user training .

these activities are conducted by the responsible parties in each life cycle phase .

in 2004 , we reported that the performance of electronic voting systems , like any type of automated information system , can be judged on several bases , including their security , accuracy , ease of use , efficiency , and cost .

we also reported that voting system performance depends on how the system was designed , developed , and implemented .

since the passage of hava , the use of electronic voting systems has increased and become the predominant method of voting .

however , concerns have been raised about the security and reliability of these systems .

as we have previously reported , testing and certifying voting systems is one critical step in acquiring , deploying , operating , and administering voting systems , which better ensures that they perform securely and reliably .

among other things , rigorous execution and careful documentation of system testing is a proven way to help ensure that system problems are found before the systems are deployed and used in an election .

to accomplish this , it is vital that the organizations that test the systems be qualified and competent to do so .

for voting systems , a key testing organization is a federally accredited , national vstl .

in general , accreditation is the formal recognition that a laboratory is competent to carry out specific types of tests or calibrations .

federally accredited laboratories perform many different types of testing and related activities on various products , ranging from inspecting grain to certifying maritime cargo gear .

the genesis of laboratory accreditation programs owes largely to agencies' need to assure themselves of the competency of the organizations responsible for testing products or services that involve the use of federal funds .

to provide national recognition for competent laboratories , the nist director established the national voluntary laboratory accreditation program ( nvlap ) in 1976 at the request of the private sector .

under this program , which is based on internationally accepted standards , nist accredits laboratories that it finds competent to perform specific types of tests or calibrations .

in june 2004 , nvlap announced the establishment , in accordance with hava , of an accreditation program for laboratories that test voting systems using standards determined by eac .

enacted in october 2002 , hava affected nearly every aspect of the voting process , from voting technology to provisional ballots and from voter registration to poll worker training .

in particular , the act authorized $3.86 billion in funding over several fiscal years to replace punch card and mechanical lever voting equipment , improve election administration and accessibility , train poll workers , and perform research and pilot studies .

hava also established eac , provided for the appointment of four commissioners , and specified the process for selecting an executive director .

generally speaking , eac is to assist in the administration of federal elections and provide assistance in administering certain federal election laws and programs .

since the passage of hava in 2002 , the federal government has taken steps to implement the act's provisions .

for example , after beginning operations in january 2004 , eac updated the existing federal voluntary standards for voting systems , including strengthening provisions related to security and reliability .

additionally , eac established an interim vstl accreditation program that leveraged a predecessor program run by the national association of state elections directors , and eac and nist then established companion accreditation programs that replaced the interim program .

federal standards for voting systems were first issued in 1990 when the federal election commission published standards .

these federal standards identified minimum functional and performance requirements , which states were free to adopt in whole , in part , or not at all , for electronic voting equipment , and specified test procedures to ensure that the equipment met those requirements .

in 2002 , the federal election commission issued its voting system standards ( vss ) , which updated the 1990 standards to reflect more modern voting system technologies .

in 2005 , we reported that these standards identified minimum functional and performance requirements for voting systems but were not sufficient to ensure secure and reliable voting systems .

as a result , we recommended that eac work to define specific tasks , measurable outcomes , milestones , and resource needs to improve the voting system standards .

until then , election administrators were at risk of relying on voting systems that were not developed , acquired , tested , operated , or managed in accordance with rigorous security and reliability standards — potentially affecting the reliability of future elections and voter confidence in the accuracy of the vote count .

following the enactment of hava in 2002 and the establishment of eac in 2004 , eac adopted the voluntary voting system guidelines ( vvsg ) in 2005 .

the vvsg specify the functional requirements , performance characteristics , documentation requirements , and test evaluation criteria for the national certification of voting systems .

accredited testing laboratories are to use the vvsg to develop test plans and procedures for the analysis and testing of systems in support of eac's voting system certification program .

the vvsg are also used by voting system manufacturers as the basis for designing and deploying systems that can be federally certified .

we reported in 2001 that the national association of state elections directors was accrediting independent test authorities to test voting equipment against the federal election commission standards .

under this program , three laboratories were accredited .

under hava , nist is to recommend laboratories for eac accreditation .

in 2006 , nist notified eac that its initial recommendations might not be available until sometime in 2007 .

as a result , eac initiated an interim accreditation program and invited the three laboratories accredited by the state elections directors to apply .

as part of the interim program , laboratories were required to attest to a set of eac - required conditions and practices , including certifying the integrity of personnel , the absence of conflicts of interest , and the financial stability of the laboratory .

in august and september 2006 , eac granted interim accreditation to two of the three laboratories invited to apply .

eac terminated its interim program in march 2007 .

hava assigned responsibilities for laboratory accreditation to both eac and nist .

in general , to reach an accreditation decision , nist is to focus on assessing laboratory technical qualifications , while eac is to use those assessment results and recommendations and augment them with its own review of related laboratory capabilities .

see table 1 for the two agencies' hava responsibilities .

the tasks that nist is to perform addressed in an annual interagency agreement executed between the institute and eac each year .

for example , the 2008 interagency agreem states that nvlap will continue to assess vstls and will coordinate with eac to continually monitor and review the performance of the laboratories .

additionally , the agreement states that the two agencies will coordinate to maintain continuity between their respective accreditation programs .

in meeting hava's requirements are the nist and eac accreditation programs can be viewed together as forming a federal vstl accreditation process that consists of a series o complementary steps .

these steps are depicted in figure 2 , where the numbers correspond to a detailed narrative description below .

as of may 2008 , eac has accredited four laboratories .

these laboratories are systest labs , llc ; wyle laboratories , inc. ; ibeta quality assurance ; and infogard laboratories , inc. a fifth laboratory , ciber inc. , has been granted nvlap accreditation and has been recommended to , but not yet accredited by , eac .

infogard laboratories , inc. , whose nvlap accreditation expires in june 2008 , has recently notified nist and eac that it would not apply to renew its accreditation , citing the volatility of the voting system environment as one reason .

the timeline for each of these accreditations , and other accreditation program activities , is found in figure 3 .

nist's defined approach to accrediting voting system laboratories largely reflects applicable hava requirements and relevant international standards , both of which are necessary to an effective program .

however , this approach is continuing to evolve based on issues realized during nist's implementation experience to date .

in particular , because nist's defined program does not , for example , specify the nature and extent of assessment documentation to generate or retain or specify the version of the voting system standards to be used , our analysis of nist's efforts in accrediting four laboratories could not confirm that the agency has consistently followed its defined accreditation program .

nist officials stated that these limitations are due in part to the relative newness of the program and that they will be addressed by updating the accreditation program handbook .

however , they said that they do not have documented plans to accomplish this .

until these limitations are addressed , nist will be challenged in accrediting voting system laboratories in a consistent and verifiable manner .

nist has defined its voting system accreditation program to address relevant hava requirements .

according to hava , nist is to conduct reviews of independent , nonfederal voting system testing laboratories and submit to eac a list of proposed voting system testing laboratories and monitor and review the performance of those proposed laboratories that eac accredits , including making recommendations to eac regarding accreditation continuance and revocation .

nist's defined voting system accreditation program satisfies both of these requirements .

with respect to the first , nist announced in june 2004 the establishment of its voting system testing laboratory accreditation program as part of nvlap , a statutorily created program for unbiased , third parties to establish the competence of national independent laboratories .

as such , nist adopted its nvlap handbook as the basis for its defined approach to reviewing vstls and has supplemented it with a handbook that is specific to voting system testing .

with respect to the second hava requirement , the supplemental handbook cited above states that the nist director will recommend nvlap - accredited vstls to eac for subsequent commission accreditation .

additionally , nist's handbooks provide for both monitoring accredited laboratories and for making recommendations regarding a laboratory's continued accreditation .

for example , the handbook states that a monitoring visit may occur at both scheduled and unscheduled times and the scope may be limited to a few items or include a full review .

it also states that a reaccreditation review shall be conducted in accordance with the procedures used to initially accredit laboratories .

further , the handbook also identifies accreditation or reaccreditation decision options , including granting , denying , or modifying the scope of an accreditation .

according to nist officials , these hava requirements are relevant and important to defining an effective voting system testing laboratory accreditation program .

by incorporating them , nist has reflected one key aspect of an effectively defined program .

nist's vstl accreditation program reflects internationally recognized standards for establishing and conducting accreditation activities .

these standards are published by the international organization for standardization ( iso ) , and the two that are germane to this accreditation program are ( 1 ) iso / iec 17011 , which establishes general requirements for accreditation bodies and ( 2 ) iso / iec 17025 , which establishes the general requirements for reviewing the competence of laboratories .

according to nist program documentation , this allows nvlap to both operate as an unbiased , third party accreditation body and to utilize a quality management system compliant with international standards .

as a result , nist has incorporated key aspects of an effective accreditation body into its voting system accreditation program .

iso / iec 17011 requires that an accrediting body have , among other things , ( 1 ) a management system for accreditation activities , ( 2 ) a policy defining the types of records to be retained and how those records will be maintained , ( 3 ) a clear description of the accreditation process that covers the rights and responsibilities of those seeking accreditation , and ( 4 ) a clear description of the accreditation activities to be performed .

nist vstl accreditation program - related documentation , including its program handbooks , satisfies each of these requirements .

in fact , nist has cross - referenced its documentation to each iso / iec 17011 requirement .

specifically , the first requirement is cross - referenced to the nvlap management system manual , which describes the overall accreditation program's management policies and control structure , and the second is cross - referenced to the program's record keeping policy , which specifies what types of records should be maintained and how they should be maintained .

the third and fourth requirements are cross - referenced to the accreditation process descriptions in both the management system manual and the general handbook .

together , these documents contain , for example , ( 1 ) the rights of laboratories applying for accreditation and ( 2 ) the scope of accreditation activities to be performed , including a preassessment review , an on - site review , and a final on - site assessment report .

iso / iec 17025 requires that accreditation reviews cover specific topics .

these include ( 1 ) laboratory personnel independence and conflicts of interest ; ( 2 ) a laboratory system for quality control ( i.e. , a framework for producing reliable results and continuous improvement to laboratory procedures ) ; and ( 3 ) a laboratory mechanism for collecting and responding to customer complaints .

additionally , the standard establishes basic technical requirements that a laboratory has to meet , and thus that reviews are to cover , including ( 1 ) competent laboratory personnel who are capable of executing the planned tests , ( 2 ) appropriate tests and test methods , and ( 3 ) clear and accurate test result documentation .

nist voting system testing laboratory accreditation program - related documents , including its program handbooks , satisfy these requirements .

first , the general handbook defines the requirement for a laboratory to have personnel that are independent and free of any conflict of interest .

second , the handbook requires that a laboratory have a management quality control system and that this system provide for reliable results and continuous improvement to laboratory procedures .

third , the handbook requires that a laboratory have a mechanism for receiving and responding to customer complaints .

last , the handbook establishes certain technical requirements that a laboratory must meet , such as having competent laboratory personnel capable of executing the planned tests , using appropriate tests and test methods , and documenting test results in a clear and accurate manner .

for several of these requirements , nist's voting - specific supplemental handbook augments the general handbook .

for example , this supplemental handbook requires laboratories to submit a quality control manual , as well as information to demonstrate the competence of laboratory administrative and technical staff .

further , it requires that a laboratory's training program be updated so that staff can be retrained as new versions of voting system standards are issued .

nist has reported on the importance of ensuring that those persons who perform accreditation assessments are sufficiently qualified and that the assessments themselves are based on explicitly defined criteria and are adequately documented .

nevertheless , nist has not fully reflected key aspects of these findings in its defined approach to accrediting voting system testing laboratories .

for example , it has not specified the basis for determining the qualifications of its accreditation assessors , and while a draft update to its handbook now includes the specific voting system standards to be used when performing an accreditation assessment , this handbook was only recently approved .

according to nist officials , these gaps are due to the newness of the accreditation program and will be addressed in the near future .

because these gaps have confused laboratories as to what standards they were to meet , and may have resulted in differences in how accreditations have been performed to date , it is important that the gaps be addressed .

nist has reported on the importance of having competent and qualified human resources to support accreditation programs .

according to these findings , an accreditation program should , among other things , provide for having experienced and qualified assessors to perform accreditation demonstrating an assessors' qualifications using defined documentation and explicit criteria that encompass the person's education , experience , and training ; and training ( initial and continuing ) for assessors .

nist's defined approach to vstl accreditation does not provide for all these requirements .

to its credit , its program handbook identifies the need for experienced and qualified assessors in the execution of accreditation activities and provides for each assessor's qualifications to be documented .

further , it has defined generic training that applies to all of its accreditation assessors .

for example , the nvlap assessor training syllabus includes training on iso / iec 17011 and 17025 , as well as training on the nvlap general handbook .

in addition , the vstl accreditation program manager stated that new assessors receive training on the 2002 vss and 2005 vvsg and that periodic training seminars are provided to assessors on changes to either the general handbook or the 2005 vvsg .

in addition , the program manager told us that candidate assessors must submit some form of documentation ( eg , a resume ) , and that this documentation is used to evaluate , rank , and select candidates that are best qualified .

the nist vstl assessors that we interviewed confirmed that they were required to submit such documentation at nist's request .

however , nist's defined approach does not cite the explicit capabilities and qualifications that an assessor must meet or the associated documentation needed to demonstrate these capabilities and qualifications .

according to the program manager , this is because the field of potential assessors in the voting system arena is small and specialized and because they focused on defining other aspects of the program that were higher priorities .

further , nist has not defined and documented the specific training requirements needed to be a vstl lead assessor or a technical assessor for the vstl program .

according to the program manager , this is because these assessors receive all the training they need by working on the job with more experienced assessors .

not specifying criteria governing assessor qualifications and training is of concern because differences in assessors' capabilities could cause inconsistencies in how assessments are performed .

nist recognizes the importance of specifying explicit criteria against which all candidate laboratories will be assessed and fully documenting the assessments that are performed .

specifically , the general handbook provides the criteria and requirements that will be used to evaluate basic laboratory capabilities .

it also states that technical requirements specific to a given field of accreditation are published in program - specific handbooks .

to that end , nist published a supplemental program - specific handbook in december 2005 that provided the voting - specific requirements to be used to evaluate vstls , additional guidance , and related interpretive information .

nist's 2005 supplemental handbook does not contain sufficient criteria against which to evaluate vstls .

it identifies specific requirements that laboratories are to demonstrate relative to the 2002 vss but not the 2005 vvsg .

for example , the handbook states that laboratories are expected to develop , validate , and document test methods that meet the 2002 vss .

however , it does not refer to the 2005 vvsg .

in addition , the program - specific checklist that accompanies this version of the handbook does not identify all the 2005 vvsg standards against which laboratories are evaluated .

specifically , this checklist makes reference to the vvsg in relation to just a few checklist requirements .

according to the nist program manager , the 2005 handbook did not refer to the 2005 vvsg requirements because only the 2002 vss requirements were mandatory at the time it was published .

he further stated that , despite the fact that the 2005 vvsg requirements were not included in that handbook , nist assessors were expected to use them when performing the first laboratory assessments .

representatives for two laboratories stated that because these requirements were not documented or identified in the nist handbooks , they did not learn that they would be required to demonstrate 2005 vvsg - based capabilities until the nist on - site assessment teams arrived .

in december 2007 , nist released draft revisions of the voting program - specific handbook and checklist , stating that labs are expected to meet both 2002 vss and 2005 vvsg .

in addition , the 2007 draft handbook clearly specifies that laboratories must demonstrate how developed test methods and planned tests trace back to and satisfy both the 2002 vss and the 2005 vvsg .

taken together , the new handbook and checklist should better identify the requirements and criteria used to evaluate a laboratory and document the results .

according to nist , the new handbook and checklist have recently been finalized , and both are now in use .

nist has found that reliable and accurate documentation provides assurance that laboratory accreditation activities have been effectively fulfilled .

however , in its efforts to date in accrediting four vstls , documentation of the assessments does not show that nist has fully followed its defined accreditation approach .

while we could not determine whether this is due to incomplete documentation of the steps performed and the decisions made during an assessment or due to steps not being performed as defined , this absence of verifiable evidence raises questions about the consistency of the assessments and the resultant accreditations .

without adequately documenting each assessment , including all steps performed and the basis for any steps not performed , such questions may continue to be raised .

to nist's credit , available documentation shows that it consistently followed some aspects of its defined approach in accrediting the four laboratories .

for example , we verified that nist received an application from each of the laboratories as required , and our review of completed checklists and summary reports shows that preassessment reviews and on - site assessments were performed for each laboratory , as was required .

according to a lead assessor , this review usually focused on the laboratories' quality assurance manuals .

moreover , the completed checklists identified whether the requirement was met or not for each listed requirement , and included comments , in some cases , as to how a laboratory addressed a requirement .

also as required , nist received laboratory responses describing how unmet requirements were addressed within specified time frames , used the responses in making accreditation decisions , and notified eac of its decisions via letters of recommendation .

furthermore , nist has recently begun reaccreditation reviews at two laboratories , as required .

however , documentation does not show that nist has consistently followed other aspects of its defined approach .

our analysis of the checklists that are to be used to both guide and document a given assessment , including identifying unmet requirements and capturing assessor comments and observations , shows some differences .

for example: one type of checklist ( the supplemental handbook checklist ) was prepared for only two of the four laboratory assessments .

according to the program manager , this is because even though a draft revision of this checklist was actually used to assess the other two laboratories , the assessment results were recorded on a different checklist ( the general handbook checklist ) .

while this is indicated on one of the two checklists , it is not indicated on the other .

on the checklist used for one laboratory , an assessor marked several sections as “ta” with no explanation as to what this means .

also , the checklist used for another laboratory did not identify whether most of the requirements were met or not met .

further , the checklist for a third laboratory had one section marked as “not applicable” but included no explanation as to why that section did not apply , while the checklist for a different laboratory marked the same section as “not applicable” but included a reason for doing so .

notwithstanding these differences , the program manager told us that each laboratory was assessed using the same requirements and all assessments to date were performed in a consistent manner .

on the basis of available documentation , however , we could not verify that this is the case .

as a result , it is not clear that nist has consistently followed its defined approach .

available documentation also does not show that nist followed other aspects of its approach .

for example: the program handbook states that each laboratory is to identify the requested scope of accreditation in its application package .

however , our analysis of the four application packages shows that two laboratories did not specify a requested scope of accreditation .

according to the program manager , the scope of accreditation for all laboratories was the 2002 vss and 2005 vvsg because , even though the latter standards were not yet in effect at the time , they were anticipated to be in effect in the near future .

however , nist did not have documentation that notified the laboratories of this scope of accreditation or that indicated whether this scope was established by eac , nist , or the laboratories .

the program handbook states that after receiving a laboratory's application package , nist will acknowledge its receipt in writing and will inform the laboratory of the next steps in the accreditation process .

however , nist did not have documentation demonstrating that this was done .

according to the program manager , this was handled via telephone conversations .

however , representatives for several laboratories noted that these calls did not clearly establish expectations , adding that some expectations were not communicated until the nist team assessors arrived to conduct the on - site assessment .

the program manager stated that these deviations from the defined approach are attributable to the relative newness of the program , but despite these discrepancies , each laboratory was assessed consistently .

however , we could not verify this , and thus it is not clear that nist has consistently followed its defined approach .

according to this official , future versions of the program handbook would address these limitations .

however , documented plans for doing so have not been developed .

eac has recently defined its voting system laboratory accreditation approach in a draft program manual .

however , this draft manual omits important content .

while addressing relevant hava requirements , the draft manual does not adequately define key accreditation factors that nist has identified , and a key accreditation feature that we have previously reported as being integral to an effective accreditation program .

moreover , not all factors and features that the draft manual does include have been defined to a level that would ensure thorough , consistent , and verifiable implementation .

because this manual was not available for eac to use on the four laboratory accreditations that it has completed , the accreditations were performed using a largely undocumented series of steps .

as a result , the thoroughness and consistency of these accreditations is not clear .

according to eac officials , these gaps are due to the agency's limited resources being focused on other issues , and will be addressed as its accreditation program evolves .

however , they said that they do not yet have documented plans to accomplish this .

until eac fully defines a repeatable vstl accreditation approach , it will be challenged in its ability to treat all laboratories consistently and produce verifiable results .

in february 2008 , eac issued a draft version of a vstl accreditation program manual for public comment .

according to hava , eac's accreditation program is to meet certain requirements .

specifically , it is to provide for voting system hardware and software testing , certification , decertification , and recertification by accredited laboratories .

additionally , it is to base laboratory accreditation decisions , including decisions to revoke an accreditation , on a vote of the commissioners , and it is to provide for a published explanation of any commission decision to accredit any laboratory that was not first recommended for accreditation by nist .

to eac's credit , its draft accreditation program manual addresses each of these requirements .

first , the manual defines the role that the laboratories are to play relative to voting system testing , certification , recertification and decertification , and it incorporates by reference an eac companion voting system certification manual that defines requirements and process steps for voting system testing and certification - related activities .

with respect to the remaining three hava requirements , the draft eac accreditation manual also requires ( 1 ) that the commissioners vote on the accreditation of laboratories recommended by nist for accreditation , ( 2 ) that eac publish an explanation for the accreditation of any laboratory not recommended by nist for accreditation , and ( 3 ) that the commissioners vote on the proposed revocation of a laboratory's accreditation .

according to eac officials , its draft approach incorporates hava requirements because the commission is focused on meeting its legal obligations in all aspects of its operations , including vstl accreditation .

in doing so , eac has addressed one important aspect of having an effective accreditation program .

beyond addressing relevant hava requirements , eac's draft accreditation manual defines an accreditation process , including program phases , requirements , and certain evaluation criteria .

however , it does not do so in a manner that fully satisfies factors that nist has reported can affect the effectiveness of accreditation programs .

moreover , it does not adequately address a set of features that our research shows are common to federal accreditation programs and that can influence a program's effectiveness .

according to eac officials , these factors and features are not fully addressed in the draft program manual because its accreditation program is still in its early stages of development and is still evolving .

until they are fully addressed , eac's accreditation program's effectiveness will be limited .

according to nist , having confidence in and ensuring appropriate use of an accredited testing laboratory requires that accreditation stakeholders have an adequate understanding of the accreditation process , scope , and related criteria .

nist further reports that confidence in the accreditation process can be traced to a number of factors that will influence the thoroughness and competence of accreditation programs , and thus these factors can be viewed as essential accreditation program characteristics .

they include having published procedures governing how the accreditation program is to be executed , such as procedures for granting , maintaining , modifying , suspending , and withdrawing accreditation ; specific instructions , steps , and criteria for those who conduct an accreditation assessment ( assessors ) to follow , such as a test methodology that is acceptable to the accreditation program ; knowledgeable and experienced assessors to execute the instructions and steps and apply the related criteria ; and complete records on the data collected , results found , and reports prepared relative to each assessment performed .

eac's draft accreditation program manual addresses one of these factors but it does not fully address the other three .

 ( see table 2. ) .

for example , while the manual requires that eac maintain records , it only addresses the retention of records associated with the testing of voting systems and not those associated with the accreditation of laboratories .

eac officials told us that testing records are meant to include accreditation records , although they added that this is not explicit in the manual and needs to be clarified .

further , the manual is silent on the steps to be followed and criteria to be applied in reviewing a laboratory's application and the qualifications required for accreditation reviewers .

by not fully addressing these factors , eac increases the risk that its accreditation reviews will not be performed consistently and comprehensively .

as we have previously reported , the nature and focus of federal programs for accrediting laboratories vary , but nevertheless include certain common features .

in particular , these programs require laboratories to provide certain information to the accrediting body , and they provide for evaluation of this information by the accrediting body in making an accreditation determination .

as we reported , the required information is to include , among other things , the laboratory's ( 1 ) organizational information , ( 2 ) records and record - keeping policy , ( 3 ) test methods and procedures , ( 4 ) conflict of interest policy , and ( 5 ) financial stability .

to its credit , eac's draft accreditation manual provides for laboratories to submit information relative to each of these features that are common to federal accreditation programs .

for example , it provides for laboratories to submit organizational information , such as location ( s ) , ownership , and organizational chart ; a written policy for maintaining accreditation - related records for 5 years ; conflict of interest policies and procedures ; test - related polices and procedures , as well as system - specific test plans ; and financial information needed to demonstrate stability .

moreover , for four of the five features , the manual identifies the specific types of information needed for accreditation and how the information is to be evaluated , including the criteria that are to be used in evaluating it .

however , for the financial stability feature , the manual does not describe what specific documents are required from the laboratory to satisfy this requirement , nor does the manual indicate how information provided by a laboratory will be evaluated .

at the time of our review , eac's director of voting system testing and certification told us that the draft accreditation manual was to be submitted for approval and that this draft did not address all of the limitations cited above .

for example , it would not contain the information needed and the evaluation approach and criteria to be used in making determinations about financial stability because this decision is to be based on what the director referred to as a “reasonableness” test that involves eac evaluation of the information relative to that provided by other laboratories .

further , while eac officials said that they plan to evolve their approach to vstl accreditation and to address these gaps , eac does not have documented plans for accomplishing this .

without clearly defining information to be used and how it is to be used , eac increases the risk that financial stability determinations will not be consistently and thoroughly made .

as of may 2008 , eac has accredited four laboratories , but the documentation associated with each of these accreditations is not sufficient to recreate a meaningful understanding of how each evaluation was performed and how decisions were made , and thus , the bases for each accreditation were not clear .

specifically , each of the accreditations occurred before eac had defined its approach for conducting them .

because of this , eac performed each one using a broadly defined process outlined in a letter to each laboratory and an associated checklist that only indicated whether certain documents were received .

our analysis of these letters showed that the correspondence sent to each laboratory was all the same , identifying three basic review steps to be performed and citing a list of documents that the laboratories were to provide as part of their applications .

however , the letters did not describe in any manner how eac would review the submitted material , including the criteria to be used .

according to eac officials , the review steps were not documented .

instead , they were derived by a single reviewer using ( 1 ) the applications and accompanying documents submitted by the laboratories , ( 2 ) familiarity with the materials used by the state election directors - sponsored accreditation program , and ( 3 ) the judgment of each reviewer .

further , while the reviews were supported by a checklist that covered each of the items that was to be included in the laboratory applications and provided space for the reviewer ( s ) to make notes relative to each of these items , the checklists did not include any guidance or methodology , including criteria , for evaluating the submitted items .

rather , the eac accreditation program director told us that he was the reviewer on all the accreditations and he applied his own , but undocumented , tests for reasonableness in deciding on the submissions' adequacy and acceptability .

our analysis of the checklists for each laboratory accreditation showed that while the same checklist was used for each laboratory , the checklists did not provide a basis for evaluating and documenting the basis for the sufficiency of those documents .

in some cases , additional communications occurred between the reviewer and the laboratory to obtain additional documents .

however , no documentation was available to demonstrate what standards or other criteria the laboratories were held to or how their submissions were otherwise reviewed .

for example , each of the checklists indicated that each laboratory provided “a copy of the laboratory's conflict of interest policy.” however , they did not specify , for example , whether the policy adequately addressed particular requirements .

nevertheless , for three of the four accredited laboratories , documentation shows that eac sought clarification on or modification to the policies provided , thus suggesting that some form of review was performed against more detailed requirements .

similarly , while the checklists indicate that the laboratories disclosed their respective coverage limits for general liability insurance policies , and in one case eac communicated to the laboratory that the limits appeared to be low , no documentation specifies the expected coverage limits .

according to the eac director of voting system testing and certification , this determination was made after comparing limits among the laboratories and was not based on any predetermined threshold .

further , while the checklists indicate that each laboratory provided audited financial statements , there is no documentation indicating how these statements were reviewed .

according to the eac program director , the lack of documentation demonstrating the basis for eac's laboratory accreditations is due to the need at the time to move quickly in accrediting the laboratories and the fact that use of the same individual to review the accreditation evaluation negated the need for greater documentation .

without such documentation , however , we could not fully establish how the accreditations were performed , including whether there was an adequate basis for the accreditation decisions reached and whether they were performed consistently .

the effectiveness of our nation's overall election system depends on many interrelated and interdependent variables , including the security and reliability of voting systems .

both nist and eac play critical roles in ensuring that the laboratories that test these two variables have the capability , experience , and competence necessary to test a voting system against the relevant standards .

nist has recently established an accreditation program that largely accomplishes this , and while eac is not as far along , it has a foundation upon which it can build .

however , important elements are still missing from both programs .

specifically , the current nist approach does not define requirements for assessor qualifications and training or ensure that assessments are fully documented .

additionally , eac has not developed program management practices that are fully consistent with what nist has found to be hallmarks of an effective accreditation program , nor has the agency adequately specified how evaluations are to be performed and documented .

as a result , opportunities exist for nist and eac to further define and implement their respective programs in ways that promote greater consistency , repeatability , and transparency — and thus improve the results achieved .

it is also important for nist and eac to follow through on their stated intentions to evolve their respective programs , building on what they have already accomplished through the development and execution of well - defined plans of action .

if they do not , both will be challenged in their ability to consistently provide the american people with adequate assurance that accredited laboratories are qualified to test the voting systems that will eventually be used in u.s. elections .

to help nist in evolving its vstl accreditation program , we recommend that the director of nist ensure that the accreditation program manager develops and executes plans that specify tasks , milestones , resources , and performance measures that provide for the following two actions: establish and implement transparent requirements for the technical qualifications and training of accreditation assessors .

ensure that each laboratory accreditation review is fully and consistently documented in accordance with nist program requirements .

to help eac in evolving its vstl accreditation program , we recommend that the chair of the eac ensure that the eac executive director develops and executes plans that specify tasks , milestones , resources , and performance measures that provide for the following action: establish and implement practices for the vstl accreditation program consistent with accreditation program management guidance published by nist and gao , including documentation of specific accreditation steps and criteria to guide assessors in conducting each laboratory review ; transparent requirements for the qualifications of accreditation reviewers ; requirements for the adequate maintenance of records related to the vstl accreditation program ; and requirements for determining laboratory financial stability .

both nist and eac provided written comments on a draft of this report , signed by the deputy director of nist and the executive director of eac , respectively .

these comments are described below along with our response to them .

in its comments , nist stated that it appreciates our careful review of its vstl program and generally concurs with our conclusions that its program must continue to evolve and improve .

however , nist also provided comments to clarify the current status of the program relative to three of our findings .

with respect to our finding that nist's defined approach for accrediting vstls does not cite explicit qualifications for the persons who conduct the technical assessments , the institute stated that it does explicitly cite assessor qualifications for its overall national laboratory accreditation program , adding that this approach to specifying assessor qualifications has a proven record of success .

it also stated that the overall program's management manual requires all assessors to meet defined criteria in such areas as laboratory experience , assessment skills , and technical knowledge , and that candidate assessors must submit information addressing each of these areas as well as factors addressing technical competence in a given laboratory's focus area ( eg , voting systems ) .

further , it stated that candidate assessors' qualification ratings and rankings are captured in work sheets .

in response , we do not disagree with any of these statements .

however , our finding is that nist's defined approach for vstl accreditation does not specify requirements for persons who assess those laboratories that specifically test voting systems .

in this regard , nist's own written comments confirm this , stating that specific requirements for assessors are not separately documented for each of its national laboratory accreditation programs , such as the vstl program .

therefore , we have not modified this finding or the related recommendation .

regarding our finding that nist's defined approach for accrediting vstls has not always cited the current voting system standards , the institute affirmed this in its comments by stating that the vstl program handbook that it provided to us only cites the 2002 system standards , as these were the only standards in place when the handbook was published .

however , nist also noted that when the 2005 system guidelines were adopted in december 2005 , it began the process of updating the handbook and associated assessment checklist , and that the handbook update was recently finalized for publication and is now being used .

in response , we stand by our finding that nist's defined approach has not always cited the current voting system standards , which nist acknowledges in its comments .

however , we also recognize that nist has recently addressed this inconsistency by finalizing its new handbook and the associated assessment checklist .

in light of nist's recent actions , we have updated the report to acknowledge the finalization of the handbook and checklist , and removed the associated recommendation that was contained in our draft report for nist to ensure that its defined approach addresses all required voting system standards .

regarding our finding that available documentation from completed accreditations does not show that nist has consistently followed all aspects of its defined approach , the institute stated that , among other things , all required documents for its vstl accreditation program are currently in use and reflect the recent update to its handbook and checklist , and that all these documents are securely maintained .

in response , we do not question these statements ; however , they are not pertinent to our finding .

specifically , our finding is that the four completed accreditations that we reviewed were not consistently documented .

as we state in our report , we reviewed the documentation associated with the accreditation assessments for these four laboratories , and we found that all four were not documented in a similar manner , even though they were based on the same version of the program handbook .

for example , neither the laboratory notifications of the scope of the assessment nor the next steps in the accreditation process were consistently documented .

therefore , we have not modified our finding , but have slightly modified our recommendation to make it clear that its intent is to ensure that all phases of the accreditation review are fully and consistently documented .

in its comments , eac described our review and report as being helpful to the commission as it works to fully develop and implement its vstl program .

it also stated that it agrees with the report's conclusions that additional written internal procedures , standards , and documentation are needed to ensure more consistent and repeatable implementation of the program .

the commission added that it generally accepts our recommendations and will work hard to implement them .

to assist it in doing so , it sought clarification about two of our recommendations , as discussed below .

eac stated that the recommendation in our draft report for the commission to develop specific accreditation steps and criteria was broadly worded , and thus the recommendation's intent was not clear .

eac also stated that it interpreted the recommendation to mean that it should define internal instructions to guide assessors in performing an accreditation , and that the recommendation was not intended to have any impact on its published requirements and procedures governing , for example , granting , suspending , or withdrawing an accreditation .

we agree with eac's interpretation , as it is in line with the intent of our recommendation .

to avoid the potential for any future misunderstanding , we have modified the wording of the recommendation to clarify its intent .

eac stated that the recommendation in our draft report for the commission to develop transparent technical requirements for the qualifications of its assessors may be confusing because , as we state in our report , only nist performs a technical accreditation review , as eac's review is administrative , non - technical in nature .

to avoid the potential for any confusion , we have modified the wording of the recommendation to eliminate any reference to technical qualification requirements .

we are sending copies of this report to the ranking member of the house committee on house administration , the chairman and ranking member of the senate committee on rules and administration , the chairmen and ranking members of the subcommittees on financial services and general government , senate and house committees on appropriations , and the chairman and ranking member of the house committee on oversight and government reform .

we are also sending copies to the chair and executive director of eac , the secretary of commerce , the deputy director of nist , and other interested parties .

we will also make copies available to others on request .

in addition , this report will be available at no charge on the gao website at http: / / www.gao.gov .

should you or your staffs have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iv .

our objectives were to determine whether the national institute of standards and technology ( nist ) and the election assistance commission ( eac ) have defined effective voting system testing laboratory ( vstl ) accreditation approaches , and whether each is following its defined approach .

to determine whether nist has defined an effective accreditation approach , we reviewed documentation from its vstl accreditation program , such as handbooks and program manuals for the national voluntary laboratory accreditation program ( nvlap ) , of which the vstl accreditation program is a part .

in doing so , we compared these documents with applicable statute , guidance , and best practices , primarily the help america vote act of 2002 ( hava ) , internationally recognized standards from the international organization for standardization ( iso ) , and federal accreditation program management guidance published by nist .

we compared program documentation with hava's nist - specific accreditation requirements to determine the extent to which the agency was fulfilling its hava responsibilities .

we also reviewed program documentation against iso / iec 17011 , which establishes general requirements for accreditation bodies , and iso / iec 17025 , which establishes the general requirements for assessing the competence of laboratories , to determine the extent to which nist's accreditation program was based on internationally recognized standards .

we also compared the documentation against nist publication nistir 6014 , which contains sections that provide guidance for laboratory accreditation programs , to determine whether the vstl accreditation program had defined other elements of effective accreditation programs .

we also interviewed the voting accreditation program manager to determine how these documents were used to guide the program .

to determine whether nist has followed its defined approach , we examined artifacts from the accreditation assessments of five vstls , including one laboratory accredited by nvlap , but not yet recommended to eac .

this material included completed assessment checklists derived from the accreditation program handbooks , additional documents supporting the assessments , and laboratory accreditation applications and supporting documentation .

we compared artifacts from these assessments to program guidance to determine the extent to which the defined process was followed .

in addition , we interviewed officials from nist and nist contract assessors and officials from eac and the four eac - accredited vstls to understand how the nist process was implemented and how it related to the process managed by eac .

to determine whether eac has defined an effective accreditation approach , we reviewed documentation from its vstl accreditation program , such as the draft voting system test laboratory accreditation program manual .

in doing so , we compared this document with applicable statute and best practices , primarily hava and federal accreditation program management guidance published by nist .

we compared the draft program manual with hava's eac - specific accreditation requirements to determine the extent to which the agency was fulfilling its hava responsibilities .

we also compared the documentation against the accreditation guidance in nistir 6014 to determine whether the accreditation program had defined other elements of effective accreditation programs .

we also interviewed the eac voting program director and executive director to determine how these documents were used to guide the program and to understand eac's defined accreditation approach prior to the development of the draft manual .

to determine whether eac has followed its defined approach , we compared artifacts from the accreditation reviews of four vstls .

we did not review a fifth laboratory , which had been accredited by nvlap , but not yet recommended to eac .

the materials reviewed included checklists completed by eac in the absence of an approved program manual .

in doing so , we compared the review artifacts to accreditation program requirements , as communicated to the laboratories , to determine the extent to which the agency followed its process , as verbally described to us .

we did not compare accreditation submissions or eac review artifacts with the draft accreditation manual because agency officials stated that the draft manual had not been used in the review of any laboratory .

in addition , we interviewed officials from nist , eac , and the four eac - accredited vstls to understand how the eac process was implemented and how it related to the process managed by nist .

to assess data reliability , we reviewed program documentation to substantiate data provided in interviews with knowledgeable agency officials .

we have also made appropriate attribution indicating the data's sources .

we conducted this performance audit at eac and nist offices in washington , d.c. , and gaithersburg , maryland , respectively , from september 2007 to september 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

paula moore , assistant director ; justin booth ; timothy case ; neil doherty ; timothy eagle ; nancy glover ; dave hinchman ; rebecca lapaze ; freda paintsil ; nik rapelje ; and jeffrey woodward made key contributions to this report .

