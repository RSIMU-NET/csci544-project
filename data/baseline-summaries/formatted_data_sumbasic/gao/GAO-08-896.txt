for decades , the department of defense ( dod ) has been challenged in modernizing its timeworn business systems .

in 1995 , we designated dod's business systems modernization program as high risk , and continue to do so today .

our reasons include the modernization's large size , complexity , and its critical role in addressing other long - standing transformation and financial management challenges .

other reasons are that dod has yet to institutionalize key system modernization management controls , and it has not demonstrated the ability to consistently deliver promised system capabilities and benefits on time and within budget .

nevertheless , dod continues to invest billions of dollars in thousands of business systems , including about a hundred that the department has labeled as business transformational programs , 12 of which account for about 50 percent of these programs' costs .

the navy enterprise resource planning ( navy erp ) program is one such program .

initiated in 2003 , navy erp is to standardize the navy's acquisition , financial , program management , maintenance , plant and wholesale supply , and workforce management business processes across its dispersed organizational environment .

as envisioned , the program consists of a series of major increments , the first of which includes three releases and is expected to cost approximately $2.4 billion over its 20-year life cycle and to be fully operational in fiscal year 2013 .

as agreed , our objective was to determine whether the department of the navy ( don ) is effectively implementing information technology ( it ) management controls on navy erp .

to accomplish this , we focused on the program's first increment by analyzing a range of program documentation and interviewing cognizant officials relative to the following management areas: architectural alignment , economic justification , earned value management ( evm ) , requirements management , and risk management .

we conducted this performance audit from june 2007 to september 2008 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

additional details on our objective , scope , and methodology are in appendix i .

don's primary mission is to organize , train , maintain , and equip combat - ready naval forces capable of winning the global war on terrorism and any other armed conflict , deterring aggression by would - be foes , preserving freedom of the seas , and promoting peace and security .

to support this mission , don performs a variety of interrelated and interdependent business functions ( eg , acquisition and financial management ) , relying heavily on it systems .

in fiscal year 2008 , don's budget for business systems and associated infrastructure was about $2.7 billion , of which $2.2 billion was allocated to operations and maintenance of existing systems and the remaining $500 million to systems in development and modernization .

of the approximately 3,000 business systems that dod reports in its current inventory , don accounts for 904 , or about 30 percent , of the total .

navy erp is one such system investment .

in july 2003 , the assistant secretary of the navy for research , development , and acquisition established navy erp to “converge” four separate pilot programs that were under way at four separate navy commands .

this program is to leverage a commercial , off - the - shelf software known as an enterprise resource planning product .

such products consist of multiple , integrated functional modules that perform a variety of business - related tasks , such as acquisition and financial management .

table 1 provides a brief description and status of each of the pilots .

according to dod , navy erp is to address the navy's long - standing problems related to financial transparency and asset visibility .

specifically , the program is intended to standardize the navy's acquisition , financial , program management , maintenance , plant and wholesale supply , and workforce management business processes across its dispersed organizational components .

when the program is fully implemented , it is to support over 86,000 users .

navy erp is being developed in a series of increments using the systems applications and products ( sap ) commercial software package , augmented as needed by customized software .

sap consists of multiple , integrated functional modules that perform a variety of business related tasks , such as finance and acquisition .

the first increment , called template 1 , is currently the only funded portion of the program and consists of three releases: 1.0 financial and acquisition , 1.1 wholesale and retail supply , and 1.2 intermediate - level maintenance .

release 1.0 is the largest of the three releases in terms of the functional requirements being addressed .

specifically , it is to provide about 56 percent of template 1 requirements .

see table 2 for a description of these releases .

don estimates the life cycle cost for the program's first increment to be about $2.4 billion , including about $1 billion for acquisition , and $1.4 billion for operations and maintenance .

the life cycle cost of the entire program has not yet been determined because future increments have not been defined .

the program office reported that approximately $400 million was spent from fiscal year 2004 through fiscal year 2007 on the first increment .

for fiscal year 2008 , about $200 million is planned to be spent .

to manage the acquisition and deployment of navy erp , don established a program management office within the program executive office for executive information systems .

the program office manages the program's scope and funding and is responsible for ensuring that the program meets its objectives .

to accomplish this , the program office is responsible for key program management areas , such as architectural alignment , economic justification , earned value management , requirements management , and risk management .

in addition , various dod and don organizations share program oversight and review activities .

a listing of key entities and their roles and responsibilities is in table 3 .

the first increment of navy erp is currently in the production and deployment phase of the defense acquisition system .

the system consists of five key program life cycle phases and three related milestone decision points .

these five phases and related milestones , along with a summary of key program activities completed during or planned for each phase , are as follows: 1 .

concept refinement: the purpose of this phase is to refine the initial system solution ( concept ) and create a strategy for acquiring the solution .

this phase began in july 2003 , at which time don began to converge the four pilot programs into navy erp and developed its first cost estimate in september 2003 .

this phase of the program was combined with the next phase , thus creating a combined milestone a / b decision point .

2 .

technology development: the purpose of this phase is to determine the appropriate set of technologies to be integrated into the investment solution by iteratively assessing the viability of the various technologies while simultaneously refining user requirements .

during the combined concept refinement and technology development phase , the program office prepared a concept of operations and operational requirements document ; performed an analysis of alternatives , business case analysis , and economic analysis ; and established its first acquisition program baseline .

it also selected sap as the commercial off - the - shelf erp software .

the combined phase was completed in august 2004 , when the mda approved milestone a / b to allow the program to move to the next phase .

3 .

system development and demonstration: the purpose of this phase is to develop a system and demonstrate through developer testing that the system can function in its target environment .

this phase was completed in september 2007 , when release 1.0 passed development testing and its deployment to navair began .

this was 17 months later than the program's original schedule set in august 2004 but on time according to the revised schedule set in december 2006 .

in september 2004 , the program office awarded a $176 million system integration contract to bearingpoint for full system design , development , and delivery using sap's off - the - shelf product and related customized software .

in january 2006 , the program office ( 1 ) reduced the contractor's scope of work from development and integration of the first increment to only development of the first release and ( 2 ) assumed responsibility and accountability for overall system integration .

according to the program office , reasons for this change included the need to change the development plan to reflect improvements in the latest sap product released and the lack of authority by the contractor to adjudicate and reconcile differences among the various navy user organizations ( i.e. , navy commands ) .

in december 2006 , the program office revised its acquisition program baseline to reflect an increase of about $461 million in the life cycle cost estimate due , in part , to restructuring the program ( eg , changing the order of the releases , changing the role of system integrator from contractor to the program office ) and resolving problems related to , among other things , converting data from legacy systems to run on navy erp and establishing interfaces between legacy systems and navy erp .

in addition , the program office awarded a $151 million contract for release 1.1 and 1.2 configuration and development to ibm in june 2007 .

in september 2007 , prior to entering the next phase , the program revised its acquisition program baseline again to reflect a $9 million decrease in the life cycle cost estimate and a 5-month increase in its program schedule .

soon after , the mda approved milestone c to move to the next phase .

4 .

production and deployment: the purpose of this phase is to achieve an operational capability that satisfies the mission needs , as verified through independent operational test and evaluation , and to implement the system at all applicable locations .

this phase began in september 2007 , focusing first on achieving initial operational capability ( ioc ) of release 1.0 at navair by may 2008 .

this date is 22 months later than the baseline established for milestone a / b in august 2004 , and 4 months later than the new baseline established in september 2007 .

according to program documentation , these delays were due , in part , to challenges experienced at navair in converting data from legacy systems to run on the new system and implementing new business procedures associated with the system .

in light of the delays at navair in achieving ioc , the deployment schedules for the other commands were also revised .

specifically , release 1.0 is still to be deployed at navsup on october 2008 , but release 1.0 deployment at spawar is now scheduled 18 months later than planned ( october 2009 ) , and deployment at navsea general fund and navy working capital fund is now scheduled to be 12 months later than planned ( october 2010 and 2011 , respectively ) .

because of the release 1.0 delays , release 1.1 is now planned for deployment at navsup 7 months later than planned ( february 2010 ) .

release 1.2 is still scheduled to be released at regional maintenance centers in october 2010 .

the program office is currently in the process of again re - baselining the program , and don plans to address any cost overruns through reprogramming of fiscal year 2008 don funds .

it estimates that this phase will be completed with full operational capability ( foc ) by august 2013 ( 26 months later than the baseline established in 2004 , and 5 months later than the re - baseline established in september 2007 ) .

5 .

operations and support: the purpose of this phase is to operationally sustain the system in the most cost - effective manner over its life cycle .

in this phase , the program plans to provide centralized support to its users across all system commands .

each deployment site is expected to perform complementary support functions , such as data maintenance .

overall , increment 1 was originally planned to reach foc in fiscal year 2011 , and its estimated life cycle cost was about $1.87 billion .

the estimate was later baselined in august 2004 at about $2.0 billion .

in december 2006 and again in september 2007 , the program was re - baselined .

foc is now planned for fiscal year 2013 , and the estimated life cycle cost is about $2.4 billion ( 31 percent increase over the original estimate ) .

key activities for each phase are depicted in figure 1 , changes in the deployment schedule are depicted in figure 2 , and cost estimates are depicted in figure 3 .

it acquisition management controls are tried and proven methods , processes , techniques , and activities that organizations define and use to minimize program risks and maximize the chances of a program's success .

using these controls can result in better outcomes , including cost savings , improved service and product quality , and a better return on investment .

for example , two software engineering analyses of nearly 200 systems acquisition projects indicate that teams using systems acquisition controls that reflected best practices produced cost savings of at least 11 percent over similar projects conducted by teams that did not employ the kind of rigor and discipline embedded in these practices .

in addition , our research shows that these controls are a significant factor in successful acquisition outcomes , including increasing the likelihood that programs and projects will be executed within cost and schedule estimates .

we and others have identified and promoted the use of a number of it acquisition management controls associated with acquiring it systems .

see table 4 for a description of several of these activities .

we have previously reported that dod has not effectively managed a number of business system investments .

among other things , our reviews of individual system investments have identified weaknesses in such things as architectural alignment and informed investment decision making , which are also the focus areas of the fiscal year 2005 defense authorization act business system provisions .

our reviews have also identified weaknesses in other system acquisition and investment management areas — such as evm , economic justification , requirements management , and risk management .

in july 2007 , we reported that the army's approach for investing about $5 billion over the next several years in its general fund enterprise business system , global combat support system - army field / tactical , and logistics modernization program did not include alignment with the army enterprise architecture or use of a portfolio - based business system investment review process .

moreover , we reported that the army did not have reliable analyses , such as economic analyses , to support its management of these programs .

we concluded that , until the army adopts a business system investment management approach that provides for reviewing groups of systems and making enterprise decisions on how these groups will collectively interoperate to provide a desired capability , it runs the risk of investing significant resources in business systems that do not provide the desired functionality and efficiency .

accordingly , we made recommendations aimed at improving the department's efforts to achieve total asset visibility and enhancing its efforts to improve its control and accountability over business system investments .

the department agreed with our recommendations .

we also reported that don had not , among other things , economically justified its ongoing and planned investment in the naval tactical command support system ( ntcss ) and had not invested in ntcss within the context of a well - defined dod or don enterprise architecture .

in addition , we reported that don had not effectively performed key measurement , reporting , budgeting , and oversight activities and had not adequately conducted requirements management and testing activities .

we concluded that , without this information , don could not determine whether ntcss , as defined , and as being developed , is the right solution to meet its strategic business and technological needs .

accordingly , we recommended that the department develop the analytical basis to determine if continued investment in ntcss represents prudent use of limited resources and to strengthen management of the program , conditional upon a decision to proceed with further investment in the program .

the department largely agreed with our recommendations .

in addition , we reported that the army had not defined and developed its transportation coordinators' automated information for movements system ii ( tc - aims ii ) — a joint services system with the goal of helping to manage the movement of forces and equipment within the united states and abroad — in the context of a dod enterprise architecture .

in addition , we reported that the army had not economically justified the program on the basis of reliable estimates of life cycle costs and benefits and had not effectively implemented risk management .

as a result , we concluded that the army did not know if its investment in tc - aims ii , as planned , was warranted or represented a prudent use of limited dod resources .

accordingly , we recommended that the department , among other things , develop the analytical basis needed to determine if continued investment in tc - aims ii represents prudent use of limited defense resources .

in response , the department agreed with our recommendations and has since reduced the program's scope by canceling future investments .

furthermore , in 2005 , we reported that don had invested approximately $1 billion in the four previously cited erp pilots without marked improvement in its day - to - day operations .

more specifically , we reported that the program office had not implemented an evm system .

we also identified significant challenges and risks as the project moved forward , such as developing and implementing system interfaces , converting data from legacy systems into the erp system , meeting its estimated completion date of 2011 at an estimated cost of $800 million , and achieving alignment with dod's bea .

to address these areas , we made recommendations that dod improve oversight of navy erp , including developing quantitative metrics to evaluate the program .

dod generally agreed with our recommendations .

dod it - related acquisition policies and guidance , along with other relevant guidance , provide an acquisition management control framework within which to manage business system programs like navy erp .

effective implementation of this framework can minimize program risks and better ensure that system investments are defined in a way to optimally support mission operations and performance , as well as deliver promised system capabilities and benefits on time and within budget .

to varying degrees of effectiveness , navy erp has been managed in accordance with aspects of this framework .

however , implementation of key management controls has not been effective .

specifically , compliance with dod's federated bea has not been sufficiently investment in the program has been economically justified on the basis of expected life cycle benefits that will likely exceed estimated life cycle costs , although some estimating limitations nevertheless exist ; earned value management has not been effectively implemented ; an important requirements management activity has been effectively a risk management process has been defined , but not effectively implemented for all risks .

the reasons that program management and oversight officials cited for why these key practices have not been sufficiently executed range from limitations in the applicable dod guidance and tools to the complexity and challenges of managing and implementing a program of this size .

each of these reasons is described in the applicable sections of this report .

by not effectively implementing all the above key it acquisition management functions , the program is at increased risk of ( 1 ) not being defined in a way that best meets corporate mission needs and enhances performance and ( 2 ) adding to the more than 2 years in program schedule delays and about $570 million in program cost increases experienced to date .

dod and other guidance , recognize the importance of investing in business systems within the context of an enterprise architecture .

moreover , the fiscal year 2005 defense authorization act requires that defense business systems be compliant with dod's federated bea .

our research and experience in reviewing federal agencies show that not making investments within the context of a well - defined enterprise architecture often results in systems that are duplicative , are not well integrated , are unnecessarily costly to interface and maintain , and do not optimally support mission outcomes .

to its credit , the program office has followed dod's bea compliance guidance .

however , this guidance does not adequately provide for addressing all relevant aspects of bea compliance .

moreover , don's enterprise architecture , which is a major component of dod's federated bea , as well as key aspects of dod's corporate bea , have yet to be sufficiently defined to permit thorough compliance determinations .

in addition , current policies and guidance do not require don investments to comply with its enterprise architecture .

this means that the department does not have a sufficient basis for knowing if navy erp has been defined to minimize overlap with and duplication of other programs' functionality and maximize interoperability among related programs .

each of these architecture alignment limitations is discussed here: the program's compliance assessments did not include all relevant architecture products .

in particular , the program did not assess compliance with the bea's technical standards profile , which outlines , for example , the standards governing how systems physically communicate with other systems and how they secure data from unauthorized access .

this is particularly important because systems like navy erp need to share information with other systems and , for these systems to accomplish this effectively and efficiently , they need to employ common standards .

a case in point is the relationship between navy erp and the global combat support system — marine corps ( gcss - mc ) program .

specifically , navy erp has identified 25 technical standards that are not in the bea technical standards profile , and gcss - mc has identified 13 technical standards that are not in the profile .

among these non - bea standards are program - unique information sharing protocols , which could limit information sharing between navy erp and gcss - mc , and with other systems .

in addition , the program office did not assess compliance with the bea products that describe system - level characteristics .

this is important because doing so would create a body of information about programs that could be used to identify common system components and services that could potentially be shared by the programs , thus avoiding wasteful duplication .

for example , our analysis of navy erp program documentation shows that it contains system functions related to receiving goods , taking physical inventories , and returning goods , which are also system functions cited by the gcss - mc program .

however , because compliance with the bea system products was not assessed , the extent to which these functions are potentially duplicative was not considered .

furthermore , the program office did not assess compliance with bea system products that describe data exchanges among systems .

as we previously reported , establishing and using standard system interfaces is a critical enabler to sharing data .

for example , navy erp program documentation indicates that it is to exchange inventory order and status data with other systems .

system interfaces are important for understanding how information is to be exchanged between systems .

however , since the program was not assessed for compliance with these products , it does not have the basis for understanding how its approach to exchanging information differs from that of other systems that it is to interface with .

compliance against each of these bea products was not assessed because dod's compliance guidance does not provide for doing so and , according to bta officials , because some bea system products are not sufficiently defined .

according to these officials , bta plans to continue to define these products as the bea evolves .

the compliance assessment was not used to identify potential areas of duplication across programs , which dod has stated is an explicit goal of its federated bea and associated investment review and decision - making processes .

more specifically , even though the compliance guidance provides for assessing programs' compliance with the bea product that defines dod operational activities , and navy erp was assessed for compliance with this product , the results were not used to identify programs that support the same operational activities and related business processes .

given that the federated bea is intended to identify and avoid not only duplications within dod components , but also between components , it is important that such commonality be addressed .

for example , bea compliance assessments for navy erp and gcss - mc , as well as two air force programs ( defense enterprise accounting and management system — air force and the air force expeditionary combat support system ) show that each program supports at least six of the same bea operational activities ( eg , conducting physical inventory , delivering property and services ) and three of these four programs support at least 18 additional operational activities ( eg , performing budgeting , managing receipt and acceptance ) .

however , since the potential overlap among these and other programs was not assessed , these programs may be investing in duplicative functionality .

reasons for this were that the compliance guidance does not provide for such analyses to be conducted and programs have not been granted access rights to use this functionality in the compliance tool .

the program's compliance assessment did not address compliance against don's enterprise architecture , which is one of the biggest members of the federated bea .

this is particularly important given that dod's approach to fully satisfying the architecture requirements of the fiscal year 2005 defense authorization act is to develop and use a federated architecture in which component architectures are to provide the additional details needed to supplement the thin layer of corporate policies , rules , and standards included in the corporate bea .

as we recently reported , don's enterprise architecture is not mature because , among other things , it is missing a sufficient description of its current and future environments in terms of business and information / data .

however , certain aspects of an architecture nevertheless exist and , according to don cio officials , these aspects will be leveraged in its efforts to develop a complete enterprise architecture .

for example , the forcenet architecture is intended to document navy's technical infrastructure .

therefore , opportunities exist for don to assess its programs in relation to these architecture products , and to understand where its programs are exposed to risks because products do not exist , are not mature , or at odds with other navy programs .

according to dod officials , compliance with the don architecture was not assessed because dod compliance policy is limited to compliance with the corporate bea , and a number of aspects of the don enterprise architecture have yet to be sufficiently developed .

the program's compliance assessment was not validated by don or dod investment oversight and decision - making authorities .

more specifically , neither the dod irbs nor the dbsmc , nor the bta in supporting both of these investment oversight and decision - making authorities , reviewed the program's assessments .

according to bta officials , under dod's tiered approach to investment accountability , these entities are not responsible for validating programs' compliance assessments .

rather , this is a component responsibility , and thus they rely on the military departments and defense agencies to validate the assessments .

however , don office of the cio , which is responsible for precertifying investments as compliant before they are reviewed by the irb , did not validate any of the program's compliance assessments .

according to office of the cio officials , they rely on functional area managers to validate a program's compliance assessments .

however , no don policy or guidance exists that describes how the functional area managers should conduct such validations .

cio officials stated that this is because these authorities do not have the resources that they need to validate the assessments , and because a number of aspects of the don architecture are not yet sufficiently developed .

validation of program assessments is further complicated by the absence of information captured in the assessment tool about what program documentation or other source materials were used by the program office in making its compliance determinations .

specifically , the tool is only configured , and thus was only used , to capture the results of a program's comparison of program architecture products to bea products .

thus , it was not used to capture the system products used in making these determinations .

the limitations in existing bea compliance - related policy and guidance , the supporting compliance assessment tool , and the federated bea , put programs like navy erp at increased risk of being defined and implemented in a way that does not sufficiently ensure interoperability and avoid duplication and overlap .

we recently completed a review examining multiple programs' compliance with the federated bea , including navy erp , for the senate armed services committee , subcommittee on readiness and management support .

we addressed the architectural compliance guidance , tool , and validation limitations as part of this review .

the investment in navy erp has been economically justified on the basis of expected life cycle benefits that far exceed estimated life cycle costs .

according to the program's benefit / cost analysis , navy erp will produce about $8.6 billion in estimated benefits for an estimated cost of about $2.4 billion over its 20-year life cycle .

while these benefit estimates were not subject to any analysis of how uncertainty in assumptions and data could impact the estimates , as called for by relevant guidance , our examination of key uncertainty variables , such as the timing of legacy systems' retirement , showed that the savings impact would be relatively minor .

however , the reliability of the cost estimate is limited because it was derived using several , but not all , key estimating practices .

for example , the estimate was not grounded in a historical record of comparable data from similar programs and was not based on a reliable schedule baseline , which are both necessary to having a cost estimate that can be considered credible and accurate .

these practices were not employed for various reasons , including dod's lack of historical data from similar programs and the lack of an integrated master schedule for the program that includes all releases .

notwithstanding the fact that these limitations could materially increase the $2.4 billion cost estimate , it is nevertheless unlikely that these factors would increase the estimate to a level approaching the program's benefit expectations .

therefore , we have no reason to believe that navy erp will not produce a positive return on investment .

forecasting expected benefits over the life of a program is a key aspect of economically justifying an investment .

the office of management and budget ( omb ) guidance advocates economically justifying investments on the basis of expected benefits , costs , and risks .

since estimates of benefits can be uncertain because of the imprecision in both the underlying data and modeling assumptions used , the guidance also provides for analyzing and reporting the effects of this uncertainty .

by doing this , informed investment decision making can occur through the life of the program , and a baseline can be established against which to compare the accrual of actual benefits from deployed system capabilities .

the most recent economic analysis , dated august 2007 , includes monetized benefit estimates for fiscal years 2004 – 2023 , in three key areas — about $2.7 billion in legacy system cost savings , $3.3 billion in cost savings from inventory reductions , and $2.7 billion in cost savings from labor productivity improvements .

collectively , these benefits total about $8.6 billion .

the program office calculated expected benefits in terms of cost savings , which is consistent with established practices and guidance .

for example , the program is to result in the retirement of 138 legacy systems ( including the 4 pilot systems ) between fiscal years 2005 and 2015 , and the yearly maintenance costs for a single system are expected to be as high as about $39 million .

according to relevant guidance , cost saving estimates should also be analyzed in terms of how uncertainty in assumptions and data could impact them .

however , the program office did not perform such uncertainty analysis .

according to program officials , uncertainty analysis is not warranted because they have taken and continue to take steps to validate the assumptions and the data , such as using the latest budget data associated with the legacy systems , and monitoring changes to the systems' retirement dates .

while these steps are positive , they do not eliminate the need for uncertainty analysis .

accordingly , we assessed key uncertainty variables , such as the timing of the legacy systems' retirement , and found that the retirement dates of some of these systems have changed since the estimate was prepared , due to , among other things , schedule delays in the program .

while the inherent uncertainty in these dates would reduce expected savings ( eg , only $11 million based on the 134 legacy systems that we examined ) , the reduction would be small relative to a total benefit estimate of $8.6 billion .

a reliable cost estimate is a key variable in calculating return on investment , and it provides the basis for informed investment decision making , realistic budget formulation and program resourcing , meaningful progress measurement , proactive course correction , and accountability for results .

according to omb , programs must maintain current and well - documented cost estimates , and these estimates must encompass the full life cycle of the program .

omb states that generating reliable cost estimates is a critical function necessary to support omb's capital programming process .

without reliable estimates , programs are at increased risk of experiencing cost overruns , missed deadlines , and performance shortfalls .

our research has identified a number of practices that are the basis of effective program cost estimating .

we have issued guidance that associates these practices with four characteristics of a reliable cost estimate .

specifically , these four characteristics are as follows: comprehensive: the cost estimates should include both government and contractor costs over the program's full life cycle , from the inception of the program through design , development , deployment , and operation and maintenance to retirement .

they should also provide an appropriate level of detail to ensure that cost elements are neither omitted nor double counted and include documentation of all cost - influencing ground rules and assumptions .

well - documented: the cost estimates should have clearly defined purposes and be supported by documented descriptions of key program or system characteristics ( eg , relationships with other systems , performance parameters ) .

additionally , they should capture in writing such things as the source data used and their significance , the calculations performed and their results , and the rationale for choosing a particular estimating method or reference .

moreover , this information should be captured in such a way that the data used to derive the estimate can be traced back to , and verified against , their sources .

the final cost estimate should be reviewed and accepted by management on the basis of confidence in the estimating process and the estimate produced by the process .

accurate: the cost estimates should provide for results that are unbiased and should not be overly conservative or optimistic ( i.e. , they should represent most likely costs ) .

in addition , the estimates should be updated regularly to reflect material changes in the program , and steps should be taken to minimize mathematical mistakes and their significance .

among other things , the estimate should be grounded in a historical record of cost estimating and actual experiences on comparable programs .

credible: the cost estimates should discuss any limitations in the analysis performed due to uncertainty or biases surrounding data or assumptions .

further , the estimates' derivation should provide for varying any major assumptions and recalculating outcomes based on sensitivity analyses , and their associated risks and inherent uncertainty should be disclosed .

also , the estimates should be verified based on cross - checks using other estimating methods and by comparing the results with independent cost estimates .

the $2.4 billion life cycle cost estimate for navy erp reflects many of the practices associated with a reliable cost estimate , including all practices associated with being comprehensive and well - documented , and several related to being accurate and credible ( see table 5 ) .

however , several important practices related to accuracy and credibility were not performed .

to be reliable , a cost estimate should be comprehensive , well - documented , accurate , and credible .

the cost estimate is comprehensive because it includes both the government and contractor costs specific to development , acquisition ( nondevelopment ) , implementation , and operations and support over the program's 20-year life cycle .

moreover , the estimate clearly describes how the various subelements are aggregated to produce amounts for each cost category , thereby ensuring that all pertinent costs are included , and no costs are double counted .

finally , cost - influencing ground rules and assumptions , such as the program's schedule , labor rates , and inflation rates are documented .

the cost estimate is also well - documented in that the purpose of the cost estimate is clearly defined , and the technical baseline includes , among other things , the hardware components and planned performance parameters .

furthermore , the calculations and results used to derive the estimate are documented , including descriptions of the methodologies used and evidence of traceability back to source data ( eg , vendor quotes , salary tables ) .

also , the cost estimate was reviewed by the naval center for cost analysis and the office of the secretary of defense , director for program analysis and evaluation , which adds a level of confidence in the estimating process and the estimate produced .

however , the estimate lacks accuracy because not all important practices related to this characteristic were performed .

specifically , while the estimate is grounded in documented assumptions ( eg , hardware refreshment every 5 years ) and periodically updated to reflect changes to the program , it is not adequately grounded in historical experience with comparable programs .

while the program office did leverage historical cost data from the navy erp pilot programs , program officials told us that the level of cost accounting on these programs did not provide sufficient data .

as stated in our guide , estimates should be based on historical records of cost and schedule estimates from comparable programs , and such historical data should be maintained and used for evaluation purposes and future estimates on comparable programs .

the importance of doing so is evident by the fact that navy erp's cost estimate has increased by about $570 million since fiscal year 2003 , which program officials attributed to , among other things , site implementation costs ( eg , training and converting legacy system data ) not included in the original cost estimate , schedule delays , and the lack of historical data from similar erp programs .

this lack of cost data for large - scale erp programs is , in part , due to dod not having a standardized cost element structure for these programs that can be used for capturing actual cost data , which is a prerequisite to capturing and maintaining the kind of historical data that can inform cost estimates on similar programs .

this means that programs like navy erp will not be able to ground their cost estimates in actual costs from comparable programs .

according to officials with the defense cost and resource center , such cost element structures are needed , along with a requirement for programs to report on their costs , but approval and resources have yet to be gained for either these structures or the reporting of their costs .

we recently completed work that addressed standardization of dod's erp cost element structure and maintenance of a database for historical erp cost data for use on erp programs .

compounding the estimate's limited accuracy are limitations in its credibility .

specifically , while the estimate satisfies some of the key practices for a credible cost estimate ( eg , confirming key cost drivers , performing sensitivity analyses , and having an independent cost estimate prepared by the naval center for cost analysis that was within 11 percent of the program's estimate ) , the program lacks a reliable schedule baseline , which is a key component of a reliable cost estimate because it serves as the basis for future work to be performed .

other factors that limit confidence in the cost estimate's accuracy are ( 1 ) past increases in the program's cost estimate ( as discussed earlier ) and ( 2 ) trends in evm data ( as discussed later ) .

taken together , the program's cost estimate is not sufficiently credible and accurate and thus not reliable .

while important cost estimating practices were not implemented , it is nevertheless unlikely that these limitations would materially increase the $2.4 billion cost estimate to a level approaching the program's $8.6 billion benefit expectations .

measuring and reporting progress against cost and schedule commitments ( i.e. , baselines ) is a vital element of effective program management .

evm provides a proven means for measuring such progress and thereby identifying potential cost overruns and schedule delays early , when their impact can be minimized .

to its credit , the program has elected to implement program - level evm , which is a best practice that has rarely been implemented in the federal government .

in doing so , however , basic evm activities have not been executed .

in particular , an integrated baseline review , which is to verify that the program's cost and schedule are reasonable given the program's scope of work and associated risks , has not been performed .

moreover , other accepted industry standards have not been sufficiently implemented , and surveillance of evm implementation by an entity independent of the program office has not occurred .

not performing these important practices has contributed to the cost overruns and lengthy schedule delays already experienced on release 1.0 , and they will likely result in more .

in fact , our analysis of the latest estimate to complete just the budgeted development work for all three releases , which is about $844 million , shows that this estimate will most likely be exceeded by about $152 million .

as we previously reported , evm offers many benefits when done properly .

in particular , it allows performance to be measured , and it serves as an early warning system for deviations from plans .

it , therefore , enables a program office to mitigate the risks of cost and schedule overruns .

omb policy recognizes the use of evm as an important part of program management and decision making .

implementing evm at the program level rather than just the contract level is considered a best practice , and omb recently began requiring it to measure how well a program's approved cost , schedule , and performance goals are being met .

according to omb , integrating government and contractor cost , schedule , and performance status should result in better program execution through more effective management .

in addition , integrated evm data can be used to better justify budget requests .

to minimize the risk associated with its decision to transition responsibility for navy erp system integration from the contractor to the government and to improve cost and schedule performance , the program office elected in october 2006 to perform evm at the program level .

we support the use of program - level evm .

however , if not implemented effectively , this program - level approach will be of little value .

a fundamental aspect of effective evm is the development of a performance measurement baseline ( pmb ) , which represents the cumulative value of planned work and serves as the baseline against which variances are calculated .

according to relevant best practice guidance , a pmb consists of a complete work breakdown structure , a complete integrated master schedule , and accurate budgets for all planned work .

to validate the pmb , an integrated baseline review is performed to obtain stakeholder agreement on the baseline .

according to dod guidance and best practices , such a review should be held within 6 months of a contract award and conducted on an as needed basis throughout the life of a program to ensure that the baseline reflects ( 1 ) all tasks in the statement of work , ( 2 ) adequate resources ( staff and materials ) to complete the tasks , and ( 3 ) integration of the tasks into a well - defined schedule .

further , the contract performance reports that are to be used to monitor performance against the pmb should be validated during the integrated baseline review .

the program office has satisfied some of the prerequisites for having a reliable pmb , such as developing a work breakdown structure and specifying the contract performance reports that are to be used to monitor performance .

however , it has not conducted an integrated baseline review .

specifically , a review was not conducted for release 1.0 , even though the contract was finalized about 30 months ago ( january 2006 ) .

also , while the review for release 1.1 was recently scheduled for august 2008 , this is 8 months later than when such a review should be held , according to dod guidance and best practices .

this means that the reasonableness of the program's scope and schedule relative to the program risks has not been assured , and has likely been , and will likely continue to be a primary contributor to future cost increases and schedule delays .

according to program officials , a review was not performed on the first release because development of this release was largely complete by the time the program office established the underlying capabilities needed to perform program - level evm .

in addition , program officials stated that an integrated baseline review has yet to be performed on the other two releases because their priority has been on deploying and stabilizing the first release .

in our view , not assuring the validity of the pmb precludes effective implementation of evm .

until a review is conducted , dod will not have reasonable assurance that the program's scope and schedule are achievable , and thus , additional cost and schedule overruns are likely .

in 1996 , dod adopted industry evm guidance that identifies 32 essential practices organized into five categories: ( 1 ) organization ; ( 2 ) planning , scheduling and budgeting ; ( 3 ) accounting ; ( 4 ) analysis and management reports ; and ( 5 ) revisions and data maintenance .

dod requires that all programs' implementation of evm undergo a compliance audit against the 32 industry practices .

in addition , dod policy and guidance state that independent surveillance of evm should occur over the life of the program to guarantee the validity of the performance data and ensure that evm is being used effectively to manage cost , schedule , and technical performance .

on navy erp , compliance with the 32 accepted industry practices has not been verified , and surveillance of evm by an independent entity has not occurred .

therefore , the program does not have the required basis for ensuring that evm is being effectively implemented on navy erp .

according to program officials , surveillance was performed by navair for release 1.0 .

however , navair officials said that they did not perform such surveillance because they did not receive the release 1.0 cost performance data needed to do so .

program officials also stated that don's center for earned value management has conducted an initial assessment of their evm management system , and that they intend to have the center perform surveillance .

however , they did not have a plan for accomplishing this .

until compliance with the standards is verified and continuous surveillance occurs , and deviations are addressed , the program will likely continue to experience cost overruns and schedule delays .

the success of any program depends in part on having a reliable schedule of when the program's work activities will occur , how long they will take , and how they are related to one another .

as such , the schedule not only provides a road map for the systematic execution of a program but also provides the means by which to estimate costs , gauge progress , identify and address potential problems , and promote accountability .

our research has identified nine practices associated with effective schedule estimating .

these practices are ( 1 ) capturing key activities , ( 2 ) sequencing key activities , ( 3 ) establishing the duration of key activities , ( 4 ) assigning resources to key activities , ( 5 ) integrating key activities horizontally and vertically , ( 6 ) establishing the critical path for key activities , ( 7 ) identifying “float time” between key activities , ( 8 ) distributing reserves to high - risk activities , and ( 9 ) performing a schedule risk analysis .

the program's estimated schedule was developed using some of these practices , but several key practices were not fully employed that are fundamental to having a schedule that provides a sufficiently reliable basis for estimating costs , measuring progress and forecasting slippages .

on the positive side , the schedule for the first two releases captures key activities and their durations and is integrated horizontally and vertically , meaning that multiple teams executing different aspects of the program can effectively work to the same master schedule .

moreover , for these two releases , the program has established float time between key activities and distributed schedule reserve to high - risk activities .

however , the program has not adequately sequenced and assigned resources to key program activities .

moreover , the estimated schedule for the first increment is not grounded in an integrated master schedule of all the releases , and thus the schedule for this increment does not reflect the program's critical path of work that must be performed to achieve the target completion date .

also , it does not reflect the results of a schedule - risk analysis across all three releases with schedule reserve allocated to high - risk activities because such risks were not examined .

see table 6 for the results of our analyses relative to each of the nine practices .

according to program documentation , they have plans to address the logical sequencing of activities ( to ensure that it reflects how work is to be performed ) , but program officials stated that they do not plan to combine all three releases into a single integrated master schedule for the entire first increment of the program because doing so would produce an overly complex and nonexecutable schedule involving as many as 15,000 activities .

however , our research of and experience in evaluating major programs' use of evm and integrated master schedules show that while large , complex programs necessitate schedules involving thousands of activities , successful programs ensure that their schedules integrate these activities .

in our view , not adequately performing these practices does not allow the program to effectively assign resources , identify the critical path , and perform a schedule risk analysis that would allow it to understand , disclose , and compensate for its schedule risks .

this means that the program is not well - positioned to understand progress and forecast its impact .

to illustrate , the program recently experienced delays in deploying its first release at navair , which according to a recent operational test and evaluation report has significantly affected the schedule's critical path .

these schedule impacts are because resources supporting the deployment at navair began to shift to the next scheduled deployment site and thus are no longer available to resolve critical issues at navair .

since the schedule baseline is not integrated across all releases , the impact of this delay on other releases , and thus the program as a whole , cannot be readily determined .

program data show a pattern of actual cost overruns and schedule delays between january 2007 and may 2008 .

moreover , our analysis of the data supports a most likely program cost growth of about $152 million to complete all three releases .

differences from the pmb are measured in both cost and schedule variances .

positive variances indicate that activities are costing less or are completed ahead of schedule .

negative variances indicate that activities are costing more or are falling behind schedule .

these cost and schedule variances can then be used in forecasting the cost and time needed to complete the program .

based on program - provided data for the first increment over a 17-month period ending may 2008 , the program has experienced negative cost variances .

specifically , while these cost variances have fluctuated during this period , they have consistently been negative .

 ( see fig .

4. ) .

moreover , our analysis of the cost to complete just the budgeted development work ( also known as the pmb ) for all three releases , which is about $844 million , will be exceeded by between about $102 million and $316 million , with a most likely overrun of about $152 million .

in contrast , the program office reports that the overrun at completion will be $55 million but has yet to provide us with documentation supporting this calculation .

moreover , our calculation does not reflect the recent problems discovered during the operational test and evaluation at navair and thus the overrun is likely to be higher .

during this same 17-month period , the program has experienced negative schedule variances and , since january 2008 , they have almost doubled each month .

further , as of may 2008 , the program had not completed about $24 million in scheduled work .

 ( see fig .

5. ) .

an inability to meet schedule performance is a frequent indication of future cost increases , as more spending is often necessary to resolve schedule delays .

because the program office has not performed important reliability checks , such as evm validation and integrated baseline reviews , as discussed above , we cannot be certain that the pmb is reliable ( i.e. , reflects all the work to be done and has identified all the risks ) .

as a result , the overrun that we are forecasting could be higher .

by not executing basic evm practices , the program has and will likely continue to experience cost and schedule shortfalls .

until the program office implements these important evm practices , it will likely not be able to track actual program costs and schedules close to estimates .

well - defined and managed requirements are recognized by dod guidance as essential , and they can be viewed as a cornerstone of effective system acquisition .

one aspect of effective requirements management is requirements traceability .

by tracing requirements both backward from system requirements to higher level business or operational requirements and forward to system design specifications and test plans , the chances of the deployed product satisfying requirements is increased , and the ability to understand the impact of any requirement changes and thus make informed decisions about such changes , is enhanced .

the program office is effectively implementing requirements traceability for its 1,733 release 1.0 system requirements .

to verify this traceability , we randomly selected and analyzed 60 of the 1,733 system requirements and confirmed that 58 of the 60 were traceable both backward to higher level requirements and forward to design specifications and test results .

the remaining 2 had been allocated to the other releases , and thus we also confirmed the program's ability to maintain traceability between product releases .

in doing so , the program utilized a tool called doors , which if implemented properly , allows each requirement to be linked from its most conceptual definition to its most detailed definition , as well as to design specifications and test cases .

in effect , the tool maintains the linkages among requirement documents , design documents , and test cases even if requirements change .

if don continues to effectively implement requirements traceability , it will increase the chances that system requirements will be met by the deployed system .

proactively managing program risks is a key acquisition management control and , if defined and implemented properly , it can increase the chances of programs delivering promised capabilities and benefits on time and within budget .

to the program office's credit , it has defined a risk management process that meets relevant guidance .

however , it has not effectively implemented the process for all identified risks .

as a result , these risks have not been proactively mitigated and either have contributed to cost and schedule shortfalls , or could potentially contribute to such shortfalls .

dod acquisition management guidance , as well as other relevant guidance advocates identifying facts and circumstances that can increase the probability of an acquisition's failing to meet cost , schedule , and performance commitments and then taking steps to reduce the probability of their occurrence and impact .

in brief , effective risk management consists of: ( 1 ) establishing a written plan for managing risks ; ( 2 ) designating responsibility for risk management activities ; ( 3 ) encouraging program - wide participation in the identification and mitigation of risks ; ( 4 ) defining and implementing a process that provides for the identification , analysis , and mitigation of risks ; and ( 5 ) examining the status of identified risks in program milestone reviews .

the program office has developed a written plan for managing risks and established a process that together provide for the above - cited risk management practices .

moreover , it has largely followed its plan and process as per the following examples: the program manager has been assigned overall responsibility for managing risks and serves as the chair of the risk management board .

also , a functional team lead ( i.e. , subject matter expert ) is assigned responsibility for analyzing and mitigating each identified risk .

program - wide participation in the identification , analysis , and mitigation of risks is encouraged .

specifically , a manager for each release is responsible for providing risk management guidance to the staff , which includes staff identification and analysis of risks .

also , according to the program office's risk management plan , all program personnel can submit a risk for approval .

in addition , stakeholders participate in risk management activities during acquisition milestone reviews .

the program office has identified and categorized individual risks .

as of june 2008 , the risk database contained 15 active risks — 3 high , 8 medium , and 4 low .

program risks are considered during program milestone reviews .

for example , during the program's critical design review , which is a key event of the system development and demonstration phase , key risks regarding implementing new business processes and legacy system changes were discussed .

furthermore , the program manager receives a monthly risk report that describes the status of program risks .

however , the program office has not consistently followed other aspects of its process .

in particular , it has not effectively implemented steps for mitigating the risks associated with ( 1 ) converting data from navair's legacy systems to run on navy erp and ( 2 ) positioning navair for adopting the new business processes embedded in navy erp .

as we have previously reported , it is important for organizations that are to operate and use commercial off - the - shelf software products , such as navy erp , to proactively manage and position themselves for the organizational impact of introducing functionality embedded in the commercial products .

if they do not , the organization's performance will suffer .

to the program office's credit , it identified numerous risks associated with data conversion and organizational change management and developed and implemented strategies that were intended to mitigate these risks .

however , it closed these risks even though they were never effectively mitigated , as evidenced by the results of recently completed don operational test and evaluation .

according to the june 2008 operational test and evaluation report for navair , significant problems relating to both legacy system data conversion and adoption of new business processes were experienced .

the report states that these problems have contributed to increases in the costs to operate the system , including unexpected manual effort .

it further states that these problems have rendered the deployed version not operationally effective and that deployment of the system to other sites should not occur until the change management process has been analyzed and improved .

it also attributed the realization of the problems to the program office and navair not having adequately engaged and communicated early with each other to coordinate and resolve differences in organizational perspectives and priorities and provide intensive pre - deployment preparation and training .

program officials acknowledged these shortcomings and attributed them to their limited authority over the commands .

in this regard , they have previously surfaced these risks with department oversight and approval authorities , but actions were not taken by these authorities that ensured that the risks were being effectively mitigated .

beyond not effectively mitigating these risks , the program office has not ensured that all risks are captured in the risk inventory .

for example , the inventory does not include the risks described in this report that are associated with not having adequately demonstrated the program's alignment to the federated bea and not having implemented program - level evm in a manner that reflects industry practices .

this means that these risks are not being disclosed or mitigated .

by not effectively addressing all risks associated with the program , these risks can and have become problems that contribute to cost and schedule shortfalls .

until all significant risks are proactively addressed , to include ensuring that all associated mitigation steps are implemented and that they accomplished their intended purpose , the program will likely experience further problems at subsequent deployment sites .

dod's success in delivering large - scale business systems , such as navy erp , is in large part determined by the extent to which it employs the kind of rigorous and disciplined it management controls that are reflected in department policies and related guidance .

while implementing these controls does not guarantee a successful program , it does minimize a program's exposure to risk and thus the likelihood that it will fall short of expectations .

in the case of navy erp , living up to expectations is important because the program is large , complex , and critical to addressing the department's long - standing problems related to financial transparency and asset visibility .

the effectiveness to which key it management controls have been implemented in navy erp varies , with one control and several aspects of others being effectively implemented , and others less so .

moreover , those controls that have not been effectively implemented have , in part , contributed to the sizable cost and schedule shortfalls experienced to date on the program .

unless this changes , more shortfalls can be expected .

while the program office is primarily responsible for ensuring that effective it management controls are implemented , other oversight and stakeholder organizations share responsibility .

for example , even though the program has not demonstrated its alignment with the federated bea , it nevertheless followed established dod architecture compliance guidance and used the related compliance assessment tool in assessing and asserting its compliance .

the root cause for not demonstrating compliance thus is not traceable to the program office but rather is due to , among other things , the limitations of the compliance guidance and tool , and the program's oversight entities not validating the compliance assessment and assertion .

also , the reason that the program's cost estimate was not informed by the cost experiences of other programs of the same size and scope is because dod does not have a standard erp cost element structure and has not maintained a historical database of costs for like programs to use .

in contrast , effective implementation of other management controls , such as implementing evm , requirements traceability , and risk management is the responsibility of the program office .

all told , addressing the management control weaknesses requires the combined efforts of the various organizations that share responsibility for managing and overseeing the program .

by doing so , the department can better assure itself that navy erp will optimally support its performance goals and will deliver promised capabilities and benefits on time and within budget .

because we recently completed work that more broadly addresses the above cited architectural alignment and comparable program cost data limitations , we are not making recommendations in this report for addressing them .

to strengthen navy erp management control and better provide for the program's success , we are making the following recommendations: to improve the reliability of navy erp benefit estimates and cost estimates , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that future navy erp estimates include uncertainty analyses of estimated benefits , reflect the risks associated with not having cost data for comparable erp programs , and are otherwise derived in full accordance with the other key estimating practices , and economic analysis practices discussed in this report .

to enhance navy erp's use of evm , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that ( 1 ) an integrated baseline review on the last two releases of the first increment is conducted , ( 2 ) compliance against the 32 accepted industry evm practices is verified , and ( 3 ) a plan to have an independent organization perform surveillance of the program's evm system is developed and implemented .

to increase the quality of the program's integrated master schedule , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that the schedule ( 1 ) includes the logical sequencing of all activities , ( 2 ) reflects whether all required resources will be available when needed , ( 3 ) defines a critical path that integrates all three releases , ( 4 ) allocates reserve for the high - risk activities on the entire program's critical path , and ( 5 ) incorporates the results of a schedule risk analysis for all three releases and recalculates program cost and schedule variances to more accurately determine a most likely cost and schedule overrun .

to improve navy erp's management of program risks , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that ( 1 ) the plans for mitigating the risks associated with converting data from legacy systems to navy erp and positioning the commands for adopting the new business processes embedded in the navy erp are re - evaluated in light of the recent experience with navair and adjusted accordingly , ( 2 ) the status and results of these and other mitigation plans' implementation are periodically reported to program oversight and approval authorities , ( 3 ) these authorities ensure that those entities responsible for implementing these strategies are held accountable for doing so , and ( 4 ) each of the risks discussed in this report are included in the program's inventory of active risks and managed accordingly .

in written comments on a draft of this report , signed by the deputy under secretary of defense ( business transformation ) and reprinted in appendix ii , dod stated that it concurred with two of our four recommendations and partially concurred with the remaining two .

further , it stated that it has taken steps to address some of our recommendations , adding that it is committed to implementing recommendations that contribute to the program's success .

the department's comments relative to both of the recommendations that it partially concurred with , as well as additional comments , are discussed below .

for our recommendation associated with improving the program's benefit and cost estimates , dod concurred with two of the recommendation's three parts , but it did not concur with one part — ensuring that future cost estimates reflect the risk of not having cost data for comparable programs .

while acknowledging that the program had limited cost data from comparable programs on which to base its cost estimate , dod stated that an uncertainty analysis had been applied to the estimate to account for the risk associated with not having such data .

the department further stated that actual experience on the program will continue to be used to refine the program's cost estimating methodology .

while we support dod's stated commitment to using actual program cost experience in deriving future estimates , we do not agree that the latest estimate accounted for the risk of not having cost data from comparable programs .

we examined the uncertainty analysis as part of our review , and found that it did not recognize this risk .

moreover , dod's comments offered no new evidence to the contrary .

for our recommendation associated with improving the program's schedule estimating , dod concurred with four of the recommendation's five parts , and partially concurred with one part — ensuring that the schedule defines a critical path that integrates all releases .

in taking this position , the department stated that a critical path has been established for each release rather than across all three releases , and it attributes this to the size and complexity of the program .

we do not take issue with either of these statements , as they are already recognized in our report .

however , dod offers no new information in its comments .

further , our report also recognizes that to be successful , large and complex programs that involve thousands of activities need to ensure that their schedules integrate these activities .

in this regard , we support the department's commitment to explore the feasibility of implementing this part of our recommendation .

in addition , while stating that it concurred with all parts of our recommendation associated with improving the program's use of evm , dod nevertheless provided additional comments as justification for having not conducted an integrated baseline review on release 1.0 .

specifically , it stated that when it rebaselined this release in december 2006 , the release's development activities were essentially complete and the release was in the latter stages of testing .

further , it stated that the risks associated with the release 1.0 schedule were assessed 3 months after this rebaselining , and these risks were successfully mitigated .

to support this statement , it said that release 1.0 achieved its “go - live” as scheduled at navair .

we do not agree with these comments for several reasons .

first , at the time of the rebaselining , about 9 months of scheduled release 1.0 development remained , and thus the release was far from complete .

moreover , the significance of the amount of work that remained , and still remains today on release 1.0 is acknowledged in dod's own comment that the scheduled integrated baseline review for release 1.1 will also include remaining release 1.0 work .

second , the release 1.0 contract was awarded in january 2006 , and dod's own guidance requires that an integrated baseline review be conducted within 6 months of a contract's award .

third , although dod states that the program achieved “go - live” as scheduled on october 1 , 2007 , the program achieved initial operational capability 7 months later than established in the december 2006 baseline .

in addition to these comments , the department also described actions under way or planned to address our recommendations .

we support the actions described , as they are consistent with the intent of our recommendations .

if fully and properly implemented , these actions will go a long way in addressing the management control weaknesses that our recommendations are aimed at correcting .

we are sending copies of this report to interested congressional committees ; the director , office of management and budget ; the congressional budget office ; the secretary of defense ; and the department of defense office of the inspector general .

we also will make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-3439 or hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

our objective was to determine whether the department of the navy is effectively implementing information technology management controls on the navy enterprise resource planning ( navy erp ) program .

to accomplish this , we focused on the first increment of navy erp and the following management areas ( 1 ) architectural alignment , ( 2 ) economic justification , ( 3 ) earned value management ( evm ) , ( 4 ) requirements management , and ( 5 ) risk management .

to determine whether navy erp was aligned with the department of defense's ( dod ) federated business enterprise architecture ( bea ) , we reviewed the program's bea compliance assessments and system architecture products , as well as versions 4.0 , 4.1 , and 5.0 of the bea , and compared them with the bea compliance requirements described in the fiscal year 2005 defense authorization act and dod's bea compliance guidance , and we evaluated the extent to which the compliance assessments addressed all relevant bea products .

we also determined the extent to which the program - level architecture documentation supported the bea compliance assessments .

we obtained documentation , such as the bea compliance assessments from the navy erp and global combat support system — marine corps programs , as well as the air force's defense enterprise accounting and management system and air force expeditionary combat support system programs .

we then compared these assessments to identify potential redundancies or opportunities for reuse and determined if the compliance assessments examined duplication across programs , and if the tool that supports these assessments is being used to identify such duplication .

in doing so , we interviewed program officials and officials from the department of the navy , office of the chief information officer and reviewed recent gao reports to determine the extent to which the programs were assessed for compliance against the department of the navy enterprise architecture .

we also interviewed program officials and officials from the business transformation agency and the department of the navy , including the logistics functional area manager , and obtained guidance documentation from these officials to determine the extent to which the compliance assessments were subject to oversight or validation .

to determine whether the program had economically justified its investment in navy erp , we reviewed the latest economic analysis to determine the basis for the cost and benefit estimates .

this included evaluating the analysis against office of management and budget guidance and gao's cost assessment guide .

in doing so , we interviewed cognizant program officials , including the program manager and cost analysis team , regarding their respective roles , responsibilities , and actual efforts in developing and / or reviewing the economic analysis .

we also interviewed officials at the office of program analysis and evaluation and naval center for cost analysis as to their respective roles , responsibilities , and actual efforts in developing and / or reviewing the economic analysis .

we did not verify the validity of the source data used to calculate estimated benefits , such as those data used to determine the yearly costs associated with legacy systems planned for retirement .

to determine the extent to which the program had effectively implemented evm , we reviewed relevant documentation , such as contract performance reports , acquisition program baselines , performance measurement baseline , and schedule estimates and compared them with dod policies and guidance .

to identify trends that could affect the program baseline in the future , we assessed cost and schedule performance and , in doing so , we applied earned value analysis techniques to data from contract performance reports .

we compared the cost of work completed with the budgeted costs for scheduled work over a 17-month period , from january 2007 to may 2008 , to show trends in cost and schedule performance .

we also used data from the reports to estimate the likely costs at completion of the program through established earned value formulas .

this resulted in three different values , with the middle value being the most likely .

we checked evm data to see if there were any mathematical errors or inconsistencies that would lead to the data being unreliable .

we interviewed cognizant officials from the naval air systems command and program officials to determine whether the program had conducted an integrated baseline review , whether the evm system had been validated against industry guidelines , and to better understand the anomalies in the evm data and determine what outside surveillance was being done to ensure that the industry standards are being met .

we also reviewed the program's schedule estimates and compared them with relevant best practices to determine the extent to which they reflect key estimating practices that are fundamental to having a reliable schedule .

in doing so , we interviewed cognizant program officials to discuss their use of best practices in creating the program's current schedule .

to determine the extent to which the program has effectively implemented requirements management , we reviewed relevant program documentation , such as the program management plan and baseline list of requirements .

to determine the extent to which the program has maintained traceability backward to high - level business operation requirements and system requirements , and forward to system design specifications , and test plans , we randomly selected 60 program requirements and traced them both backward and forward .

this sample was designed with a 5 percent tolerable error rate at the 95 percent level of confidence so that , if we found 0 problems in our sample , we could conclude statistically that the error rate was less than 5 percent .

in addition , we interviewed program officials involved in the requirements management process to discuss their roles and responsibilities for managing requirements .

to determine the extent to which the program implemented risk management , we reviewed relevant risk management documentation , such as the program's risk management plan and risk database reports demonstrating the status of the program's major risks and compared the program office's activities with dod acquisition management guidance and related industry practices .

we also reviewed the program's mitigation process with respect to key risks to determine the extent to which these risks were effectively managed .

in doing so , we interviewed cognizant program officials , such as the program manager and risk manager , to discuss their roles and responsibilities and obtain clarification on the program's approach to managing risks associated with acquiring and implementing navy erp .

we conducted this performance audit at dod offices in the washington , d.c. , metropolitan area and annapolis , md. , from june 2007 to september 2008 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

in addition to the individual named above , key contributors to this report were neelaxi lakhmani , assistant director ; monica anatalio ; harold brumm ; neil doherty ; cheryl dottermusch ; nancy glover ; mustafa hassan ; michael holland ; ethan iczkovitz ; anh le ; josh leiling ; emily longcore ; lee mccracken ; madhav panwar ; karen richey ; melissa schermerhorn ; karl seifert ; sushmita srikanth ; jonathan ticehurst ; and adam vodraska .

