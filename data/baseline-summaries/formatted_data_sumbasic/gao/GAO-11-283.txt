a recent influx of federal funds has breathed new life into the prospect of developing an expanded national passenger rail network in the united states .

specifically , the american recovery and reinvestment act of 2009 ( recovery act ) appropriated $8 billion — significantly more than congress provided in recent years — to develop high speed and intercity passenger rail service .

interest in these funds was high , and in january 2010 the federal railroad administration ( fra ) — an agency within the department of transportation ( the department ) — selected 62 applications in 23 states and the district of columbia to receive the money .

the vast majority ( almost 90 percent ) of the $8 billion awarded went to develop new or substantially improved passenger rail corridor projects , which , in several cases , expect to deliver high speed rail service reaching speeds of more than 150 miles per hour .

the remaining funding generally went to projects focusing on upgrades and improvements to existing rail service ( typically up to 79 miles per hour ) .

with the recovery act funding , fra recognized that it needed to transform itself from essentially a rail safety organization to one that can make and oversee multibillion dollar investment choices .

this report assesses how fra made the first of those choices and ensured that national investment goals are being met .

it focuses on the extent to which fra ( 1 ) applied its established criteria to select projects ; ( 2 ) followed recommended practices for awarding discretionary grants ; and ( 3 ) communicated outcomes to the public , compared with selected other recovery act competitive grant programs .

these topics are the main focus of the report .

in addition , we are also reporting on the extent to which selected projects align with legislative and federal goals .

 ( see app .

i. ) .

our overall approach to addressing these topics was to ( 1 ) review publicly available information , such as federal legislation , plans , and other guidance , about the high speed intercity passenger rail program's evaluation , selection and communication approach , and compare it to practices used by other competitive grant programs ; ( 2 ) review documents that fra used in reviewing applications and selecting awardees to determine the extent to which fra applied its established criteria ; ( 3 ) analyze fra data on technical review scores to determine the statistical relationship between some of fra's published criteria and the selection decisions ; and ( 4 ) interview a cross - section of officials from 12 of the 40 states and the district of columbia which submitted either a preapplication or an application for recovery act funding ( selected to reflect a range of application outcomes , award amounts , number of applications , and geographic location ) , a random sample of 18 of the 44 department reviewers which included at least one person from each applicant review panel , and other fra officials who oversaw the evaluation and selected awards .

we focused our review on projects selected by fra in january 2010 and funded through the recovery act , which included applications submitted for ready - to - go projects ( called “track 1a” ) , the completion of environmental and preliminary engineering requirements necessary to prepare projects for future funding ( called “track 1b” ) , and projects to develop new high speed rail or intercity passenger services or substantially upgrade existing corridor service ( called “track 2” ) .

we assessed the reliability of fra's scoring data by conducting a series of data tests , reviewing documents and reports about fra's data systems , and speaking with officials familiar with the data .

we determined that these data are sufficiently reliable for our reporting purposes .

 ( additional information on our scope and methodology is contained in app .

ii. ) .

we conducted this performance audit from april 2010 to march 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

fra is the primary federal agency responsible for overseeing safety in the railroad industry , as well as for distributing federal funds for intercity passenger rail service .

fra also administers federal operating and capital grants to the national railroad passenger corporation ( known as amtrak ) , which have averaged between $1 billion and $1.3 billion per year since fiscal year 2003 .

fra also approves railroad rehabilitation and improvement financing loans and rail line relocation and improvement capital grants , and is the granting agency for the $120 million in fiscal year 2008 and fiscal year 2009 capital funds for intercity passenger rail projects .

recent legislation has vastly increased the federal role in and federal funds for developing intercity passenger rail service .

the passenger rail investment and improvement act of 2008 ( priia ) , enacted in october 2008 , authorized more than $3.7 billion for three different federal programs for high speed rail , intercity passenger rail , and congestion reduction grants .

priia also called for fra to create a preliminary national rail plan within 1 year after passage of the act as well as a long - range national rail plan that promotes an integrated and efficient national rail system .

fra released a preliminary national rail plan in october 2009 and a subsequent progress report in september 2010 .

the recovery act , enacted in february 2009 , appropriated $8 billion for the three priia - established intercity passenger rail programs .

unlike priia , which authorized an 80 percent federal share for a project's capital costs , the recovery act provided up to 100 percent federal funding available for obligation through fiscal year 2012 and expenditure through fiscal year 2017 .

the recovery act required that the department develop a strategic plan to use these funds .

in april 2009 , fra released its strategic plan for developing high speed rail in america and distributing federal funds .

priia and the recovery act created new responsibilities for fra to plan , award , and oversee the use of new federal funds for intercity passenger rail .

in response , fra launched the high speed intercity passenger rail ( hsipr ) program in june 2009 by issuing a funding announcement and interim guidance , which outlined the requirements and procedures for obtaining federal funds .

fra further outlined the vision and goals of the program through a number of outreach events and meetings , including seven regional workshops and more than 25 one - on - one site visits and conference calls with potential state applicants .

states expressed a great deal of enthusiasm for the new program , requesting $102 billion across 278 preapplications , which fra used to gauge initial interest and anticipate its staffing needs to manage the program .

states , including the district of columbia , ultimately submitted 229 applications for $57.8 billion in recovery act funds .

fra asked applicants to submit recovery act project applications under three tracks: 1a , 1b , and 2 .

track 1 was intended to primarily address economic recovery goals , and could either focus on ready - to - go projects ( track 1a ) or the completion of environmental and preliminary engineering requirements necessary to prepare projects for future funding ( track 1b ) .

track 2 focused on much larger , long - term projects to develop new high speed rail services or substantially upgrade existing corridor service .

while track 1 and track 2 applications were submitted and reviewed at different times , fra used a similar approach to assess them , and applied the same criteria during three independent steps: eligibility determination , technical review , and selection .

 ( see fig .

1. ) .

the eligibility determination was conducted by panels of officials experienced in environmental requirements or passenger and commuter rail .

these officials used worksheets to aid in assessing application completeness and determining whether applicants and proposed projects were eligible to receive funds .

eligibility panels also made preliminary determinations as to whether applicants had substantially completed environmental requirements and whether the projects they submitted were ready to begin .

for the most part , applications deemed not yet ready or ineligible were not forwarded for technical review ; because the track 1 eligibility and technical review periods overlapped , there were two recovery act applications that received technical review scores and were later deemed not yet ready or ineligible and removed from award consideration .

the technical review was conducted by panels of officials with experience in several fields , such as passenger and commuter rail , grants management , and environmental requirements .

the technical review differed slightly for track 1 applications , which were reviewed by 12 panels composed of three reviewers , and track 2 applications , which were reviewed by a single panel of eight reviewers .

for both tracks , reviewers used guidebooks to assess applications against six technical review criteria: ( 1 ) transportation benefits , ( 2 ) economic recovery benefits , ( 3 ) other public benefits ( eg , environmental quality and energy efficiency ) , ( 4 ) project management approach , ( 5 ) sustainability of benefits , and ( 6 ) timeliness of project completion .

 ( see table 1. ) .

the guidebooks provided detailed descriptions of what was included within each of these criterion , as well as step - by - step instructions on reviewing applications that included a suggested scoring method using a scale from one ( lowest ) to five ( highest ) .

for example , the track 2 guidebook suggested applications that included more than one major weakness , were nonresponsive , or failed to address a particular criterion be given a technical review score of a one for that criterion .

applications that technical panelists determined were responsive , and included major and minor strengths and no major or very few minor weaknesses in a particular criterion , were to be given a technical review score of a five for that criterion .

after completing an individual evaluation of each application , reviewers convened within their panel to discuss their overall thoughts on the application and technical review scores for each criterion , which they could revise based on input from other panelists .

to arrive at a final score for each application , fra officials used a formula that averaged individual scores and weighted the scores based on established priorities identified in the funding announcement .

in addition , program officials standardized track 1 application scores to correct for potential inconsistencies across review panels .

after the technical review , senior department and fra officials — deputy secretary , under secretary for policy , fra administrator , and fra deputy administrator , among others — selected projects to recommend to the secretary of transportation .

they considered the technical review scores along with four additional pre - established selection criteria identified in the funding announcement: ( 1 ) region and location , ( 2 ) innovation , ( 3 ) partnerships , and ( 4 ) track type and funding round .

 ( see table 2. ) .

hsipr program officials gave five briefings to senior officials on the results of the technical review and possible factors to consider in making award decisions , such as potential project cost , service speed , shared benefits , and readiness .

program officials also provided additional information , including funding scenarios , facts sheets on individual applications , and corridor maps upon request .

according to fra , senior officials considered this information when making their recommendations , but did not numerically score or rank applications .

on january 27 , 2010 , the fra administrator recommended 62 applications for funding and the secretary of transportation concurred with these recommendations .

on january 28 , 2010 , dot announced the selections .

the selections were spread across several types of intercity passenger rail , including projects for emerging high speed rail ( operating at speeds up to 90 miles per hour ) , regional corridors ( operating at speeds between 90 and 124 miles per hour ) , and core express corridors ( operating at speeds between 125 and 250 miles per hour or more ) .

for example , the department selected one project to receive $35 million to rehabilitate track and provide service from portland to brunswick , maine at speeds up to 70 miles per hour .

another project was to receive more than $50 million to construct 11 miles of dedicated passenger rail track near rochester , new york , which will allow for service speeds up to 110 miles per hour .

a third project was selected to receive almost $2.3 billion to initiate the first part of the california's high speed rail system , which will allow for more than 200 miles per hour service between los angeles , san francisco and the central valley , and eventually , san diego .

these selections were consistent with the criteria in priia , the recovery act , and fra's strategic plan , which included broad goals that gave fra discretion in developing a national passenger rail system .

 ( additional information on the legislative and program goals , and how the selected projects fit into them , is contained in app .

i. ) .

fra applied its established criteria to determine eligibility and assess applications' technical merit .

however , its rationales for selecting projects were typically too general to determine how it applied the additional selection criteria .

when asked for more information on certain applications , fra provided specific reasons for its selection decisions , but , in our opinion , creating a detailed , comprehensive record alongside the final selections is preferable .

officials reported that they used the technical review scores as a starting point from which to apply each of the four selection criteria , which is partially supported by our analysis of fra data .

for example , we found that applications receiving a higher technical review score were about seven to eight times more likely to be selected for an award compared to those receiving a lower technical review score .

we found that fra applied eligibility criteria established in its funding announcement when determining whether applications were eligible .

specifically , eligibility criteria listed in the funding announcement aligned with criteria outlined in the worksheets used by the panelists to verify that applications were eligible .

panelists were given separate worksheets to conduct the track 1 and track 2 eligibility reviews , and each of these worksheets included eligibility criteria listed in the funding announcement .

for instance , as outlined in the funding announcement , the track 1 worksheet required eligibility panelists to indicate if the application was submitted on - time , by an eligible applicant , and with all of the required supporting documents .

similarly , the track 2 worksheet included questions regarding applicant eligibility , qualifications , and construction grant prerequisites which aligned with the eligibility criteria listed in the funding announcement .

fra also applied the established technical review criteria communicated in its funding announcement by including these criteria in the guidebooks provided to technical panelists to assess the technical merits of each application .

specifically , the guidebooks fra provided to panelists for reviewing track 1a , 1b , and 2 applications were divided into six sections that aligned with each of the six technical review criteria listed in the funding announcement .

moreover , the criteria within these sections of the guidebook often matched the criteria in the funding announcement very closely and , in some cases , word - for - word .

for example , the funding announcement stated that an applicant's experience administering similar projects would be considered under the technical review criteria of project management approach , which was included word - for - word in the project management approach section of the guidebooks .

we spoke to at least one representative from each technical review panel ; these representatives confirmed that panelists used the criteria listed in the guidebooks and did not use other criteria during their evaluation of individual applications .

senior department and fra officials recommended to the secretary of transportation applications to receive awards and the proposed amounts of the awards .

when deciding which applications to recommend for awards , senior fra officials told us that they used the results of the technical review panels and the four selection criteria .

these four criteria were described in fra's june 2009 funding announcement as: ( 1 ) region and location ( eg , ensuring geographic balance , integration into the nationwide transportation network , and assistance to economically distressed regions ) , ( 2 ) innovation ( eg , pursuing new technology with a favorable public return , promoting domestic manufacturing , and developing human capital capacity for sustainable rail development ) , ( 3 ) partnerships ( eg , multi - state planning and investment and workforce diversity ) , and ( 4 ) tracks and round timing ( eg , longer - term track 2 corridor development balanced with ready - to - go track 1 investments ) .

for example , officials stated that they used the innovation criterion to select applications with higher proposed speeds of service .

in particular , senior officials reported using this criterion to reinforce the selection of the california and florida intercity passenger rail projects , which were the only eligible projects with the potential for service above 150 miles per hour .

in another example , officials reported that they applied the partnership criterion by assessing applicants' track record with implementing large transportation projects as well as demonstrated relationships with key stakeholders , such as private railroads .

senior fra officials stated they developed their funding amount recommendations based on their professional judgment and national high speed intercity passenger rail program goals .

officials told us they accounted for the risks related to the total cost of the project during selection discussions and weighed them against the overall policy goals of developing a national high speed passenger rail network .

officials also stated that they used their professional judgment about rail systems to recommend the award amounts for each application , paying particular attention to the amounts distributed to the large , track 2 projects , and that they are continuing to assess the effect of changes to the requested funding amounts during the scope of work negotiations with awardees .

according to fra , its rationales for selecting applications are recorded in a recommendation from the fra administrator to the secretary of transportation and in a memorandum from the secretary to the administrator concurring on the recommendations and specifying potential funding amounts .

the rationales stated in these memorandums were typically vague , such as “aligns well with fra's published evaluation criteria” and “will result in significant transportation benefits preserve and create jobs.” these rationales most often restated the criteria listed in the funding announcement generally ( eg , result in significant transportation benefits ) rather than providing insight into why the department viewed projects as meritorious .

in addition , the memorandums did not provide any information on why other applications were not recommended for selection , which prevents us from assessing how the department viewed the merits of successful applications over unsuccessful ones .

for example , we found several instances in which , without documentation , it was difficult to determine the reasons why some projects were selected and others were not .

specifically , fra decided not to select six track 1a applications from new york that received higher technical review panel scores , and selected a lower scoring track 1a application from the same applicant .

fra officials subsequently told us that the lower scoring application was selected for a number of reasons , including improving the reliability of the passenger trains on the rail line , ensuring that the project will become part of the infrastructure of any significant improvements to passenger rail service west of albany , and improving the fluidity of both passenger and freight rail operations on this heavily used rail route .

similarly , fra selected a lower scoring track 1a application from illinois , but not a relatively high scoring one .

fra officials subsequently told us that they selected this application because it is an essential part of a long - standing program of projects to improve the fluidity of rail traffic in the highly congested chicago area .

fra officials also told us that the scope of the relatively high scoring track 1a application was included in illinois' selected track 2 application .

this level of information , which provides some insight into the merits of projects , was not included in the department's record of its decisions .

in addition to the memorandums , fra posted descriptions on its web site of the selected projects , their expected benefits , and prospective award amounts .

however , these descriptions are not particularly useful in understanding why these projects were selected because the cited benefits — such as reducing travel times , increasing travel speed and ridership , providing attractive transportation alternatives , and creating jobs — were supposed to be integral to all projects .

for example , fra's web site describes one project as increasing on - time performance and ultimately allowing speeds of up to 110 miles per hour on its segment , but does not give any indication why this project was meritorious .

other descriptions were similar .

fra also sent letters to individual applicants regarding its decision , and , if the application was not selected , a brief explanation as to why it was not selected .

for example , a number of these letters explained that applications were not selected because they did not meet a prerequisite , had application materials that did not provide sufficient support for the proposed activities , or did not submit all application materials necessary to adequately evaluate the project .

however , these letters did not provide further details on how the proposed projects did not meet the prerequisite , how the application materials were insufficient , or which application materials were not received .

other decision letters provided applicants with similarly broad explanations .

fra officials also told us that they called all applicants , as well as their state secretaries of transportation and state governors , to inform them of fra's decisions .

several of the states that we contacted reported that the primary purpose of these calls was for fra to provide feedback on their individual projects and , when requested , give explanations for why projects were not selected .

while applicants stated that this information will be helpful during future application rounds , there is no required written record of these conversations and , therefore , they do not provide others with insight on why selection decisions were made .

documentation of agency activities is a key part of accountability for decisions .

the department has a financial assistance guidance manual to assist agencies with administering awards competitions and which fra officials told us that they used to develop the competition framework .

the manual recommends that all discretionary project selections , such as the intercity passenger awards , include an explanation of how the projects were selected based on the established funding priorities , but does not lay out expectations for the level of explanation .

in particular , the manual recommends that officials document decisions if projects with the highest priority are not funded .

while the department documented its decisions , as required by its financial assistance manual guidance , the absence of an insightful internal record of the reasons behind award recommendations , and the final selections where they differ , can give rise to challenges to the integrity of the decisions made .

while fra was able to provide us with specific reasons on a case - by - case basis for why projects were selected , almost a year after these decisions were made , we believe creating a sufficiently detailed record has increased relevance in high - stakes , high - profile decisions , such as the intercity passenger rail awards competition in which there are vocal critics and ardent supporters of the program .

similar arguments apply for creating an internal record for amounts recommended for awards .

fra officials understood that the available recovery act funds were not sufficient to fully fund a number of the projects and sought to fund projects or portions of projects that could provide transportation benefits if no additional federal funds were available .

for these decisions , fra proposed awarding 10 states ( including the district of columbia ) all ( 100 percent ) of the funds they applied for , 8 states nearly all ( 91 – 99 percent ) of the funds they applied for , 5 states some ( 47 – 86 percent ) of the funds they applied for , and one state with slightly more ( 104 percent ) than it applied for .

 ( see fig .

2 .

see also app .

iii for dollar amounts associated with fig .

2. ) .

the applicant notification letters did not offer an explanation for why fra proposed award amounts that differed from requests , and applicants we spoke with did not report that fra had provided such information to them .

given that infrastructure projects have an inclination for cost growth , developing a record that explains why the recommended costs are appropriate for the proposed project provides integrity to the final decisions .

the current economic climate has also increased the importance of providing an internal rationale for large differences between requested funds and proposed award amounts .

many states have faced large budget deficits in 2010 that will require them to make difficult budget decisions about the future use of state funds , particularly where the recovery act awards will not provide all the funding expected to be needed to complete a project .

for example , as of june 2010 , florida had made $3 billion in budget cuts to close its budget deficit .

for its high speed rail award , florida is slated to receive less than half of what it said is needed to complete the proposed tampa to orlando high speed rail express project .

an official from the florida department of transportation is hopeful that florida will receive additional federal grants , but is unsure where the remaining funds will come from otherwise .

additionally , washington state applied for 16 separate projects totaling $976 million and was selected to receive a composite award of $590 million .

washington state officials acknowledged that the award amount will not fund all 16 of the projects , and have since reduced the scope of the application to the 11 projects that could be completed with the awarded amount while still providing the maximum benefit to the corridor .

fra officials stated they awarded amounts that differed from those requested in applications as a result of their recognition that many of the projects were based on preliminary work that was not well - refined , and that states differed in their ability to accurately estimate costs .

in contrast , north carolina received 4 percent more funding than originally requested .

according to fra , the additional funding was allocated to north carolina for possibly adding additional train frequencies for a recovery act project .

while we recognize that fra may have developed these proposed award amounts for good reasons , without a written record of the department's rationale for these adjustments , after the fact reconstructions of funding amount decisions invite outside criticism of the decisions .

one of your interests was in how the results of technical review panels aligned with final award decisions .

as discussed earlier , while fra considered the technical review panels to be an important part of its decision making , they were not the sole basis for selecting projects .

this was detailed in fra's funding announcement , which described how applications were first to be assessed against six technical review criteria and then final recommendations would be made using the technical review results and four selection criteria .

while the technical review panel evaluations alone were not meant to designate final selections , we found that of 179 eligible recovery act applications , senior management recommended 92 percent ( 57 of 62 ) of higher scoring applications for funding ; that is they received review panel scores of 3 or higher out of 5 possible points .

 ( see fig .

3. ) .

within these recommended applications , most received a technical review score of 3 or 4 , and three of the five applications that received a technical review score of 5 were recommended for selection .

one of the two applications that scored a 5 and was not selected for funding was included in a selected track 2 application .

in a few cases though , senior officials recommended applications that received a lower technical review score ( i.e. , a score of 2 ) because , according to fra , they believed these projects included freight and commuter rail service partners that were willing to make cost contributions in line with their potential benefit share , were strategically important to other selected applications , or helped to achieve regional balance .

these considerations were included in the four selection criteria senior department and fra officials said they used to evaluate applications .

for example , one of the two applications from rhode island requested $1.2 million to complete preliminary engineering and environmental reviews and received a lower overall technical review score , in part because technical reviewers did not believe the applicant had sufficiently quantified the transportation and economic recovery benefits .

this application was later recommended for selection .

according to fra , senior officials recommended applications receiving lower technical review scores , such as this rhode island application , in part to achieve greater regional balance .

additionally , fra indicated this particular application was one of the few applications proposed for the northeast corridor , which further supported the region / location selection criteria .

in another instance , senior officials selected a track 2 application from california that requested $194 million for preliminary engineering and environmental requirements for a large corridor application that received a lower technical review panel score .

according to fra , senior officials recommended some applications receiving lower technical review scores due to the projects' strategic importance to other selected applications .

officials stated that they recommended the track 2 california application because the completion of preliminary engineering and environmental requirements were necessary to move forward on several other large california projects also recommended for an award .

officials also told us that some applications receiving a higher technical review score ( i.e. , 3 , 4 , or 5 ) were not selected in order to ensure regional balance , especially when an applicant had already been selected for other large awards .

for example , a track 1a application from north carolina received a higher technical review panel score due , in part , to the anticipated transportation benefits of increased ridership and on - time - performance , and the applicant's estimates that the project would create more than 400 new jobs .

most of the projects that north carolina applied for under this application were also included as part of a larger , intercity passenger rail application that was later recommended for selection , and the state was awarded an estimated total of $545 million for high and conventional speed rail projects .

department and senior fra officials reported that higher evaluated applications were not selected if the proposed project was already included in larger selected projects , to avoid duplicative selections .

another example was fra's decision not to select a higher scoring track 1a application from florida that requested $270 million to acquire 61 miles of right - of - way .

this application was scored highly due in part to its immediate benefits and substantial contribution of state funds but , similar to north carolina , florida had already been awarded $1.25 billion for a separate large , track 2 corridor project .

in addition , almost 90 percent of the applications that scored a 4 and were not selected were submitted by applicants that had either already received a large award or had submitted a relatively high number of applications .

to provide further insight into the attributes that were consistent with being selected for recovery act awards , we examined technical review score and application data using a statistical model and found that two out of four variables we included in our model , technical review scores and the number of applications submitted per state , were significantly related to the likelihood of an application being selected for an award .

applications with higher scores ( i.e. , scores of 3 , 4 , or 5 ) were about seven to eight times more likely to be selected than those with scores of 1 or 2 .

for example , an application receiving a technical review score of 5 , the highest possible score , was more than nine times more likely to be selected for an award as an application receiving a technical review score of a 1 or 2 .

this analysis supports statements from senior department and fra officials indicating that the technical review scores were largely the basis for their selection deliberations .

additionally , we found that states submitting fewer applications ( i.e. , between one and three ) were more than three times more likely to have their application selected than states submitting higher numbers of applications ( i.e. , between four and nine ) .

this result suggests that selection officials attempted to spread the awards across different applicants , which is consistent with fra's reported efforts to attain geographic distribution .

however , the results differed somewhat for the four states that submitted 10 or more applications .

in this case two of the states had a lower likelihood of being selected for an award than states submitting fewer than 10 applications , while one state had a higher likelihood of being selected .

one additional state had about the same likelihood of being selected as states submitting between four and nine applications .

when asked about these differences across states , fra officials said that the number of applications submitted did not affect their selection decisions .

we identified six recommended practices used across the federal government to ensure a fair and objective evaluation and selection of discretionary grant awards .

these practices are based on policies and guidance used by the office of management and budget and other federal agencies — including the department , and our work .

fra substantially followed these practices , including communicating key information to applicants , planning for the competition , using a technical merit review panel with desirable characteristics , assessing applicants' ability to account for funds , and notifying applicants of awards decisions .

 ( see table 3. ) .

in our opinion , fra partially met one recommended practice: documenting the rationale for funding decisions .

as discussed previously , we believe it would have been beneficial to provide more detail about the rationales for these decisions .

according to fra officials , the methods they used to evaluate and select applications were based on best practices collected from several other federal government agencies , which we believe likely helped them meet a number of the recommended practices we identified .

communicate with potential applicants prior to the competition .

fra issued a funding announcement that included information on the $8 billion in available funding , key dates , the competition rules , the funding priorities and relative importance for each one , and the types of projects fra would consider for federal grants .

applicants we spoke with praised fra's communication and stated that fra officials did a good job providing information and answering questions during the period leading up to the preapplication and application deadlines .

for example , officials from several states indicated that fra officials participated in biweekly conference calls , which were helpful in understanding the technical aspects of how to apply .

applicants also indicated that the outreach events , particularly the site visits , helped them refine their applications and ensure projects met program requirements .

plan for administering the technical review .

fra developed two plans for determining technical merit: ( 1 ) the track 1 technical review used 12 panels each comprised of three reviewers and ( 2 ) the track 2 technical review used one panel of eight reviewers .

track 1 applications were randomly assigned across the panels , while the track 2 panel reviewed all of the eligible applications .

fra identified and asked for volunteers to participate in the technical reviews from within fra and across several other agencies within the department .

fra officials also provided reviewers with guidebooks to document their application assessments and instructed them to input the results , including scores and comments , into a centralized database .

fra standardized final track 1 application scores to account for any unintentional differences in the way panels assessed and scored applications , but did not need to standardize track 2 scores because the review was conducted by a single panel .

finally , according to officials , fra oversaw the review by examining technical review scores and comments , and conducting daily meetings with representatives from each panel to ensure panelists were consistently applying the criteria .

develop a technical review panel with certain characteristics .

fra compiled technical review panels that included staff with background in several relevant fields , such as grants management , passenger and commuter rail , and environmental requirements , and made other knowledgeable staff available if panelists had questions .

fra officials stated that panelists were also required to sign or submit a previously completed conflict of interest form to attest to their independence .

in addition , panelists were given guidebooks to assess applications that included the technical review criteria and were told by fra program officials to apply only these criteria during their efforts .

fra also trained panelists during a 1-day orientation session .

assess applicants' capabilities to account for funds .

fra required applicants to provide information on their ability to account for funds .

specifically , applicants were asked to describe their experience , if any , managing rail investment projects .

if applicants reported that they did not have experience on projects similar to the one they were proposing , fra instead asked applicants to include a plan for building the capacity to manage the project .

the application also required applicants to provide information on their financial management capability , including previous audit results , and the applicants' ability to manage potential cost overruns and financial shortfalls .

in addition , fra required applicants to submit supplemental materials such as a detailed capital cost budget , which provided a breakdown of the activities included in each application and their anticipated cost .

these pieces of information were assessed by fra through an eligibility panel , to ensure the application was complete , and a technical review panel , to evaluate the applicants' overall ability to manage the project .

notify applicants of awards decisions .

fra officials provided each applicant with a letter indicating which applications were selected and a general reason why individual applications were not selected .

while fra did not include estimated award amounts in these notification letters , this information was made publicly available on the department's web site and distributed through a press release .

in addition , most of the applicants we spoke with indicated that fra provided informal feedback on applications via telephone calls shortly after the awards were announced .

for example , an official from one applicant stated that fra provided information on ways to improve applications that were not selected , which the applicant used when applying for funds in future rounds .

document rationale for awards decisions .

according to the guidance from the department , department of commerce , the department of education , and our work , agencies should document their rationale for award decisions .

as stated previously , fra documented how it applied the technical criteria for selected projects , and provided applicants with a general explanation for selecting or rejecting individual projects .

however , as discussed in a previous section , in our view fra typically did not clearly document specific reasons for selecting individual projects , reasons for not selecting other projects , or how changes made to requested funding amounts might affect applicants' ability to achieve project goals .

according to fra , officials used lessons from a number of other government programs when developing the method for evaluating and selecting projects .

for example , one of the officials responsible for developing the funding announcement , technical review guidebooks , and the format of the technical review panels stated that he relied on his experience working with large transit grants to create a review that was both quantifiable and allowed for subjective professional judgment .

in addition , this official noted that fra examined the methods used by other agencies , such as the department of health and human services , the department of justice , and the federal transit administration , to develop and implement a list of best practices for awarding discretionary grants .

fra publicly communicated outcome information , such as a list of awards and the award amounts , at a level similar to or greater than most other recovery act competitive grant programs that we examined .

specifically , fra communicated information on award decisions to the public , but did not communicate the results of the technical review that had contributed to these decisions .

only one of the programs that we examined — the department of education's state innovation grants ( known as race to the top ) — publicly communicated the results of its technical review , which include technical scores and comments ; however , this program used a much different approach for selecting awardees than the hsipr program .

members of congress and the president have emphasized the need for accountability , efficiency , and transparency in the expenditure of recovery act funds and have made it a central principle of the act .

however , the act did not define the attributes of transparency or how deeply an agency's actions should be transparent .

we also did not find any non - recovery act requirement or guidance instructing federal programs to publicly disclose the reasons for their selection decisions .

to assess the extent to which fra publicly communicated outcome information , we compared the hsipr program to 21 other recovery act competitive grant programs , including race to the top .

 ( see fig .

4. ) .

we selected 20 of these programs randomly from a list of almost 200 competitively awarded grant programs that distributed recovery act funds .

we included the 21st program , race to the top , because it was of interest to you .

fra publicly communicated at least as much outcome information as all but one recovery act competitive grant programs we reviewed .

specifically , fra publicly communicated through its web site the selection decisions , including the amount of funds requested , general benefits from the project , and the potential award amounts for the 62 recovery act applications that it selected .

it did not communicate the results of the technical review .

out of the other 21 competitively awarded recovery act programs we examined , 13 communicated selection information similar to fra , including awards and award amounts , but not the results of the technical review .

for example , the department of health and human services' national institutes of health published a list of 21,581 award winners for nearly $9 billion , but , similar to the hsipr program , did not report the results of the technical review .

eight other programs conveyed less information than fra and did not publicly communicate the results of the technical review or the awards and award amounts .

race to the top was the only program we examined that publicly provided the results of its technical review .

these results , which were posted on the department of education's web site , included scores and comments from reviewers for each applicant , but were not connected to individual reviewers by name .

according to its web site , the department of education decided to release this level of detailed information because the $4 billion race to the top program was larger than any other discretionary program the department of education had previously administered , and officials sought to ensure the highest level of integrity and transparency .

unlike the hsipr program , however , race to the top used these scores as the sole basis for selecting awards and only chose applicants receiving the highest scores .

as described in the previous section , the technical review scores were an important component for making hsipr selection decisions , but did not include consideration of additional pre - established selection criteria designed to ensure long - term success and sustainability of the program .

as such , publishing them without additional decision making information on the specific reasons for selecting and not selecting individual applications could lead to erroneous conclusions about fra's decisions .

according to fra officials , the results of the technical review were not communicated because department officials were concerned that associating technical review scores and comments with a specific reviewer could discourage reviewers from participating in future department competitive grant evaluations .

furthermore , in their view , this might also prevent reviewers in future funding rounds from providing candid evaluations .

however , as the race to the top program demonstrated , it would be possible for fra to present overall technical panel review assessments or their individual comments without linking individuals' names to comments , if it chooses to do so .

fra officials stated that the anonymous disclosure of technical scores and comments would still prevent fra and department leadership and staff from frankly expressing their individual judgments , as they might still be concerned over how theses opinions would reflect on the fra and hsipr program if they were made public .

the $8 billion appropriated by the recovery act for the high speed intercity passenger rail program represents a large investment in the development of a national passenger rail network .

fra established a fair and objective approach for distributing these funds and substantially followed recommended discretionary grant award practices used throughout the government .

the exception is what we view as incomplete documentation of why some applications were chosen and not others , and how fra decided to distribute the funds at the time those decisions were made .

this incomplete documentation is notable given the robust documentation of the other steps used to determine eligibility and assess technical merit .

we believe that establishing a record that provides insight into why decisions were made , rather than merely restating general technical review and selection criteria , including amounts to be provided , would enhance the credibility of fra's awards decisions to the extent that this record confirms that selected projects aligned with established criteria and goals .

by not establishing this record , fra invites skepticism about the overall fairness of its decisions , even if they are sound , and hinders meaningful disclosure of how it made its decisions , if it chooses to do so .

to help ensure accountability over federal funds , we recommend that the secretary of transportation direct the administrator of the federal railroad administration to create additional records that document the rationales for award decisions in future hsipr funding rounds , including substantive reasons ( 1 ) why individual projects are selected or not selected and ( 2 ) for changes made to requested funding amounts .

we provided a draft of this report to the department of transportation for its review and comment .

the department told us that it carefully constructed the grant processes for the hsipr program based on extensive review and consideration of best practices both within and outside the agency with the intent of providing a comprehensive and transparent process .

the department indicated that its overall intent was to select the best projects that offered the greatest available and achievable benefit to the nation .

the department told us that it would carefully consider our recommendation to determine if there are means to further enhance the transparency of its grant selection process with additional documentation , without creating a process that is unduly burdensome to administer .

the department also offered technical comments which we incorporated as appropriate .

as agreed with your office , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

we are sending copies of this report to congressional subcommittees with responsibilities for surface transportation issues ; the director , office of management and budget ; the secretary of transportation ; and the administrator of the federal railroad administration .

in addition , this report will be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have any questions regarding this report , please contact me at ( 202 ) 512-2834 or flemings@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are owen bruce , matthew cook , colin fallon , michele fejfar , maria gaona , grant mallie , james ratzenberger , douglas sloane , matthew voit , and crystal wesco .

we examined the extent to which american recovery and reinvestment act of 2009 ( recovery act ) projects selected by the federal railroad administration ( fra ) align with legislative and the administration's goals to develop high speed and conventional rail networks .

congress provided its most recent expectations for high and conventional speed rail in the passenger rail investment and improvement act of 2008 ( priia ) and the recovery act .

in this regard , priia speaks generally about supporting improvements to high and conventional speed rail and does not set out any expectations for relative attention to high and conventional speed passenger rail improvements .

the recovery act appropriated $8 billion for both forms of rail service broadly .

however , it required that fra give priority to projects that support the development of intercity high speed service .

further , the act required that fra develop a strategic plan that describes how fra will use recovery act funding to improve and deploy high speed systems .

fra had wide latitude to achieve goals laid out in its strategic plan .

fra has outlined its vision for developing intercity passenger rail service in its strategic plan , as required by the recovery act , and in both its preliminary national rail plan issued in 2009 and the plan's update nearly a year later .

fra's vision documents — the strategic plan and its updated national rail plan — described broad goals , such as for transportation , safety , and economic competitiveness , and established categories for the type of high speed rail projects it intends to support .

for example , the strategic plan notes the high speed rail program aims to generate construction and operating jobs , while providing a steady market for various industries producing rail , control systems , locomotives , and passenger cars .

in addition , the plan notes that investments in high speed rail can result in competitive trip times and rail transport can also result in higher - density development as compared to other modes of transportation .

similarly , the updated national rail plan sets a goal of connecting communities through high speed rail while , among other things , reducing congestion , boosting economic growth , and promoting economic sustainability .

however , these vision documents provide limited details on goals for the high speed rail program .

for example , while the strategic plan emphasizes investments that will yield tangible benefits to rail performance and improve connections between different modes of transportation , it does not describe how and when fra intends to realize these benefits .

in addition , as we reported last june , the preliminary rail plan did not offer specific recommendations for future action and was designed to serve as a springboard for further discussion with states and freight railroads .

while the update to this plan included improving rail performance as a goal and provided some measurements for high speed rail performance , such as competitive trip times , it did not provide any specific targets for these metrics , or any time line showing when fra hopes to attain these improvements .

consistent with the recovery act's direction to give priority for high speed rail service , about half ( 45 percent ) of the applications selected were for core express corridors ( high speed service of 125 – 250 miles per hour or more ) or regional corridors ( higher - speed service of 90 – 124 miles per hour ) using categories of service similar to those fra established in its vision documents .

 ( see table 4. ) .

fra did not establish specific targets for the number of each type of project it intended to support .

in addition to providing priority for high speed projects , the applications that fra selected were consistent with near - term economic recovery goals and its long - term development goals .

while most selected projects are short - term in nature and are intended to support economic recovery goals established by the recovery act , most funding was provided to several long - term , high speed corridor projects .

specifically , we found that 48 of the 62 applications selected were track 1 applications , which are smaller projects designed to be completed within 2 years .

 ( see table 5. ) .

these projects represent about 11 percent of the funding provided for high speed rail and intercity passenger rail through the recovery act .

the remaining 89 percent of funding was provided for 14 track 2 applications , which are primarily long - term , corridor projects .

the funding allocation aligns with fra's focus on long - term investments that will support development of a high speed passenger rail network as described in the funding announcement .

while fra announced in january 2010 awards of nearly $8 billion in grants for the program , many of these projects have only recently begun .

as of december 31 , 2010 , fra had obligated $4.2 billion , or about 54 percent of the funding awarded in january , and about $50 million has been spent for projects selected under track 2 .

in may 2009 , fra issued a plan for spending recovery act funds , which it updated in july 2010 .

fra missed its may 2009 targets for obligations and spending through 2010 estimate because it had planned to announce awards — and begin obligating funds — in the autumn of 2009 .

however , fra did not make those announcements until january 2010 and did not begin to obligate funds until may 2010 .

fra then revised its estimates in july 2010 .

fra surpassed the calendar year 2010 goals for obligating and spending funds in the july 2010 plan .

 ( see table 6. ) .

during calendar year 2010 , fra obligated about 11 times as much as anticipated in the july 2010 plan , while awardees have spent about 7 times as much as planned over the same time period .

the recovery act authorized obligation of funds through september 30 , 2012 , and fra intends to obligate all funds by this date .

passenger rail investments are often long - term efforts that must be carried out in partnership between the state and others , notably private railroads .

for example , in order to begin design and construction on many of these projects , grant recipients must negotiate and secure agreements with private freight railroads to use their tracks for passenger rail trains .

however , officials from these railroads are concerned that sharing tracks would create safety risks and liability concerns , prevent freight expansion , and cause rail congestion .

some of the states have experienced delays finalizing these agreements with the railroads and , accordingly , have not completed agreements with fra to obligate awarded funding .

to determine the extent to which fra applied its established criteria to select projects , we identified the criteria that it planned to use from its june 23 , 2009 , funding announcement outlining its evaluation and selection approach .

we then compared these criteria to the worksheets and guidebooks that fra used to determine eligibility and assess technical merit .

finally , we interviewed fra officials who participated in evaluating and selecting projects to obtain information on whether and how they applied the established criteria .

specifically , we randomly selected 1 technical reviewer from each of the 12 track 1 , 3 , and 4 panels ( 12 out of 36 reviewers ) , and 6 of the 8 reviewers from the track 2 panel .

in addition , we interviewed senior fra officials to further understand how senior department of transportation ( the department ) and fra officials applied the selection criteria , selected projects , and determined the amount of funding provided for each project .

we also asked fra officials to provide reasons for why several lower scored applications were selected , while other higher scored applications were not .

we conducted semi - structured interviews with officials from 10 of the 40 states and the district of columbia that submitted a preapplication or an application for track 1 and track 2 funding about how fra communicated its approach to reviewing applications and award results .

we selected these states on the basis of four characteristics: ( 1 ) the extent to which applicants progressed through the preapplication , application , evaluation , and selection stages ; ( 2 ) geographic regions ; ( 3 ) the number of applications submitted ; and ( 4 ) the amount of funding .

we also contacted officials in two additional states ( ohio and washington ) to understand the effect of fra's funding decisions on the scope of these states' proposed rail program .

our efforts were limited to applications requesting funding under track 1 and track 2 of the high speed intercity passenger rail program ( hsipr ) in august 2009 and october 2009 and awarded recovery act funding for projects in january 2010 .

we did not review fra's rationale for its decision in december 2010 to redistribute $1.195 billion from two projects in ohio and wisconsin to on - going high speed rail projects in 13 states .

we also assessed whether fra's approach to calculating reviewers' individual scores and compiling them for an overall panel score reflected the criteria and weights for each criterion as published in the funding announcement as well as the overall reliability of the data used to make these calculations .

to do this , we reviewed documentation about the system used to collect the information and spoke with officials knowledgeable about the data .

we found some inaccuracies in how fra calculated the technical review scores .

specifically , we found that some standardized scores were incorrect due to the inclusion of three duplicate records and three applications deemed not yet ready or ineligible .

in addition , we noted fra incorrectly weighted some technical evaluation scores for applications submitted under track 1b .

however , we determined that these errors would not materially affect our findings and for the purposes of examining the effect of the scores on application selection , we found the data to be sufficiently reliable .

fra officials said that they would correct their calculations for future rounds of rail funding .

further we performed tests to determine the variables ( eg , technical review scores and number of applications submitted ) that had a significant statistical relationship with being selected for an award .

our approach is described in app .

iv .

to determine the extent to which fra used recommended practices for awarding discretionary grants , we examined office of management and budget guidance , guidance from several federal agencies , and our reports on this issue .

 ( see table 7. ) .

we identified key grant practices recommended across executive branch agencies and compared them to practices analyzed in our prior work .

specifically , we identified six recommended practices relating to ( 1 ) communicating with potential applicants prior to the competition , ( 2 ) planning for administering the review of applications , ( 3 ) developing a technical review panel with certain characteristics , ( 4 ) assessing applicants' abilities to manage grant funds , ( 5 ) notifying applicants of decisions , and ( 6 ) documenting reasons for award decisions .

we compared these practices to information from the 2009 funding announcement , guidance to applicant reviewers , and to statements made by fra officials regarding their implementation of their grants award program .

for this effort , one analyst carried out the comparison and a second analyst verified the comparison results .

where differences existed , the two analysts discussed them and reached agreement .

we also discussed the extent of fra's use of several of these practices with the officials from our sample of 10 states .

to determine the extent fra publicly communicated information about the results of its award competition , we compared the information it communicated to the public about its awards to the types of information communicated by a random sample of 20 other competitively awarded recovery act programs .

 ( see table 8. ) .

we selected the sample from 193 recovery act programs identified in the catalog of federal domestic assistance as competitive grant programs using recovery act funds .

in addition , we compared the information communicated about fra's awards to the information communicated by the innovation grants program ( race to the top ) — a discretionary grant program run by the department of education .

we included the race to the top program because you expressed interest in it .

we first reviewed materials on fra's web site and other public releases , such as press releases and outreach presentations to determine what fra publicly communicated .

we then discussed these results with fra officials to confirm our results .

for each of the 21 other recovery act programs , we reviewed three public information sources: ( 1 ) the program's catalog of federal domestic assistance award announcement , ( 2 ) internet search results , and ( 3 ) grants.gov , which provides information on more than 1,000 grant programs .

for each program , we searched these sources for information about final award results ( project description , why the project was selected , and award amount ) and for information that demonstrated how applications fared at different states of the process ( eligibility determination and internal reviews , such as technical review panels ) .

we defined the results of any technical review as either scores or comments , and when at least one of these elements was listed in at least one of the three sources of information , we concluded that technical review information was publicly communicated about the program .

in carrying out this assessment , one analyst carried out the work and a second analyst independently performed the same tasks .

the two analysts then compared their results and resolved any differences .

the results of our comparison to a sample of other recovery act programs are not generalizable across all recovery act programs .

to determine the extent to which hsipr applications align with statutory and other goals , we reviewed federal laws , including the recovery act and priia .

we analyzed fra's federal register notice describing its approach for selecting applications , its strategic vision for high speed rail , and its preliminary national rail plan and its subsequent update to gather information on any goals the agency has established for high speed rail networks and conventional service and the types of projects it seeks to support .

we did not assess whether the applications selected by fra will achieve the stated benefits or costs .

we reviewed information submitted by applicants , namely the type of project proposed , the funding requested and awarded , and the estimated future speed of the projects .

we used this data to sort projects into three categories developed by fra: core express corridors , regional corridors , and emerging high speed rail routes .

fra's definition of top speeds within these categories overlap , which we modified slightly to provide discrete endpoints .

of the 62 applications selected by fra , 13 did not provide data on anticipated top speed after project completion .

these 13 applications include a variety of improvements , including station rehabilitations , the reconfiguration of rolling stock , and existing tracks and grade crossings upgrades for which one would not expect top speed information .

we used these data as background on selected applications and did not assess them for reliability .

we also reviewed fra's recovery act plans and compared fra goals for obligating and spending awarded funds to its actual rates of obligating and spending from january 2010 through december 2010 .

after reviewing a department of transportation inspector general audit report on its financial management system and speaking with department officials familiar with the system , we determined that these data were sufficiently reliable .

in january 2010 , fra proposed to provide 18 of the 24 states , including the district of columbia , selected for awards all or nearly all ( 91 percent or more ) of the money that they requested .

 ( see table 9. ) .

the agency proposed to provide one state , north carolina , with slightly more than it requested and the remaining five states with amounts varying from 47 percent to 86 percent of the amounts requested .

in december 2010 , nearly a year after making these proposals , fra announced that $1.195 billion in recovery act funds for high speed rail — representing most of the $810 million for wisconsin's milwaukee - madison corridor and $385 million for ohio's cincinnati - columbus - cleveland “3c” route , originally designated for these states in january 2010 — would be redirected to high speed rail projects already underway in 13 other states .

in making these changes , fra noted that wisconsin has suspended work under its existing high speed rail agreement and the incoming governors in wisconsin and ohio have both indicated that they will not move forward to use high speed rail money received under recovery act .

the adjusted amounts resulted in fra proposing to provide all or nearly all of the original request amounts ( 91 percent or more ) for one additional state ( oregon ) .

 ( see table 10. ) .

while most of the funding was redistributed to three states ( california , florida , and washington ) , the total funding awarded to these three states was less than 80 percent of their original requests .

this appendix contains information related to our statistical analyses of fra and application data to examine possible relationships between several variables and fra's selection decisions .

we obtained the data for our analysis from the application review module of grantsolutions , the database fra used to store application information and technical review scores .

our analysis examined all 206 out of 259 submitted applications which fra deemed eligible and ready to receive federal funds .

eligible applications included those requesting recovery act funds , tracks 1 and 2 , as well as those requesting annual appropriations , tracks 3 and 4 .

we included track 3 and 4 applications in our analysis because fra reviewed , weighted , and calculated the results for tracks 1 , 3 , and 4 applications as a group rather than by distinct tracks .

to assess the reliability of the data in application review module , we reviewed database user manuals , spoke with officials knowledgeable about the data , and conducted a series of data tests .

we found some inaccuracies in how fra calculated the technical review scores .

specifically , we found that some final technical review scores were incorrect due to the inclusion of three duplicate technical review scores and three applications later determined to be not yet ready or ineligible .

in addition , fra had mistakenly applied incorrect weights to the track 1b application technical review scores , which resulted in 15 final scores that were one point higher than they should have been and another 5 final scores that were one point lower than they should have been .

we determined that these errors would not materially affect our findings and for the purposes of examining the effect of scores on application selection , found these data are sufficiently reliable .

to determine the extent to which specific variables were related to the department's selection of applications , we considered a set of bivariate tables and conducted a series of bivariate and multivariate regression analyses .

from the tables and regression analyses we were interested in determining how the department's decision to select an application for an award was affected by four variables: ( 1 ) the technical review scores , ( 2 ) application track , ( 3 ) the requested funding amount , and ( 4 ) the number of applications submitted by state or groups of states .

our analyses provide us with estimates , called odds ratios , which indicate the differences in the odds of applications being selected for an award across certain categories of the different variables we examined .

an odds ratio of 1.0 would indicate that applications in different categories were equally likely to be selected for an award .

an odds ratio of less than 1.0 implies that applications in the category to which the odds ratio applies were less likely to be selected relative to those they are being compared to ( known as the “reference” category ) .

for example , if applications receiving a technical review score of a 3 had an odds ratio of 0.5 it would indicate that they were half as likely to be selected for an award as applications that received a score of 1 or 2 ( the reference category ) .

inversely , an odds ratio greater than 1.0 suggests that applications with that characteristic were more likely to be selected .

for example , if applications receiving a technical review score of a 5 had an odds ratio of 3.0 , we would conclude that applications receiving that score were three times more likely to be selected relative to the reference category .

the primary reason for preferring odds ratios to describe the relationships across variables is because the significance of the differences between specific odds ratios can be easily tested and the ratios can be re - estimated after considering other variables .

we first examined the effect of technical review scores on the likelihood of being selected for an award , and found that the odds of being selected for an award were in general greater for applications receiving higher technical review scores than for applications receiving lower ones .

for the purposes of our analyses we combined scores of 1 and 2 , as there were only five applications that had received a score of 1 and there was no evidence that they were significantly different from applications that had been assigned scores of 2 , in terms of being selected for funding .

a smaller percentage of the applications that received scores of 1 or 2 were selected for funding ( 16 percent selected ) than applications that had received a score of 3 ( 46 percent selected ) or 4 ( 42 percent selected ) , and applications that received a score of 5 had the highest percentage of being selected for funding ( 69 percent selected ) .

in addition , the odds ratios of 4.30 , 3.67 , and 11.57 indicate that applications receiving a higher technical review score ( 3 , 4 , or 5 , respectively ) were at least three times more likely to be selected for an award than those receiving a lower technical review score ( i.e. , 1 or 2 ) .

 ( see table 11. ) .

we followed several steps to calculate the odds ratios of 4.30 , 3.67 , and 11.57 .

first , we derived the odds that applications with certain technical review scores would be selected for an award .

for example , to determine the selection odds for applications receiving a score of 1 or 2 , we divided the number of applications receiving a score of 1 or 2 that were selected by the number applications receiving those scores that were not selected .

seven were selected for awards , whereas 36 were not ; the resulting odds ( 7 / 36 ) equal 0.19 .

this means that 19 applications receiving a score of 1 or 2 would be selected for an award for every 100 that were not .

by comparison , the odds on being selected for applications receiving a technical review score of 3 were 41 / 49 , or 0.84 , which indicates that 84 applications receiving a score of 3 would be selected for an award for every 100 that were not .

the odds ratio comparing these two odds is 0.84 / 0.19 equal to 4.30 .

this odds ratio suggests that the odds of being selected for an award are more than four times greater for applications receiving a technical review score of 3 than for applications receiving a score of 1 or 2 .

we also found that there were sizable differences in the likelihood of applications submitted under different tracks being selected for an award .

while only between one - quarter and one - third of the applications in tracks 1a and 1b were selected , 61 percent of track 2 applications were selected , as were nearly three - fourths of the applications in tracks 3 and 4 .

these differences are also apparent from looking at the odds and odds ratios in table 12 .

the odds on being selected for funding were slightly lower ( by a factor of 0.77 ) for track 1b applications than for track 1a , but they were more than three times greater for track 2 applications than for track 1a applications , and more than five times greater for track 3 and track 4 applications than for track 1a applications .

 ( see table 12. ) .

there were also sizable differences in the likelihood of applications being selected based on the amount requested .

slightly more than half of the applications that requested less than $1 million were funded , as were exactly half of the applications that requested $50 million or more .

at the same time , roughly 40 percent of applications requesting between $1 million and $10 million were funded , and less than one - fourth of the applications requesting $10 to $50 million were funded .

the odds and odds ratios indicate that applications requesting the lowest amounts were the most likely to be selected and that applications requesting the highest amounts were almost as likely as those requesting the lowest amounts to be selected .

applications requesting more than $1 million but less than $50 million were somewhat less likely to be selected than applications requesting less than $1 million or more than $50 million .

 ( see table 13. ) .

we examined the differences of applications being selected by each state and , ultimately , by the number of applications submitted in many of the states .

we first considered the number of applications selected for each of the 34 states that submitted eligible applications .

of these 34 , 4 states submitted more than 10 applications , 22 states submitted 3 or fewer applications , and 9 states submitted a single application .

we found a statistically significant relationship indicating there was a much greater tendency for applications to be selected when they came from states in which a maximum of three applications were submitted .

given the low number of applications submitted by many of the states , however , we could not control for all of the differences between states in a multivariate analysis in which the effects of the other variables are estimated simultaneously .

therefore , we combined the states with smaller numbers of applications into two groups: one group contained states which submitted one to three applications and the other group contained states submitting four to nine applications .

these groupings did not result in the loss of any significant information with respect to differences in the likelihood of applications being selected across states .

the results of these state groupings indicate that the percentage of applications selected for funding from states with one to three applications ( 70 percent selected ) were considerably higher than the percentage of funded applications from states with four to nine applications ( 40 percent selected ) .

in addition , those states that submitted more than nine applications showed considerable differences in the percent of applications selected .

california ( 37 percent selected ) and missouri ( 75 percent selected ) had a relatively high percentage of applications selected , and new york ( 18 percent selected ) and washington state ( 5 percent selected ) had a relatively low percentages of applications selected .

as in the previous tables , the odds and odds ratios give us the same sense of the association that the percentages reveal ; the odds on being selected for funding were more than three times higher for applications from states submitting one to three applications than for applications from california or from states submitting four to nine applications ( 2.33 / 0.68 , which equals 3.43 ) .

in addition , new york and washington state were much less likely to likely to have an application selected than california and missouri .

 ( see table 14. ) .

we also examined the data using bivariate and multivariate logistic regression models to estimate the effects of these different variables on the likelihood of applications being selected for funding .

the bivariate models examine the effects of the technical review score , track , amount requested , and state or group of states from which the application arose , one at a time .

the odds ratios from these models are the same as those produced from the observed frequencies in the different two - way tables described above .

from these models , however , we obtain specific tests of the significance of the differences between each variable category to determine more generally whether there are differences between any of the categories .

in the bivariate regression model we find that many , but not all , of the odds ratios describing these differences are significant .

 ( see table 15. ) .

the multivariate model estimates the net effects of these different variables on the likelihood of applications being selected for funding , or the effects of each variable when the effects of other variables are considered simultaneously , rather than one at a time .

our results indicate that the differences between tracks and amount requested categories are rendered insignificant when technical review scores and state and state group are taken into account , while the effect of technical review score and the differences between state and state group remain sizable and in most cases significant .

specifically , we found that , when we accounted for all four variables , applications receiving a technical review score of 3 , 4 , or 5 were about seven to eight times more likely to be selected for funding than applications which scored 1 or 2 .

in addition , applications from states that submitted one to three applications , and applications from missouri , were three and nine times as likely , respectively , to be selected as those from california , while those from washington state were less than one - tenth as likely as those from california to be selected .

the remaining variable categories were not significant in the multivariate model and , therefore , do not provide a statistical explanation for why applications were more or less likely to be selected for an award .

