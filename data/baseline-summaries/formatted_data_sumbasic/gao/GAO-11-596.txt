department of homeland security ( dhs ) acquisitions represent hundreds of billions of dollars in life - cycle costs to support a wide range of missions , including securing the nation's borders , mitigating natural disasters , screening airline passengers and baggage , and investigating security threats .

dhs acquisition spending has increased by 50 percent from $9.1 billion in fiscal year 2004 to $13.6 billion in fiscal year 2010 .

a key goal of dhs's acquisitions process is ensuring that programs and technologies meet technical and performance specifications and are tested in dhs's operational environment , and that the results of these tests are evaluated before these programs and technologies are allowed to progress toward purchase and deployment .

the homeland security act of 2002 created dhs and , within it , established the science and technology directorate ( s&t ) .

the act provided s&t with responsibility for conducting national research , development , test and evaluation ( t&e ) , and procurement of technology and systems for , among other things , detecting , preventing , protecting against , and responding to terrorist attacks .

s&t's test & evaluation and standards office ( tes ) is responsible for , among other things , conducting oversight over t&e activities across all of dhs's components to ensure that major acquisitions — those with life - cycle costs of over $300 million dollars — being considered by dhs are appropriately tested and evaluated by dhs component agencies prior to their purchase and deployment .

we have previously reported on several major dhs acquisitions that were deployed before appropriate t&e was successfully completed .

for example , in october 2009 , we reported that the transportation security administration ( tsa ) procured and deployed explosives trace portal equipment — a machine which detects traces of explosives on airline passengers by using puffs of air to dislodge particles from the passengers' body or clothing into an analyzer — even though tsa officials were aware that earlier tests did not demonstrate reliable performance in an airport environment .

we recommended that tsa conduct an evaluation and determine whether it was cost effective to continue to use these machines .

tsa concurred with this recommendation and later halted further deployment of these machines due to performance , maintenance , and installation problems .

as of april 2011 , tsa reported that it had removed all 101 machines that it had deployed from airports .

furthermore , in january 2010 , we reported that dhs had not effectively managed key aspects of the testing of customs and border protection's ( cbp ) secure border initiative network ( sbinet ) , a multibillion dollar program to deliver surveillance and decision - support technologies along the u.s. border with mexico and canada .

among other things , we reported that test procedures were largely not executed as written and changes to these procedures were not made according to a documented quality assurance process which increases the risk that the procedures would not support test objectives or reflect the system's ability to perform as intended .

we made a number of recommendations related to the content , review , and approval of test planning documentation and resolution of system problems , with which dhs generally concurred .

in january 2011 , the secretary of homeland security directed cbp to end sbinet as originally conceived and to develop a new border technology deployment plan .

these past problems highlight the importance that t&e plays in the successful development of major acquisition programs and technologies , as well as the importance of overseeing the t&e efforts of dhs's components .

in may 2009 , dhs issued a t&e directive requiring components to , among other things , ensure adequate and timely t&e is performed to support informed acquisition decision making and also requires that s&t's tes oversee these activities by reviewing component t&e activities and documentation , and approving those t&e activities and documentation related specifically to operational testing .

you asked us to evaluate s&t's efforts to oversee t&e across dhs and identify any challenges that it faces in performing this mission .

specifically , this report addresses: ( 1 ) the extent to which tes oversees t&e of selected dhs major acquisition programs throughout the system acquisition process ; and ( 2 ) what challenges , if any , tes and component officials report facing in coordinating and overseeing t&e across dhs acquisition programs .

to address our objectives , we reviewed dhs departmental and component - level policies and guidance , such as dhs's acquisition directive and guidebook , t&e directive , and our past reports .

we also reviewed relevant program documentation , including memoranda of acquisition decision events in the acquisition process .

further , we conducted interviews with relevant dhs and component officials involved in the acquisition process including t&e of programs and technologies .

we focused our review generally on the period after may 2009 when dhs issued its t&e directive since there were no specific t&e requirements prior to its issuance .

we focused our review on tes's t&e staff and activities and did not review the efforts of tes staff who were engaged in developing national standards to meet homeland security mission needs or tes staff in two testing facilities , since developing standards and conducting tests did not relate to overseeing t&e across dhs components and thus , were outside the scope of our review .

regarding our objective to determine the extent to which tes oversees t&e of selected dhs major acquisition programs throughout the system acquisition process , we initially selected a nonprobability sample of 12 programs of the 86 on dhs's major acquisitions list for fiscal year 2010 and , for 11 of the 12 programs , we analyzed related acquisition and t&e documentation , such as the program's test plans , as well as memoranda documenting approval of operational test agents ( test agents ) , and interviewed dhs component officials to help determine the extent that tes reviewed these plans and agents and documented its approval during fiscal year 2010 .

selections were based on three factors — programs which had undergone a senior dhs management review in fiscal year 2010 , programs representing different dhs components , and programs which were overseen by all nine of tes's test area managers .

for 11 programs , we conducted semistructured interviews and collected information from component - level officials involved in the development of t&e documentation .

the results of these analyses cannot be generalized to tes's efforts to oversee all major acquisition programs , but they provide informative examples of the extent to which tes carried out its oversight responsibilities .

we reviewed documentation of tes's oversight efforts and compared them to the requirements in the dhs acquisition and t&e directive to determine the extent to which tes fulfilled its responsibilities .

we also compared tes efforts to document their oversight to standards for internal control in the government .

further , we conducted site visits to s&t's transportation security laboratory and tsa's transportation security integration facility to obtain an overview of testing in general and to specifically observe testing of transportation security technologies .

see appendix i for a description of the 11 dhs major acquisition programs selected for our analysis .

to determine what challenges , if any , tes and component officials report facing in coordinating and overseeing t&e of major dhs acquisition programs , we interviewed tes and component officials responsible for t&e from the 11 selected programs and conducted semistructured interviews with all nine tes test area managers who were primarily responsible for conducting t&e oversight in fiscal year 2010 .

we analyzed and categorized the challenges that these officials said they faced in conducting t&e of their programs .

we discussed the major challenges we identified with tes officials , who agreed with our evaluation .

we also reviewed various tes and dhs initiatives to address these challenges .

for example , we reviewed minutes from tes's t&e council , which was formed to promote t&e best practices and lessons learned in establishing consistent t&e policies and processes for use in acquisition programs throughout dhs .

in addition , we reviewed minutes of t&e council working groups , which were formed to address particular challenges , such as testing and evaluating information technology acquisitions .

we also reviewed documents and interviewed tes and component officials with regard to t&e training and certification efforts .

further , we interviewed the under secretary for s&t and other s&t executives regarding s&t's t&e efforts and the challenges that s&t and tes face in overseeing t&e across dhs .

finally , we reviewed our past work related to these challenges .

we conducted this performance audit from may 2010 through june 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

dhs acquisitions support a wide range of missions and investments including ships and aircraft , border surveillance and screening equipment , nuclear detection equipment , and systems to track the department's financial and human resources .

in support of these investments , dhs , in 2003 , established an investment review process to help reduce risk and increase the chances for successful acquisition outcomes by providing departmental oversight of major investments throughout their life cycles and to help ensure that funds allocated for investments through the budget process are being spent wisely , efficiently , and effectively .

our work over the past several years has consistently pointed to the challenges dhs has faced in effectively managing and overseeing its acquisition of programs and technologies .

in november 2008 , we reported that dhs had not effectively implemented its investment review process , and as a result , the department had not provided the oversight needed to identify and address cost , schedule , and performance problems for its major acquisitions .

specifically , we reported that of the 48 major investments reviewed requiring milestone or annual reviews , 45 were not reviewed in accordance with the departments' investment review policy , and 18 were not reviewed at all .

four of these investments had transitioned into a late acquisition phase — production and deployment — without any required reviews .

we recommended and dhs concurred that dhs identify and align sufficient management resources to implement oversight reviews in a timely manner throughout the investment life cycle .

in june 2010 , we reported that over half of the 15 dhs programs we reviewed awarded contracts to initiate acquisition activities without component or department approval of documents essential to planning acquisitions , setting operational requirements , and establishing acquisition program baselines .

our work noted that without the development , review , and approval of these key acquisition documents , agencies are at risk of having poorly defined requirements that can negatively affect program performance and contribute to increased costs .

in january 2011 , dhs reported that it has begun to implement an initiative to assist programs with completing departmental approval of acquisition program baselines .

in our february 2011 biennial update of the status of high - risk areas needing attention by congress and the executive branch , we continued to designate dhs's implementation and transformation , which includes the department's management functions , as a high - risk area .

for example , because of acquisition management weaknesses , major programs , such as sbinet , have not met capability , benefit , cost , and schedule expectations .

further , dhs had not fully planned for or acquired the workforce needed to implement its acquisition oversight policies as we previously recommended .

as of january 2011 , dhs reported that it had increased its acquisitions management staffing and planned to hire more staff to develop cost estimates .

dhs has taken several actions to address these recommendations and implement more discipline and rigor in its acquisition processes .

specifically , dhs created the acquisition program management division in 2007 to develop and maintain acquisition policies , procedures , and guidance as a part of the system acquisition process .

dhs also issued an interim acquisition directive and guidebook in november 2008 for programs to use in preparing key documentation to support component and departmental making .

in january 2010 , dhs finalized the acquisition directive which established acquisition life - cycle phases and senior - level approval of each major acquisition program at least three times at key acquisition decision events during a program's acquisition life - cycle .

this directive established the acquisition life - cycle framework with four phases: ( 1 ) identify a capability need ( need phase ) ; ( 2 ) analyze and select the means to provide that capability ( analyze / select phase ) ; ( 3 ) obtain the capability ( obtain phase ) ; and ( 4 ) produce , deploy , and support the capability ( produce / deploy / support phase ) .

each acquisition phase culminates in a presentation to the acquisition review board ( arb ) , which is to review each major acquisition ( that is , those designated as level 1 or level 2 programs ) at least three times at key acquisition decision events during a program's acquisition life cycle .

the acquisition decision authority — the chief acquisition officer or other designated senior - level official — is to chair the arb and decide whether the proposed acquisition meets certain requirements necessary to move on to the next phase and eventually to full production .

the directive outlines the extent and scope of required program , project , and service management ; level of reporting requirement ; and the acquisition decision authority based on whether the acquisition is classified as level 1 , 2 , or 3 .

the acquisition decision authority for major acquisitions — level 1 and level 2 — is to be at the department or component level and the acquisition decision authority for nonmajor acquisitions — level 3 — is to be at the component level.an acquisition may be raised to a higher level acquisition level by the arb .

the arb supports the acquisition decision authority in determining the appropriate direction for an acquisition at key acquisition decision events .

following an arb meeting , the acquisition program management division is to prepare an acquisition decision memorandum as the official record of the meeting to be signed by the acquisition decision authority .

this memo is to describe the approval or other decisions made at the arb and any action items to be satisfied as conditions of the decision .

the arb reviews are to provide an opportunity to determine a program's readiness to proceed to the following life - cycle phase .

however , we reported in march 2011 that the arb had not reviewed most of dhs's major acquisition programs by the end of fiscal year 2009 and programs that were reviewed had not consistently implemented action items identified as part of the review by established deadlines .

our prior work has shown that when these types of reviews are skipped or not fully implemented , programs move forward with little , if any , early department - level assessment of the programs' costs and feasibility , which contributes to poor cost , schedule , and performance outcomes .

as a part of its responsibilities , the acquisition program management division has identified major dhs acquisition programs , projects , or services for oversight through the arb process .

according to acquisition program management division officials , beginning in fiscal year 2009 , the list was to be updated on a yearly basis through interviews with and documentation from component program offices .

in may 2010 , the undersecretary for management identified 86 programs on dhs's major oversight list for fiscal year 2010 , 62 of which tes and component officials determined required t&e oversight — that is programs that were in an acquisition phase where t&e was being planned or conducted .

several of the 62 programs consisted of multiple subprojects , such as tsa's passenger screening program .

for more information on these 86 major acquisition programs , see appendix ii .

dhs's 2010 acquisition directive also includes guidance for preparing documentation to support component and departmental decision making and specifies requirements for developmental and operational t&e as a part of the acquisition review process .

developmental t&e may include a variety of tests , such as system qualification testing , system acceptancetesting , and software testing .

developmental testing may be carried out by the user and may be conducted in simulated environments , such as laboratories , test facilities , or engineering centers that might or might not be representative of the complex operational environment .

operational t&e is a field test , performed under realistic conditions by actual users in order to determine the operational effectiveness and suitability of a system , and the corresponding evaluation of the data resulting from the test .

to carry out its responsibilities for overseeing t&e , s&t established tes in 2006 and created the position of director of tes in june 2007 .

tes's mission is to establish and manage dhs t&e policies and procedures and to oversee and coordinate t&e resources to verify attainment of technical performance specifications and operational effectiveness and suitability .

to carry out its t&e oversight , in fiscal year 2010 , tes had a budget of about $23 million and as of february 2011 had a staff of 26 , which includes the tes director , 19 staff dedicated to t&e activities , and 6 dedicated to developing standards .

in may 2009 , dhs issued a delegation which specified the responsibilities and duties of the director of operational test & evaluation .

the tes director and director of operational test and evaluation , while distinct positions in the t&e directive , share some advisory , review , and oversight responsibilities .

for example , both are responsible for advising program managers in developing t&e documentation and approving test and evaluation master plans .

the tes director is responsible for developing dhs t&e policy and the director of operational test and evaluation is to approve operational test plans and report to the arb after assessing operational test reports .

since may 2009 , the director of operational test and evaluation position has not been continuously filled according to the current tes director .

in a november 2010 memo , the under secretary for science and technology designated one person as both the director of tes and the director of operational test and evaluation until further notice .

the t&e directive outlines the responsibilities of the tes director and the director of operational test and evaluation .

according to the directive , the tes director is to establish the department's testing and evaluation policies and processes and the director of operational test and evaluation is to administer those policies and processes .

the directive also outlines tes's responsibilities in overseeing t&e across dhs components and its role in the acquisition review process .

table 1 describes tes's t&e responsibilities as outlined in the t&e directive for all level 1 , level 2 , and special oversight acquisition programs .

the t&e directive requires tes to review and approve required component acquisition documentation before an arb meets for an acquisition decision event .

these documents are meant to be reviewed and , if required , approved in a sequential order associated with the acquisition phase , because these documents build upon one another .

figure 1 presents tes's responsibilities throughout the four dhs acquisition phases as defined in the acquisition directive .

to carry out these responsibilities for the 62 acquisition programs under its oversight in fiscal year 2010 , tes has test area managers who assist component officials in fulfilling their t&e responsibilities and provide guidance and clarification in regard to the requirements in the t&e directive .

according to tes , each major acquisition program is assigned a test area manager and as of february 2011 , tes employed nine test area managers .

tes met its oversight requirements when approving test plans and test reports in accordance with dhs acquisition and t&e directives for the 11 major acquisition programs we selected for review .

however , tes did not consistently document its review and approval of operational test agents or its review of other required acquisition documentation , which could provide more assurance that components were meeting t&e directives when tes reviewed these documents .

further , tes does not plan an independent assessment of tsa's advanced spectroscopic portal's operational test results , as required by the t&e directive .

tes is to oversee t&e of major dhs acquisition programs by ensuring that the requirements set forth in the t&e directive are met and by working with component program officials to develop t&e documentation , such as test and evaluation master plans , as required by dhs's acquisition directive .

tes's t&e oversight responsibilities set forth in the t&e and acquisition directives pertain to programs primarily in the analyze / select and obtain phases of the acquisition process because most testing and evaluation efforts occur in these phases .

as a result , the requirements of the t&e directive and tes's oversight vary depending on when a program progresses through certain phases of the acquisition process .

for example , when a program is in the produce / deploy / support phase there is usually little to no t&e activity , so tes's involvement is limited .

we reviewed tes's t&e oversight efforts for 11 dhs programs and found that tes had conducted oversight of components' test plans and test reports , as set forth in the acquisition and t&e directives , as it asserted .

the 11 programs , each managed by different dhs components , were in one phase of the acquisition process or had two or more subprojects simultaneously in different phases of the acquisition process .

for example , coast guard's h - 65 helicopter program has 6 discrete subprojects , each with its own completion schedule , including 4 subprojects in the produce / deploy / support phase and 2 subprojects in the obtain phase .

acquisition program management division , tes , and component officials determine if subprojects need to develop separate sets of acquisition documents as they progress through the acquisition process .

figure 2 provides an overview of these programs and their associated acquisition phases .

additional details on these programs can be found in appendix i .

as shown in figure 3 , for the 11 selected dhs programs , tes reviewed and approved test and evaluation master plans for 6 of the 7 programs that were required to develop such plans by the t&e and acquisition directives and had documented their approval of these plans .

for the one program that was in the phase that required such a plan — atlas tactical communications — the program had not yet drafted its test and evaluation master plan .

the remaining 4 programs had plans in draft form that had not yet been submitted to tes for review .

as a result , tes was not yet required to review these plans .

component officials from each of these six programs stated that tes provided input to the development of the test and evaluation master plans .

for example , office of health affairs officials stated that tes officials suggested that the biowatch gen - 3 program office incorporate an additional test event to ensure that the program was tested under specific environmental conditions described in the program's operational requirements document , which resulted in more tests .

in addition , u.s. customs and border protection ( cbp ) officials stated that tes participated in a line - by - line review of the sbinet test plan and provided detailed suggestions .

further , tes suggested that the criteria used for operational testing in the test and evaluation master plan needed to be expanded , and that an update may be required for sbinet to progress to the next acquisition phase .

all of the component program officials who had undergone tes review or approval told us that tes test area managers provided their input in a variety of ways , including participating in t&e working groups , in specific meetings to discuss t&e issues , or by providing written comments culminating in tes's approval of the plan .

after the test and evaluation master plan is developed , the test agent is to develop operational test plans , which detail field testing of the system under realistic conditions for determining that the system's overall effectiveness and suitability for use before deployment of the system .

as shown in figure 4 , of the 11 selected acquisition programs , tes reviewed and approved operational test plans for the 4 programs that were required to develop such plans by the acquisition directive and documented their approval of these plans .

component officials from these 4 programs said that tes provided input into their test plans .

for example , national protection and programs directorate officials from the national cybersecurity protection system program stated that tes had significant comments on their operational test plan , such as including confidence levels associated with the syste key performance requirements and helping program officials select a sample size necessary to measure statistically significant results .

in addition , tes officials requested that the plan include different testing scenarios in order to demonstrate a varied use of the system .

in a officials from the transportation security administration's ( tsa ) advanced technology - 2 program indicated that tes provided significan input to their plan through a working group .

the remaining 7 of the 11 programs had not yet begun to develop their operational test plan .

at the conclusion of operational testing , the test agent is to write a re on the results of the test .

the t&e directive specifies that tes is to receive the operational test report , which is to address all the critic issues and provide an evaluation of the operational suitability and operational effectiveness of the system .

after reviewing the operatio test report , tes then is to write a letter of assessment — which is an d independent assessment of the adequacy of the operational test an provides tes's concurrence or nonconcurrence on the test agent evaluation of operational suitability and operational effectiveness .

tes is to provide the letter of assessment to the arb as it is determining whe a program should progress to the production and deployment phase .

of the 11 programs we selected to review , tes developed a letter of assessment for the 1 program — tsa's advanced technology 2 — that had completed operational testing and had a written operational t&e report on the results .

the assessment concluded that while the t&e activities were adequate to inform the arb as to system performance , tes did not concur with tsa's test agent's assessment as to system effectiveness because the system did not achieve a key performance parameter during testing .

the arb considered the letter of assessment and tes's input and granted tsa permission to procure and deploy a limited number of screening machines .

tsa will have to go before the arb again to determine if full - scale production can proceed after tsa has provided the arb with a business case and risk mitigation plan related to testing issues .

the remaining 10 selected programs had not completed operational testing and thus , were not ready for letters of assessment .

in addition to letters of assessment , tes officials told us that they regularly discuss t&e issues and concerns either verbally or through e - mails with acquisition program management division officials , who are responsible for organizing arb meetings .

for example , acquisition program management division officials stated that they rely on tes to provide candid information about the suitability of various programs' t&e and whether these issues impact their program's readiness to go before the arb .

further , the officials told us that tes's input at the arbs , if any , is to be documented in acquisition decision memorandums .

acquisition program management division officials also noted that tes's input may be used in making the decision about when to hold an arb for a particular program .

t&e input from tes is one of many factors the arb uses in overseeing acquisitions .

for example , according to s&t officials , the arb considers the current threat assessments and the extent to which the program , if implemented sooner , would help to address that threat .

the arb also considers factors such as the cost of the program and potential costs of conducting more testing and whether the results of operational testing were sufficient to achieve the intended benefits of the program .

as a result , the arb may accept a higher level of risk and allow a program to proceed even if testing concerns have been raised , if it determines that other reasons for quicker implementation outweigh these concerns .

tes officials also stated that they work extensively with components prior to arb meetings to ensure that t&e issues are addressed , with the goal to address these issues before going before the arb .

tes meets with component officials during regular acquisition review team meetings to resolve various issues before arb meetings are convened .

for example , due to concerns about the results of system qualification tests , tes recommended to sbinet program and arb officials that the program should not proceed to the next milestone — site preparation , tower construction , and sensor and communication equipment installation at the ajo , arizona test site — until after operational testing was completed at the tucson , arizona test site .

in may 2009 , the arb authorized sbinet to proceed with plans for the ajo , arizona site despite tes's advice to the contrary , and directed tes to work with component officials to revise test plans , among other things .

while tes's oversight of the test plans and reports for major acquisition programs selected for review is in accordance with provisions in the t&e directive , it did not consistently document its review and approval of certain acquisition documentation or document the extent to which certain requirements in the t&e directive were met .

the t&e directive requires that an operational test agent — a government agency or independent contractor carrying out independent operational testing for major acquisition programs — is to meet certain requirements to be qualified and approved by tes , but does not specify how tes's approval is to be documented .

according to the t&e directive , the test agent may be within the same component , another government agency , or a contractor , but is to be independent of the developer and the development contractor .

because the responsibilities of a test agent are significant throughout the t&e process , this independence is to allow the agent to present objective and unbiased conclusions regarding the system's operational effectiveness and suitability to dhs decision makers , such as the arb .

for example , some the test agent's responsibilities in the t&e directive include: being involved early in the acquisition cycle by reviewing draft requirements documents to help ensure that requirements are testable and measurable .

assisting the component program manager in the preparation of the test and evaluation master plan .

planning , coordinating , and conducting operational tests , and preparing the operational t&e report .

reporting operational test results to the program manager and tes .

according to tes officials , the test agent is also to meet other requirements in order to be approved by tes , such as having the expertise or knowledge about the product being tested and having the capacity and resources to execute the operational tests .

to ensure that criteria for test agents are met , the t&e directive requires tes to approve all agents for major acquisition programs .

as shown in figure 5 , of the 11 programs we reviewed , 8 programs had selected a test agent and the others were in the process of selecting a test agent .

tes provided documentation , such as memoranda , of its approval for 3 of these 8 programs .

for the remaining 5 programs , there was no documentation of the extent to which these test agents had met the criteria and that tes had approved them .

according to tes officials , they did not have a mechanism in place requiring a consistent method for documenting their review and approval of component agents or the extent to which criteria used in reviewing these agents were met .

in the absence of such a mechanism in fiscal year 2010 , tes's approval of test agents was not consistently documented .

tes and component officials stated that the approval for the five programs was implicit or provided verbally without documentation regarding whether the test agent met the t&e directive requirements .

the t&e directive states that the test agent is to be identified and approved as early as possible in the acquisition process to , among other things , assist the component program officials in developing the test and evaluation master plan and review draft requirements documents to provide feedback regarding the testability of proposed requirements .

tes and component officials stated that they assumed that test agents were approved using various approaches .

specifically , of the five programs that had test agents sign the test and evaluation master plan , one program had documented approval from tes .

for example , coast guard and office of health affairs officials stated that they did not have explicit documentation of tes's approval of their agents ; however , they believed that tes's approval was implicit when tes approved their test and evaluation master plan since the test agent and tes are both signatories on the plan .

cbp and national protection and programs directorate officials told us that tes provided verbal approval for their test agents .

since there is no mechanism requiring tes to document its approval of the agent , and approval was granted verbally , there is no institutional record for dhs or an independent third party to validate whether tes followed its criteria when approving these test agents and whether the test agent was identified and approved before the test and evaluation master plan and requirements documents were finalized , as outlined in the t&e directive .

with regard to the three programs in which tes had documented its approval in memoranda , these memoranda detailed tes's agreement or nonagreement with a particular agent and highlighted whether the agent met the criteria outlined in the t&e directive .

for example , tes provided interim approval to all three of the programs with the conditions that the programs prove at a later date that the test agents met all the requirements .

for example: in april 2010 , tes wrote a memo and granted interim approval with “serious reservations” for 1 year to tsa's test agent for the passenger screening program .

in the memo , tes cited concerns about the organizational structure and the lack of independence of the test agent since the test agent was part of the same tsa office responsible for managing the program .

the memo outlined several steps that tsa should take , including the implementation of interim measures , such as new procedures , to ensure the necessary independence critical to testing and evaluation efforts as required by dhs directives .

tes officials told us that by documenting tes's interim approval in a memo , they were able to communicate their concerns about the test agent's independence to tsa and dhs decision makers and set forth interim measures that tsa needed to address regarding their concerns .

in july 2010 , tes granted conditional approval to the test agent for the u.s .

citizenship and immigration services' ( uscis ) transformation program's test agent .

tes made its approval contingent on the program developing a plan to ensure that the test agent was familiar with the component's business practices .

according to tes officials , after component officials gave a briefing to tes , they determined that the test agent met the requirements and it was approved .

in january 2011 , tes granted conditional approval for the u.s. secret service's information integration and transformation program to bring its selected test agent on board .

tes's final approval will be given after program officials brief tes on the test agent's operational testing approach , which is to demonstrate that the test agent has knowledge of the product and has the capacity to execute the tests .

tes officials told us that they do not have approval memos for all of the test agents that have been hired by program offices since the t&e directive was implemented in may 2009 .

because tes did not consistently document their approvals of test agents , it is unclear whether tes has ever disapproved a test agent .

tes officials acknowledged that they did not consistently document that the test agents met t&e requirements and did not document their approval of test agents .

tes officials said that it would be beneficial to do so to ensure that agents met the criteria required in the t&e directive .

in addition , standards for internal control in the federal government and associated guidance state that agencies should document key decisions in a way that is complete and accurate , and that allows decisions to be traced from initiation , through processing , to after completion .

these standards further state that documentation of key decisions should be readily available for review .

without a mechanism for documenting its review and approval of test agents for major acquisition programs , it will be difficult for dhs or an independent third party to validate tes's decision - making process to ensure that it is effectively overseeing component testing .

moreover , it will be difficult for tes to provide reasonable assurance that these agents met the criteria outlined in the t&e directive , such as the requirement that they be independent of the program being tested .

in addition to reviewing and approving test plans , under the t&e directive , tes is required to review certain component acquisition documents , including the mission need statements , operational requirements document , concept of operations , and developmental test reports , amongst others .

these documents , which are required at the need , analyze / select , and obtain phases of the acquisition process , are to be reviewed by tes to assist component program managers in identifying and resolving technical , logistical , and operational issues early in the acquisition process and to ensure that these documents meet relevant criteria .

specifically , as outlined in the t&e directive , tes is to review the mission need statement to establish awareness of the program and help ensure that the required standards are developed and that the component has identified the appropriate resources and support needed to conduct testing .

tes is also to review the operational requirements document , including the key performance parameters and critical operational issues that specify the operational effectiveness and operational suitability issues that the test agent is to examine in order to assess the system's capability to perform the mission .

further , tes is to review the concept of operations , since this document describes how the technology or equipment will be used in an operating environment .

tes is to review the developmental test reports to maintain knowledge of contractor testing and to assist in its determination of the program's readiness to progress to operational testing .

we have previously reported that inadequate attention to developing requirements results in requirements instability , which can ultimately cause cost escalation , schedule delays and fewer end items .

further , we reported that without the required development and review of key acquisition data , dhs cannot provide reasonable assurance that programs have mitigated risks to better ensure program outcomes .

tes officials stated that they do not have a mechanism to document or track those that they did review , what criteria they used when reviewing these documents , and the extent to which the documents reviewed met those criteria .

for the 11 dhs programs that we reviewed , 8 programs had component - approved mission need statements ; 2 programs , atlas tactical communications and transformation , had not yet completed such statements ; and 1 program , the initial sbinet program , had completed a mission need statement in october 2006 before the t&e directive was issued and did not develop a separate mission need statement for the block 1 increment of the program .

of the 8 programs that had mission need statements , 6 components told us that they did not have evidence that tes reviewed the mission need statement in accordance with the t&e directive .

further , tes could not demonstrate that it had received or reviewed these documents .

since tes did not have documentation of its review , it is difficult to determine the extent to which the documents were reviewed and the extent to which these documents met the review criteria .

tes officials told us that they do not usually provide substantial input into the mission need statements and that they receive these documents to establish awareness of a new program .

further , while one tes test area manager told us that he reviews all developmental test reports , another test area manger told us that some programs do not routinely send him developmental test reports .

also , for example , secret service officials said that for the information integration and transformation program they provided the operational requirements document , concept of operations , and integrated logistic support plan to tes .

specifically , the officials said that tes officials were very helpful in providing input on draft documents and made improvements to the documents by suggesting , for example , that the tests be more realistic by including personnel from field offices , headquarters , and external agencies in the live / production test environment .

in contrast , officials from tsa stated that while they provided their mission need statement , concept of operations , integrated logistics support plan , and acquisition program baseline documents for the advanced technology 2 ( at - 2 ) program to tes , tes officials did not provide input or comments on any of those documents .

tes officials told us that the at - 2 program was initiated and developed some acquisition documentation prior to may 2009 when the t&e directive was issued .

specifically the operational requirements document was approved and finalized by tsa in june 2008 prior to the t&e directive and provided later to tes in february 2010 when the program was being reviewed .

when tes reviewed the operational requirements document along with other documents such as the test and evaluation master plan , tes wrote a memo to tsa in march 2010 requesting that detection performance requirements be clarified and that users concur with the requirements .

after several months of discussion , tsa and tes agreed on an approach which was used as the basis for initial operational t&e .

standards for internal controls in the federal government , as outlined earlier , state that agencies should document key decisions , and further that documentation of key decisions should be readily available for review .

tes officials stated that they do not have a mechanism requiring that they document their review of certain acquisition documentation or the extent to which the document met the criteria used in reviewing these documents , and recognized that doing so would be beneficial .

developing a mechanism for tes to document its review of key acquisition documents could better position tes to provide reasonable assurance that it is reviewing key documentation and providing input that is important for determining the outcome of future testing and evaluation efforts , as required by the t&e directive .

moreover , such a policy could help to ensure that an institutional record exists for dhs or an independent third party to use in determining whether tes is effectively overseeing component t&e efforts and assisting in managing dhs major acquisition programs .

according to the t&e directive , tes is to conduct an independent assessment of the adequacy of an operational test , provide a concurrence or nonconcurrence on the test agent's evaluation of operational suitability and operational effectiveness , and provide any further independent analysis it deems necessary for all major dhs acquisition programs .

tes is to document this independent assessment by writing a letter of assessment within 30 days of receiving the operational test report from the components' test agent and provide the letter of assessment to the arb , who then uses the assessment in making its determination of whether the program can proceed to purchase and implementation .

while tes has developed a letter of assessment for the two other programs undergoing an arb decision to enter into the production and deployment phase since the t&e directive was issued in may 2009 , tes officials told us that they do not plan to write such an assessment for the advanced spectroscopic portal ( asp ) program because they are the test agent for asp and thus , are not in a position to independently assess the results of testing that they conducted .

in april 2008 , over a year before the t&e directive was issued , senior level executives from dhs , s&t , cbp , and the domestic nuclear detection office ( dndo ) signed a memorandum of understanding regarding arrangements for asp operational testing .

the memo designated pacific northwest national lab , a u.s. department of energy laboratory , as the test agent .

however , the memo also outlined the roles and responsibilities of tes , many of which reflected the duties of a test agent , such as developing and approving all operational test plans , responsibility for the management of testing and field validation , and developing and approving operational test reports .

tes officials told us that they were using pacific northwest national lab staff to carry out the operational tests , but are acting , for all intents and purposes , as the test agent for asp .

tes and dndo officials told us that this arrangement was made after repeated testing issues arose with the asp program .

in september 2008 , we reported that asp phase 3 testing by dndo provided little information about the actual performance capabilities of asp and that the resulting test report should not be used in determining whether asp was a significant improvement over currently deployed equipment .

specifically , we found that the asp phase 3 test results did not help determine an asp's “true” level of performance because dndo did not design the tests to assess asp performance with a high degree of statistical confidence .

in response to our report , dhs convened an independent review team to assist the secretary in determining whether he should certify that there will be a significant increase in operational effectiveness with the procurement of the asp system .

the independent review team found that the test results and measures of effectiveness were not properly linked to operational outcomes .

in may 2009 , we reported that dhs had increased the rigor of asp testing in comparison with previous tests .

for example , dndo mitigated the potential for bias in performance testing ( a concern we raised about prior testing ) by stipulating that there would be no asp contractor involvement in test execution .

however , the testing still had limitations , such as a limited set of scenarios used in performance testing to conceal test objects from detection .

moreover , we also reported that tes was to have the lead role in the final phase of asp testing .

as of february 2011 , tes officials told us that the final phase of testing , consisting of 21 days of continuous operation , had not yet been scheduled .

with tes acting as the test agent , it is not in a position to exercise its responsibilities during the operational testing phase , such as approving the operational test plan or writing a letter of assessment of the final results of operational testing .

as it has done for two other recent dhs acquisition programs , tes was able to confirm through its independent assessment whether the test agent conducted operational testing as described in the test and evaluation master plan and operational test plan .

for example , tes outlined concerns in its letter of assessment to the arb that the at - 2 system did not meet a stated operational requirement key performance parameter — a throughput measure of bags per hour — for the majority of the time under test which resulted in a “not effective” determination by tes .

tes officials recognized that , as the test agent , they are not in a position to conduct an independent assessment of operational test results and write a letter of assessment for asp and that they are the highest level organization within dhs for both t&e oversight and operational test expertise .

they further stated that the decision to have tes serve as the test agent was made prior to the issuance of the t&e directive and that it was too late in the program's development to go back and select another agent .

nevertheless , tes officials recognized that this one - time situation would result in the lack of an independent assessment of asp test results and there were no plans to conduct or contract for such an independent assessment .

while we acknowledge that this decision was made prior to the t&e directive and the requirement that tes write a letter of assessment of all major acquisition programs , it is nonetheless important that asp undergo an independent assessment of its test results since its operational test plan , which was developed by tes , was not subject to oversight .

because asp has faced testing issues , many of which we have reported on in past years , it is important that this program undergo oversight to help avoid similar problems from reoccurring .

without an independent assessment of asp's operational test results , it will be difficult to ensure that operational testing was properly planned , conducted , and that the performance results are useful .

in addition , arranging for an independent assessment of operational tests results could provide the arb with critical information on testing and evaluation efforts to help it determine whether asp should be approved for purchase and implementation .

tes and component officials reported challenges faced in coordinating and overseeing t&e across dhs components that fell into four primary categories: ( 1 ) ensuring that a program's operational requirements — the key requirements that must be met for a program to achieve its intended goals — can be effectively tested ; ( 2 ) working with dhs component program staff that have limited t&e expertise and experience ; ( 3 ) using existing t&e directives and guidance to oversee complex information technology acquisitions ; and ( 4 ) ensuring that components allow sufficient time and resources for t&e while remaining within program cost and schedule estimates .

both tes and dhs , more broadly , have begun initiatives to address some of these challenges , but it is too early to determine their effectiveness .

both tes and component officials stated that one of their challenges is developing requirements that are testable , consistent , accurate , and complete .

specifically , six of the nine tes test area managers told us that working with dhs components to ensure that operational requirements can be tested and are suitable to meet mission needs is important because requirements development is one of the biggest challenges facing dhs .

for example , one tes test area manager described the difficulty in drafting a test and evaluation master plan if operational requirements are not testable and measurable .

another tes test area manager indicated that programs' operational requirements documents often do not contain user needs or operational requirements for system performance .

this leads to difficulties in testing those requirements later .

further , six of the nine tes test area managers cited that some components' operational requirements are difficult to test as written , which results in delays in drafting t&e documents as well as impacting the program cost and schedule parameters .

our prior work has found that program performance cannot be accurately assessed without valid baseline requirements established at the program start .

according to dhs guidance , the baseline requirements must include a threshold value that is the minimum acceptable value which , in the user's judgment , is necessary to satisfy the need .

in june 2010 , we reported that if threshold values are not achieved , program performance is seriously degraded , the program may be too costly , or the program may no longer be timely .

in addition , we reported that inadequate knowledge of program requirements is a key cause of poor acquisition outcomes , and as programs move into the produce and deploy phase of the acquisition process , problems become much more costly to fix .

to help remedy these issues , we have made a number of recommendations to address them .

dhs has generally agreed with these recommendations and , to varying degrees , has taken actions to address them .

for example: in may 2010 , we reported that not all of the sbinet operational requirements that pertain to block 1 — a surveillance , command , control , communications , and intelligence system being fielded in two portions of the international border in arizona — were achievable , verifiable , unambiguous , and complete .

for example , a november 2007 dhs assessment determined that 19 operational requirements , which form the basis for the lower - level requirements used to design and build the system , were not complete , achievable , verifiable , or affordable .

further , the dhs assessment noted that a requirement that the system should provide for complete coverage of the border was determined to be unverifiable and unaffordable because defining what complete coverage meant was too difficult and ensuring complete coverage , given the varied and difficult terrain along the border , was cost prohibitive .

to address these issues , we recommended that the currently defined block 1 requirements , including key performance parameters , are independently validated as complete , verifiable , and affordable and any limitations found in the requirements are addressed .

furthermore , cbp program officials told us that they recognized the difficulties they experienced with requirements development practices with the sbinet program .

within cbp , the office of technology , innovation , and acquisition has responsibility for managing the sbinet program .

office of technology , innovation , and acquisition officials told us that their office was created to strengthen expertise in acquisition and program management of sbinet .

in may 2009 , we reported that asp testing uncovered multiple problems in meeting the requirements for successful integration into operations at ports of entry .

as a result , we recommended that dhs assess asps against the full potential of current equipment and revise the program schedule to allow time to conduct computer simulations of asp's capabilities and to uncover and resolve problems with asps before full - scale deployment .

we also reported that other tsa technology projects were delayed because tsa had not consistently communicated clear requirements in order to test the technologies .

we recommended that tsa evaluate whether current passenger screening procedures should be revised to require the use of appropriate screening procedures until it is determined that existing emerging technologies meet their functional requirements in an operational environment .

in march 2011 testimony , the under secretary for s&t stated that s&t had begun working with the dhs under secretary for management to use their collective expertise and resources to better address the “front end” of the acquisition cycle , namely , the translation of mission needs into testable requirements .

further , in response to this challenge , s&t has reorganized and established an acquisition support and operations analysis group , which is to provide a full range of coordinated operations analysis , systems engineering , t&e , and standards development support for dhs components .

in addition , tes's t&e council is currently focusing on the challenges related to requirements development .

specifically , tes test area managers have presented specific briefings to component officials at council meetings which provide information on how to better generate requirements .

further , in response to our previously mentioned report designating dhs on the high - risk list , dhs developed a strategy to , among other things , strengthen its requirements development process .

dhs's january 2011 strategy describes the establishment of a capabilities and requirements council to evaluate and approve operational requirements early in the acquisition process .

specifically , the capabilities and requirements council is to , among other things , reconcile disagreements across program offices and approve analyses of alternatives and operational requirement documents .

we stated in a march 2011 response to dhs on its strategy that it was unclear how the introduction of new governance groups will streamline the process and address previously identified issues because it appeared that the governance groups are chaired by the deputy secretary and have many of the same participants .

since the s&t reorganization has only recently taken place and the t&e council and the department's strategy have only recently begun to address the challenge of requirements generation , it is too soon to determine the effectiveness of these actions in addressing this challenge .

tes officials told us that t&e experience and expertise within dhs components varies , with some components possessing staff with extensive t&e experience and expertise and others having relatively little .

for example , tes officials noted that the coast guard and tsa have t&e policies and procedures in place , as well as staff with extensive t&e experience , which limited their dependence on tes for t&e expertise .

other components in dhs told us they rely more on tes or contractors for t&e expertise .

for the 11 dhs programs we reviewed , officials from components which do not have many acquisition programs , such as the office of intelligence and analysis , reported needing more assistance from tes in identifying and selecting appropriate and qualified test agents , for example .

conversely , components with more acquisition programs , such as the coast guard , told us that they have well - established test agents and procedures in place , and require little guidance from tes .

for example , we reported in april 2011 that most coast guard major acquisition programs leverage navy expertise , in some way , to support a range of testing , engineering , and other program activities .

furthermore , cbp recently established a new office whose goal is to strengthen expertise in acquisition and program management , including t&e , and ensure that cbp's technology efforts are focused on its mission and integrated across the agency .

in response to this challenge , tes has worked with dhs's acquisition workforce office to develop t&e certification requirements and training for components .

tes officials told us that they have worked with the acquisition workforce branch and developed pilot courses on t&e for component t&e staff , including fundamentals of test and evaluation , intermediate test and evaluation , and advanced test and evaluation .

in april 2010 , dhs issued an acquisition workforce policy which establishes the requirements and procedures for certification of dhs t&e managers .

the policy allows t&e managers to be certified at a level that is commensurate with their education , training , and experience .

component staff from 6 of the 11 programs we reviewed said they participated in tes's certification training program and believed that the training would assist them in carrying out their t&e responsibilities .

in addition , tes is in the process of hiring four additional staff to assist the test area managers in their t&e oversight responsibilities and hoped to have the additional staff hired by the end of fiscal year 2011 .

lack of dhs staff to conduct acquisition oversight , including t&e , is a departmentwide challenge .

in our previous reports , dhs acquisition oversight officials said that funding and staffing levels have limited the number of programs they can review .

we recommended that dhs identify and align sufficient management resources to implement oversight reviews in a timely manner .

dhs generally concurred with the recommendation and , as of january 2011 , has reported taking action to address it by identifying needed capabilities and hiring staff to fill identified gaps .

further , to address this challenge , in 2009 and 2010 , t&e council representatives from the acquisition workforce branch made presentations at council meetings to update members on the status of various acquisition workforce issues , including t&e certification .

for example , presenters asked t&e council members to inform their respective components about new t&e certification courses and to provide information on how to sign up for the courses .

in 2010 , the acquisition workforce policy was implemented by dhs , which allowed the department to begin to certify t&e acquisition personnel .

while dhs has undertaken efforts to help address these challenges , it is too soon to evaluate the impact that these efforts will have in addressing them .

effectively managing it acquisitions is a governmentwide challenge .

tes and component officials we interviewed told us that t&e guidance , such as specific guidance for integrating developmental testing and operational testing , may not be sufficient for the acquisition of complex it systems .

specifically , component officials stated that the assessment of risks and environmental factors are different for it programs than other acquisitions and that conducting testing in an operational environment may not be necessary for it programs because the operational environment is no different than the test environment .

in addition , four of the nine test area managers told us that aspects of the existing t&e guidance may not directly apply to it acquisitions .

the department is in the process of making modifications to its acquisitions process to better accommodate information technology acquisitions .

according to the previously mentioned january 2011 strategy submitted to gao , dhs is piloting a new model for it acquisitions .

this model , which is to be consistent with the department's overall acquisition governance process , is to have many of the steps in the modified process that are similar or the same as what currently exists but time frames for different types of acquisitions would be instituted .

for example , acquisition programs designated as it programs may go through a more streamlined acquisition process that may better fit the rapidly changing it environment , and the arb would have the option to delegate oversight responsibilities to an executive steering committee .

in other cases , tes and component officials are investigating the possibility of conducting integrated testing — the combination of developmental and operational testing — for some programs although this process may take longer to plan and pose greater risks because testing is being done simultaneously .

further , the t&e best practices integrated working group , a subgroup of the t&e council , including tes , acquisition program management division , and office of chief information officer officials , was working to identify and promote t&e best practices for it system acquisition .

this group drafted an operational test agent risk assessment process to validate the streamlining process approach while adhering to acquisition and t&e policy and directives , and as of march 2011 , one component , uscis , has made use of this process .

additionally , three other programs are investigating the possible use of this process and the possibility of tailoring or eliminating t&e deliverables or operational t&e requirements for it programs , with the approval of tes .

the group has identified three it acquisition programs to serve as a pilot for this effort .

as dhs considers modifications to its t&e process for it programs , it also must consider the effect such a change could have on determining a system's technical performance and evaluating the system's operational effectiveness and suitability .

for example , we have previously reported on testing problems with sbinet , a cbp program designated as an it program .

we found that sbinet testing was not performed in a manner that would adequately ensure that the system would perform as intended .

among the factors contributing to these problems was insufficient time for reviewing and approving test documentation , which in part , led to test plans and test cases not being well - defined .

as a result , we recommended that test schedules , plans , cases , and procedures are adequately reviewed and approved consistent with the revised test and evaluation master plan .

since the efforts dhs is taking to address this challenge have only recently been initiated , it is too early to tell what impact they will have on the overall challenges of t&e for it programs .

both tes and component officials stated that balancing the need to conduct adequate t&e within the confines of a program's costs and schedule is a recurring challenge , and a challenge that is difficult to solve .

we have previously reported on the challenges associated with balancing the need to conduct testing within program cost and schedules .

our past review of the department of defense's ( dod ) director of operational test and evaluation found that while the acquisition community has three central objectives — performance , cost , and schedule — the director of operational test and evaluation has but one - - operational testing of performance .

we reported that these distinct priorities can lead to testing disputes .

we reported that these disputes encompassed issues such as ( 1 ) how many and what types of test to conduct ; ( 2 ) when testing should occur ; ( 3 ) what data to collect , how to collect them , and how b to analyze them ; and ( 4 ) what conclusions were supportable , given th e analysis and limitations of the test program .

the foundation of most of these disputes laid in different notions of the costs and benefits of testing and the levels of risk that were acceptable when making full - rate production decisions .

the dod director of operational test and evaluation consistently urged more testing ( and consequently more time , resources , and cost ) to reduce the level of risk and number of unknowns before the decision to proceed to full - rate production , while the services consistently sought less testing and accepted more risk when making production decisions .

these divergent dispositions frequently led to est healthy debates about the optimal test program , and in a small number of cases , the differences led to contentious working relations .

tes and dhs component officials expressed views similar to those expressed in our past work at dod .

of the nine tes test area managers we talked with , four told us that allowing appropriate time and resources for t&e within program cost and schedule is a challenge .

according to the test area manager's , component program management officials often do not incorporate sufficient time within their schedule for t&e or reduce the time allowed for t&e to save time and money .

in one test area managers' view , doing so can reduce the effectiveness of testing or negatively impact the results of the tests .

however , tsa officials told us that tes wanted to insert new test requirements for the at - 2 program — including the involvement of more tsa staff in the tests — after the program schedule was established and it was difficult to accommodate the changes and resulted in some delays .

tes officials told us that these test requirements were in lieu of other planned field testing , which were not consistent with the program's concept of operations and that tsa officials agreed with the new test requirements .

according to tes and component officials we spoke with , both the program officials and tes understand the views and perspectives of one another and recognize that a balance must be struck between effective t&e and managing programs within cost and schedule .

as a result , tes is working with program officials through the t&e council or t&e working groups to discuss these issues early in the acquisition cycle ( before it is too late ) , particularly while developing the test and evaluation master plan , which outlines the time allowed for testing and evaluation .

timely and accurate information resulting from t&e of major acquisitions early in the acquisition process can provide valuable information to dhs's senior level managers to make informed decisions about the development , procurement , deployment , and operation of dhs's multibillion dollar portfolio of systems and services .

improving the oversight of component t&e activities is but one part of the significant challenges dhs faces in managing its acquisitions .

components themselves are ultimately responsible for the management and implementation of their programs and dhs senior level officials are responsible for making key acquisitions decisions which lead to production and deployment .

tes helps support acquisition decisions by providing oversight over major acquisitions' t&e , which can help reduce , but not eliminate , the risk that new systems will not be operationally effective and suitable .

since the homeland security act creating dhs was enacted in 2002 , s&t has had the responsibility for overseeing t&e activities across the department .

however , s&t did not have staff or the acquisition and t&e directives in place to conduct such oversight across dhs components until may 2009 when dhs issued its t&e directive .

since then , tes has implemented some of the requirements and overseen t&e of major acquisitions we reviewed , as well as provided independent assessments of operational test results to the arb .

however , tes has not consistently documented its compliance with the directives .

documenting that tes is fulfilling the requirements within dhs acquisition and t&e directives and the extent to which the criteria it is using to review and approve these documents are met , including approving operational test agents and reviewing key acquisition documentation , would assist tes in demonstrating that it is conducting t&e oversight and meeting requirements in these directives .

furthermore , without an independent assessment of operational test results for the advance spectroscopic portal program , a key t&e oversight requirement in the t&e directive , the arb will lack t&e oversight and input it needs to determine whether asp is ready to progress toward production and deployment .

this is especially important , given that program's troubled history , which we have highlighted in a series of prior reports .

to better ensure that testing and evaluation requirements are met , we recommend that the secretary of homeland security direct the under secretary for science & technology to take the following two actions: develop a mechanism to ensure that tes documents its approval of operational test agents and the extent that the test agents meet the requirements in the t&e directive , and criteria that tes use in reviewing these test agents for major acquisition programs .

develop a mechanism to ensure that tes documents its required review of component acquisition documents , including the mission need statements , concept of operations , operational requirements documents , developmental test reports , test plans , and other documentation required by the t&e directive , the extent that these documents meet the requirements in the t&e directive , and criteria that tes uses in reviewing these documents .

to ensure that the arb is provided with an independent assessment of the operational test results of the advanced spectroscopic portal program to help determine whether the program should be approved for purchase and implementation , we recommend that the secretary of homeland security take the following action: arrange for an independent assessment , as required by the t&e directive , of asp's operational test results , to include an assessment of the adequacy of the operational test and a concurrence or nonconcurrence on the operational test agent's evaluation of operational suitability and operational effectiveness .

we received written comments on a draft of this report from dhs on june 10 , 2011 , which are reproduced in full in appendix iii .

dhs concurred with all three of our recommendations .

dhs concurred with our first recommendation ( 1 ) that s&t develop a mechanism to ensure that tes documents its approval of operational test agents , ( 2 ) the extent that the test agents meet the requirements in the t&e directive , and ( 3 ) the criteria that tes uses in reviewing these test agents for major acquisition programs .

specifically , dhs stated that the director of tes issued a memorandum to test area managers and tes staff regarding the operational test agent approval process which describes the responsibilities , considerations for selection , and the process necessary to select an operational test agent .

in addition , dhs stated that tes is drafting memos approving operational test agents using the new test agent approval process .

dhs also concurred with our second recommendation that s&t develop a mechanism to ensure that tes documents ( 1 ) its required review of component acquisition documents required by the t&e directive , ( 2 ) the extent that these documents meet the requirements in the t&e directive , and ( 3 ) the criteria that tes uses in reviewing these documents .

dhs stated that the director of tes issued a memorandum to test area managers and tes staff detailing the role of tes in the document review process and the process that tes staff should follow for submitting their comments to these documents .

finally , dhs concurred with our third recommendation that s&t arrange for an independent assessment of asp's operational test results .

dhs stated that the asp program is under review and does not have an operational test scheduled .

however , tes is investigating the option of using a separate test agent to conduct operational testing of asp , which would allow tes to perform the independent assessment and fulfill its independent oversight role as outlined in dhs policy .

such actions , if taken , will fulfill the intent of this recommendation .

dhs also provided technical comments on the report , which we incorporated as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to interested congressional committees and the secretary of homeland security .

the report will be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have questions regarding this report , please contact me at ( 202 ) 512-9627 or at maurerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iv .

an effort to develop nd deploy technologie to llow custom nd border protection to detect ncler or rdiologicl mteri from conveynce , such as trck , entering the united ste t lnd nd port of entry .

u.s. immigrtion nd custom enforcement ( ice ) an effort to modernize ice'scticl commniction tem nd equipment , tht ice gent nd officer use to support mission - criticl commniction from otdted log tem to modern nd ndrdized digittem .

project 25 pgrde will modernize tcticl commniction nd deploy ite infrastrctre nd end - user subscrier rdio .

interoperable rpid deployment stem ( irds ) will otfit ice with trportable commniction tem to support rpid deployment requirement fro rotine , emergency nd disaster repone , nd pecil opertion .

the progrm i divided into ix egment , inclding: ( 1 ) p25 pgrde for the atlnt region , ( 2 ) p25 pgrde for the boton region , ( ) p25 pgrde for the denver region , ( 4 ) p25 pgrde for the centrl hub infrastrctre , ( 5 ) irds moile rdio commniction kit thsupport disaster nd emergency repone opertion , nd ( 6 ) irds moile commniction stem ( mcs ) moile commniction vehicle thsupport disaster nd emergency repone opertion .

in mrch 2011 , the component acquition exective determined tht the taccom progrm wold e conolidted with other ice infrastrctre progr nd tht the progrm wold e required to submit quition docmenttion to the arb prior to aust 2011 .

segment 1: prodce / deploy / support segment 2-6: in the process of eing pdted .

a ntionwide , interoperting network of detector / identifier tht i to provide autonomous ir - sampling ly of the environment for iologicgent of concern .

the tem i to enable detection , identifiction , nd reporting of recognized orgni within 6-hor period .

 ( 4 ) obsolete component moderniztion ( ocm ) – replce obsolete component nd substem ; ( 5 ) ship helicopter secre trvere & stem ( shsts ) – provide the ability to automticlly ecre the ircrft to the flight deck nd trvere it into the hnger ; nd , ( 6 ) atomtic flight control stem ( afcs / avionic ) modernize digitl common avionic architectre stem ( caas ) common with the h - 60t pgrde nd digitl atomtic flight control stem .

an effort to modernize secret service's it infrastrctre , commniction tem , ppliction , nd process .

the progrm i divided into for dicrete egment: ( 1 ) enabling cabilitie: it infrastrctre moderniztion / cyer secrity / dabase architectre ; ( 2 ) commniction cabilitie ; ( ) control cabilitie ; nd , ( 4 ) mission support cabilitie .

in feuary 2011 , the arb grnted quition deciion event 2a nd quition deciion event 2b deciion for the enabling cabilitie egment .

the remining three egment remined in the anlyze / select phase .

an integrted tem of intrusion detection , lyticl , intrusion prevention , nd informtion - ring cabilitie thre to used to defend the federl civilin government's informtion technology infrastrctre from cyer thre .

inclde the hrdwre , oftwre , supporting process , trining , nd ervice thre to e developed nd quired to support the mission .

the inititem , known as eintein , was renmed as block 1.0 nd incldeabilitie such as centrlized d torge .

 ( 1 ) block 2.0 i to dd n intrusion detection stem ( ids ) which i to assss network trffic for the preence of mlicious ctivity ; ( 2 ) block 2.1 i to provide secrity incident nd event mgement which i to enable d ggregtion , correltion , nd visualiztion .

 ( ) block .0 i to provide n intrusion prevention cability .

a joint inititive etween intelligence nd anly ( i&a ) nd the office of the chief informtion officer which i to ring nified , enterpripproch to the mgement of ll classified informtion technology infrastrctre inclding: ( 1 ) homelnd secre d network ( hsdn ) for ecret level commniction infrastrctre ; ( 2 ) homelnd top secret network ( htsn ) for top ecret commniction infrastrctre ; nd ( ) homelnd secre commniction ( hsc ) for classified voice nd video teleconference cabilitie .

a next genertion of x - ry technology tht i to complement the trditionl x - ry technology nd provide new technicl cabilitie , such as automted detection lgorithm , thret imge projection , lternte viewing tion , bulk exploive lgorithm , nd expnded thret lit tht incorporte emerging thre to vition ecrity .

a tem which i to provide trporttion secrity officerability to creen passenger' crry - on baggge irporttionwide .

a progrm which i to deliver surveillnce nd deciion - support technologie tht crete virtual fence nd ituationreness long the u.s. order with mexico nd c. the firsbinet deployment of the block i tem took plce in the ton , arizon tion .

the econd deployment of the block i tem took plce in the ajo , arizon tion .

in juary 2011 , the sbinet progrm ended as originlly conceived ; however , limited deployment of technology , inclding 15 enor tower nd 10 commniction tower , remined deployed nd opertionl in arizon .

the t&e result on thee tower were to e reported ometime in april 2011 .

tasc i to develop nd field n integrted finncil mgement , asset mgement , nd procrement mgement tem oltion .

the progrm i to usndrd business process nd ingle line of cconting complint with the common governmentwide cconting classifiction trctre .

the tasc exective steering committee determined tht the federl emergency mgement agency will e the firt dhs component to migrte to tasc .

u.s. citizenhip nd immigrtion service ( uscis ) an effort to move immigrtion ervice from per - based model to n electronic environment .

the progrm i to deliver implified , we - based tem for enefit eeker to submit nd trck their ppliction .

the new , ccont - based tem i to provide customer with improved ervice .

in fiscal year 2010 , there were 86 acquisition programs on the acquisition program management division's oversight list , which included the acquisition level and designation as an information technology acquisition .

table 2 lists information on these 86 acquisition programs , and in addition , includes information on the acquisition phase for each program as of april 2011 and whether the program was subject to the test and evaluation ( t&e ) directive .

for example , some programs , such as customs and border protection's acquisition of border patrol facilities would not involve any t&e activities and therefore would not be subject to the requirements in the t&e directive or dhs science and technology directorate's test and evaluation and standards office ( tes ) oversight .

in addition to the contact named above , christopher currie ( assistant director ) , nancy kawahara , bintou njie , melissa bogar , jessica drucker , caitlin white , richard hung , michele fejfar , labony chakraborty , tracey king , paula moore , dan gordon , michele mackin , molly traci , and sean seales made significant contributions to this report .

