performance.gov is intended to serve as the public window to the federal government's goals and performance in key areas .

the office of management and budget ( omb ) created it in response to a requirement of the gpra modernization act of 2010 ( gprama ) .

gprama requires important performance information , including agency performance plans and reports , to be available via a central , government - wide website .

gprama further requires omb to post a list of all federal programs identified by agencies with related budget and performance information , and quarterly updates on agency priority goals ( apg ) and cross - agency priority ( cap ) goals on the website .

in june 2013 , we reported on performance.gov's development and identified issues related to unclear performance measures , inconsistent user experience , and accessibility , navigation , and search capability challenges .

we recommended omb systematically collect customer feedback , enhance the usability of the website design , and collect performance information to develop goals .

omb agreed with our recommendations and indicated it would take steps to implement them .

this report is part of our response to a mandate to periodically assess the implementation of the gpra modernization act of 2010 .

this report assesses omb's ( 1 ) efforts to ensure performance.gov's usefulness , and ( 2 ) strategic plan for performance.gov .

to address our objectives , we reviewed the 22 digitalgov.gov requirements for federal websites and digital services , and selected 9 for our assessment of performance.gov .

the selected requirements are those most associated with customer feedback and outreach , usability , performance measures , and records management .

we assessed omb's efforts ( in collaboration with the performance improvement council ( pic ) and the general services administration ( gsa ) ) to ensure the usefulness of performance.gov and omb's strategic plan by comparing the steps taken and documentation for each to the selected requirements .

to further address our first objective , we examined documentation on how omb was ( 1 ) seeking information from various audiences about their needs concerning performance.gov ; ( 2 ) ensuring the website was clarifying ways audiences can use it ; and ( 3 ) tracking a broader range of performance and customer satisfaction measures , and setting goals for those measures .

we also used the information to follow up on the status of the recommendations from our 2013 report .

in addition , we reviewed requirements outlined in gprama for performance.gov , including public reporting requirements for apgs , cap goals , and the federal program inventory .

we also interviewed staff from omb's office of performance and personnel management , the pic , and gsa to determine the actions taken thus far to address our prior recommendations .

for reports with open recommendations as they relate to performance.gov , see appendix i .

to further address our second objective , we requested documentation on omb's strategic plan for performance.gov , its social media and customer outreach strategy , and its web records plan .

however , the agency has not developed these documents .

we interviewed staff from omb , the pic , and gsa on performance.gov's strategic plan , social media and customer outreach strategy , web records plan , and accessibility on mobile devices .

we compared any steps taken on those actions to digitalgov.gov requirements .

see appendix ii for additional information about the objectives , scope , and methodology of this report .

we conducted this performance audit from july 2015 to august 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

gprama required omb to establish a single , performance - related website by october 1 , 2012 .

the site would provide program and performance information readily accessible and easily found on the internet by the public and members of congress .

omb made performance.gov available to the public in august 2011 .

omb's stated goals for performance.gov include providing ( 1 ) a public view into government performance to support transparency , and ( 2 ) executive branch management capabilities to enhance senior leadership decision making .

performance.gov is a repository of federal government performance information .

one of omb's main goals of the website is to link agency programs , their relationships , and contributions to strategic objectives .

this is intended to increase the utility of this information through enhanced agency comparisons , supporting both benchmarking and best practice identification .

gprama lists the data elements that are required to be reported on performance.gov , including quarterly updates for apgs and cap goals .

omb guidance provides more detailed direction to facilitate the submission of that information .

apgs are target areas where agency leaders want to achieve near - term performance increases .

apgs are often referred to as the agencies' highest priority performance goals .

cap goals are outcome - oriented goals covering a limited number of crosscutting policy areas , as well as goals to improve management across the federal government .

these goals are intended to cover areas where increased cross - agency collaboration is needed to improve progress towards shared , complex policy , or management objectives .

this includes attracting foreign investment to spur job growth and enabling agencies to recruit and hire the best talent .

in march 2013 , federal agencies added the first performance updates for cap goals and apgs to performance.gov .

several entities maintain and operate performance.gov .

figure 1 provides an overview of the performance.gov governance structure .

office of management and budget .

omb is responsible for setting the direction , vision , policy , and guidance of performance.gov and ensuring its effective operation .

omb has partnered with gsa and the pic , and has contract support from ekuber ventures inc. to provide key services for the site .

omb also collaborated with gsa to establish a performance management line of business ( pmlob ) to further guide the administration of performance.gov .

agencies .

gprama requires that agencies make their respective strategic plans , performance plans and reports , and information about their apgs ( as applicable ) , including quarterly updates , available for publication on performance.gov .

twenty - two agencies have web pages on performance.gov that provide links to their strategic plans , annual performance plans , and annual performance reports ; report agency progress on government - wide management initiatives ; and show agency contributions to the cap goals .

there are 31 additional agencies that do not have a dedicated web page on performance.gov .

instead , they provide links to their strategic plans or to their agency plans and reports pages .

agencies submit their annual and , as applicable , quarterly performance information for publication on the website through the performance reporting entry portal ( prep ) system .

while omb and pic give feedback on updates to the information or suggest changes , ultimately , the agency decides what information is published on its performance.gov page .

general services administration .

gsa builds the technical platform , provides project management of performance.gov , and determines the business requirements and priorities .

performance improvement council .

the pic is chaired by omb's deputy director for management and is composed of performance improvement officers ( pio ) from each of the 24 chief financial officers ( cfo ) act agencies as well as other pios and individuals designated by the chair .

the pic facilitates the exchange of useful practices to strengthen agency performance management , such as through cross - agency working groups .

the pic is supported by an executive director and a team of eight full - time staff who conduct implementation planning and coordination on crosscutting performance areas .

in coordination with omb , the pic provides several types of guidance to agencies .

it also trains agency officials responsible for updating the quarterly information on performance.gov and provides liaisons to answer those officials' questions .

contractor ( ekuber ventures inc. ) .

in august 2015 , gsa awarded software services company ekuber ventures a contract to provide operations support and maintenance of performance.gov and the prep system .

according to the contract , ekuber will provide training and information technology help desk support .

the current contract runs through august 2020 .

performance management line of business ( pmlob ) .

pmlob is an interagency effort to develop government - wide performance management capabilities to help meet gprama transparency requirements .

it is also designed to support government - wide performance management efforts .

pmlob's key objectives , according to its 2013 charter , include , among others , developing performance.gov into a gprama - compliant data tool .

when first established , performance.gov was funded by gsa's e - government fund .

beginning in 2013 , this funding was supplemented by agency fees .

according to the pmlob program charter and pic staff , e - government funding ( now funding from the federal citizen services fund ) was to be primarily used for enhancing the site , developing new functionality , and focusing on specific areas identified by management .

however , according to agency staff , due to limited funding , they are focusing on system maintenance rather than new development .

staff also said performance.gov received $1,029,922 from the federal citizen services fund for fiscal year 2016 .

however , omb and pic staff told us gsa used $700,000 for other purposes leaving $329,922 for the website .

additional funding for performance.gov is collected from 15 agencies through interagency agreements with gsa .

these agreements document the services gsa will provide and the fees the agencies will pay .

through the interagency agreements , gsa collects approximately $795,000 annually — $53,000 from each of the 15 agencies .

according to the pmlob charter , agency fees are to cover the data collection capabilities and some operational and maintenance costs .

pic staff told us they plan to request a 4 percent increase in agency fees in fiscal year 2017 to meet the increasing costs for operating the website .

digitalgov.gov contains a checklist of requirements for federal websites and digital services that are based on relevant statutes , regulations , executive orders , or policy documents .

digitalgov.gov is managed by gsa and is designed to help agencies provide digital services and information to the public .

our review focused on requirements related to customer feedback and outreach , usability , performance measures , and records management .

see appendix ii for additional information on our selected requirements .

while omb , gsa , and the pic took several steps to improve performance.gov , their actions neither meet digitalgov.gov or gprama requirements , nor completely address our prior recommendations .

for example , omb , gsa , and the pic collect some customer feedback , but have not engaged broader audiences .

while gsa , on behalf on omb , also conducted a usability test , omb has not addressed all of the findings from that test .

further , omb and the pic track 18 of 24 recommended performance measures , but have not set goals for those measures .

in addition , our prior work identified several areas where omb is not fully meeting gprama requirements for performance.gov .

this includes making all the required information for apgs , cap goals , and the federal program inventory available on the website .

omb has not yet implemented the recommendations we made related to these findings .

omb , gsa , and pic staff took some steps to prioritize , store , and address user feedback for the website .

according to digitalgov.gov requirements , agencies need to understand the needs of their customers by collecting and addressing customer feedback , and use those data and feedback to continuously improve programs .

moreover , a focus of gprama is to make federal performance information more accessible to the public .

the following methods are used to collect customer feedback and improve the website: website survey .

pic staff told us that a website survey is configured to appear on performance.gov for about 20 percent of visits .

the website survey asks users to rate their overall experience on performance.gov , how likely they are to return , and how likely they are to recommend it to someone else , among other things .

survey results from october 2014 to december 2015 showed that 48 percent of survey respondents rated their overall experience on performance.gov an 8 out of 10 or above .

of the 568 respondents during this period , 69 percent were likely or very likely to return to the website , and 61 percent of respondents were likely or very likely to recommend performance.gov to someone else .

feedback button .

omb and the pic also collect feedback through the “feedback” button on performance.gov's home page .

the “feedback” button leads the user to a web form that sends feedback directly to gsa , as shown in figure 2 .

examples of feedback submitted through the form include: notification of broken links , comments about outdated agency information , and questions about where specific information can be found on the website .

prep working group .

omb staff stated that they have been focused on enhancing the internal prep system that is used by agency officials to submit the annual and quarterly performance information required to be on the website by gprama .

to help in this effort , the prep working group — a voluntary group of agency officials responsible for updating their respective agency's performance reporting information — provides feedback to the contractor on the prep users' needs .

for example , the working group members were given the opportunity to test updates to the prep system and suggest improvements .

according to pic staff , feedback submitted through the website survey , “feedback” button , prep working group , and e - mails to the performance.gov help desk are prioritized and logged into the product backlog , a system used by gsa to store feedback .

according to pic staff , feedback is prioritized based on several factors , including the value it would bring to a larger audience , the cost and estimated time to implement , and the risk and opportunity cost of addressing the feedback .

the highest priority items on the product backlog system are placed on the monthly prioritized list and are handled by the contractor .

examples of addressed feedback from the product backlog include modifying graphs to accurately display apg data and updating performance.gov's frequently asked questions page to reflect current information .

gsa , on behalf of omb , conducted a usability test on performance.gov and issued the findings in september 2013 .

digitalgov.gov recommends conducting usability testing to collect general feedback from users about the design and functionality of a website , offering invaluable insights into what needs improvement .

further , digitalgov.gov states that simple usability tests are a quick way to identify major problems and give agencies the tools to take immediate action to improve the website .

the september 2013 usability test report found several problems with performance.gov and made recommendations to improve the usability of the website , detailed in table 1 .

the specific findings of the usability test and status of actions taken to address those findings are as follows: accessibility .

the 2013 gsa usability test found portions of performance.gov that were inaccessible to people with visual disabilities .

according to omb staff , changes were made to the website to address the identified accessibility issues consistent with section 508 requirements .

pic staff also told us that when the pic adds new content to the website , it is tested for accessibility with tools such as screen readers .

agencies are responsible for ensuring the content they submit to the website is accessible , such as the quarterly updates .

digitalgov.gov's guidance on section 508 states that accessibility testing should be conducted before a web page launches or when making significant changes to digital products and services .

purpose .

some usability testers who participated in the 2013 gsa usability test also reported that they were confused about the purpose of the website because the main page did not clearly explain either performance.gov's purpose or the target audience .

digitalgov.gov plain writing requirements state that content should be written in a clear , concise , and well - organized manner .

we reviewed related federal websites and found examples of clearly stated purposes on the home page .

figure 3 compares the data.gov home page to that of performance.gov .

data.gov has a simple explanation of the site and uses icons to depict the available topics .

performance.gov has two large paragraphs of text explaining the benefits of establishing apgs and cap goals , but it does not clarify its purpose as a central , government - wide website where users can find these apgs and cap goals for federal agencies .

performance.gov's lack of a simple and clear explanation of its purpose could potentially confuse users .

without guidance about tasks that can be accomplished on a website , along with explanations of the different areas of the website and navigation assistance , website users may be unable to successfully achieve their objectives .

data visualizations .

the gsa usability test also reported that some users had trouble locating graphs or data visualizations , and understanding whether agencies had met target goals .

for example , the usability test noted that users wanted a “goal line” showing target values .

in 2013 , we also reported on the importance of making the information and data on a performance - reporting website engaging and easily understandable .

figure 4 shows how “measures of australia's progress,” a website designed to show users how australia is progressing , provides color - coded indicators of how a performance metric is performing relative to the goal .

in contrast , performance.gov does not have color - coded indicators or other data visualizations that help users understand if agencies are meeting their goals .

search .

the 2013 gsa usability test also examined how well the home page's search function performed .

the test revealed that search terms were not highlighted in search results and results of the search did not necessarily match the search terms .

for example , when users searched for the “national institutes of health,” search results returned the home page for the national science foundation instead of pages directly related to the national institutes of health .

pic staff told us that the search algorithm was modified to highlight the search terms in the search results .

however , the algorithm has not been updated to return search results more reflective of the search terms .

the contract with ekuber discusses an option of improving the search function .

however , omb staff told us they have not decided whether they will invest in enhancing the website's search capability .

omb staff told us that they have not implemented all the usability test recommendations because of limited resources .

additionally , while the ekuber contract , awarded august 19 , 2015 , has options to address some usability issues , omb has not prioritized which usability issues need to be addressed first or a timeline for addressing these issues .

specifically , the contract contains an option to enhance the usability of the site , which gsa can exercise at a later date during the contract period .

these contract options include improving the website search function and enhancing data visualizations on performance.gov , among other things .

in 2013 , we also found that some performance.gov users reported issues with accessibility , navigation , and search capabilities .

specifically , we found that omb had not articulated ways that intended audiences , such as members of the public , congress , and agency staff , could use the website or the information available through it to accomplish other specific tasks .

for example , the website gave no indication or examples of the ways that various audiences could use performance.gov to facilitate coordination or communication about goals , activities , and performance between agencies .

at that time , we recommended omb clarify ways that intended audiences could use the information on performance.gov to accomplish specific tasks and identify the design changes that would be required to facilitate that change .

omb agreed with our recommendation and subsequently conducted the usability test .

although the actions omb has taken are a step in the right direction , they do not fully address our prior findings with regard to how people could use the website to complete specific tasks .

as a result , our prior recommendation remains open .

thus , if usability issues are not addressed , performance.gov users could continue to have difficulties using and understanding the content posted on the site .

addressing usability issues could also help resolve another open recommendation from our 2013 report concerning engaging wider audiences — such as congressional staff and interested members of the public — to ensure the site is meeting a broad set of user needs .

specifically , in 2013 , we recommended that omb seek to more systematically collect information on the needs of a more varied audience , including through the use of customer satisfaction surveys and other approaches recommended by leading practices .

omb also agreed with this recommendation .

while the efforts omb and the pic have taken on usability testing and collecting and using customer feedback are steps in the right direction , these actions do not completely address our prior recommendation on engaging a potentially broader audience about the website's usefulness .

as a result , our previous recommendation remains open .

digitalgov.gov requirements state managers should track , analyze , and report on a minimum baseline set of performance , search , customer satisfaction , and other measures .

this allows staff to get a holistic view of how well online information and services are delivered .

as of may 2016 , omb and the pic were tracking 18 of the 24 recommended digitalgov.gov measures .

pic staff told us that they began using the digital analytics program ( dap ) to track performance measures for performance.gov in march 2014 .

a dap staff member further explained that the web analytics program is not currently customized to track the six remaining measures for performance.gov .

this represents some improvement from 2013 , when we found omb and gsa monitored visitors' use of performance.gov by tracking 15 of the 24 website performance measures .

at that time , we recommended that omb seek to ensure that all performance , search , and customer satisfaction measures , consistent with leading practices , are tracked for the website , and , where appropriate , omb should create goals for those measures to help identify and prioritize potential improvements to performance.gov .

while omb agreed with the recommendation , it still does not track all the measures we recommended and that are required by digitalgov.gov .

omb and the pic are now tracking four customer satisfaction measures that were not tracked in 2013 .

however , two other measures related to searches — ”top referring search terms” and “percentage of visitors using site search” — that omb and the pic tracked in 2013 are not being tracked in 2016 .

table 2 shows the performance measures tracked for performance.gov in 2013 as compared to the performance measures tracked in 2016 .

additionally , omb and the pic have not set goals or targets for any of the measures they are tracking .

under the strategic planning requirements established under gpra and enhanced by gprama — which can also serve as leading practices for planning for individual initiatives — agencies are to establish performance goals to define the level of performance to be achieved .

in addition , agencies are required to establish performance indicators to assess the progress towards the goal , and later evaluate whether the goal has been met .

furthermore , our 2013 report found that goals or targets had not been established for the performance measures the agency was tracking .

we recommended that omb develop goals or targets for those measures .

our prior recommendation remains open .

pic staff told us that goals were not set for performance.gov performance measures , and improvements have not been made because of limited staffing resources .

in february 2016 , the pic hired a new digital services director who will be responsible for reviewing performance measures tracked by dap and making recommendations for changes to the website accordingly , among other things .

without tracking all recommended search and customer satisfaction measures , and establishing goals or targets for these measures , it will be difficult for omb and the pic to know if they are meeting customer needs and if they are delivering information to identify and prioritize potential improvements to the website .

our prior work has found that omb has not met all of the gprama public reporting requirements for performance.gov .

in particular , our work identified several areas where omb was not fully meeting apg and cap goal public reporting requirements .

additionally , the inventory of federal programs had not been updated on performance.gov since the inventory's initial release in may 2013 .

based on recent communications with omb staff , these issues have not been fully resolved .

apgs .

in september 2015 , we reported that performance.gov provided limited information on the quality of performance information used to measure progress on selected apgs .

gprama requires agencies to publicly report on how they are ensuring the accuracy and reliability of the performance information they use to measure progress towards these apgs .

the six agencies we reviewed for the 2015 report used various sections of performance.gov to discuss some of the performance information quality requirements for apgs .

but none of the agencies met all five gprama requirements for their individual apgs .

moreover , while there is no place on the website that is set aside to discuss the quality of performance information for each apg , we found hyperlinks on performance.gov to the selected agencies' performance plans and reports .

however , there was no explanation of where to find discussions on the quality of performance information in these plans and reports .

we concluded that while omb had directed agencies to discuss the quality of apg performance information in their annual performance plans and reports for several years , the selected agencies' plans and reports often did not .

we made two recommendations to omb aimed at improving public reporting .

we recommended that omb , working with the pic , identify practices participating agencies can use to improve their public reporting in their performance plans and reports of how they are ensuring the quality of performance information used to measure progress towards apgs .

pic staff took steps to address this recommendation .

in may 2016 , pic staff reported that they had summarized the self - assessments completed by the performance improvement officers ( pios ) and their deputies on their agency's data quality policies and procedures .

the pic staff summary we reviewed identified aspects of data quality in which agencies had generally rated their performance highest , and other aspects of data quality in which agencies had rated their performance lowest .

we believe the pic's efforts should help pios examine their agency's data quality policies and procedures , and ultimately improve data quality and the information provided to external audiences .

we also recommended that omb , working with the pic , identify additional changes that are needed for its guidance to agencies on ensuring the quality of performance information for apgs on performance.gov .

as of june 2016 , omb has not updated its guidance .

cap goals .

in may 2016 , we reported that omb and the pic had incorporated lessons learned from the prior cap goal period ( 2012-2014 ) and provided ongoing assistance to cap goal teams .

based in part on our june 2014 recommendation , omb and the pic updated guidance and developed a new reporting template to help improve public reporting on the implementation of cap goals .

omb and the pic also implemented strategies to build agency capacity to work across agency lines , such as assigning agency goal leaders , providing ongoing guidance and assistance to cap goal teams , and holding senior - level reviews .

we also found that the selected cap goal's quarterly progress updates — published on performance.gov — met a number of gprama reporting requirements , including identifying contributors and reporting strategies for performance improvement and quarterly results .

furthermore , in our may 2016 report , while we found that most of the selected cap goal teams were consistently reporting the status of quarterly milestones to track goal progress , they had not established quarterly targets as required by gprama .

also , all of the selected cap goal teams reported that they were working to develop performance measures .

while they were at various stages of the process , they were not reporting on these efforts consistently .

in that report , we recommended that omb and the pic report on performance.gov the actions that cap goal teams are taking , or plan to take , to develop performance measures and quarterly targets .

omb staff generally agreed with the recommendation .

with improved reporting of performance information , the cap goal teams will be better positioned to demonstrate goal progress at the end of 4-year goal period .

omb has not yet confirmed the specific actions it plans to take in response to this recommendation .

federal program inventory .

in october 2014 , we found the approach used by omb and agencies to develop an inventory of all federal programs along with related budget and performance information had not fully met gprama requirements .

gprama requires omb to make this information publicly available on performance.gov .

pic staff reported that the federal program inventory requirement was initially met in fiscal year 2013 by presenting data from agency plans and reports as pdf attachments on performance.gov .

however , we found that omb had not published an inventory of federal programs on performance.gov since 2013 .

moreover , omb did not expect an update of the program inventories to happen before may 2017 because the staff who would work on the program inventories were heavily involved in digital accountability and transparency act of 2014 ( data act ) implementation .

further , in october 2014 , we reported that implementation of the data act could be tied to the program inventories because the act requires federal agencies to publicly report information about any funding made available to , or expended by , an agency or a component of the agency at least quarterly .

we also found that agency reporting for both sets of requirements was web based .

this could more easily enable linkages between the two or facilitate incorporating information from each other .

in july 2015 , we recommended that omb should accelerate efforts to determine how best to merge data act purposes and requirements with the gprama requirement to produce a federal program inventory .

in april 2016 , pic staff told us that they will work with the data act implementation team to determine how to best integrate the gprama and data act requirements .

however , they did not provide specific details .

until omb determines a strategy to integrate gprama and data act requirements , omb will not know what resources or steps it needs to take to ensure the requirements are met and incorporated on performance.gov .

omb and pic officials have told us that they are focused on ensuring performance.gov is gprama compliant and are aware that the website is not fully consistent with gprama requirements .

without all the required gprama information , performance.gov will not be transparent and may fall short of meeting user needs .

omb does not have a strategic plan for the website that will help guide officials in the future .

specifically , we found that omb does not have a customer outreach strategy that incorporates , as appropriate , information about how omb intends to ( 1 ) inform users of changes on performance.gov , ( 2 ) use social media as a method of communication , and ( 3 ) use mobile devices and applications .

finally , omb lacks an archiving plan that details how omb plans to manage the data and content on performance.gov .

omb has not developed a strategic plan to guide the future of performance.gov .

agency - wide strategic planning practices required under gpra , and enhanced by gprama , can serve as leading practices for planning at lower levels within federal agencies , such as individual programs or initiatives .

under this law , strategic plans are the starting point and basic underpinning for results - oriented management .

among other things , strategic plans should contain goals and objectives , approaches , and resources needed to achieve those long - term goals and objectives .

when we began our review , omb staff said that they had not developed a strategic plan for performance.gov because a new contractor , ekuber , had just started a few months prior , and they wanted to allow time for the contractor to transition into its new role .

ekuber took over the performance.gov contract in the late summer of 2015 .

since that time , omb and the pic have taken an important first step towards developing a strategic plan by hiring a digital services director in february 2016 .

the digital services director's responsibilities include developing a strategic plan and managing the long - term development of performance.gov .

hiring a new director who has responsibility for outlining needed improvements is a move in the right direction .

however , without a plan for the future , omb will not know what resources it will need or steps it needs to take to ensure all requirements are met and incorporated on performance.gov .

such a plan could prove especially valuable in maintaining continuity during the upcoming presidential transition .

omb and the pic have not developed a customer outreach strategy .

the digital government: building a 21st century platform to better serve the american people strategy calls for federal websites to become more customer centric by responding to the needs of users and making it easier to find and share information , and accomplish important tasks “anytime , anywhere , any device.” it also calls on agencies to embrace new technologies to drive participation and to develop innovative , transparent , user - facing products and services efficiently and effectively .

without developing a user outreach plan , omb risks being unable to provide services to its users where they need it most .

specifically , we identified three areas where performance.gov did not have a customer outreach strategy that incorporates , as appropriate , information about how omb intends to ( 1 ) inform users of changes on performance.gov , ( 2 ) use social media as a method of communication , and ( 3 ) use mobile devices and applications .

performance.gov does not inform users when a new quarterly update has been published .

website usability guidelines created by gsa in conjunction with health and human services ( hhs ) call for websites to inform users when changes are made .

most of the time , omb publishes a blog post alerting users that new quarterly data have been updated ; however , this blog post is made available through www.whitehouse.gov , not performance.gov .

while a blog post on whitehouse.gov will inform some users that the data have been updated , more performance.gov users would be informed of the updates if they were cross posted to the performance.gov home page .

the lack of alerts is particularly problematic because omb does not always meet the deadlines published in omb circular no .

a - 11 ( which provides guidance to agencies ) for the apg and cap goal quarterly updates .

for example , in the 2015 version of omb circular no .

a - 11 , omb estimated the update for the first quarter of 2016 would be published around march 17 , 2016 .

however , these data were not published until march 30 , 2016 .

pic staff told us that , in some cases , omb held the publication date until it could be coordinated with a press release or a blog posting , which was the case for this quarterly update .

the delay in publishing the quarterly updates highlights the need to inform users of when the website has been updated .

other federal agencies have updated users about new information on their website by including a “latest news” or “updates” section on their home page or by implementing date and time stamps .

for example , the u.s. census bureau home page provides users with a “latest news” section , along with a calendar of events for the upcoming week , to allow users to easily access the most up - to - date information ( see figure 5 ) .

omb and pic staff said that most of the site's users are other federal agencies and employees who already generally know when information will be updated .

the public and members of congress , however , would not know when information has been updated .

in may 2016 , pic staff told us they are considering a number of options to inform users of changes on performance.gov .

this includes a banner on the home screen identifying new information or using time stamps and a timeline to highlight quarterly updates , among other options .

while pic staff are in the early stages of planning , we continue to believe communicating changes on the website will help enhance the usability for all performance.gov users .

without a systematic method for informing users of when the website has been updated , omb and the pic are missing an opportunity to disseminate information to a broader base of users and meet users where they are .

on a related issue , in some instances , we were unable to determine when some information on the website was last updated .

for example , we reviewed the content on seven web pages – overview , strategies , progress update , next steps , indicators , continuing programs and other factors , and related goals – associated with 22 apgs .

we found that none of them had a time or date stamp of when the information was last updated .

date or time stamps can provide users with a clear idea of when a web page was last reviewed or updated .

this increases a website's transparency .

we also reviewed the progress update page of each of the 22 apgs .

on three of those pages , we were unable to identify any text that would tell the reader when the page was last updated , such as the quarter or year of the update .

according to pic staff , the information on the apg pages always reflects the most current information .

as previously noted , gprama requires the apgs to be updated quarterly .

without a date or time stamp or some indication of when the data were last updated , system users , including decision makers , are unable to determine how current the website's data are .

performance.gov provides users with social media links to share information on the website .

however , omb and the pic are not using social media services , such as twitter or facebook , to interact with the site's users .

digitalgov.gov's social media requirement states that staff should use social outreach tools to interact with customers and improve the customer experience .

furthermore , we found that other federal websites have social media pages and updates linked on the home page .

for example , the u.s. department of housing and urban development ( hud ) website has a feed on its home page showing tweets from its twitter account .

these tweets provide users with the latest information from hud , including policy updates and recent news and events related to hud's mission ( see figure 6 ) .

according to omb and pic staff , they have not used social media for customer outreach because they have limited staffing resources to manage a social media strategy .

however , in may 2016 , omb and pic staff told us they are planning to hire a communications advisor who will be responsible for creating a social media strategy , among other tasks .

this is an important first step .

moving forward with a social media strategy should help publicize performance.gov and improve user experience .

without such a strategy , omb and the pic are missing an opportunity to interact with customers and to improve the customer experience .

performance.gov is accessible on mobile devices , such as mobile phones and tablets , as digitalgov.gov recommends .

the performance.gov home page and the eight main tabs are accessible and readable on a mobile device .

we were able to access agency information and read about strategic goals and apgs on a mobile device .

however , omb does not have a mobile application ( also known as an app ) for performance.gov .

the u.s. digital services playbook outlines key successful practices from the private sector that would help the government build effective digital resources , including mobile applications .

according to the playbook , it is important for staff to understand how users access the website , whether it is through a computer or mobile device .

digital products may better cater to how the users interact with the website .

omb and pic staff told us they have not determined whether it will be beneficial to develop a mobile application for its users .

considering the need for a mobile application when developing a customer outreach plan may help omb determine future resource requirements .

as of the end of february 2016 , omb and the pic no longer maintain a full archive of performance.gov .

this means that the data that were previously published as a part of the 2012-2013 apgs are no longer available to the public .

additionally , on an ongoing basis , omb and the pic do not publish the progress updates from a prior quarter's update for any apg , once the newest quarter's data are released .

however , previous iterations of cap goal progress have been maintained for the public to access via the cap goal update pages .

pic staff told us that the platform used to previously archive the site was no longer supported .

pic staff told us they would like to migrate the archived data back onto performance.gov so that it can be accessed publicly again , if funding for the project becomes available .

the digitalgov.gov requirement for records management cites national archives and records administration ( nara ) guidance on managing web records , which is based on statutory requirements .

this guidance states federal websites are part of an agency's approach to serving the public and agencies should conduct risk assessments to determine what parts of their websites should be documented and have records kept .

once an agency determines which content to keep or archive , staff should then develop a web records schedule to document and store that content .

the pic does not have a web records schedule to determine how to manage performance.gov content and data .

instead , the pic told us gsa is instituting a new method of managing and tracking the progress of its web projects .

a web records schedule would provide stakeholders with important information about plans to archive the data and content published on performance.gov .

further , without an archive of performance.gov , users can no longer compare long - term agency priority goals and progress made toward those goals .

this affects the website's transparency .

since performance.gov was launched in 2011 , omb has worked with the pic and gsa to develop a single website that will meet federal requirements for the public reporting of agency performance information .

while omb , gsa , and the pic have taken several steps to improve performance.gov , their actions do not fully meet digitalgov.gov requirements or completely address our prior recommendations .

for example , while omb and gsa conducted a usability test of the website , they have not addressed all of the test's findings .

further , omb has experienced several challenges implementing apg and cap goal reporting , and the federal program inventory requirements outlined in gprama .

without improving usability and fully implementing gprama requirements , performance.gov will have difficulty serving its intended purpose as a central website where users can find government - wide performance information easily .

omb has not developed a strategic plan for performance.gov .

omb and the pic took an important first step by hiring a digital services director for the pic .

omb now needs to outline the goals and objectives , approaches , and resources needed for the future development of performance.gov .

further , omb does not have a customer outreach strategy that explores additional ways to display updated information on the performance.gov home page and to use social media and mobile applications for outreach .

additionally , outlining a plan to manage and archive the content and data on performance.gov in a systematic way will increase the transparency of the website .

without a strategic plan that incorporates all of these areas , it will be difficult for decision makers to prioritize and plan for future website improvements .

such a plan could prove especially valuable in maintaining continuity during the upcoming presidential transition .

we recommend the director of the office of management and budget , in consultation with the performance improvement council and general services administration , take the following three actions: 1 .

ensure the information presented on performance.gov consistently complies with gprama public reporting requirements for the website's content .

2 .

analyze and , where appropriate , implement usability test results to improve performance.gov .

3 .

develop a strategic plan for the future of performance.gov .

among other things , this plan should include: the goals , objectives , and resources needed to consistently meet digitalgov.gov and gprama requirements ; a customer outreach plan that considers how ( 1 ) omb informs users of changes in performance.gov , ( 2 ) omb uses social media as a method of communication , and ( 3 ) users access performance.gov so that omb could , as appropriate , deploy mobile applications to communicate effectively ; and a strategy to manage and archive the content and data on performance.gov in accordance with nara guidance .

we provided a draft of this report to the director of omb and the administrator of gsa for review and comment .

on august 5 , 2016 , we met with omb , pic and gsa staff .

omb and pic staff provided us with oral comments on the report and we made technical changes as appropriate .

omb staff agreed with the recommendations in the report .

gsa did not have comments on the report .

we are sending copies of this report to the director of omb and the administrator of gsa as well as appropriate congressional committees and other interested parties .

in addition , this report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff members have any questions about this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iii .

status update 1 .

the agency has not yet provided reporting on cross agency priority ( cap ) goal progress , we recommend that the director of the office of management and budget ( omb ) , working with the performance improvement council ( pic ) , take the following action: report on performance.gov the actions that cap goal teams are taking , or plan to take , to develop performance measure and quarterly targets .

information on what actions it has taken in response to this recommendation .

1 .

to help participating agencies improve 1 .

we reviewed updates omb published to their public reporting , the director of omb , working with the pic executive director , should identify additional changes that need to be made in omb's guidance to agencies related to ensuring the quality of performance information for agency performance goals ( apg ) on performance.gov .

circular no .

a - 11 in july 2016 .

circular no .

a - 11 continues to direct agencies to provide data quality information for publication on performance.gov or to provide a hyperlink from performance.gov to relevant explanation in agencies' performance reports , which was a requirement omb added in june 2015 in response to our preliminary findings .

however , our review found that omb also needed to update the template agencies complete for performance.gov updates to make it easier for agencies to publish this information on the website .

the july 2016 update of circular no .

a - 11 does not indicate whether this template has been updated or whether additional changes to a - 11 are needed .

we will continue to monitor omb and the pic's efforts to address our recommendation .

1 .

to ensure that federal program spending 1. data are provided to the public in a transparent , useful , and timely manner , the director of omb should accelerate efforts to determine how best to merge digital accountability and transparency act ( data act ) purposes and requirements with the gpra modernization act of 2010 ( gprama ) requirement to produce a federal program inventory .

in april 2016 , omb staff told us that identifying programs for the purposes of data act reporting would not be completed until after may 2017 .

however , they said they have convened a working group to develop and vet a set of options to establish a government - wide definition for program that is meaningful across multiple communities and contexts ( such as budget , contracting , and grants ) .

status update 1. federal program inventory requirements and to make the inventories more useful , the director of omb should better present a more coherent picture of all federal programs: revise relevant guidance to direct agencies to collaborate with each other in defining and identifying programs that contribute to common outcomes ; revise relevant guidance to provide a time frame for what constitutes “persistent over time” that agencies can use as a decision rule for whether to include short - term efforts as programs ; and define plans for when additional agencies will be required to develop program inventories .

in june 2016 , omb staff stated that they have not taken any actions in response to our recommendations related to the federal program inventory , as they continue to determine how best to implement inventory requirements in coordination with those of the data act .

in our july 2015 testimony on data act implementation , we recommended that omb accelerate efforts to determine how to best merge data act purposes and requirements with the gprama requirement to produce a federal program inventory .

however , at the same hearing , the acting deputy director for management and controller at omb stated that he did not expect an update of the program inventories to happen before may 2017 .

2 .

to ensure the effective implementation of 2 .

as of june 2016 , omb had not taken federal program inventory requirements and to make the inventories more useful , the director of omb should , to better present a more coherent picture of all federal programs , include tax expenditures in the federal program inventory effort by designating tax expenditure as a program type in relevant guidance ; and developing , in coordination with the secretary of the treasury , a tax expenditure inventory that identifies each tax expenditure and provides a description of how the tax expenditure is defined , its purpose , and related performance and budget information .

action to include tax expenditures in the federal program inventory .

gprama requires omb to publish a list of all federal programs on a central , government - wide website .

the federal program inventory is the primary tool for agencies to identify programs that contribute to their goals , according to omb's guidance .

by including tax expenditures in the inventory , omb could help ensure that agencies are properly identifying the contributions of tax expenditures to the achievement of their goals .

although omb published an initial inventory covering the programs of 24 federal agencies in may 2013 , omb decided to postpone further development of the inventory in order to coordinate with the implementation of the data act .

in our july 2015 testimony , we recommended that omb accelerate efforts to merge data act purposes with the production of a federal program inventory .

status update 3 .

in june 2016 , omb staff stated that they have not taken any actions in response to our recommendations related to the federal program inventory , as they continue to determine how best to implement inventory requirements in coordination with those of the data act .

in our july 2015 testimony on data act implementation , we recommended that omb accelerate efforts to determine how best to merge data act purposes and requirements with the gprama requirement to produce a federal program inventory .

however , at the same hearing , the acting deputy director for management and controller at omb stated that he did not expect an update of the program inventories to happen before may 2017 because the staff that would work on the program inventories were heavily involved in data act implementation .

revise relevant guidance to direct agencies to consult with relevant congressional committees and stakeholders on their program definition approach and identified programs when developing or updating their inventories ; revise relevant guidance to direct agencies to identify in their inventories the performance goal ( s ) to which each program contributes ; and ensure , during omb reviews of inventories , that agencies consistently identify , as applicable , the strategic goals , strategic objectives , apgs , and cap goals each program supports .

status update 1 .

omb has taken some steps to address could use the information on the performance.gov website to accomplish specific tasks and specify the design changes that would be required to facilitate that use .

this recommendation , but additional actions are needed .

the general services administration , on behalf of omb , issued a usability test report on performance.gov in september 2013 .

the test found that ( 1 ) sections of the website were not accessible ; ( 2 ) users were unclear about the purpose of performance.gov , its intended audiences , and what users can do on the website ; ( 3 ) users had difficultly locating graphics and understanding if agencies had met their goals ; and ( 4 ) the search functionality produced poor results and the search terms were not highlighted .

this usability test produced several recommendations based on these findings .

according to omb and pic staff in august 2015 and may 2016 , they have taken some actions to address the recommendations .

for example , staff addressed the accessibility issue and partly addressed the search issue .

however , omb has not yet addressed the other two recommendations .

further , omb and the pic have not clarified the ways that intended audiences can use the information on the website to accomplish specific tasks , or specified design changes that would be required to facilitate that use , as we described in our report .

we will continue to monitor progress .

status update 2 .

omb , gsa and the pic systematically information on the needs of a broader audience , including through the use of customer satisfaction surveys and other approaches recommended by federal guidance .

collect information on the needs of its users by implementing a website survey , a feedback form on performance.gov , and a working group focused on improving the prep system .

staff have set up a backlog system to prioritize , store , and address user feedback .

according to pic staff , feedback is prioritized based on several factors .

these factors include the value it would bring to a larger audience , the cost and estimated time to implement , and the risk and opportunity cost of addressing the feedback .

the highest priority items on the product backlog system are placed on the monthly prioritized list for the contractor to begin work on .

however , omb and the pic have not identified , engaged , or collected information on the needs of a broader audience , such as interested members of the public , and how those needs might be addressed through the website .

3. seek to ensure that all performance , 3 .

as of may 2016 , omb and the pic were search , and customer satisfaction metrics , consistent with leading practices outlined in federal guidance , are tracked for the website , and , where appropriate , create goals for those metrics to help identify and prioritize potential improvements to performance.gov .

monitoring 18 of the 24 recommended performance measures .

pic staff said that they now track the performance measures through the digital analytics program , which does not track the remaining six measures .

omb and pic staff have not created goals for the measures they track to help identify and prioritize potential improvements to performance.gov .

we will continue to monitor progress .

1 .

to ensure that the pic has a clear plan 1 .

the pic developed a strategic plan for for accomplishing its goals and evaluating its progress , the director of omb should work with the pic to update its strategic plan and review the pic's goals , measures , and strategies for achieving performance , and revise them if appropriate .

2015 , which identified its mission , goals and strategies , and core responsibilities for achieving them .

pic staff reported that they plan to update the document for 2016 with a more robust plan .

status update 1 .

according to information provided by guidance are made , the director of omb should work with the pic to test and implement these provisions .

omb and pic staff in june 2015 , although omb revised its guidance as we recommended , it did not work with the pic to test implementation of these provisions .

instead , they told us that both pic and omb staff ensure agencies are implementing these provisions of their guidance when reviewing agencies' quarterly update submissions for apgs .

however , our analysis of agencies' updates in july 2014 found implementation of these provisions continues to be mixed .

we will continue to monitor progress .

performance.gov to include additional information about apgs , the director of omb should ensure that agencies adhere to omb's guidance for website updates by providing complete information about the organizations , program activities , regulations , tax expenditures , policies , and other activities — both within and external to the agency — that contribute to each apg .

ensure that agencies provide complete information about tax expenditures contributing to their apgs .

according to information provided by omb staff in april 2015 , agencies were asked to identify organizations , program activities , regulations , policies , tax expenditures , and other activities contributing to their 2014-2015 apgs .

this process began as part of the september 2014 update to performance.gov , with opportunities for revisions in subsequent quarterly updates .

we found that while agencies had made progress in identifying external organizations and programs for their apgs , they did not present this information consistently on performance.gov .

although each apg web page has a location where agencies are to identify contributing programs , agencies did not always identify external organizations and programs there .

instead , they identified these external contributors elsewhere , such as apg overview or strategy sections .

in june 2015 , omb staff said they would work with agency officials to help ensure information is presented in the appropriate area of performance.gov in future updates .

performance.gov to include additional information about apgs , the director of omb should ensure that agencies adhere to omb's guidance for website updates by providing a description of how input from congressional consultations was incorporated into each apg .

performance.gov for the 2014-2015 apgs generally found that either agencies did not include this information or they had not updated it to reflect the most recent round of stakeholder engagement .

in june 2015 , omb staff reported that they will focus agency attention on this issue during the development of the 2016-2017 apgs , to be published in october 2015 .

we will continue to monitor progress .

this report is part of our response to a mandate to assess the implementation of the gpra modernization act of 2010 ( gprama ) .

this report assesses the office of management and budget's ( omb ) ( 1 ) efforts to ensure the usefulness of performance.gov , and ( 2 ) strategic plan for performance.gov .

to address our objectives , we reviewed the 22 digitalgov.gov requirements for federal websites and digital services , and selected 9 for our assessment of performance.gov , as shown in table 4 .

the selected requirements are those most associated with customer feedback and outreach , usability , performance measures , and records management .

we assessed omb's efforts ( in collaboration with the performance improvement council ( pic ) and the general services administration ( gsa ) ) to ensure the usefulness of performance.gov and omb's strategic plan by comparing steps taken and documentation for each to the selected requirements .

to further address our first objective , we examined documentation on how omb was ( 1 ) seeking information from various audiences about their needs concerning performance.gov , ( 2 ) ensuring the website was clarifying ways audiences can use performance.gov , and ( 3 ) tracking a broader range of performance and customer satisfaction measures and setting goals for those measures .

we used the information to follow up on the status of the recommendations in our 2013 report .

further , we collected documentation on customer service feedback , including survey data collected from the website survey and customer service feedback logs , and analyzed the results .

we reviewed the findings of the september 2013 usability test gsa conducted .

we collected documentation on the performance measures tracked for performance.gov and compared it with the recommended measures on digitialgov.gov .

we reviewed requirements for agency performance plans established under the government performance and results act of 1993 and enhanced by gprama , and used them as a source for leading practices on setting goals for collected performance measures and customer service feedback .

we reviewed requirements outlined in gprama for performance.gov , including public reporting requirements for agency priority goals ( apgs ) , cross - agency priority ( cap ) goals , and the federal program inventory .

we reviewed other related guidance , such as omb circular no .

a - 11 , preparation , submission , and execution of the budget .

we reviewed several of our prior related reports and summarized the findings and recommendations related to omb's implementation of the selected gprama requirements .

we selected these reports because they represent our most current reports on the implementation of these selected gprama requirements .

we also interviewed staff from omb's office of performance and personnel management , the pic , and gsa to determine the actions taken in response to our recommendations about clarifying ways intended audiences can use performance.gov , systematically collecting feedback from a broader audience , and tracking recommended performance measures .

further , we communicated with omb and pic staff to determine the actions taken thus far to address our prior recommendations in relation to apgs , cap goals , and the federal program inventory .

to further address our second objective , we requested documentation on omb's strategic plan for performance.gov , social media and customer outreach strategy , and web records plan .

we found the agency has not developed these documents .

we also interviewed staff from omb , the pic , and gsa on the performance.gov website's strategic plan , social media and customer outreach strategy , web records plan , and accessibility on mobile devices .

we compared any steps taken on those actions to digitalgov.gov requirements .

we reviewed the digital services playbook , which outlines key successful practices from the private sector that would help the government build effective digital resources .

the playbook provides guidance on interacting with users through different channels .

it was used to analyze the extent to which performance.gov is accessible on mobile devices .

we reviewed performance.gov to determine if it was accessible on mobile devices by visiting each main tab of the website on multiple devices .

we also identified nara guidance on web records management .

this guidance assists agencies in how to properly manage and schedule web records .

we reviewed the research - based web design and usability guidelines for guidelines about informing website users of changes to the website .

the research - based web design and usability guidelines provide guidance on a broad range of web design and communication issues .

we also reviewed a selection of apg pages on performance.gov to document whether the page had an indication of the last time it was updated .

we concentrated on evaluating the 22 agencies listed on performance.gov , which are mostly department level agencies .

the additional 31 agencies listed on performance.gov do not have dedicated pages on the website , and therefore were not evaluated as part of this analysis .

for the 22 selected agencies , we focused on the apg's for fiscal years 2014 to 2015 because the number of apgs for fiscal years 2016 to 2017 was not yet finalized at the time of the analysis .

we randomly selected one apg from each of the 22 agency pages for review .

for each selected apg , we reviewed all of the tabs to determine if there were any date or time stamps to indicate the last time the page was updated .

we also reviewed in more detail the progress update and indicators pages to evaluate whether users could determine through page context when the information or data on a given page was last updated .

we conducted this performance audit from july 2015 to august 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , lisa pearson , assistant director ; sonya phillips , analyst - in - charge , supervised the development of this report .

caroline prado , robyn trotter , and edith yuh made significant contributions to all aspects of this report .

karin fangman , robert gebhart , kirsten lauber , and donna miller provided additional assistance .

