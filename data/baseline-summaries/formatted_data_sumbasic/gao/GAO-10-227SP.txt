i am pleased to present gao's second annual assessment of selected large - scale nasa projects .

this report provides a snapshot of how well nasa plans and executes major acquisitions — a topic that has been on gao's high - risk list since the list's inception in 1990 .

in this year's report , we found that nasa frequently exceeded its own acquisition cost and schedule estimates , even when those estimates were relatively new .

in fact , 9 out of 10 projects that have been in implementation for several years signiﬁ cantly exceeded their cost or schedule baseline estimates — all in the last 3 years .

nasa's ongoing struggle to meet budget and schedule demands comes at a time when the agency is on the verge of major changes .

the space shuttle is slated to retire this year after nearly 30 years of service , the international space station draws closer to its scheduled retirement in 2016 , and a new means of human space ﬂ ight is under development , and the very future of which has been hotly debated and recently reviewed by an independent commission , and awaits a presidential decision .

amid all this change , one thing that will remain constant is nasa's need to manage programs and projects with a budget that has remained relatively constant in recent years .

this will require hard choices among competing priorities within the organization , which must balance its core missions in science , aeronautics , and human space ﬂ ight and exploration .

in addition , nasa will be competing for an ever - shrinking share of discretionary spending against other national priorities , such as the economy , combatting terrorism , and health care reform .

we believe that this report can provide insights that will help nasa place programs in a better position to succeed and help the agency maximize its investments .

our work has shown that reducing the project challenges that can lead to cost and schedule growth this report identiﬁ es hinges on developing a sound business case that includes ﬁ rm requirements , mature technologies , a knowledge - based acquisition strategy , realistic cost estimates , and sufﬁ cient funding .

to its credit , nasa has continued to take steps to improve its acquisition process along these lines .

the revisions aim to provide key decision - makers with increased knowledge needed to make informed decisions before a program starts , and to maintain discipline once it begins .

implementation of these revisions , however , will require senior nasa leaders to have the will to terminate projects that do not measure up , to recognize and reward savings , and to hold appropriate parties accountable for poor outcomes .

the national aeronautics and space administration's ( nasa ) portfolio of major projects ranges from highly complex and sophisticated space transportation vehicles , to robotic probes , to satellites equipped with advanced sensors to study the earth .

in many cases , nasa's projects are expected to incorporate new and sophisticated technologies that must operate in harsh , distant environments .

these projects have also produced ground - breaking research and advanced our understanding of the universe .

however , one common theme binds most of the projects — they cost more and take longer to develop than planned .

we reported last year that 10 out of 13 nasa projects experienced signiﬁ cant cost and / or schedule growth from baselines established only 2 or 3 years earlier .

for example , the glory project , a science satellite designed to help understand how the sun and particles in the atmosphere affect earth's climate , saw its development costs increase more than 50 percent — from $169 to $259 million — since 2008 .

congress reauthorized the glory project in ﬁ scal year 2009 and new cost and schedule baselines were then established .

similarly , technical issues delayed the mars science laboratory by 2 years , and the project , which was already over budget , is now scheduled to cost over $660 million more than estimated in 2007 — an increase of over 68 percent in development costs .

in prior years , programs such as the x - 33 and x - 34 , which were meant to demonstrate technology for future reusable launch vehicles , were cancelled because of technical difﬁ culties and cost overruns after nasa spent more than $1 billion on the programs .

nasa acknowledges the problem and is striving to improve its cost estimating and program execution .

the agency notes that most missions are one of a kind and complex and that external factors , such as launch scheduling and spotty performance by development partners , also cause delays and cost increases .

although space development programs are complex and difﬁ cult by nature , our work consistently ﬁ nds that inherent risks are exacerbated by poor acquisition management .

moreover , the reality of cost and schedule increases can have secondary impacts when projects that are seemingly on track end up being the bill - payer for troubled projects .

this also makes it hard to manage the portfolio and make investment decisions .

congress has expressed concern about nasa's performance and has identiﬁ ed the need to standardize the reporting of cost , schedule , and content for nasa research and development projects .

in 2005 , congress required nasa to report cost and schedule baselines — benchmarks against which changes can be measured — for all nasa programs and projects with estimated life - cycle costs of at least $250 million that have been approved to proceed to the development stage , known as implementation , in which components begin to take physical form .

it also required that nasa report to congress when development cost is likely to exceed the baseline estimate by 15 percent or more , or when a milestone is likely to be delayed beyond the baseline estimate by 6 months or more .

in response , nasa began establishing cost and schedule baselines in 2006 and has been using them as the basis for annual project performance reports to the congress provided in its annual budget submission each year .

while establishing the baselines required by the congress enabled a more consistent reporting among nasa projects , it also made past cost and schedule growth less transparent .

consequently , the cost and schedule breaches presented in this report represent only increases from the baselines established after the 2005 congressional requirement .

recently , nasa was appropriated over $1 billion through the american recovery and reinvestment act of 2009 to help spur technological advances in science .

the agency's science and exploration systems mission directorates were each appropriated $400 million under this supplemental appropriation .

as of october 2009 , the projects covered in this assessment are scheduled to receive $470 million as a part of the total allocation that nasa intends to use to assist with such items as developing instruments and spacecraft , maintaining the current workforce , and building test facilities .

appendix v provides a listing of nasa projects in our review receiving funding under the american recovery and reinvestment act of 2009 and the intended use of those funds for each project .

the explanatory statement of the house committee on appropriations accompanying the fiscal year 2009 omnibus appropriations act directed gao to prepare project status reports on selected large - scale nasa programs , projects , or activities .

this report responds to that mandate by assessing 19 nasa projects , each with an estimated life - cycle cost over $250 million .

the combined estimated life - cycle cost for these 19 projects exceeds $66 billion .

each assessment is presented in a two - page summary that analyzes the project's cost and schedule status and project challenges we identiﬁ ed with the objective to identify risks that , if mitigated , could put nasa in a better position to succeed .

we also provide general observations about the performance of nasa's major projects and the agency's management of those projects during development .

in doing so , the report expands on the importance of developing a knowledge - based acquisition strategy and to provide decision - makers with an independent , knowledge - based assessment of individual systems that identiﬁ es potential risks and allows the decision - makers to take actions to put projects that are early in the development cycle in a better position to succeed .

nasa provided updated cost and schedule data as of october 2009 for 14 of the 19 projects .

we reviewed and compared that data to previously established cost and schedule baselines for each of those 14 projects .

we took appropriate steps to address data reliability .

our approach included an examination of the current phase of a project's development and how each project was advancing .

each project we reviewed was in either the formulation phase or the implementation phase of the project life cycle .

in the formulation phase , the project deﬁ nes requirements — what the project is being designed to do — matures technology , establishes a schedule , estimates costs , and produces a plan for implementation .

in the implementation phase , the project carries out these plans , performing ﬁ nal design and fabrication as well as testing components and system assembly , integrating these components and testing how they work together , and launching the project .

this phase also includes the period from project launch through mission completion .

we assessed each project's cost and schedule and characterized growth in either as signiﬁ cant if it exceeded the baselines that trigger reporting to the congress under the law .

based on our previous reviews and discussions with project ofﬁ cials and drawing on gao's established criteria for knowledge - based acquisitions and on other gao work on system acquisitions , we identiﬁ ed six challenges that can contribute to cost and schedule growth in these projects: technology maturity , design stability , contractor performance , development partner performance , funding issues , and launch manifest issues .

this list of challenges is not exhaustive , and we believe these challenges will evolve as we continue this work into the future .

to assess technology maturity , we examined the projects' reported critical technology readiness levels — a measure that nasa devised and that is now used at other agencies as well .

we looked at the technology readiness level at the time of the project's preliminary design review , which occurs just before it enters the implementation phase , and compared that against the level of maturity that best practices call for at that stage to minimize risks .

based in part on our discussions with ofﬁ cials for the individual projects and data submitted by the projects , we identiﬁ ed the extent to which project cost and schedule were negatively impacted by challenges integrating heritage — or pre - existing — technology into their projects .

to assess design stability , we examined the percentage of engineering drawings completed or projected to be completed by the critical design review — which is usually held about midway through the project's development .

we asked project ofﬁ cials to provide this information , and we compared it against gao's best practices' metric of 90 percent of drawings released by the critical design review .

we also discussed the extent to which contractors' and development partners' challenges in developing and delivering project hardware affected overall project cost and schedule .

to assess funding issues , we interviewed project ofﬁ cials and reviewed budget documents to determine if increases to cost or schedule resulted from interrupted or delayed funding , or if project ofﬁ cials indicated that the project had poor phasing of the project's funding plan .

to assess launch manifest issues , we interviewed launch services ofﬁ cials to determine what projects had to reschedule launch dates based on an inability to be ready for launch or other factors .

the individual project ofﬁ ces were given an opportunity to provide comments and technical clariﬁ cations on our assessments prior to their inclusion in the ﬁ nal product .

we conducted this performance audit from april 2009 to february 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufﬁ cient , appropriate evidence to provide a reasonable basis for our ﬁ ndings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our ﬁ ndings and conclusions based on our audit objectives .

appendix ii contains detailed information on our scope and methodology .

we do not provide recommendations in this report .

many of nasa's projects are one - time articles , meaning that there is little opportunity to apply knowledge gained to the production of a second , third , or future increments of spacecraft .

in addition , nasa often partners with other domestic partners and other space - faring countries , including several european nations , japan , and argentina .

these partnerships go a long way to foster international cooperation in space , but they also subject nasa projects to added risk such as when partners do not meet their obligations or run into technical obstacles they cannot easily overcome .

while space development programs are complex and difﬁ cult by nature , and most are one - time efforts , the nature of its work should not preclude nasa from achieving what it promises when requesting and receiving funds .

we have reported that nasa would beneﬁ t from a more disciplined approach to its acquisitions .

the development and execution of a knowledge - based business case for these projects can provide early recognition of challenges , allow managers to take corrective action , and place needed and justiﬁ able projects in a better position to succeed .

our studies of best practice organizations show the risks inherent in nasa's work can be mitigated by developing a solid , executable business case before committing resources to a new product development .

in its simplest form , this is evidence that ( 1 ) the customer's needs are valid and can best be met with the chosen concept , and ( 2 ) the chosen concept can be developed and produced within existing resources — that is , proven technologies , design knowledge , adequate funding , and adequate time to deliver the product when needed .

a program should not go forward into product development unless a sound business case can be made .

if the business case measures up , the organization commits to the development of the product , including making the ﬁ nancial investment .

our best practice work has shown that developing business cases based on matching requirements to resources before program start leads to more predictable program outcomes — that is , programs are more likely to be successfully completed within cost and schedule estimates and deliver anticipated system performance .

at the heart of a business case is a knowledge - based approach to product development that is a best practice among leading commercial ﬁ rms .

those ﬁ rms have created an environment and adopted practices that put their program managers in a good position to succeed in meeting expectations .

a knowledge - based approach requires that managers demonstrate high levels of knowledge as the program proceeds from technology development to system development and , ﬁ nally , production .

in essence , knowledge supplants risk over time .

this building of knowledge can be described over the course of a program , as follows: when a project begins development , the customer's needs should match the developer's available resources — mature technologies , time , and funding .

an indication of this match is the demonstrated maturity of the technologies needed to meet customer needs — referred to as critical technologies .

if the project is relying on heritage — or pre - existing — technology , that technology must be in appropriate form , ﬁ t , and function to address the customer's needs within available resources .

the project will normally enter development after completing the preliminary design review , at which time a business case should be in hand .

then , about midway through the product's development , its design should be stable and demonstrate it is capable of meeting performance requirements .

the critical design review takes place at that point in time because it generally signiﬁ es when the program is ready to start building production - representative prototypes .

if design stability is not achieved , but a product development continues , costly re - designs to address changes to project requirements and unforeseen challenges can occur .

by the critical design review , design should be stable and capable of meeting performance requirements .

finally , by the time of the production decision , the product must be shown to be producible within cost , schedule , and quality targets and have demonstrated its reliability , and the design must demonstrate that it performs as needed through realistic system - level testing .

lack of testing increases the possibility that project managers will not have information that could help avoid costly system failures in late stages of development or during system operations .

our best practices work has identiﬁ ed numerous other actions that can be taken to increase the likelihood that a program can be successfully executed once that business case is established .

these include ensuring cost estimates are complete , accurate , and updated regularly and holding suppliers accountable through such activities as regular supplier audits and performance evaluations of quality and delivery .

moreover , we have recommended using metrics and controls throughout the life cycle to gauge when the requisite level of knowledge has been attained and when to direct decision makers to consider criteria before advancing a program to the next level and making additional investments .

the consequence of proceeding with system development without establishing and adhering to a sound business case is substantial .

gao and others have reported that nasa has experienced cost and schedule growth in several of its projects over the past decade , resulting from problems that include failing to adequately identify requirements and underestimating complexity and technology maturity .

we have found that the need to meet schedule is one of the main reasons why programs cannot execute as planned .

short cuts , such as developing technology while design work and construction are already underway , and delaying or reducing tests , are taken to meet schedule .

ultimately , when a schedule is set that cannot accommodate the work that needs to be done , costs go up and capability is delayed .

delaying the delivery of these capabilities can also have a ripple effect throughout nasa projects as staff must then stay on a given project longer than intended , thus increasing the project's costs , and crippling other projects that had counted on using newly available staff to move forward .

in 2005 , we reported that nasa's acquisition policies did not conform to best practices for product development because those policies lacked major decision reviews at several key points in the project life - cycle that would allow decision makers to make informed decisions about whether a project should be authorized to proceed in the development life cycle .

based in part on our recommendations , nasa issued a revised policy in march 2007 that institutes several key decision points ( kdp ) in the development life cycle for space ﬂ ight programs and projects .

at each kdp , a decision authority is responsible for authorizing the transition to the next life - cycle phase for the project .

in addition , nasa's acquisition policies also require that technologies be sufﬁ ciently mature at the preliminary design review before the project enters implementation , that the design is appropriate to support proceeding with full - scale fabrication , assembly , integrating and test at the critical design review , and that the system can be fabricated within cost , schedule , and performance speciﬁ cations .

these changes brought the policy more in line with best practices for product development .

a more detailed discussion of nasa's acquisition policy and how it relates to best practices is provided in appendix iii of this report .

further , in response to gao's designation of nasa acquisition management as a high risk area , nasa developed a corrective action plan to improve the effectiveness of nasa's program / project management .

the approach focuses on how best to ensure the mitigation of potential issues in acquisition decisions and better monitor contractor performance .

the plan identiﬁ es ﬁ ve areas for improvement — program / project management , cost reporting processes , cost estimating and analysis , standard business processes , and management of ﬁ nancial management systems — each of which contains targets and goals to measure improvement .

as part of this initiative , nasa has taken a positive step to improve management oversight of project cost , schedule , and technical performance with the establishment of a baseline performance review reporting to nasa's senior management .

through monthly reviews , nasa intends to highlight projects that are predicted to exceed internal nasa cost and / or schedule baselines , which are set lower than cost and schedule baselines submitted to congress , so the agency can take preemptive actions to minimize the projects' potential cost overruns or schedule delays .

during our data collection efforts , we reviewed several projects' monthly and quarterly status reports , which gave us insight into their status , risks , and issues .

while this reporting structure might enable management to be aware of the issues projects are facing , it is too early to tell if the monthly reviews are having the intended impact of enabling nasa management to take preemptive cost saving actions , such as delaying a design review or canceling a project .

as a part of the continuing effort to improve its acquisition processes , nasa has begun a new initiative — joint cost and schedule conﬁ dence levels ( jcl ) — to help programs and projects with management , cost and schedule estimating , and maintenance of adequate levels of reserves .

under this new policy , cost , schedule , and risk are combined into a complete picture to help inform management of the likelihood of a project's success .

utilizing jcl , each project will receive a cost estimate with a corresponding conﬁ dence level — the percentage probability representing the likelihood of success at the speciﬁ ed funding level .

nasa believes the application of this policy will help reduce the cost and schedule growth in its portfolio and improve transparency , and increase the probabilities of meeting those expectations .

nasa's goal is for all projects that have entered the implementation phase to have a jcl established by spring 2010 .

while these efforts are positive steps , it is too early to assess their impact and they will be limited if project ofﬁ cials are not held accountable for demonstrating that elements of a knowledge - based business case are demonstrated at key junctures in development .

for projects to have better outcomes not only must they demonstrate a high level of knowledge at key junctures , but decision makers must also use this information to determine whether and how best a project should proceed through the development life cycle .

if done successfully , these measures should enable nasa to foster the expansion of a business - oriented culture , reduce persistent cost growth and schedule delays , and maximize investment dollars .

we assessed 19 large - scale nasa projects in this review .

four of these projects were in the formulation phase where cost and schedule baselines have yet to be established , while 15 had entered implementation .

nine of the 15 projects experienced signiﬁ cant cost and / or schedule growth from their project baselines , while ﬁ ve of the remaining projects had just entered implementation and their cost and schedule baselines were established in ﬁ scal year 2009 .

nasa provided cost and schedule data for 14 of the 15 projects in the implementation phase of the project life cycle .

despite being in implementation , nasa did not provide cost or schedule data for the magnetospheric multiscale ( mms ) project .

nasa will not formally release its baseline cost and schedule estimates for this project until the ﬁ scal year 2011 budget submission to congress , and late in our review process agency ofﬁ cials notiﬁ ed us that they will not provide project estimates to gao until that time .

nasa also did not provide formal cost and schedule information for the projects in formulation , citing that those estimates were still preliminary .

see ﬁ gure 1 for a summary of these projects .

based on our analysis , development costs for projects in our review increased by an average of over 13 percent from their baseline cost estimates — including one project that increased by over 68 percent — and an average delay of almost 11 months to their launch dates .

these averages were signiﬁ cantly higher when the four projects that just entered implementation are excluded .

speciﬁ cally , there are 10 projects of analytical interest because ( 1 ) they are in the implementation phase , and ( 2 ) their baselines are old enough to begin to track variances .

most of these 10 projects have experienced signiﬁ cant cost and / or schedule growth , often both .

these projects had an average development cost growth of 18.7 percent — or almost $121.1 million — and schedule growth of over 15 months , and a total increase in development cost of over $1.2 billion .

over half of this total increase in development cost — or $706.6 million — occurred in the last year .

these cost growth and schedule delays have all occurred within the last 3 years , and a number of these projects had experienced considerable cost growth before baselines were established in response to the 2005 statutory reporting requirement .

see table 1 below for the cost and schedule growth of the nasa projects in the implementation phase .

despite having baselines established in ﬁ scal year 2008 , two projects have sought reauthorization from congress because of development cost growth in excess of 30 percent .

congress reauthorized the glory project in ﬁ scal year 2009 , and new cost and schedule baselines were established after the project experienced a 53 percent cost growth and 6-month launch delay from original baseline estimates .

the glory project has since breached its revised schedule baseline by 16 months and exceeded its development cost baseline by over 14 percent — for a total development cost growth of over 75 percent in just 2 years .

project ofﬁ cials also indicated that recent technical problems could cause additional cost growth .

similarly , the mars science laboratory project is currently seeking reauthorization from congress after experiencing development cost in excess of 30 percent .

all six factors we assessed can lead to project cost and schedule growth: technology maturity , design stability , contractor performance , development partner performance , funding issues , and launch manifest issues .

these factors — characterized as project challenges — were evident in the projects that had reached the implementation phase of the project life cycle , but many of them began in the formulation phase .

we did not speciﬁ cally correlate individual project challenges with speciﬁ c cost and / or schedule changes in each project .

the degree to which each speciﬁ c challenge contributed to cost and schedule growth varied across the projects in this review and we did not assign any speciﬁ c challenge as a primary factor for cost and / or schedule growth .

table 2 depicts the extent to which each of the six challenges occurred for each of the 19 projects we reviewed .

technology maturity was by far the most prevalent challenge , affecting 15 of the 19 projects .

when combined with design instability — another metric related to technical difﬁ culty — 17 projects were affected .

a discussion of each challenge follows .

our past work on systems acquisition has shown that beginning an acquisition program before requirements and available resources are matched can result in a product that fails to perform as expected , costs more , or takes longer to develop .

we have found that these problems are largely rooted in the failure to match customer's needs with the developer's resources — technical knowledge , timing , and funding — when starting product development .

in other words , commitments were made to deliver capability without knowing whether the technologies needed could really work as intended .

time and costs were consistently underestimated , and problems that surfaced early cascaded throughout development and magniﬁ ed the risks facing the program .

our best practices work has shown that a technology readiness level ( trl ) of 6 — demonstrating a technology as a fully integrated prototype in a relevant environment — is the level of maturity needed to minimize risks for space systems entering product development .

nasa's acquisition policy states that a trl of 6 is desirable prior to integrating a new technology .

technology maturity is a fundamental element of a sound business case , and the absence is a marker for subsequent problems , especially in design .

similarly , our work has shown that the use of heritage technology — proven components that are being modiﬁ ed to meet new requirements — can also cause problems when the items are not sufﬁ ciently matured to meet form , ﬁ t , and function standards by the preliminary design review ( pdr ) .

nasa states in its systems engineering handbook that particular attention must be given to heritage systems because they are often used in architectures and environments different from those in which they were designed to operate .

although nasa distinguishes critical technologies from heritage technologies , our best practices work has found critical technologies to be those that are required for the project to successfully meet customer requirements , regardless of whether or not they are based on existing or heritage technology .

therefore , whether technologies are labeled as “critical” or “heritage,” if they are important to the development of the spacecraft or instrument — enabling it to move forward in the development process — they should be matured by pdr .

of the 14 projects for which we received data and that had entered the implementation phase , four entered this phase without ﬁ rst maturing all their critical technologies , and 10 encountered challenges in integrating or modifying heritage technologies .

additionally , two projects in formulation — ares i and orion — also encountered challenges with critical or heritage technologies .

these projects did not build in the necessary resources for technology modiﬁ cation .

for instance , the recent cost and schedule growth in the mars science laboratory ( msl ) highlights the problems that can be realized when a project proceeds past the formulation phase with immature technologies .

msl reported seven critical technologies were not mature at the time of its preliminary design review , and over a year later two of these technologies were still immature at the critical design review ; however , the project moved forward into the implementation phase with established cost and schedule baselines and the lack of technology maturity contributed to an unstable design .

in part as a result of immature technologies and an unstable design , msl delayed its launch date by 25 months , and development costs have grown by more than $660 million .

in november 2008 , the grail project also moved beyond its pdr with an immature heritage technology — the reaction wheel assembly .

this technology has been ﬂ own on other nasa missions , but the project team must modify it for grail by integrating electronics into the assembly .

nasa acknowledges in its systems engineering handbook that modiﬁ cation of heritage systems is a frequently overlooked area in technology development and that there is a tendency on the part of project management to overestimate the maturity and applicability of heritage technology .

nasa recognizes that as a result of not placing enough emphasis on the development of heritage technologies , key steps in the development process are not given appropriate attention , and critical aspects of systems engineering are overlooked .

the importance of establishing a stable design at a project's critical design review ( cdr ) is also critical .

the cdr provides assurance that the design is mature and will meet performance requirements .

an unstable design can result in costly re - engineering and re - work efforts , design changes , and schedule slippage .

quantitative measures employed at cdr , such as percentage of engineering drawings , can provide evidence that the design is stable and “freeze” it to minimize changes in the future .

our work has shown that release of at least 90 percent of engineering drawings at the cdr provides evidence that the design is stable .

though nasa's acquisition policy does not specify how a project should achieve design stability by cdr , nasa's systems engineering handbook adheres to this metric of 90 percent drawings released by the cdr .

eight projects in our assessment have already held their cdr and were able to provide us with the number of engineering drawings completed and released .

none of these 8 projects met the 90 percent standard for design stability at cdr ; however , nasa believes that some of these projects had stable designs and pointed to other activities that occurred prior to cdr as evidence .

nevertheless , the percentage of engineering drawings released at cdr by these 8 projects averaged less than 40 percent , and more than three - fourths of these projects had signiﬁ cant cost and / or schedule growth from their established baselines after their cdr when their design was supposed to be stable .

although all the cost and schedule growth for these projects cannot be directly attributed to a lack of design stability , we believe that this was a contributing factor .

discussions with project ofﬁ cials showed the metric was used inconsistently to gauge design stability .

for example , goddard space flight center requires greater than 80 percent drawings released at cdr , yet we were told by several project ofﬁ cials that the rule of thumb for nasa projects is between 70 and 90 percent drawings released at cdr .

however , there was no consensus among the ofﬁ cials .

for example , one project manager from goddard space flight center told us the project is planning to have 70 percent of the drawings released at cdr ; a project manager from the jet propulsion laboratory cited that having 85 to 90 percent of the drawings released is what he prefers at cdr ; otherwise , he does not consider the project design to be complete .

goddard's chief engineer said that , as a member of a design review board , he will generally question projects that have less than 95 percent of engineering drawings released , especially if the project is using heritage technologies .

ofﬁ cials added that at cdr it is more important to have drawings completed that relate to critical technologies than those related to integration activities .

in addition to released drawings , nasa often relies on subject matter experts in the design review process and other methods to assure that a project has a stable design .

some projects indicated that completing engineering models , which are preproduction prototypes , and holding sub - system level cdr's for instruments and components helped to assess design stability , at least in part .

ofﬁ cials for these projects indicated that use of engineering models helps decrease risk of ﬂ ight unit development ; projects that did not use engineering models indicated they might have caught problems earlier had they used them .

for example , at cdr the mars science laboratory's engineering models were incomplete and could have been a cause of concern .

mars science laboratory project ofﬁ cials were aware that avionics were an issue at cdr , but were unaware of future problems with other project components , such as the actuators .

project ofﬁ cials told us that if the engineering models for all subsystems had been completed at cdr , many of the later problems would have been caught and mitigated earlier in the process , thereby avoiding schedule delays .

however , these project ofﬁ cials added that engineering models are expensive to employ and not all projects have the available funding required to utilize them .

nasa relies heavily on the work of its contractors .

ofﬁ cials at ﬁ ve of the projects we reviewed indicated that the contractors for their projects had trouble moving their work forward after experiencing technical and design problems with hardware that disrupted development progress .

since about 85 percent of nasa's annual budget is spent by its contractors , the performance of these contractors is instrumental to the success of the projects .

shifts in the industrial base and a lack of expertise at the contractors affected performance .

for example , project ofﬁ cials for the sofia project reported that the contractor for the aircraft modiﬁ cation was bought and sold several times during the development process .

project ofﬁ cials further reported that the contractor had limited experience with this type of work and did not fully understand the statement of work .

consequently , the contractor had difﬁ culty completing this work , which led to signiﬁ cant cost overruns .

while project ofﬁ cials told us that issues with that contractor have since been resolved , this year another sofia contractor that is responsible for developing hardware and software has performed poorly , which ofﬁ cials attribute to a recent buyout of the company .

in addition , agency ofﬁ cials said that nasa is a low priority for the contractor , and the project is ﬁ nding it difﬁ cult to exert pressure to ensure better performance .

project ofﬁ cials told us that they currently have three people at the contractor's site as a permanent presence .

they added that if the contract were to be cancelled due to poor performance , this work would be brought in - house and would result in a one year delay .

in addition , the glory project has struggled for several years to develop a key instrument .

the glory project manager cited management inefﬁ ciencies with the instrument's contractor including senior leadership changes , a loss of core competencies because of a plant closure , and a lack of proper decision authority .

the contractor agreed that the plant closure and the need to re - staff were major project challenges .

six projects in our review encountered challenges with their development partners .

in these cases the development partner could not meet their commitments to the project within the planned schedules .

for example , nasa collaborated with the european space agency ( esa ) on the herschel space observatory .

nasa delivered its two instruments to esa in a timely manner , but esa encountered difﬁ culties developing its instruments , and the result was a 14-month delay in herschel's schedule .

because of this delay , nasa incurred an estimated $39 million in cost growth because of the need to fund component developers for a longer period of time than originally planned .

we found that of the projects that are currently in implementation and have experienced cost and / or schedule growth , those with international or domestic partners experienced more than one - and - a - half times as much schedule growth on average as those with no partner .

table 3 below shows the average schedule growth for projects with partners as compared to those without partners .

during the course of our review , we identiﬁ ed six projects in the implementation phase , as well as three projects still in formulation , that had experienced issues related to the project's funding because of issues such as agency - directed funding cuts early in the project life - cycle and projects whose budgets do not match the work expected to be accomplished .

for example , nasa management cut $35 million from the kepler project's ﬁ scal year 2005 budget — a cut amounting to one - half of the project's budget for the year .

contractor ofﬁ cials told us that this forced the shutdown of signiﬁ cant work , interrupted the overall ﬂ ow and scheduling for staff and production , and required a renegotiation of contracts .

this funding instability , according to a nasa project ofﬁ cial , contributed to an overall 20-month delay in the project's schedule and about $169 million in cost growth .

the funding instability for kepler affected more than that one project .

the wise project had to extend the formulation phase since funding was unavailable at the time of the conﬁ rmation review in november 2005 .

according to nasa and contractor ofﬁ cials , the wise project experienced funding cuts when nasa took money from that project to offset increased costs for the kepler project .

as a result of the extended formulation phase , the wise project manager told us that development costs increased and the launch readiness date slipped 11 months .

this is an example of how , when problems arise , one project can become the bill - payer for another project , making it difﬁ cult to manage the portfolio and make investment decisions .

we also identiﬁ ed several projects where , according to nasa ofﬁ cials , the projected budget was inadequate to perform work in certain ﬁ scal years .

for example , the constellation program's poorly phased funding plan has diminished both the ares i and orion projects' ability to deal with technical challenges .

nasa initiated the constellation program relying on the accumulation of a large rolling budget reserve in ﬁ scal years 2006 and 2007 to fund program activities in ﬁ scal years 2008 through 2010 .

thereafter , nasa anticipated that the retirement of the space shuttle program in 2010 would free funding for the constellation program .

the program's risk management system identiﬁ ed this strategy as high risk , warning that shortfalls could occur in ﬁ scal years 2009 through 2012 .

according to the constellation program manager , the program's current funding shortfalls have reduced the ﬂ exibility to resolve technical challenges .

in addition , the james webb space telescope project had to delay its scheduled launch date by one year in part because of poor phasing of the project's funding plan .

we identiﬁ ed four projects in our assessment that are experiencing launch delays or other launch manifest - related challenges .

by their nature , launch delays can contribute signiﬁ cantly to cost and schedule growth , as months of delay can translate into millions of dollars in cost increases .

for example , the solar dynamics observatory ( sdo ) project missed its scheduled launch date in august 2008 because of test scheduling and spacecraft parts problems .

this delay resulted in the sdo project moving to the end of the manifest for the atlas v launch vehicles on the east coast , causing an 18-month launch delay and $50 million cost increase .

while the primary reason for the cost growth is that the sdo project could not meet its original schedule for launch , the project is incurring additional costs to maintain project staff longer than originally planned as they await their turn in the launch queue .

according to sdo ofﬁ cials , this has also affected stafﬁ ng at goddard space flight center since these personnel were scheduled to move to other projects .

furthermore , launch delays of one project can potentially impact the launch manifest for other projects .

the 25-month delay of the mars science laboratory project has the potential to cause disruptions for other projects on the launch manifest in late 2011 , including those outside of nasa , since planetary missions — those missions that must launch in a certain window because of planetary alignments — receive launch priority to take advantage of optimal launch windows .

some nasa projects are also experiencing launch manifest - related challenges .

for example , the gravity recovery and interior laboratory project is monitoring the availability of trained launch personnel as that mission is the last to launch on the delta ii vehicle .

united launch alliance ofﬁ cials told us that they are taking active steps , such as cross - utilizing the delta ii personnel with other launch vehicles , to ensure that trained launch personnel are available for all the remaining delta ii launches .

in addition , the recent failure of the taurus xl launch vehicle during the launch of the orbiting carbon observatory has the potential to delay the glory mission if the taurus xl is not cleared for use before glory has corrected its technical problems .

the 2-page assessments of the projects we reviewed provide a proﬁ le of each project and describe the challenges we identiﬁ ed .

on the ﬁ rst page , the project proﬁ le presents a general description of the mission objectives for each of the projects ; a picture of the spacecraft or aircraft ; a schedule timeline identifying key dates for the project ; a table identifying programmatic and launch information ; and a table showing the baseline year cost and schedule estimates and the most current available cost and schedule data ; a table showing the challenges relevant to the project ; and a project status narrative .

on the second page of the assessment , we provide an analysis of the project challenges and the extent to which each project faces cost , schedule , or performance risk because of these challenges .

in addition , nasa project ofﬁ ces were provided an opportunity to review drafts of the assessments prior to their inclusion in the ﬁ nal product , and the projects provided both technical corrections and more general comments .

we integrated the technical corrections as appropriate and characterized the general comments below the detailed project discussion .

see ﬁ gure 2 below for an illustration of the layout of each two - page assessment .

we provided a draft of this report to nasa for review and comment .

in its written response , nasa agrees with our ﬁ ndings and states that it will strive to address the challenges that lead to cost and schedule growth in its projects .

nasa agrees that gao's cost and schedule growth ﬁ gures reﬂ ect what the agency has experienced since the baselines were established in response to the 2005 statutory reporting requirements .

importantly , nasa has begun to provide more data regarding cost growth prior to these baselines , and we look forward to working with nasa to increase transparency into cost and schedule information of large - scale projects even further in the future .

nasa noted that its projects are high - risk and one - of - a - kind development efforts that do not lend themselves to all the practices of a “business case” approach that we outlined since essential attributes of nasa's project development differ from those of a commercial or production industry .

we agree , however nasa could still beneﬁ t from a more disciplined approach to its acquisitions whereby decisions are based upon high levels of knowledge .

currently , inherent risks are being exacerbated due to projects moving forward with immature technologies and unstable designs and difﬁ culties working with contractors and international partners , leading to cost and schedule increase which make it hard for the agency to manage its portfolio and make informed investment decisions .

nasa's comments are reprinted in appendix i. nasa also provided technical comments , which we addressed throughout the report as appropriate and where sufﬁ cient evidence was provided to support signiﬁ cant changes .

we will send copies of the report to nasa's administrator and interested congressional committees .

we will also make copies available to others upon request .

in addition , the report will be available at no charge on gao's web site at http: / / www.gao.gov .

should you or your staff have any questions on matters discussed in this report , please contact me at ( 202 ) 512-4841 or chaplainc@gao.gov .

contact points for our ofﬁ ces of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vi .

our objectives were to report on the status and challenges faced by nasa systems with life - cycle costs of $250 million or more and to discuss broader trends faced by the agency in its management of system acquisitions .

in conducting our work , we evaluated performance and identiﬁ ed challenges for each of 19 major projects .

we summarized our assessments of each individual project in two components — a project proﬁ le and a detailed discussion of project challenges .

we did not validate the data provided by the national aeronautics and space administration ( nasa ) .

however , we took appropriate steps to address data reliability .

speciﬁ cally , we conﬁ rmed the accuracy of nasa - generated data with multiple sources within nasa and , in some cases , with external sources .

additionally , we corroborated data provided to us with published documentation .

we determined that the data provided by nasa project ofﬁ ces were sufﬁ ciently reliable for our engagement purposes .

we developed a standardized data collection instrument ( dci ) that was completed by each project ofﬁ ce .

through the dci , we gathered basic information about projects as well as current and projected development activities for those projects .

the cost and schedule data estimates that nasa provided were the most recent updates as of october 2009 ; performance data that nasa provided were the most recent updates as of november 2009 .

at the time we collected the data , 4 of the 19 projects were in the formulation phase and 15 were in the implementation phase .

nasa only provided cost and schedule data for 14 projects in implementation .

despite being in the implementation phase , nasa did not provide cost or schedule data for the magnetospheric multiscale ( mms ) project .

to further understand performance issues , we talked with ofﬁ cials from most project ofﬁ ces and nasa's ofﬁ ce of program analysis and evaluation ( pa&e ) .

the results collected from each project ofﬁ ce , mission directorate , and pa&e were summarized in a 2-page report format providing a project overview ; key cost , contract , and schedule data ; and a discussion of the challenges associated with the deviation from relevant indicators from best practice standards .

the aggregate measures and averages calculated were analyzed for meaningful relationships , e.g .

relationship between cost growth and schedule slippage and knowledge maturity attained both at critical milestones and through the various stages of the project life cycle .

we identiﬁ ed cost and / or schedule growth as signiﬁ cant where , in either case , a project's cost and / or its schedule exceeded the baselines that trigger reporting to the congress .

to supplement our analysis , we relied on gao's work over the past years examining acquisition issues across multiple agencies .

these reports cover such issues as contracting , program management , acquisition policy , and estimating cost .

gao also has an extensive body of work related to challenges nasa has faced with speciﬁ c system acquisitions , ﬁ nancial management , and cost estimating .

this work provided the context and basis for large parts of the general observations we made about the projects we reviewed .

additionally , the discussions with the individual nasa projects helped us identify further challenges faced by the projects .

together , the past work and additional discussions contributed to our development of a short list of challenges discussed for each project .

the challenges we identiﬁ ed and discussed do not represent an exhaustive or exclusive list .

they are subject to change and evolution as gao continues this annual assessment in future years .

our work was performed primarily at nasa headquarters in washington , d.c .

in addition , we visited nasa's marshall space flight center in huntsville , alabama ; dryden flight research center at edwards air force base in california ; and goddard space flight center in greenbelt , maryland , to discuss individual projects .

we also met with representatives from nasa's jet propulsion lab in pasadena , california and a provider of nasa launch services , the united launch alliance .

nasa only provided speciﬁ c cost and schedule estimates for 14 of the 19 projects in our review .

for one project , the magnetospheric multiscale project , nasa will not formally release its baseline cost and schedule estimates until the ﬁ scal year 2011 budget submission to congress , and late in our review process agency ofﬁ cials notiﬁ ed us that they will not provide project estimates to gao until that time .

for three of the projects that had not yet entered implementation , nasa provided internal preliminary estimated total ( life - cycle ) cost ranges and associated schedules , from key decision point b ( kdp - b ) , solely for informational purposes .

nasa formally establishes cost and schedule baselines , committing itself to cost and schedule targets for a project with a speciﬁ c and aligned set of planned mission objectives at key decision point c ( kdp - c ) , which follows a non - advocate review ( nar ) and preliminary design review ( pdr ) .

kdp - c reﬂ ects the life - cycle point where nasa approves a project to leave the formulation phase and enter into the implementation phase .

nasa explained that preliminary estimates are generated for internal planning and ﬁ scal year budgeting purposes at kdp - b , which occurs mid - stream in the formulation phase , and hence , are not considered a formal commitment by the agency on cost and schedule for the mission deliverables .

nasa ofﬁ cials contend that because of changes that occur to a project's scope and technologies between kdp - b and kdp - c , estimates of project cost and schedule can change signiﬁ cantly heading toward kdp - c .

finally , nasa did not provide data for the global precipitation measurement mission because nasa ofﬁ cials said it did not have a requirement for a kdp - b review , because it was authorized to be formulated prior to the requirements of npr 7120.5d being in place .

this section of the 2-page assessment outlines the essentials of the project , its cost and schedule performance , and its status .

project essentials reﬂ ect pertinent information about each project , including , where applicable , the major contractors and partners involved in the project .

these organizations have primary responsibility over a major segment of the project or , in some cases , the entire project .

project performance is depicted according to cost and schedule changes in the various stages of the project life cycle .

to assess the cost and schedule changes of each project we obtained data directly from nasa pa&e and from nasa's integrated budget and performance documents .

for systems in implementation , we compared the latest available information with baseline cost and schedule estimates set for each project in the ﬁ scal year 2007 , 2008 , or 2010 budget request .

all cost information is presented in nominal “then year” dollars for consistency with budget data .

baseline costs are adjusted to reﬂ ect the cost accounting structure in nasa's ﬁ scal year 2009 budget estimates .

for the ﬁ scal year 2009 budget request , nasa changed its accounting practices from full - cost accounting to reporting only direct costs at the project level .

the schedule assessment is based on acquisition cycle time , which is deﬁ ned as the number of months between the project start , or formulation start , and projected or actual launch date .

formulation start generally refers to the initiation of a project ; nasa refers to project start as key decision point a , or the beginning of the formulation phase .

the preliminary design review typically occurs during the end of the formulation phase , followed by a conﬁ rmation review , referred to as key decision point c , which allows the project to move into the implementation phase .

the critical design review is held during the ﬁ nal design period of implementation and demonstrates that the maturity of the design is appropriate to support proceeding with full scale fabrication , assembly , integration , and test .

launch readiness is determined through a launch readiness review that veriﬁ es that the launch system and spacecraft / payloads are ready for launch .

the implementation phase includes the operations of the mission and concludes with project disposal .

we assessed the extent to which nasa projects exceeded their cost and schedule baselines .

to do this , we compared the project baseline cost and schedule estimates with the current cost and schedule data reported by the project ofﬁ ce in october 2009 .

to assess the project challenges for each project , we submitted a data collection instrument to each project ofﬁ ce .

we also held interviews with representatives from most of the projects to discuss the information on the data collection instrument .

these discussions led to identiﬁ cation of further challenges faced by nasa projects .

these challenges were largely apparent in the projects that had entered the implementation phase .

we then reviewed pertinent project documentation , such as the project plan , schedule , risk assessments , and major project reviews .

to assess technology maturity , we asked project ofﬁ cials to assess the technology readiness levels ( trl ) of each of the project's critical technologies at various stages of project development .

originally developed by nasa , trls are measured on a scale of one to nine , beginning with paper studies of a technology's feasibility and culminating with a technology fully integrated into a completed product .

 ( see appendix iv for the deﬁ nitions of technology readiness levels. ) .

in most cases , we did not validate the project ofﬁ ces' selection of critical technologies or the determination of the demonstrated level of maturity .

however , we sought to clarify the technology readiness levels in those cases where the information provided raised concerns , such as where a critical technology was reported as immature late in the project development cycle .

additionally , we asked project ofﬁ cials to explain the environments in which technologies were tested .

our best practices work has shown that a technology readiness level of 6 — demonstrating a technology as a fully integrated prototype in a relevant environment — is the level of maturity needed to minimize risks for space systems entering product development .

in our assessment , the technologies that have reached technology readiness level 6 are referred to as fully mature because of the difﬁ culty of achieving technology readiness level 7 , which is demonstrating maturity in an operational environment — space .

projects with critical technologies that did not achieve maturity by the preliminary design review were assessed as having a technology maturity project challenge .

we did not assess technology maturity for those projects which had not yet reached the preliminary design review at the time of this assessment .

to assess the complexity of heritage technology , we asked project ofﬁ cials to assess the trl of each of the project's heritage technologies at various stages of project development .

we also interviewed project ofﬁ cials about the use of heritage technologies in their projects .

we asked them what heritage technologies were being used , what effort was needed to modify the form , ﬁ t , and function of the technology for use in the new system , whether the project encountered any problems in modifying the technology , and whether the project considered the heritage technology as a risk to the project .

heritage technologies were not considered critical technologies by several of the projects we reviewed .

based on our interviews , review of data from the data collection instruments , and previous gao work on space systems , we determined whether complexity of heritage technology was a challenge for a particular project .

to assess design stability , we asked project ofﬁ cials to provide the percentage of engineering drawings completed or projected for completion by the preliminary and critical design reviews and as of our current assessment .

in most cases , we did not verify or validate the percentage of engineering drawings provided by the project ofﬁ ce .

however , we collected the project ofﬁ ces' rationale for cases where it appeared that only a small number of drawings were completed by the time of the design reviews or where the project ofﬁ ce reported signiﬁ cant growth in the number of drawings released after cdr .

in accordance with gao's best practices , projects were assessed as having achieved design stability if they had released at least 90 percent of projected drawings by the critical design review .

projects that had not met this metric were determined to have a design stability project challenge .

though some projects used other methods to assess design stability , such as computer and engineering models and analyses , we did not analyze the use of these other methods and therefore could not assess the design stability of those projects .

we could not assess design stability for those projects that had not yet reached the critical design review at the time of this assessment .

to assess whether projects encountered challenges with contractor performance , we interviewed project ofﬁ cials about their interaction and experience with contractors .

we also relied on interviews we held in 2008 with contractor representatives from orbital sciences corporation , ball aerospace and technologies corporation , and raytheon space systems about their experiences contracting with nasa .

we were informed about contractor performance problems pertaining to their workforce , the supplier base , and technical and corporate experience .

we also discussed the use of contract fees with nasa and contractor's representatives .

we assessed a project as having this challenge if these contractor performance problems — as conﬁ rmed by nasa and , where possible , the project contractor — caused the project to experience a cost overrun , schedule delay , or decrease in mission capability .

for projects that did not have a major contractor , we considered this challenge inapplicable to the project .

to assess whether projects encountered challenges with development partner performance , we interviewed nasa project ofﬁ cials about their interaction with international or domestic partners during project development .

development partner performance was considered a challenge for the project if project ofﬁ cials indicated that domestic or foreign partners were experiencing problems with project development that impacted the cost , schedule , or performance of the project for nasa .

these challenges were speciﬁ c to the partner organization or caused by a contractor to that partner organization .

for projects that did not have an international or domestic development partner , we considered this challenge not applicable to the project .

to assess whether projects encountered challenges with funding , we interviewed ofﬁ cials from nasa's program analysis and evaluation division , nasa project ofﬁ cials , and project contractors about the stability of funding throughout the project life - cycle .

funding stability was considered a challenge if ofﬁ cials indicated that project funding had been interrupted or delayed resulting in an impact to the cost , schedule , or performance of the project , or if project ofﬁ cials indicated that the project budgets do not have sufﬁ cient funding in certain years based on the work expected to be accomplished .

we corroborated the funding changes and reasons with budget documents when available .

to assess whether projects encountered challenges with their launch manifests , we interviewed nasa launch services ofﬁ cials and ofﬁ cials from one of nasa's contracted providers for launch services about project launch scheduling , launch windows , and projects that missed their opportunities .

launch manifest was considered a challenge if , after establishing a ﬁ rm launch date , a project had difﬁ culty rescheduling its launch date because it was not ready , if the project could be affected by another project slipping its launch , or if there were launch vehicle ﬂ eet issues .

projects that have not yet entered into the implementation phase have not yet set a ﬁ rm launch date and were therefore not assessed .

in addition , nasa received an appropriation from the american recovery and reinvestment act of 2009 ( arra ) .

nasa provided a record of projects involved in our review that received arra funds .

the individual project ofﬁ ces were given an opportunity to comment on and provide technical clariﬁ cations to the 2-page assessments prior to their inclusion in the ﬁ nal product .

we conducted this performance audit from april 2009 to february 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufﬁ cient , appropriate evidence to provide a reasonable basis for our ﬁ ndings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our ﬁ ndings and conclusions based on our audit objectives .

gao has previously conducted work on nasa's acquisition policy for space - ﬂ ight systems , and in particular , on its alignment with a knowledge - based approach to system acquisitions .

the ﬁ gure below depicts this alignment .

as the ﬁ gure shows , nasa's policy deﬁ nes a project life cycle in two phases — the formulation and implementation phases , which are further divided into incremental pieces: phase a through phase f. project formulation consists of phases a and b , during which time the projects develop and deﬁ ne the project requirements and cost / schedule basis and design for implementation , including an acquisition strategy .

during the end of the formulation phase , leading up to the preliminary design review ( pdr ) and non - advocate review ( nar ) , the project team completes its preliminary design and technology development .

nasa interim directive nm 7120-81 for nasa procedural requirements 7120.5d , nasa space flight program and project management requirements , specify that the project complete development of mission - critical or enabling technology , as needed , with demonstrated evidence of required technology qualiﬁ cation ( i.e. , component and / or breadboard validation in the relevant environment ) documented in a technology readiness assessment report .

the project must also develop , document , and maintain a project management baseline that includes the integrated master schedule and baseline life - cycle cost estimate .

implementing these requirements brings the project closer to ensuring that resources and needs match , but it is not fully consistent with knowledge point 1 of the knowledge - based acquisition life - cycle .

our best practices show that demonstrating technology maturity at this point in the system life cycle should include a system or subsystem model or prototype demonstration in a relevant environment , not only component validation .

as written , nasa's policy does not require full technology maturity before a project enters the implementation phase .

after project conﬁ rmation , the project begins implementation , consisting of phases c , d , e , and f. during phases c and d , the project performs ﬁ nal design and fabrication as well as testing of components and system assembly , integration , test , and launch .

phases e and f consist of operations and sustainment and project closeout .

a second design review , the critical design review ( cdr ) , is held during the implementation phase toward the end of phase c. the purpose of the cdr is to demonstrate that the maturity of the design is appropriate to support proceeding with full scale fabrication , assembly , integration , and test .

though this review is not a formal decision review , its requirements for a mature design and ability to meet mission performance requirements within the identiﬁ ed cost and schedule constraints are similar to knowledge expected at knowledge point 2 of the knowledge - based acquisition life - cycle .

furthermore , after cdr , the project must be approved at kdp d before continuing into the next phase .

the nasa acquisition life - cycle lacks a major decision review at knowledge point 3 to demonstrate that production processes are mature .

according to nasa ofﬁ cials , the agency rarely enters a formal production phase due to the small quantities of space systems that they build .

none ( paper studies and analysis ) invention begins .

once basic principles are observed , practical applications can be invented .

the application is speculative and there is no proof or detailed analysis to support the assumption .

examples are still limited to paper studies .

none ( paper studies and analysis ) active research and development is initiated .

this includes analytical studies and laboratory studies to physically validate analytical predictions of separate elements of the technology .

examples include components that are not yet integrated or representative .

analytical studies and demonstration of nonscale individual components ( pieces of subsystem ) .

basic technological components are integrated to establish that the pieces will work together .

this is relatively “low ﬁ delity” compared to the eventual system .

examples include integration of “ad hoc” hardware in a laboratory .

low ﬁ delity breadboard .

integration of nonscale components to show pieces will work together .

not fully functional or form or ﬁ t but representative of technically feasible approach suitable for ﬂ ight articles .

fidelity of breadboard technology increases signiﬁ cantly .

the basic technological components are integrated with reasonably realistic supporting elements so that the technology can be tested in a simulated environment .

examples include “high ﬁ delity” laboratory integration of components .

high ﬁ delity breadboard .

functionally equivalent but not necessarily form and / or ﬁ t ( size weight , materials , etc ) .

should be approaching appropriate scale .

may include integration of several components with reasonably realistic support elements / subsystems to demonstrate functionality .

lab demonstrating functionality but not form and ﬁ t. may include ﬂ ight demonstrating breadboard in surrogate aircraft .

technology ready for detailed design studies .

representative model or prototype system , which is well beyond the breadboard tested for trl 5 , is tested in a relevant environment .

represents a major step up in a technology's demonstrated readiness .

examples include testing a prototype in a high ﬁ delity laboratory environment or in simulated realistic environment .

prototype .

should be very close to form , ﬁ t and function .

probably includes the integration of many new components and realistic supporting elements / subsystems if needed to demonstrate full functionality of the subsystem .

high - ﬁ delity lab demonstration or limited / restricted ﬂ ight demonstration for a relevant environment .

integration of technology is well deﬁ ned .

prototype near or at planned operational system .

represents a major step up from trl 6 , requiring the demonstration of an actual system prototype in a realistic environment , such as in an aircraft , vehicle or space .

examples include testing the prototype in a test bed aircraft .

prototype .

should be form , ﬁ t and function integrated with other key supporting elements / subsystems to demonstrate full functionality of subsystem .

flight demonstration in representative realistic environment such as ﬂ ying test bed or demonstrator aircraft .

technology is well substantiated with test data .

technology has been proven to work in its ﬁ nal form and under expected conditions .

in almost all cases , this trl represents the end of true system development .

examples include developmental test and evaluation of the system in its intended weapon system to determine if it meets design speciﬁ cations .

actual application of the technology in its ﬁ nal form and under mission conditions , such as those encountered in operational test and evaluation .

in almost all cases , this is the end of the last “bug ﬁ xing” aspects of true system development .

examples include using the system under operational mission conditions .

there are 7 nasa projects in our review , including all of those in formulation , that are receiving money from the american recovery and reinvestment act ( arra ) of 2009 .

see table 4 below for the nasa projects in our review receiving this funding and the intended use of those funds .

in addition to the contact named above , jim morrison , assistant director ; jessica m. berkholtz ; greg campbell ; richard a. cederholm ; kristine r. hassinger ; jeff r. jensen ; kenneth e. patton ; brian a. tittle ; and letisha t. watson made key contributions to this report .

