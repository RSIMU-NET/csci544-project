the army has spent over $765 million of the $1 billion estimated total cost for the maneuver control system ( mcs ) which is to provide battlefield information to maneuver commanders .

since 1980 , the mcs program has experienced numerous problems , such as fielding inadequate computer software and canceling the development of one software version due to design flaws , cost growth , and schedule slips .

given the program's past difficulties and the important role of mcs in the army's battlefield automation efforts , we reviewed the army's development and acquisition plans for mcs .

specifically , our objectives were to determine whether ( 1 ) the current mcs software development strategy is appropriate to overcome prior development problems and ( 2 ) 207 new computers for mcs related training should be procured as planned .

the goal of the army's mcs program is to develop and field a computer system that provides automated critical battlefield assistance to maneuver commanders and their battle staff at the corps - to - battalion level .

mcs is intended to enable the command staff to collect , store , process , display , and disseminate critical data to produce and communicate battle plans , orders , and enemy and friendly situational reports .

it is a key component of the army tactical command and control system , which is also intended to enhance the coordination and control of combat forces through automated management of five key battlefield areas , including maneuver control .

given its role to communicate battle plans , orders , and enemy and friendly situation reports , mcs is also a key component of the army's ongoing efforts to digitize ( automate ) its battlefield operations .

in 1980 , the army fielded the first mcs system — with limited command , control , and communications capabilities — to vii corps in europe .

in 1982 , the army awarded a 5-year contract to continue mcs development , and by 1986 mcs software had evolved to version 9 , also fielded in europe .

in 1987 , the army performed post - deployment tests on version 9 in germany .

the results of those tests led the army materiel systems analysis activity to conclude that mcs did not exhibit adequate readiness for field use and recommend that further fielding not occur until the system's problems were resolved .

however , the army awarded a second 5-year contract that resulted in version 10 , which was fielded by april 1989 and remains in the field today .

in november 1989 , the army materiel systems analysis activity reported that mcs had met only 30 percent of its required operational capabilities and again recommended that the system not be released for field use .

in may 1990 , operational testers again questioned the system's functional ability and effectiveness because it could not produce timely , accurate , and useful information in a battle environment .

while earlier versions of mcs were being fielded and withdrawn , the development of software continued .

in 1988 , the army awarded a contract for the development of version 11 .

by february 1993 , the army stopped development of version 11 software due to multiple program slips , serious design flaws , and cost growth concerns .

the program was then reorganized with a plan approved by the office of the secretary of defense in april 1993 .

under the reorganized program , a group of contractors and government software experts have been working to develop the next version of mcs software — version 12.01 — utilizing software segments that could be salvaged from the failed version 11 effort .

in addition to software , the mcs system consists of computers procured under the army's common hardware and software ( chs ) effort , which was undertaken to reverse the proliferation of program - unique computers and software .

the army planned to acquire 288 of the chs computers in fiscal years 1997 and 1998 to support the mcs training base , and has already acquired 81 .

those computers were used in a training base assessment to support a decision to acquire the remaining 207 computers .

since its reorganization in 1993 , mcs program experience indicates continuing problems in the system's development .

specifically , ( 1 ) the mcs initial operational test and evaluation of version 12.01 has slipped twice , ( 2 ) interim developmental level tests and a customer test done to support a decision to award a contract to develop follow - on software show that significant problems continue , and ( 3 ) development of follow - on version 12.1 was begun despite the results of the customer test and prior program history .

after the 1993 program reorganization , version 12.01 was scheduled to undergo initial operational testing and evaluation in november 1995 .

the test slipped to november 1996 and is now scheduled for march 1998 .

program officials stated that the test date slipped initially because the chs computers to be used were not yet available .

during august and september 1996 , version 12.01 underwent a system confidence demonstration to determine whether it was ready for the november 1996 initial operational test and evaluation .

because the software was not ready , further work and two additional system confidence demonstrations followed in august and september 1996 .

both demonstrations indicated that the system was not ready for operational testing .

additionally , the software still had an open priority one software deficiency and priority three and four deficiencies that would have negatively impacted the conduct of the operational test .

both the army's operational test and evaluation command and the department of defense's ( dod ) director of operational test and evaluation ( dot&e ) had stated that there could be no open priority one or two software deficiencies before the operational test .

they had also stated that there could not be any open priority three and four deficiencies that , in combination , were likely to have a detrimental effect on the system's performance .

dot&e staff told us that there were a number of open priority three and four software deficiencies that they believe would have had a detrimental effect .

when mcs program officials realized that these deficiencies would not be resolved in time for the initial operational test , they downgraded the test 3 weeks before it was to occur to a limited user test , utilizing $8.5 million appropriated for the mcs operational test in fiscal years 1996 and 1997 .

that test was conducted in november 1996 .

while the test report has not been finalized , a draft version states that mcs — in the tested configuration — is not operationally effective or suitable .

throughout the development of version 12.01 , interim software builds have undergone numerous performance tests to determine the current state of software development , and build 4 was subjected to a customer test .

the results of those tests identified continuing problems as the number of builds proceeded .

for example , a december 1995 performance test report on build 3.0 stated that , if the problems found during the test were not quickly corrected in build 3.1 , then the risk to the program might be unmanageable .

the follow - on april 1996 performance test report of build 3.1 stated that significant problems in system stability prevented proper testing of several requirements .

the report further stated that messaging between battlefield functional areas was extremely difficult and problematic and that the system had other stability problems .

a september 1996 performance test report stated that of 568 previously open deficiency reports from builds 5.1 through 5.2c , 165 , almost 29 percent , still remained open .

this report , the last published on an mcs performance test , reflected the state of the mcs software shortly before the downgraded limited user test , in which mcs failed to demonstrate either operational effectiveness or suitability .

more recent performance tests of later builds have been done ; however , separate reports on those test events have not been issued .

rather , the program office plans to prepare an integrated test report in october or november 1997 .

“developed by a confederation of contractors who have built this current version of mcs on the salvaged 'good' portions of the abruptly terminated development of mcs version 11 , it needs to stand the rigor of an independent operational test and evaluation .

 .

 .

before a mcs block iv contract is awarded.” to help determine the level of risk in proceeding under the army's development strategy , dot&e stated in a june 1995 memorandum that an operational test of version 12.01 be conducted to measure the software's maturity before the award of a contract for the development of follow - on versions .

as a result , an operational assessment — called the mcs customer test — was conducted on version 12.01 in april 1996 to support the award of a $63.1 million contract for the development of mcs block iv software — mcs versions 12.1 , 12.2 , and 12.3 .

no pass / fail criteria were set for the customer test .

however , dot&e directed that four operational issues be tested .

those issues related to ( 1 ) the capacity of the system to store and process required types and amounts of data , including the ability of the staff users to frequently update the information database ; ( 2 ) the capabilities of the mcs network to process and distribute current and accurate data using the existing communications systems ; ( 3 ) the impact of computer server outages on continuity of operations ; and ( 4 ) the system administration and control capabilities to initialize the system , become fully operational , and sustain operations .

in its report on the customer test , the army's test and experimentation command stated that , at the time of the test , mcs was evolving from a prototype system to one ready for initial operational test and evaluation and , as such , possessed known limitations that were described to the system users during training .

the command reported that the test's major limitations included ( 1 ) software that did not contain the full functional capability planned for the initial operational test and evaluation ; ( 2 ) a need to reboot the system after crashes caused by the use of the computer's alternate function key ; ( 3 ) two changes in software versions during training ; and ( 4 ) the fact that 65 percent of the system manager functions had not been implemented or trained .

table 1 provides more detail on the customer test results .

in addition to these findings , the mcs test officer stated the following: “system performance degraded over time causing message backlogs , loss of data , and numerous system reboots .

over a period of 12 operational hours the slowed down and created message backlogs of up to 4 hours .

to remain functional , the entire network of [mcs] systems must be shut down and reinitialized in proper sequence.” “the staff users had great difficulty using ... applications.” “the software pertaining to system management functions was immature , incomplete and lacked documentation .

this capability is critical to the effective use and operation of the [mcs] system.” even though the customer test did not involve pass / fail criteria , based on our review of the test report and the test officer's comments , we believe that only the third operational issue — impact of computer server outages on continuity of operations — was met .

despite the results of the customer test and the program's prior history , the under secretary of defense for acquisition and technology approved the army's plan to award a concurrent contract for mcs block iv software development — mcs versions 12.1 , 12.2 and 12.3 .

in september 1996 , the army awarded a contract for the development of mcs software versions 12.1 , 12.2 , and 12.3 to a different contractor than the developers of mcs version 12.01 .

at that time , version 12.01 was still scheduled to undergo its initial operational testing in november 1996 .

the start of the follow - on development could have been timed to occur after version 12.01 had completed that operational testing .

at most , this action would have delayed the contract award 2 months , assuming that the initial operational test had occurred in november 1996 as scheduled .

however , the contract was awarded before the initial operational test , and the planned 5 month concurrency in the development of versions 12.01 and 12.1 became 18 months when the operational test slipped to march 1998 .

the current program schedule indicates that ( 1 ) version 12.1 is expected to undergo its operational assessment / test about 1 year after the fielding of version 12.01 is started and ( 2 ) version 12.1 fielding is to be done 5 months after initial operational capability of version 12.01 is achieved .

if the scheduled version 12.01 operational test and evaluation slips again and the version 12.1 contractor is able to maintain its development schedule , version 12.1 could become available before version 12.01 .

by may 1997 , the army requested dod approval of a revised acquisition program baseline that changes the planned follow - on operational test and evaluation of versions 12.1 , 12.2 , and 12.3 to operational assessments / operational tests .

program officials said that , although the name of the tests had changed , the planned scope of the tests had not .

however , the officials said that the name change complies with guidance from dot&e , which lists multiple levels of operational test and evaluation ( from an abbreviated assessment to full operational test ) and outlines a risk assessment methodology to be used to determine the level of testing to be performed .

the officials further stated that the use of the generic term operational test / operational assessment permits possible changes to the level of testing for version 12.1 and follow - on software increments based on the risk assessment process .

the contractors competing for the mcs block iv ( mcs versions 12.1 , 12.2 , and 12.3 ) development were given access to the government's 12.01 code and allowed to reuse as much of it as they chose .

the block iv developer is not required to reuse any of version 12.01 .

rather , the block iv contract requires the development of software to provide specific functions .

given that ( 1 ) version 12.01 software has not passed or even undergone an initial operational test and evaluation and ( 2 ) the mcs block iv contractor building version 12.1 is not the contractor that is building version 12.01 and is only required to develop the version 12.1 to provide specified functions , we believe that the version 12.1 development effort should not be viewed as building upon a proven baseline .

instead , it should be viewed as a new effort .

the army's current development plan for version 12.1 and beyond , as shown in figure 1 , continues an approach of building a follow - on version of software on an incomplete and unstable baseline — the uncompleted preceding version of software .

additionally , according to an official in the dod's office of the director of test , systems engineering , and evaluation , the army's development process allows requirements that are planned for one software version , which cannot be accomplished in that version's development as planned , to be deferred to a later version's development .

as a result , this process makes judging program risk and total cost very difficult .

the mcs program has previously demonstrated the problem of deferring requirements .

for example , during mcs version 11 development , we reported that the army had deferred seven mcs functions that were to have been developed by june 1992 and included in the software version to undergo operational testing .

even though the version 11 operational test had slipped twice , from may 1992 to september 1992 and then to may 1993 , the army continued to defer those functions , and the operational test was planned for less than the complete software package originally scheduled to be tested .

in commenting on a draft of this report , dod said that they had made progress not reflected in that draft .

specifically , they noted that there were no priority one or two , and only 22 priority three software deficiencies open as of september 11 , 1997 , as compared with 10 priority one , 47 priority two , and 67 priority three deficiencies open on august 16 , 1996 .

while we agree these results indicate that some known problems have been fixed , they provide no indication of the number or severity of still unknown problems .

for example , mcs version 12.01 development showed enough progress entering the november 1996 scheduled initial operational test and evaluation to reach a commitment of resources and personnel .

however , that test was later downgraded to a limited user test because of software immaturity .

successful completion of an initial operational test and evaluation should provide a more definitive indication of the mcs program's progress .

before the slip of the mcs initial operational test and evaluation from november 1996 to march 1998 , the army planned to acquire 288 computers — 150 in fiscal year 1997 and 138 in fiscal year 1998 — for the mcs training base .

these computers were to be acquired after a full - rate production decision at a total cost of about $34.8 million — $19.1 million in fiscal year 1997 and $15.7 million in fiscal year 1998 .

after the initial operational test and evaluation slipped , dod approved the army's acquisition of a low - rate initial production of 81 computers in fiscal year 1997 for a training base operational assessment .

the purpose of the assessment , which was performed from february to may 1997 , was to judge the merits of allowing the army to procure the remaining computers prior to successful completion of the slipped operational test .

on the basis of the results of that assessment , the acting under secretary of defense for acquisition and technology authorized the army in july 1997 to proceed with its acquisition plans .

the acting under secretary noted that the dot&e had reviewed the assessment and agreed that version 12.01 was adequate for use in the training base .

the acting under secretary also authorized the army to move the training base computer funds from the mcs budget to the army's automated data processing equipment program budget line .

this action was necessary because , according to both army and dod officials , it was determined that the computers to be acquired do not meet the legislated reasons in 10 u.s.c .

2400 for low - rate initial production .

that legislation allows the early acquisition of systems to ( 1 ) establish an initial production base , ( 2 ) permit an orderly increase in the production rate for the system that is sufficient to lead to full - rate production upon successful completion of operational test and evaluation , and ( 3 ) provide production - representative items for operational test and evaluation .

even though the army now plans to acquire the computers under a different budget line , the intended use of the computers remains unchanged .

mcs program officials said that the computers are needed in the mcs training base before operational testing to adequately support future fielding of mcs and the larger army battle command system , of which the army tactical command and control system and mcs are key components .

this rationale is the same one the acting under secretary cited in his july 1997 memorandum .

in that memorandum , he stated that the “requirement to train army - wide on commercial equipment is a recognized requirement not only for mcs but for a host of other digital .

 .

 .

systems.” the acting under secretary further noted that the funds to be moved were for equipment needed to support integrated training of multiple systems throughout the army and concluded that “training on a digital system , even if it is not the system that is ultimately fielded , is important to the army in order to assist in making the cultural change from current maneuver control practice to a digitized approach.” mcs program officials stated that the mcs course curriculum needs to be developed and that equipping the training base before the completion of operational testing avoids a 2-year lag between the completion of operational testing and the graduation of trained students .

the officials also commented that the computers could be used elsewhere , since they would be compatible with other army programs .

the legislated requirement that major systems , such as mcs , undergo initial operational test and evaluation before full - rate production serves to limit or avoid premature acquisitions .

the army has had previous experience acquiring ineffective mcs equipment , which is indicative of the need for adequate testing before systems are fielded .

in july 1990 , the army began withdrawing over $100 million of militarized mcs hardware from the field due to both hardware and software deficiencies .

additionally , the army subsequently decided not to deploy other mcs equipment it had procured for light divisions at a cost of about $29 million because the equipment was too bulky and heavy .

the mcs program's troubled development and acquisition history has continued since the program's 1993 reorganization .

however , the army awarded a new contract to develop future software versions and plans to procure computers without fully resolving the problems of earlier versions .

this strategy does not minimize the possibility of future development problems and ensure that the army will ultimately field a capable system .

also , since mcs software version 12.1 is being developed concurrently by a different contractor to functional specifications , it would be prudent to subject the version 12.1 software to the level of operational testing required to support a full - rate production decision , as planned for version 12.01 .

accordingly , we believe a more appropriate strategy would require that future software versions be developed using only fully tested baselines , and that each version be judged against specific pre - established criteria .

we recommend that you direct the secretary of the army to set specific required capabilities for each software version beyond version 12.01 , test those versions against specific pass / fail criteria for those capabilities , and only award further development contracts once problems highlighted in that testing are resolved ; perform a full operational test and evaluation of mcs software version 12.1 to ensure that it provides the full capabilities of version 12.01 ; and procure additional mcs computers only after an initial operational test and evaluation and a full - rate production decision have been completed .

in commenting on a draft of this report , dod agreed with our recommendation that specific required capabilities for each mcs software version beyond version 12.01 are needed , that those versions should be tested against specific pass / fail criteria for those capabilities , and that the army should not award further development contracts until problems highlighted in prior tests are resolved .

dod noted that the army has already set specific required capabilities for those software versions and will test those versions against specific pass / fail criteria to ensure system maturity and determine that the system remains operationally effective and suitable .

dod further stated that it will not support the award of further development contracts until the army has successfully resolved any problems identified during the testing of related , preceding versions .

dod partially agreed with our recommendation that the army be directed to perform a full - operational test and evaluation of mcs software version 12.1 to ensure that it provides the full capabilities of version 12.01 .

dod stated that the army will comply with dod regulation 5000.2r and will follow guidance from director of operational test and evaluation , which lists multiple levels of operational test and evaluation ( from an abbreviated assessment to full operational test ) and outlines a risk assessment methodology to be used to determine the level of testing to be performed .

dod did not , however , indicate whether it would require the army to conduct a full operational test .

we continue to believe that the version 12.1 development effort should not be viewed as building upon a proven baseline .

instead , version 12.1 development should be viewed as a new effort .

as a result , we still believe that the prudent action is to require that version 12.1 be subjected to the same level of operational test and evaluation as version 12.01 , the level required to support a full - rate production decision .

dod agreed with our recommendation that it direct the army to not procure more mcs computers until the completion of an initial operational test and evaluation and a full - rate production decision .

it stated , however , that no further direction to the army is needed as it had already provided direction to the army on this issue .

specifically , the department stated that it has directed the army to extract the training base computers from the mcs program and to not procure or field more mcs hardware to operational units until successfully completing an initial operational test and evaluation .

our recommendation , however , is not limited to the hardware for operational units , but also encompasses the computers the army plans to buy for the training base .

given the program's prior history and the fact that the training base computers are not needed to satisfy any of the legislated reasons for low - rate initial production , we continue to believe that the army should not be allowed to buy those computers until mcs has successfully completed its initial operational test and evaluation — the original plan prior to the mcs initial operational test and evaluation's multiple schedule slips .

dod's comments are reprinted in their entirety in appendix i , along with our evaluation .

in addition to those comments , we have revised our report where appropriate to reflect the technical changes that dod provided in a separate letter .

to determine whether the current mcs software development strategy is appropriate to overcome prior problems and to determine whether the army should procure 207 new computers for the expansion of the mcs training base , we interviewed responsible officials and analyzed pertinent documents in the following dod offices , all in washington , d.c.: director of operational test and evaluation ; director of test , systems engineering , and evaluation ; assistant secretary of defense for command , control , communications , and intelligence ; under secretary of defense ( comptroller ) ; and defense procurement .

in addition , we interviewed responsible officials and analyzed test reports from the office of the army's project manager , operations tactical data systems , fort monmouth , new jersey ; and the army's operational test and evaluation command , alexandria , virginia .

to meet our second objective , we also interviewed responsible officials and analyzed pertinent documents from the army's combined arms center , fort leavenworth , kansas .

we conducted our review from march to september 1997 in accordance with generally accepted government auditing standards .

we are sending copies of this report to the chairman and ranking minority members , senate and house committees on appropriations , senate committee on armed services , and house committee on national security ; the director , office of management and budget ; and the secretary of the army .

we will also make copies available to others on request .

as you know , the head of a federal agency is required by 31 u.s.c .

720 to submit a written statement on actions taken our recommendations to the senate committee on governmental affairs and the house committee on government reform and oversight not later than 60 days after the date of this report .

a written statement must also be submitted to the senate and house committees on appropriations with the agency's first request for appropriations made more than 60 days after the date of the report .

please contact me at ( 202 ) 512-4841 if you or your staff have any questions concerning this report .

major contributors to this report were charles f. rey , bruce h. thomas , and gregory k. harmon .

the following are gao's comments on the department of defense's ( dod ) letter dated october 2 , 1997 .

1 .

in partially agreeing with this recommendation , dod states that the army will comply with dod regulation 5000.2r and will follow guidance from director of operational test and evaluation — guidance which lists multiple levels of operational test and evaluation ( from an abbreviated assessment to full operational test ) and outlines a risk assessment methodology to be used to determine the level of testing to be performed .

dod does not , however , indicate how they agree or disagree with our recommendation or state whether they will implement the recommendation .

as we stated in the body of this report , given that a different contractor is building version 12.1 under a requirement to provide specific functionality , we believe that this development effort should not be viewed as building upon a proven baseline .

instead , version 12.1 development should be considered a new effort .

as a result , we continue to believe that it is prudent to require that version 12.1 be subjected to the level of operational test and evaluation required to support a full - rate production decision .

2 .

dod's direction to the army only partially implements our recommendation .

our recommendation is not limited to the hardware for operational units , but also encompasses the computers the army plans to buy for the training base .

we continue to believe that the army should not be allowed to buy the planned training base computers until mcs has successfully completed its initial operational test and evaluation — the original plan prior to the mcs initial operational test and evaluation's schedule slips .

the training base computers are not required to satisfy any of the three purposes the law indicates for low - rate initial production — to ( 1 ) establish an initial production base , ( 2 ) permit an orderly increase in the production rate for the system sufficient to lead to full - rate production upon successful completion of operational test and evaluation , and ( 3 ) provide production - representative items for operational test and evaluation .

since the training base computers are not needed to satisfy one of the above legislated conditions , we continue to believe that the army should refrain from buying any additional mcs computers prior to a full - rate production decision .

the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 37050 washington , dc 20013 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 202 ) 512-6061 , or tdd ( 202 ) 512-2537 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

