the effective operation of federal agencies has a significant effect on the health , safety , and security of the american public , and how federal leaders manage the operations and performance of their agencies influences their ability to achieve important outcomes .

our previous work has found that leadership's active use of performance information to guide decision making leads to better managed programs and improved results .

our work has also found that federal agencies can use performance information to identify and correct performance problems , improve program implementation and organizational processes , and make other important management and resource allocation decisions .

however , our past surveys of federal managers have identified continuing weaknesses in the use of performance information by agencies that can hinder their ability to achieve critical results .

to improve performance and results by increasing the use of performance information by agency leaders and managers , congress included in the gpra modernization act of 2010 ( gprama ) a requirement that agencies review progress on agency priority goals ( apg ) at least once a quarter .

this requirement was based on the “stat” model of frequent , in - person , leadership - driven reviews of performance — a model that has been widely adopted by state and local governments .

in its guidance to agencies on how to implement reviews , the office of management and budget ( omb ) has emphasized that frequent , data - driven performance reviews provide a mechanism for agency leaders to: ( 1 ) assess the organization's performance ; ( 2 ) bring together the people , resources , and analysis needed to drive progress on agency priorities ; ( 3 ) diagnose performance problems and identify improvement opportunities through the analysis of data ; ( 4 ) identify lessons learned from past experience ; and ( 5 ) decide on next steps to increase performance and productivity .

as omb has also noted , these practices are designed to shift the emphasis away from the passive collection and reporting of performance information to a model where performance information is actively used by agency officials to inform decision - making .

the latter is more likely to lead to performance improvements .

omb circular no .

a - 11 , preparation , submission , and execution of the budget , pt .

6 , section 270.3 ( august 2014 ) .

improvements .

this report builds on that earlier work , and is part of our response to a statutory requirement that we evaluate how the implementation of the gpra modernization act of 2010 ( gprama ) is affecting performance management in federal agencies , and whether performance management is being used by agencies to improve the efficiency and effectiveness of agency programs.examines ( 1 ) the extent to which agencies are conducting data - driven performance reviews in a manner consistent with gprama requirements , omb guidance , and leading practices for reviews ; and ( 2 ) how data - driven performance reviews have affected performance , collaboration , accountability , and efficiency within agencies , and how positive effects can be sustained .

to address both objectives , we conducted a survey of performance improvement officers ( pios ) at 23 executive agencies .

we asked pios for information about the frequency of review meetings ; leadership of , and participation in , review meetings ; preparation for , execution of , and follow - up on , review meetings ; challenges ; and perceived impacts on agency performance , collaboration , efficiency , and accountability for results .

we received responses from all 23 agency pios .

to further address both objectives , we selected five case - study agencies for a more in - depth assessment of their data - driven review processes .

to provide additional detail and illustrative examples to supplement government - wide data collected through our survey , we used these more in - depth reviews to gather information about agency practices and officials' perceptions of effects that review meetings have had .

we selected a sample of agencies that reflect a range of characteristics , including agency size and the extent to which agency leadership uses quarterly performance reviews to drive progress toward goals , as reported by respondents to our 2013 federal managers survey .

we selected the departments of commerce ( commerce ) , health and human services ( hhs ) , and transportation ( dot ) ; the general services administration ( gsa ) ; and social security administration ( ssa ) .

at each agency , we selected at least two agency priority goals ( apg ) to obtain the perspective of apg leaders and their staff on the agency's data - driven review process .

to allow us to corroborate information collected through surveys , interviews , and observations , and strengthen our confidence in the reliability of the self - reported survey responses , we requested supporting documents from 12 agencies , representing more than 50 percent of the agencies surveyed , to verify survey responses related to review meeting frequency , leadership , participation , content , and follow - up .

the 12 agencies included the 5 agencies selected for more in - depth review , several agencies whose survey responses required clarification , and several additional agencies selected at random .

examples of documents submitted by agencies included review meeting attendance or invitations , agendas , presentation slides , and briefings and summary reports .

through the survey , and subsequent follow - up , we learned that the department of homeland security ( dhs ) does not hold the omb - required , in - person review meetings , and has not done so since december 2013 .

for this reason , the summaries of survey responses in this report exclude dhs , with the exception of table 2 , which describes the frequency of review meetings at each agency .

gprama established the position of agency chief operating officer ( coo ) and required that the deputy head of the agency , or equivalent — such as a deputy secretary — serve in this role .

the function of the coo is to: 1 ) provide overall organization management to improve agency performance and achieve the mission and goals of the agency through the use of strategic and performance planning , measurement , analysis , regular assessment of progress , and use of performance information to improve the results achieved ; 2 ) advise and assist the head of agency in carrying out the performance planning , reporting , and review requirements of gprama ; 3 ) oversee agency - specific efforts to improve management functions within the agency and across government ; and 4 ) coordinate and collaborate with relevant personnel within and external to the agency who have a significant role in contributing to and achieving the mission and goals of the agency .

31 u.s.c .

§ 1123. performance staff to periodically come together to discuss performance review practices in their agencies .

we observed agency review meetings at hhs , gsa , and ssa .

we also observed one sub - agency - level review meeting at gsa .

observing review meetings allowed us to gain firsthand knowledge of how the meetings are conducted in these agencies .

this provided context , increased our familiarity with the process , and corroborated information gained through other means .

while we requested to observe a review meeting at both commerce and dot , we were not allowed to do so .

the agencies cited concerns that our presence could inhibit open discussion .

lastly , to address the first objective , we compared what we learned about the review processes at all 23 agencies with requirements for review meetings established in gprama , as well as standards set forth in omb guidance and leading practices for data - driven reviews we previously to address the second objective , we analyzed our survey identified.data to determine how agencies characterized the effects of their review meetings .

we also used interviews with officials and documentation from our selected agencies to identify illustrative examples of the effects review meetings have had , and to identify actions they have taken to sustain the positive effects of the reviews .

we conducted our work from july 2014 to july 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusion based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

data - driven performance reviews are regularly scheduled , structured meetings used by organizational leaders and managers to review and analyze data on progress toward key performance goals and other management - improvement priorities .

they are generally used to target areas where leaders want to achieve near - term performance improvements , or to accelerate progress through focused senior leadership attention .

over the past several years , congress and the executive branch have taken steps to improve federal performance management by requiring that agencies conduct regular data - driven review meetings .

in 2010 , omb released a memorandum establishing the expectation that federal agencies would hold data - driven reviews at least once every quarter to review progress on their priority goals and assure that follow - up steps would be taken to achieve improved outcomes .

the memorandum specified that discussions during these meetings were to be guided by analyses of performance data , to focus on progress toward desired outcomes , to explore why variations between targets and actual outcomes occurred , and to prompt adjustments when needed .

congress , through the passage of gprama , made the expectation that agencies would hold regular data - driven reviews a statutory requirement .

specifically , gprama requires that , not less than quarterly , the head of each agency and coo , with the support of the pio , should review progress on agency priority goals ( see text box ) .

gprama requirement for quarterly priority progress reviews gprama requires that , not less than quarterly , at all agencies required to develop agency priority goals , the head of the agency and chief operating officer , with the support of the agency performance improvement officer , shall: for each agency priority goal , review with the appropriate goal leader the progress achieved during the most recent quarter , overall trends , and the likelihood of meeting the planned level of performance ; coordinate with relevant personnel within and outside the agency that contribute to the accomplishment of each agency priority goal ; assess whether relevant organizations , program activities , regulations , policies , and other activities are contributing as planned to the agency priority goals ; categorize agency priority goals by risk of not achieving the planned level of performance ; and for agency priority goals at greatest risk of not meeting the planned level of performance , identify prospects and strategies for performance improvement , including any needed changes to agency program activities , regulations , policies , or other activities .

while gprama established requirements for agencies to conduct the reviews , gprama also required that omb prepare guidance on the implementation of gprama .

in 2011 , omb released guidance for federal agencies that reinforced the requirements in gprama , specified that the reviews should be held in person , and outlined the specific purposes of the data - driven review meetings , the roles and responsibilities of agency leaders involved in the review process , and how the reviews should be conducted .

in 2012 , omb released updated guidance for data - driven reviews.throughout this report .

gao - 13-228 .

and federal level who shared their experiences and lessons learned.these practices , along with additional insights on why the application of these practices is important , are noted and summarized throughout this report .

nine leading practices identified by gao that can be used to promote successful data - driven performance reviews reviews are conducted frequently and regularly .

leaders use data - driven reviews as a leadership strategy to drive performance improvement .

key players attend reviews to facilitate problem solving .

rigorous preparations enable meaningful performance discussions .

there is capacity to collect accurate , useful , and timely performance data .

staff have skills to analyze and clearly communicate complex data for decision making .

reviews ensure alignment between goals , program activities , and resources .

leaders hold managers accountable for diagnosing performance problems and identifying strategies for improvement .

participants engage in rigorous and sustained follow - up on issues identified during reviews .

taken together , the gprama requirements , omb guidance , and leading practices identify the elements necessary to carry out effective data - driven reviews: ( 1 ) those that are used to engage agency leaders in the rigorous assessment of agency performance ; ( 2 ) support faster and better informed responses to identified performance problems ; ( 3 ) improve communication and collaboration across an agency ; and ( 4 ) enhance individual and collective accountability for improving progress toward agency goals .

gprama requirement: quarterly reviews gprama requires that agency leaders conduct reviews on progress toward agency priority goals ( apgs ) not less than quarterly .

omb guidance: quarterly reviews omb guidance directs agency leaders to run data - driven performance reviews on each of their apgs at least quarterly .

this guidance also stresses that reviews should be conducted in person , as significant experience in federal agencies , states , localities , and other countries demonstrates that in - person engagement of senior leaders greatly accelerates learning and performance improvement .

leading practice for data - driven reviews: frequency and regularity data - driven review meetings should be frequent and regularly scheduled .

data - driven performance review meetings that are held frequently and regularly help foster a culture of active and ongoing performance management , problem solving , and continuous improvement .

as omb has noted , the purpose of conducting performance reviews at least quarterly is to ensure that agency leaders regularly review agency performance on top priorities , along with the short - and long - term actions agencies are taking to improve performance , and bring together the people , resources , and analysis needed to drive progress on priority goals .

of the 23 cfo act agencies surveyed , 20 agencies reported that they hold data - driven review meetings at least quarterly , with some agencies holding them more frequently .

see table 2 for a summary of the frequency with which each agency holds in - person review meetings to , among other things , review progress on apgs .

as shown in the table , the department of homeland security ( dhs ) does not hold the required in - person review meetings .

the five case - study agencies we selected for more in - depth review – the departments of commerce ( commerce ) , health and human services ( hhs ) , and transportation ( dot ) ; general services administration ( gsa ) ; and social security administration ( ssa ) – all hold in - person review meetings involving agency leaders and apg goal leaders at different frequencies .

this reflects differences in leadership preferences and organizational structures and processes .

see appendix ii for more detailed information on the approach used by each of the five selected case - study agencies .

omb guidance is clear that reviews should be held in person to bring together senior leaders and officials involved in all levels of program delivery .

this can help ensure coordination across agency silos and enable rapid decision making .

this guidance states that while written communication may replace in - person review meetings in rare circumstances , it should only be a stopgap measure to continue performance reviews in a process that otherwise operates primarily in person .

a few agencies , including the department of agriculture ( usda ) , dhs , and hhs , reported that they do not hold in - person reviews of progress on apgs at least quarterly as called for in gprama and omb guidance .

the department of state ( state ) reported that agency officials participate in one data - driven review meeting each quarter ; however , each meeting is used to review progress on only one apg .

agriculture .

according to usda officials , the deputy secretary meets weekly with officials and staff from the office of budget and program analysis ( obpa ) , including the pio , to discuss budget and regulatory issues , which also provides opportunities to discuss apg progress and performance - related issues .

usda officials also told us that written updates on apgs and performance data are provided quarterly , and that the deputy secretary and pio meet as necessary to review progress toward , and discuss issues related to , specific apgs .

however , these are not regularly scheduled meetings .

in addition , they told us that staff from the obpa have frequent conversations with program officials as part of the review of regulatory documents , funding availability notices , executive budget documents , and other related documents so they have not had separate , regularly scheduled meetings to discuss progress toward the apgs .

however , in subsequent discussions with usda officials , they informed us that they intend to begin holding regularly scheduled quarterly meetings led by the coo and involving senior usda leadership , as directed by omb in circular a - 11 guidance .

homeland security .

dhs reported that in - person review meetings ceased due to competing priorities and demands at the end of 2013 , when a change in leadership brought alternative management priorities .

agency leaders continue to review written performance updates quarterly .

according to a dhs official , a meeting involving the deputy secretary and apg goal leaders to review goal results from fiscal year 2015 is being scheduled .

health and human services .

hhs leaders hold in - person review meetings for each apg twice a year and review apg progress two more times a year through reviews of written progress updates .

officials from hhs said that , due to the longer - term nature of the agency's apgs , performance data that are tracked for each goal show little meaningful change from quarter to quarter , so agency officials have not considered meeting quarterly to be an effective use of participants' time .

one hhs official said that managers convene meetings with their program teams more frequently to track progress on efforts contributing to each goal .

state .

each year , state holds one joint , in - person meeting to review progress on the apg to support the implementation of low emission development strategies , which it co - leads with the u.s. agency for international development ( usaid ) .

state also holds one meeting each year to review progress on the apg to improve consular service delivery .

state officials attend three other reviews on apgs led by usaid held throughout the year .

therefore , progress on each apg is only reviewed by officials from state in an in - person review meeting once a year .

a state official stated that the performance measures used to track progress on the two apgs for which the agency either leads or co - leads show little meaningful change from quarter to quarter , and believes it would not be beneficial for stakeholders to attend meetings more often than annually .

the official also said , however , that the agency's pio reviews the data and written updates provided by apg goal leaders each quarter , and updates the deputy secretary .

as gprama requirements and omb guidance are clear that in - person review meetings should be held at least once a quarter , and that progress on each apg should be reviewed each quarter in these meetings , the approaches of these four agencies – dhs , hhs , state , and usda – are not consistent with requirements for the frequency and expected characteristics of reviews .

furthermore , omb guidance states that these reviews should not be conducted through written documents , and that agency leaders should use performance review meetings as an opportunity to engage those involved in all levels of program delivery .

the lack of frequent , regular , in - person review meetings could result in missed opportunities for leaders and key officials at these four agencies to have regular , in - depth discussions of performance on top agency priorities .

such meetings could also allow them to actively promote ongoing coordination and accountability , address identified challenges or problems in a timely manner , and encourage continuous improvement in agency performance and operations .

as omb guidance also clarifies , apgs are defined as a “near - term” result or achievement that agency leaders want to accomplish within approximately 24 months through focused leadership attention .

while the guidance states that apgs can advance progress toward longer - term , outcome - focused strategic goals and objectives , the apgs are designed to be near - term improvements in outcomes , customer service , or efficiency .

even in those instances when new quantitative performance data are not available for review in meetings , more frequent in - person reviews still provide the opportunity to review goal leader progress in completing shorter - term milestones or initiatives contributing to progress on the goals , and promptly address any identified problems .

in fact , gprama and omb guidance both state that agencies should have clearly - defined , quarterly milestones to track progress on their apgs .

gprama requirement: leadership gprama requires that the agency head and chief operating officer ( coo ) conduct reviews with the support of the performance improvement officer ( pio ) .

omb guidance: leadership omb guidance directs that the agency head and / or coo must conduct reviews with the support of the pio and the pio's office .

leading practice for data - driven reviews: leadership agency leaders should be directly and visibly engaged in the reviews .

according to omb , significant experience at federal agencies , states , localities , and other countries demonstrates that in - person engagement of senior leaders in review meetings greatly accelerates learning and performance improvement .

the personal engagement of agency leaders in the review meetings also demonstrates their commitment to improvement across the agency and , as mentioned above , facilitates coordination across agency silos and rapid decision making .

as omb has also noted , frequent , data - driven reviews also send a signal throughout the organization that agency leaders are focused on effective and efficient implementation to improve the delivery of results .

gprama recognized that the direct involvement of leaders is a critical factor to drive performance improvement within an agency .

thus , it requires that agency heads and coos conduct the reviews .

omb's guidance for agencies on how to conduct the reviews also emphasized the importance of leadership involvement in data - driven performance reviews , directing the agency head , coo , or both to conduct the review .

as we have previously reported , the commitment of agency leaders to make decisions and manage programs on the basis of performance information , and inspire others to embrace such a model – which review meetings can be used to do – is critical to increase the use of performance information throughout an agency .

we found through our survey that 19 of 22 agencies that held review meetings reported that the meetings were led by their agency head or coo , or jointly by the coo and pio .

see table 3 below for specific agency responses .

furthermore , we found through our survey that review meetings are used as a tool to enhance the engagement of top agency leadership in an agency's performance management process .

in fact , all 22 agencies reported that their reviews have had a positive effect on the engagement of top agency leadership in the agency's performance management process , with 13 reporting a large positive effect .

although their exact roles varied , officials from our five selected case - study agencies reported that agency heads or coos were actively involved in their agency's review processes , and led the meetings through the following activities: focusing on agency priorities and directly communicating their expectations , asking questions , reinforcing individual and collective accountability , encouraging collaboration , offering assistance with problem solving or identifying available resources , and sharing perspectives from discussions with external stakeholders .

for example , during the review meeting we observed at gsa , the administrator and other leaders engaged in the discussion by challenging assumptions about the status of goal progress , asking questions about factors driving changes in specific performance measures , and encouraging goal leaders to identify areas of risk and plans for addressing challenges .

some case - study agency officials we interviewed stated that a key role of agency leaders in the meetings is to set a positive example by demonstrating their commitment to , and involvement in , agency performance management processes , and by communicating that participation in reviews is a priority .

for example , at ssa , the agency head personally presided over bi - monthly review meetings and created a new office ( the office of the chief strategic officer ) to support expanded performance management and data analysis efforts .

according to ssa officials we spoke with , in one review meeting the agency head brought focused attention from across the agency on a priority goal that was showing insufficient progress .

she convened a follow - up meeting requesting that offices throughout ssa articulate how they would contribute to progress on the goal .

this proved to be a useful technique for establishing a broader sense of accountability for contributions to the goal and helped identify new strategies to improve progress .

less than half of the agencies ( 8 of 22 ) reported in our survey that getting or sustaining the participation of top agency leadership in the reviews was a challenge .

however , we heard of instances in which logistical challenges can make it difficult for the agency head or coo to participate in each review meeting .

for example , a department of transportation ( dot ) official reported that leadership participation at the review meetings , which are held separately with representatives of each of dot's 10 operating administrations each quarter , was a moderate challenge because key leaders may not be able to attend due to last - minute scheduling changes or conflicts .

the agency has decided that in these situations they will continue with scheduled meetings with other leadership team members , such as the general counsel of dot , leading the meeting .

officials at dot said that ensuring the review meetings are held regularly is important because it avoids logistical challenges presented by rescheduling meetings , helps minimize preparation time by ensuring staff do not have to recreate meeting materials , encourages attendance , and leads to more productive discussions because staff are assured they will have a regular opportunities to raise issues for high - level attention .

usda , the department of defense ( dod ) , and state reported in our survey that the agency head or coo does not lead meetings that are used to review progress on apgs , as specified by gprama and omb guidance .

agriculture .

in its survey response and additional follow - up communication , usda reported that the meetings between the deputy secretary and pio that are held to discuss apg progress are led by the pio , who presents information to the deputy secretary in these meetings .

defense .

dod reported that its apgs are reviewed in meetings of the defense business council ( dbc ) , which has responsibility for the development and review of dod's performance goals .

dbc meetings , however , are led by the deputy chief management officer , who is the pio of dod , rather than the deputy secretary of defense , who is the coo .

according to meeting attendance lists shared by dod , neither the agency head nor deputy secretary / coo lead or regularly attend these reviews .

state .

state reported that its pio leads the review meeting for the one apg that state leads , and co - leads the review meeting with usaid for the one apg that is shared by the two agencies .

a state official explained that the agency feels the pio is appropriately suited for the role as leader of the review meetings as she also serves as the agency's senior budget official .

this dual role , according to the official , allows her to integrate performance with knowledge of agency resources .

neither the agency head nor the deputy secretary / coo lead or regularly attend these reviews .

these practices , however , are not consistent with omb guidance , which clearly states that agency heads or coos should conduct in - person meetings used to review progress on apgs .

leading practices emphasize that having leaders actively engaged in the reviews helps ensure that participants take the reviews seriously .

as omb has similarly noted , the involvement of coos is critical to bringing a broader set of actors together to solve problems across the organization .

therefore , because the agency head or coo does not lead review meetings at these three agencies , the review process may be viewed as less of a priority by agency officials .

this could have a detrimental effect on participation in reviews .

it could also reduce opportunities for top agency leaders to reinforce responsibility and accountability , and to personally communicate their priorities and perspective to agency managers and staff .

gprama requirement: participation of priority goal leaders and other relevant personnel gprama requires that agency leaders include agency priority goal ( apg ) leaders in their reviews and coordinate with other relevant personnel within and outside the agency that contribute to the accomplishment of each apg .

omb guidance: participation of priority goal leaders and other relevant personnel omb guidance reinforces this requirement by requiring that agency leaders include apg goal leaders , or their designees , in the reviews , along with , as appropriate , relevant personnel within and outside the agency who contribute to the accomplishment of each apg .

leading practices for data - driven reviews: participation by key personnel reviews should include personnel with programmatic knowledge and responsibility for the specific performance issues being discussed .

in addition , the participation of officials with functional management responsibilities , such as information technology , financial management , and human capital , can facilitate problem solving by providing managers from across the agency with a forum to communicate with each other .

when officials from various offices and levels of management participate in review meetings , the meetings provide opportunities to have honest , informed discussions about performance with all key players present , and facilitate collaboration and group problem solving .

officials representing their program or area of responsibility may also feel increased accountability for results when forced to report on progress in front of leadership and peers .

survey responses show that participation of pios in review meetings is strong , with 20 of 22 agencies reporting that pios always attend review meetings , and 2 reporting that their pios often attend .

see figure 1 for reported frequency of participation in review meetings by agency leadership and other key contributors .

as the highest officials dedicated to managing agency - wide performance management efforts , pios hold a unique position within their agencies and are key participants in the review meetings .

pios and agency performance staff also engage in a variety of activities that directly support successful review meetings .

through discussions with agency officials and survey results , we found that responsibilities of pios and performance staff may include overseeing preparations for review meetings , including the collection and analysis of data , creation of presentation materials , and convening preparatory meetings ; co - leading review meetings ; and managing follow - up on action items identified in review meetings .

participation by apg goal leaders in review meetings is also strong .

twenty - one out of 22 agencies reported that their goal leaders always or often participate in review meetings .

through our discussions with goal leaders we learned that they also play a key role in the review meetings , and present information on progress toward goals , respond to questions from agency leaders , identify problems or challenges and propose strategies to address them , and request support or assistance .

most agencies also reported that other key officials with responsibility for agency financial management , human capital , information technology , and legal matters attend their review meetings .

while there was variation across agencies on the frequency with which these officials participate in review meetings , 11 agencies reported that their chief financial officers ( cfo ) , chief human capital officers ( chco ) , chief information officers ( cio ) , chief acquisition officers ( cao ) , and representatives from their office of general counsel ( ogc ) always or often attend the reviews .

officials from three of our five selected case - study agencies discussed the benefits of including chief officers in their reviews .

these benefits include providing a cross - cutting agency perspective and specialized expertise to inform decisions , and offering assistance , resources , and problem solving support .

for example , one dot official described how the discussions in review meetings often focus on regulations that are under review by the office of information and regulatory affairs ( oira ) at omb , which , in some instances , reviews regulations before they can be finalized .

according to dot officials , in the department's review meetings , officials discuss the progress and plans of rulemakings with the secretary's office or ogc , such as facilitating early engagement with oira to address analytical issues .

according to dot officials , this is significant because the issuance of rules is an important tool that the department uses to promote progress toward its apgs .

for instance , in its reporting on progress toward the apg to reduce the rate of roadway fatalities , dot identified a number of proposed and final rules designed to reduce the risk of fatalities and serious injuries through enhancements to the safety of vehicles and roadways .

usda and state provided responses indicating that participation in their reviews is not fully consistent with requirements , guidance , and leading practices .

agriculture .

usda responded to our survey that apg goal leaders participate in review meetings about half of the time .

through follow - up communication , usda officials clarified that meetings between the deputy secretary and pio in which apg progress is reviewed generally do not involve goal leaders .

officials also said , however , that when the deputy secretary had specific questions on apg progress , the office of budget and program analysis ( obpa ) would schedule a follow - up meeting attended by the deputy secretary , pio , goal leaders , and , occasionally , performance staff .

usda officials also reported that their cfo , chco , cio , and cao are rarely involved in the meetings , and that the general counsel never attends .

in subsequent follow - up communication , usda officials stated that if additional information or action is needed from administrative , program , or policy officials , then the pio and obpa staff will act as a liaison , relaying questions and information between these officials and the deputy secretary .

usda officials said that given usda's large size and the complex and diverse nature of its multiple missions , it is generally easier logistically to have the pio meet with the deputy secretary , rather than trying to schedule a meeting involving additional senior officials .

state .

state responded to our survey that the cfo , chco , cio , cao , and general counsel never attend review meetings .

upon subsequent follow - up with state officials , they could not provide an example of when these officials had been invited or attended review meetings , but said that they plan to invite officials with functional management responsibilities as appropriate in the future .

not involving apg goal leaders in regular reviews of goal progress is inconsistent with gprama requirements and omb guidance ; by not doing so , usda may be missing opportunities for direct communication between agency leaders and relevant program staff about progress , challenges , and strategies for improvement .

in addition , by not regularly including officials with functional management or legal expertise , as leading practices suggest , usda and state may also miss opportunities to address performance issues in which human capital , information technology , acquisitions , or legal expertise could play a significant role in the development of solutions .

as we reported in our earlier evaluation of agency performance review meetings , omb guidance and leading practices indicate that including key players from other agencies can lead to more effective collaboration and goal achievement .

specifically , omb guidance states that agencies should include , as appropriate , relevant personnel from outside the agency who contribute to the accomplishment of an apg or other priority .

when key players are excluded from performance reviews , agencies may miss opportunities to have all the relevant parties apply their knowledge of the issues and participate in developing solutions to performance problems .

instead , agencies will need to rely on potentially duplicative parallel coordination mechanisms , which could result in less than optimal performance improvement strategies .

only two agencies , state and usaid , reported that they always or often include officials from outside the agency in their review meetings .

these are also the two agencies that hold joint sessions to review progress on their shared apg .

most agencies , however , reported that external contributors never participate in their reviews .

in february 2013 , we recommended that omb work with the pic and other relevant groups to identify and share promising practices to help agencies extend their performance reviews to include , as relevant , representatives from outside organizations that contribute to achieving their agency performance goals .

omb generally concurred with the recommendation , and in july 2014 , staff from omb and the pic told us that meetings of the pic internal reviews working group have been used to discuss the inclusion of representatives from external organizations in agency reviews , as appropriate .

in march 2015 , omb staff said that while they have found that at times it is useful to engage external stakeholders in improving program delivery , agencies view reviews as internal agency management meetings .

thus , they believe it would not always be appropriate to regularly include external representatives .

according to pic staff , the pic continues to work with agencies to identify examples where agencies have included representatives from outside organizations in quarterly reviews , and to identify promising practices based on those experiences .

as those promising practices are identified , pic staff plan to disseminate them through the pic internal reviews working group and other venues .

we will continue to monitor these efforts and periodically report on their status .

gprama requirement: review of quarterly and trend data on priority goal progress gprama requires that participants review , for each apg , progress achieved during the most recent quarter , overall trend data , and the likelihood of meeting the planned level of performance .

omb guidance: review of quarterly and trend data on priority goal progress omb guidance reinforces this requirement by directing participants to review progress achieved during the most recent quarter , overall trend data , and the likelihood of meeting the planned level of performance .

it also says that , in the reviews , agency leaders should hold goal leaders accountable for knowing the quality of their data , for having a plan to improve it if necessary , and for filling critical evidence or other information gaps .

leading practice for data - driven reviews: collecting and analyzing performance data participants in a data - driven review meeting must have up - to - date , accurate data on performance to have a meaningful discussion about progress toward goals and milestones .

the capacity to collect relevant and timely data and the ability to analyze it to identify key trends , areas of strong or weak performance , and possible causal factors are critical to successful reviews .

as we have previously reported , the capacity to collect and analyze accurate and useful data is critical to successful data - driven reviews .

the collection and analysis of valid , up - to - date performance data in advance of data - driven review meetings is necessary to ensure that the most timely data and information are used to inform discussions in meetings , and that key trends or areas of strong or weak performance have been identified .

the collection and analysis of up - to - date data for review meetings is also necessary because gprama and omb guidance require that reviews be used to review progress toward apgs .

all 22 agencies reported that they always or often collect data on apg performance measures and milestones in advance of their review meetings .

furthermore , all 22 agencies reported that they always or often analyze these data to identify key performance trends or patterns and areas of strong or weak performance .

see figure 2 below for information on the frequency with which agencies reported that they take specific data collection and analysis actions prior to their review meetings .

all five of our selected case - study agencies established processes for collecting and analyzing performance data in advance of their review meetings .

at each of the agencies , officials told us that those managing the preparation for review meetings collect updated performance data from goal leaders and their staffs .

some agencies used a standardized template to collect and organize the performance data and other relevant information about progress toward goals and milestones such as risks , challenges , and future actions .

gsa also used an online spreadsheet that offices were required to regularly update with new information on progress toward specific agency goals or milestones .

see figure 3 for a screenshot of this spreadsheet .

collecting accurate and timely data is critical for successful performance reviews , but our survey found that 19 of 22 agencies identified this as a challenge .

this finding is consistent with our previous survey of agency pios , as well as past surveys conducted by the pic , which found that the primary challenges agencies faced when implementing reviews included access to data and limitations in the capability of their data systems .

it appears , however , that the attention and scrutiny data receive through the review process can help agencies identify and address problems or limitations .

in fact , 20 of 22 agencies reported that their reviews have had a positive impact on the quality of the performance data used to track progress and inform decision making within their agencies .

according to omb staff , in february 2015 , omb and the pic also formed a cross - agency working group on data quality comprised of agency and omb representatives .

the stated objectives of the working group , which will meet through august 2015 , are to identify guidelines and practices that would improve the reliability and quality of performance data and the reporting process , and establish standards and consistency across the federal government .

we are assessing the quality of publicly reported information on apgs in selected agencies and plan to discuss this cross - agency working group in more detail in an upcoming report in the summer of 2015 .

officials from some of our five selected case - study agencies described the actions they have taken to address challenges presented by lagging or limited performance data .

for example , to track progress on the reduction of improper payments , the goal leader for ssa's improper payments apg previously received relevant data annually .

to increase the frequency with which new data are available , the pio initiated a conversation on increasing the frequency with which payment accuracy data is received and the goal leader worked with the ssa office of quality review , which collects the data , to increase the frequency to every 6 months .

an ssa official said that having more current data has given the agency a better indication , at an earlier stage , of its progress in a given year and of any impacts its actions may be having .

at hhs , a key indicator tracked for the hhs early childhood education apg is the number of states with quality rating and improvement systems that meet seven benchmarks .

the data for this indicator , however , are only available annually .

because more frequently updated data are not available , hhs officials asked the goal leader to disaggregate the data by geographic region to allow for a more granular examination of conditions and trends across regions .

the office of child care analyzed state progress toward implementing quality rating and improvement systems that met the seven benchmarks .

as part of the analysis , the office of child care identified the most common gaps in the states , and created a map that provided a visual representation of state progress toward the goal .

officials from two of our five selected case - study agencies also reported specific challenges related to their capacity to perform data analysis to inform performance management .

as we have previously reported , this helps ensure performance information is analyzed and communicated for example , an ssa official effectively , and used in a meaningful way.said that the agency had insufficient analytical capacity to perform deep , detailed analysis of data on the use of ssa services and the relationships between the use of these services and other factors .

to address this limitation , ssa created an office of performance management and business analytics to collaborate with other ssa offices to gather and analyze agency data , and to perform complex data analyses .

ssa also facilitates initiatives like an internal training program where senior data analysts train other staff .

the agency is also seeking to hire an advanced data scientist .

in april 2013 , we reported on the importance of ensuring that agency performance management staff have sufficient capacity to support performance management in federal agencies , and recommended that the director of the office of personnel management ( opm ) , in coordination with the pic and the chief learning officer council , work with agencies to identify competency areas needing improvement within agencies , and identify training that focuses on needed performance opm and omb staff agreed with this management competencies.recommendation .

in july 2014 , opm told us that it had coordinated with the pic on this recommendation , and that the pic would take responsibility for the remaining actions needed to implement this recommendation .

in march 2015 , omb and pic staff said that the pic has created a number of training programs designed to provide agency officials with information about performance management , and approaches for using performance management to improve agency performance .

the pic has also created a public website , learnperformance.gov , with informational resources on a range of topics , including measurement , data and analysis , and reporting and communicating performance information .

we will continue to monitor these efforts as training and other knowledge sharing efforts are implemented and expanded .

gprama requirement: identifying “at risk” goals gprama requires that agencies categorize agency priority goals ( apgs ) by risk of not achieving the planned level of performance .

omb guidance: identifying “at risk” goals omb guidance also directs agencies to identify apgs ( or other priorities ) at risk of not achieving the planned level of performance and work with goal leaders to identify strategies that support performance improvement .

it also directs them to review variations in performance trends across the organization and delivery partners , identify possible reasons for the variance , and understand whether the variance points to promising practices or problems needing greater attention .

leading practice for data - driven reviews: rigorous preparation rigorous preparation is critical for effective performance reviews , as key participants must be prepared to discuss issues related to their performance and progress toward goals .

after data have been collected and analyzed , they must be effectively communicated to participants .

following the completion of data collection and analysis , offices responsible for supporting reviews will often compile summary materials to help leaders and participants prepare for the reviews .

these efforts to ensure that participants are aware of the status of goals and milestones , and key questions likely to be raised and discussed in the meetings , can also be critical to the success of reviews .

as we have also previously reported , frequent and regular communication of performance information is also critical to remind agency officials of their commitment to achieve the agency's goals , and to keep those goals in mind as they pursue their day - to - day activities .

it also helps ensure that leaders and managers have opportunities to review information in time to take action to make improvements .

all 22 agencies reported that they always or often develop presentation slides or other meeting materials to communicate key data and analyses to participants .

furthermore , all 22 agencies also reported that they always or often distribute these materials to participants for review before the meetings .

see figure 4 below for information on how frequently agencies report they take specific actions to prepare for review meetings .

all five of our selected case - study agencies developed presentation slides , or other meeting materials , and distributed them to participants in advance of their review meetings .

in addition to presenting information on progress toward agency goals and milestones , meeting materials may also include discussions of key strategies and initiatives being employed to influence progress , and any risks , challenges , or opportunities those managing the goals are facing .

in accordance with the gprama and omb requirement that agencies categorize apgs by risk of not achieving the planned level of performance , materials produced for meetings at all five of our selected agencies included information or color - coded graphics to indicate the likelihood a goal will be achieved and whether a goal is “off track” or “at risk.” for two examples of materials prepared for review meetings at ssa and hhs , see interactive figures 5 and 6 .

fourteen of 22 agencies reported that they always or often held a preparatory session to review the agenda , data , and key discussion points with participants before their review meetings .

officials from our five selected case - study agencies described preparatory meetings that officials from their agencies hold in advance of review meetings .

officials from some of the five agencies also described how these preparatory sessions can be valuable , as they allow agency leaders and goal leaders to familiarize themselves with the data and discuss responses to potential questions with knowledgeable staff .

general services administration .

two of the agency's bureaus , the public building service and the federal acquisition service , hold regular meetings where managers from each service review and discuss performance data presented later at the agency - level performance review meetings .

officials see these meetings as not only preparation for the agency - level review meetings , but as vital to effectively managing the business of the services and making progress toward identified goals .

social security administration .

to prepare for the quarterly review meetings with the acting commissioner of ssa , the chief strategic officer / pio meets with ssa's deputy commissioners , goal leaders , and appropriate staff to discuss progress toward apgs , the status of efforts being employed to achieve them , and the order in which goals should be discussed in the quarterly review .

this preparatory meeting is held 10 days before the quarterly review meeting is scheduled to be held .

five days before the quarterly review meeting , the chief strategic officer / pio meets with the acting commissioner to prepare for the quarterly meeting .

at this meeting , they discuss goal progress and trends , issues to be discussed during the review meeting , and potential questions the acting commissioner could ask .

materials prepared by the apg goal teams for the quarterly review are sent to the acting commissioner 48 hours in advance of this preparatory meeting .

transportation .

an official from the federal railroad administration ( fra ) explained that , prior to fra's review meetings with the deputy secretary of transportation , staff hold briefings for the fra administrator and each associate administrator .

this official said that these preparatory meetings for fra leadership are valuable because they allow fra leadership to ask questions of knowledgeable staff to better understand the data and information they will ultimately present at the review meeting with dot leadership .

gprama requirement: review of priority goal progress and identification of improvements gprama requires that agency leaders review progress on agency priority goals ( apgs ) ; assess whether relevant organizations , programs , regulations , and policies are contributing as planned ; and identify strategies for performance improvement for those goals at greatest risk of not achieving their planned levels of performance .

omb guidance: review of priority goal progress and identification of improvements omb guidance reinforces this requirement by directing agency leaders to use in - person review meetings to review progress on apgs ; hold goal leaders accountable for knowing whether their performance indicators are trending in the right direction and , if not , having a plan to accelerate progress on the goal ; identify apgs or other priorities at risk of not achieving planned levels of performance ; and work with goal leaders to identify strategies that support improvement .

leading practice for data - driven reviews: accountability agency leaders should use review meetings to hold goal leaders and other responsible managers accountable for knowing the progress being made in achieving goals and , if progress is insufficient , understanding why and having a plan for improvement .

as mentioned throughout this report , a fundamental purpose of data - driven review meetings is to provide a mechanism for agency leaders to assess an agency's progress on key goals and milestones ; analyze and discuss data to identify goals at risk , performance problems , and improvement opportunities ; and ensure that goal contributors are held accountable for their performance .

through our survey , we found that most agencies reported they always or often use their review meetings to assess progress and contributions , and identify goals at risk .

however , as shown in figure 7 below , there is some variation reported across the 22 agencies on the frequency of specific types of actions taken during review meetings .

assessing progress on apgs .

reviewing progress on apgs on a regular and ongoing basis is a key requirement of gprama , and helps ensure that agency leaders , goal leaders , and other contributors have frequent opportunities to review recent progress and trends .

twenty of 22 agencies reported that their data - driven review meetings are always or often used to review progress on apgs , including recent progress , overall trends , and the status of related milestones .

analyzing data to identify goals at risk and hold goal leaders accountable .

gprama also requires that agencies identify and categorize goals at risk of not achieving the planned level of performance .

twenty - one of 22 agency pios reported that their review meetings are always or often used to identify goals at risk , and to hold goal leaders accountable for explaining why the goal is at risk , as well as strategies for performance improvement .

discussing contributions of program activities , policies , and regulations and whether they should be changed to improve their impact on priority goals .

assessing the contributions that organizations , program activities , policies , and regulations are making toward the achievement of goals is another critical part of efforts to use review meetings to ensure accountability for the completion of commitments , and to identify potential problems , effective practices , or strategies for improvement .

gprama requires that agencies include these assessments as part of their reviews .

twenty of 22 agencies reported always or often discussing whether specific organizations or program activities were contributing as planned to priority goals , and 18 of 22 reported always or often discussing the contributions of relevant policies toward priority goals .

in this way , data - driven review meetings can be used to reinforce the alignment of higher - level agency goals with the milestones and day - to - day activities of program officials contributing to each goal .

however , only 13 of 22 agencies reported always or often discussing whether program activities , policies , and regulations should be changed to improve their alignment with priority goals .

ssa officials described how they have used their review meetings to discuss the contributions of programs , policies , and regulations , and necessary changes .

for example , ssa has an apg to expand the use of video technology to hold benefit determination hearings .

according to ssa officials , initial discussions on this goal in quarterly review meetings identified the challenge that those scheduled for video hearings were opting out at the last minute , which led to unpredictable schedules and down time for administrative law judges .

to address this issue and help achieve the broader goal to expand the use of video hearings , the agency determined that a regulatory change was needed to require claimants to decline a video hearing within 30 days after the date the claimant receives notice that the agency may schedule the claimant to appear at a hearing by video teleconferencing .

this change was designed to decrease last - minute hearing cancellations and help them more efficiently schedule video hearings .

the milestones that were developed to track progress on the development and implementation of the regulatory change were regularly discussed in the quarterly meetings , which is one of the factors that led to the agency working with omb to expedite the release of the regulation .

ssa believes that this june 2014 regulatory change will have long - term benefits .

however , ssa has acknowledged that in the short term they may receive more opt - outs due to the 30-day notice requirement .

for this reason , they are tracking the opt - out rate for video hearings to measure the impact of the new regulation , and are reviewing these data in their quarterly meetings .

in the quarterly review meeting we observed after the regulation had been implemented , participants discussed several potential consequences of the regulation , including the potential for an increase in the opt - out rate , and possible strategies for addressing them , such as additional regulatory or process changes .

some agencies reported through our survey that they take certain actions during review meetings about half of the time or less frequently .

for example , the department of labor ( labor ) reported that it reviews apg progress about half the time in review meetings .

however , a labor official explained through follow - up communication that it responded this way because each quarter it holds performance review meetings for each of its 16 components and not all components have responsibility for one or more of the apgs .

therefore , apg progress is discussed only during review meetings for components that contribute to apgs .

the agency specified , however , that each quarter it reviews progress on each of its apgs .

three agencies — the departments of energy ( energy ) and health and human services , and the national science foundation ( nsf ) — reported that they rarely discuss whether program activities , policies , and regulations should be changed to improve their alignment with priority goals in review meetings .

two other agencies — the department of defense ( dod ) and the national aeronautics and space administration ( nasa ) — reported that they never hold these discussions .

through follow - up , officials from energy , hhs , nasa , and nsf clarified their responses to this question and explained how their review meetings were used to identify and discuss weaknesses or risks that could impact the achievement of their goals , and discuss suggestions for improvement .

energy .

while energy's quarterly review meetings were used to discuss apgs identified as “off track,” and review future plans and milestones for these goals , an agency official said that other goal - specific meetings were held to drive action on at - risk goals .

the quarterly review meetings also served as an opportunity for senior leaders not involved in the other meetings to discuss goal progress and actions being taken to improve efforts in those areas that are off track .

health and human services .

an hhs official stated that the agency's response was due mainly to the fact that review meetings are generally not used to discuss regulatory changes , with some exceptions , such as reviews held for the health information technology apg .

instead , the official said that discussions in hhs review meetings are focused primarily on improving progress on apgs through better implementation and execution of program activities and other management initiatives .

hhs leaders that attend the review meetings , however , may use the information gained to inform decisions on longer - term policy or regulatory changes .

nasa .

a nasa official stated that the agency's apgs are closely aligned with specific agency programs and projects , and that monthly data - driven performance review meetings are used to discuss potential cost , schedule , technical , and programmatic risks to meeting their milestones , as well as strategies for improving performance .

nasa does not , however , discuss realigning or changing programs or policies to meet those milestones .

for example , in their quarterly reviews of the james webb space telescope program , participants have had discussions of actions the program will undertake to meet its milestones , but not of changing the program , or of reassigning this work to a different agency program , as the program is the only one with the capability to implement the work .

national science foundation .

nsf officials stated that they have no relevant regulations to discuss in the agency's review meetings , but nsf officials do discuss potential changes to program activities and policies at review meetings as the need for program or policy changes become apparent , which is generally about half of the time .

for example , nsf has an apg to improve the nation's capacity in data science by focusing nsf investments in human capital , partnerships , and infrastructure that support data science .

initial plans for this apg were set at the time the goal was established and expressed as a series of quarterly milestones .

the timing of achievement of these milestones is occasionally altered , and nsf review meetings have been used to discuss these changes .

in one such recent change , nsf officials originally planned to support big data regional innovation hubs in fiscal year 2014 , but decided to gather more community input to increase the specificity and quality of its proposals .

request for comments was published in the federal register , with a public comment period ending november 1 , 2014.according to an nsf official , the submissions to the request were used to refine the solicitation for big data regional innovation hubs that was subsequently released in the second quarter of fiscal year 2015 .

these timing changes were presented at each review meeting and discussed as necessary .

big data regional innovation hubs are designed to be consortiums of members from academia , industry , and government that would foster collaboration amongst partners , and focus on key big data challenges and opportunities in their regions of service .

our survey that participants review progress on apgs only about half the time , rarely identify goals at risk , and never discuss whether program activities , policies , or regulations should be changed to improve their alignment with priority goals .

this is consistent with our own review of documentation from dbc meetings , which indicated that a review of apgs was not always included on the agenda .

in those instances when apg progress was reviewed , the information on apg progress included in meeting materials was limited .

for instance , materials prepared for some meetings had only one slide with an aggregate count of how many apgs were on or off track , and limited information on the status of individual apgs .

without information on the status of individual apgs , dod's review meetings are unlikely to foster meaningful discussions about progress and trends .

further , if these review meetings are not regularly used to assess progress on individual apgs , and to identify at - risk goals and potential improvements , it could mean missed opportunities for dod to address performance problems or accelerate progress .

dod officials informed us , however , that over the next year , they plan to revise their review process to ensure they conduct regular , quarterly reviews of apg progress that involve discussions on progress achieved in the most recent quarter , performance trends , and status of related milestones ; discussions of potential organization , program activity , policy , or regulatory changes to improve alignment with , and impact on , priority goals ; and the identification of at - risk goals .

omb guidance: follow - up omb guidance directs agency leaders to agree on follow - up actions at each review meeting and track timely follow - through .

leading practice for data - driven reviews: follow - up rigorous and sustained follow - up on issues identified during meetings , including the identification of the individual or office responsible for each follow - up action , is critical to ensure the success of reviews as a performance improvement tool .

identifying and agreeing upon actions that need to be taken following a review meeting , and rigorously tracking the status of these actions to completion , is a key element of omb guidance as well as a leading practice .

rigorous follow - up is also critical to the overall success of reviews as a tool for addressing identified deficiencies and improving performance .

according to our survey results , most agencies reported that they are generally taking steps to identify and follow up on action items identified in review meetings .

however , this is an area where our survey indicated there is less consistency in how frequently agencies are employing specific practices .

figure 8 shows the frequency with which agencies reported conducting specific follow - up actions .

the variation in how systematically agencies identify and follow - up on action items from review meetings is also illustrated by the different approaches that our five selected case - study agencies reported using to identify and follow - up on action items , which are described in table 4 .

the analysis of responses to our survey indicated that there is a statistically significant , positive correlation between the frequency with which an agency identifies and agrees on specific follow - up actions and the perceived impact of review meetings on performance improvement .

specifically , as shown in figure 9 , all 13 agencies that reported that their review meetings have had a major impact on performance improvement also always or often identify and agree on follow - up actions during review meetings .

agencies that reported their review meetings have had a minor impact on performance improvement reported identifying and agreeing on follow - up actions during review meetings less frequently .

similarly , our analysis found that a statistically significant , positive correlation exists between the frequency with which an agency uses its review meetings to review the status of follow - up actions from the previous meeting , and the perceived impact those reviews have on performance improvement .

these findings are consistent with surveys of agency pios administered in the past by the pic .

these surveys found that agencies where reviews have had a major impact on agency performance are more likely to document specific action items with clear owners and due dates , and review follow - up actions from previous meetings .

while omb guidance and leading practices are clear that participants in each review meeting should agree on follow - up actions and track follow - through , four agencies – dod , energy , nsf , and the small business administration ( sba ) – reported through our survey that they identify and agree on specific follow - up actions about half the time or less frequently .

through follow - up with energy , sba , and nsf , officials from those agencies further explained the actions they are taking , or have taken , to identify , document , and track follow - up items , are consistent with omb guidance .

energy .

energy reported through our survey that participants identify and agree on specific follow - up actions in quarterly review meetings about half of the time .

according to an energy official , however , in instances where follow - up actions are identified , those items are documented in a “summary of actions.” in addition to using quarterly review meetings to identify follow - up actions , the official stated that other topic - specific meetings are used to identify and address follow - up items for specific apgs .

for example , the summary of actions from energy's august 2014 quarterly review meeting indicated that the deputy secretary would hold a meeting with a specific agency official to review the off - track status of a priority goal in more detail .

small business administration .

sba reported through our survey that participants would rarely identify and agree on specific follow - up actions to be taken after meetings .

during the course of our review , however , sba officials instituted changes to the agency's review processes as a result of new leadership , and have given the sba office of performance management responsibility for ensuring that all action items from their review meetings , as well as “key takeaways” for discussion at the next review , are recorded .

national science foundation .

nsf reported through our survey that participants rarely identify and agree on specific follow - up actions .

however , nsf officials stated that they chose this response because their goals are based primarily on the achievement of milestones and goal teams have already outlined the specific actions they will be taking in goal documentation .

the status of actions to complete each of these milestones is then reviewed in each review meeting .

nsf officials also said that in the event a follow - up action or course correction is identified in a quarterly meeting , the status of these actions will be discussed in bi - weekly meetings between the pio and coo , who determine whether the actions have been adequately addressed or whether additional steps are required .

in contrast , dod reported through our survey that participants in review meetings rarely identify and agree on specific follow - up actions .

after subsequent follow - up with the agency , we found that dod practices are not consistent with omb guidance or leading practices .

through our review of documents from dod review meetings , we also found there was no information included in materials prepared before , or after , these meetings to indicate that they are used to identify follow - up actions related to apgs .

in our follow - up communication with them , dod officials acknowledged the need to regularly identify follow - up actions , and informed us that over the next year they plan to integrate the identification of specific follow - up actions into their reviews .

clearly identifying and documenting follow - up items , identifying the individual or office responsible , and monitoring their status are important to ensure that agreed upon actions are taken after dod's review meetings .

this is supported by the results of our analysis , which showed that systematically identifying and following up on action items is associated with review meetings having a greater impact as a performance improvement tool .

furthermore , a failure to clearly identify and document follow - up actions may lead to a situation at dod in which there is no commonly - held list of specific actions that will be taken after review meetings , and a limited ability to hold accountable those responsible for the completion of action items .

the results of our survey on agency data - driven review practices indicate that review meetings have had positive effects on progress toward agency goals , collaboration between agency officials , the ability to hold agency officials accountable for progress toward goals , and the ability to identify opportunities to improve agency operations .

coos , pios , apg goal leaders , and staff that we spoke with at the five selected agencies reinforced these findings , and also shared examples that illustrate the positive effects their data - driven review meetings are having in these areas .

nearly all agencies reported that their data - driven review meetings have had a positive effect on progress toward the achievement of agency goals , and on their ability to identify and mitigate risks to goal achievement .

as illustrated in figure 10 , all 22 agencies reported that their reviews have had a positive effect on progress toward their apgs , and 21 of 22 reported that their reviews have had a positive effect on their agency's ability to identify and mitigate risks to achieving priority goals .

in our discussions with officials from selected agencies , data - driven review meetings were described as venues for agency leaders and managers to assess progress toward key goals and milestones , the status of ongoing initiatives and planned actions , potential solutions for problems or challenges hindering progress , and additional support or resources needed to improve performance .

agency officials emphasized that discussions in their review meetings tend to focus on those goals or issues most in need of attention , where the achievement of a goal or milestone is at risk .

in this way , reviews can serve as early warning systems and facilitate focused discussions on external , technical , or operational obstacles that may be hindering progress , and the specific actions that should be taken to overcome them .

for example , ssa has an apg to increase the number of registrations for its my social security portal by 15 percent per year in fiscal years 2014 and 2015 .

in 2014 , however , through the review of data for ssa's third quarter review meeting , it became apparent to ssa leadership that the agency was not on track to achieve its target for this goal .

according to officials , as part of the quarterly review process agency officials completed a more thorough examination of reasons for this and found that the agency would not be able to complete the development of additional features , such as the ability to request a replacement social security card , which were expected to drive higher volumes of traffic to the portal .

understanding these limitations , ssa's focus shifted to what could be done by offices throughout the agency , using currently available or attainable resources and technology , to support efforts to increase the number of registrations .

to achieve this , ssa leadership had different offices within the agency , including communications , policy , and budget , specify the contributions they would make to help increase the number of registrations .

for example , the office of communications developed a document outlining 26 activities the office was taking , or planned to take , to promote my social security to potential users .

since then , the agency's quarterly review meetings have been used to review and reinforce the commitments each office made .

in the quarterly review meeting that we observed , a representative of ssa's communications office emphasized that supporting efforts to increase my social security registrations is the office's top priority , and discussed an ongoing national marketing campaign , and marketing activities targeted to advocates in the aging and disability communities and third party tax preparers .

while ssa was unable to meet the registration goal for fiscal year 2014 , according to ssa officials , these efforts recently undertaken as a result of the review process have helped generate an increase in registrations .

data from ssa's fiscal year 2015 first quarter review show that there was a 46 percent increase in new account registrations in october 2014 compared to the number of new registrations in october 2013 , and a 26 percent increase in december 2014 relative to december 2013 .

many agencies reported that they are also using their review meetings to review progress on a broader suite of performance goals that go beyond the requirement to review apgs .

nineteen of 22 agencies reported that they always or often discuss progress on agency - wide goals or initiatives beyond the apgs in their review meetings , while 20 of 22 agencies reported that reviews have had a positive effect on their progress toward the achievement of other performance goals .

for example , according to a gsa official , a long - standing challenge of the public buildings service ( pbs ) has been finalizing occupancy agreements in a timely fashion.2014 , agency leadership made improving performance in this area a specific goal for pbs , which was then often discussed during gsa's review meetings .

according to gsa officials , due to the increased attention on the status of goal progress and leadership commitment to improving performance , the agency has surpassed its goals in this area .

according to gsa's performance report , in fiscal year 2014 , the agency improved the on - time activation of occupancy agreements in owned space to 98 percent and leased space to 90 percent , exceeding the targets of 90 percent in owned space and 82 percent in leased space .

this is also an improvement from the on - time activation rates of 86 in percent for owned space and 75 percent for leased space in fiscal year 2013 .

twenty - one of 22 agencies reported that their data - driven reviews have had a positive effect on collaboration between officials from different offices or programs within the agency .

similarly , agency officials with whom we spoke emphasized that review meetings provide opportunities to bring together the people , analytical insights , and resources from across an agency that are needed to improve progress on agency priorities and to address any identified performance problems or challenges .

as we heard from agency officials , and summarized in figure 11 , bringing leaders and officials from across an agency together regularly to focus on shared goals and milestones can establish a shared sense of purpose , encourage ongoing collaboration , and reduce organizational silos .

the review meetings also serve as action - forcing events that provide an opportunity for officials from across an agency to develop and implement collaborative solutions to identified problems .

these insights into the positive effects review meetings can have on collaboration within agencies also reinforce their potential value as a tool for promoting increased collaboration across agencies .

as noted above , this should encourage those who lead and manage agency reviews to follow omb guidance on the issue and be mindful of opportunities to leverage reviews to involve relevant stakeholders from external agencies or organizations .

department of health and human services ( hhs ) officials reported that promoting collaboration between different offices that contribute to individual apgs has been one of the most important effects of their reviews .

they also emphasized that reviews have been used to bring apg contributors from across hhs together to discuss what can collectively be done to support progress on the goals .

for example , hhs has an apg to increase the number of eligible providers who receive incentive payments for the successful adoption or demonstration of meaningful use of certified electronic health record ( ehr ) technology .

according to hhs officials , the goal requires a great deal of coordination between the office of the national coordinator for health it ( onc ) and the centers for medicare & medicaid services ( cms ) .

an hhs official involved in these efforts explained that the two offices realize that , given their shared ownership of the goal , they are expected to coordinate effectively , and the reviews are used to reinforce this expectation and ensure that ongoing coordination is occurring .

one way this has manifested itself is in the improved data sharing arrangement that now exists between onc and cms .

under this arrangement , cms collects data related to the medicare and medicaid ehr incentive programs , which provide financial incentives for the “meaningful use” of certified ehr technology by health care providers .

according to onc officials , the review process has helped encourage more regular data sharing between cms and onc , with onc receiving monthly updates on ehr incentive program registration , attestation , and payment data from cms .

the team supporting the priority goal is now using these data to conduct ongoing evaluations of the characteristics of providers at different stages in the program .

according to an onc official , data on program participation are also shared by onc and cms during monthly presentations to the federal advisory committees on health it , which illustrates how they are also using these data to facilitate partnerships and public - facing discussions with other stakeholders .

similarly , officials from gsa stated that they believe that the most significant effect of the agency's review meetings has been to enable collaborative problem solving , where ideas for potential solutions can be freely shared , and officials can request assistance from their colleagues in offices throughout gsa .

for example , as part of a 2013 reorganization , the office of administrative services ( oas ) was given responsibility for the management of gsa - occupied real estate .

according to gsa officials , through review meetings agency leaders identified that regional offices were estimating project costs and footprints using different assumptions .

oas was able to partner with pbs , which had experience working with other agencies to estimate project costs and footprints , to leverage their experience in the consideration of internal agency real estate needs .

this led to the creation of a new streamlined process for project plans , with the head of oas reviewing all plans to ensure they conform to consistent standards .

agency leaders said this coordination likely would not have happened if the two offices had remained siloed in their approaches , as they were before they began coordinating in gsa's regular review meetings .

having regular review meetings that require goal leaders and other contributors to report out on their progress , and respond to direct questions about the actions that they are taking , provide leaders with important opportunities to clarify and reinforce responsibilities , motivate actions necessary to complete milestones or improve performance , and hold goal leaders and managers accountable .

the involvement of agency leaders in review meetings also helps ensure that the meetings are taken seriously and viewed as a priority .

twenty - one of 22 agencies reported that their data - driven reviews have had a positive effect on their agency's ability to hold goal leaders and other officials accountable for progress towards goals and milestones.according to officials from selected agencies , the transparency of performance information , and a review process that ensures it receives appropriate scrutiny , produces an increased sense of accountability for results .

several agency officials emphasized that it is important for those leading the meetings to establish a constructive , solution - oriented environment in which officials can be open and honest about their progress , and any problems and challenges they are facing .

this outlook is also reinforced by omb guidance , which directs agency leaders to establish an environment that promotes learning and openly sharing successes and challenges .

at the same time , while agency leaders must maintain an environment in which participants feel comfortable raising problems and challenges , officials we spoke to also emphasized that leaders must use the meetings to hold goal leaders and managers accountable for having well - thought - out strategies for how to overcome them or mitigate their impact .

leaders from commerce said that they are using their regular review meetings with bureau heads and goal leaders to support a cultural change throughout the agency and to reinforce accountability for performance at multiple levels of the organization .

this change is one that emphasizes regular and ongoing reviews of performance , accountability for the completion of action items , more frequent and regular follow - up through review meetings , and an increased urgency and pace of implementation .

for example , in 2014 , commerce established an apg designed to increase the percentage of companies assisted by the global markets ( gm ) program that achieve their export objectives.commerce officials , the decision to focus on this new measure , which places the focus on the satisfaction of clients , came out of input from gm clients and staff , as well as discussions in departmental review meetings about the need to clarify the objectives of the program and improve the quality of assistance that trade and commercial specialists provide .

the key measure used to track progress on the priority goal was designed to help drive changes in internal processes and behaviors by focusing more clearly on activities and outcomes gm staff are able to directly influence .

now , at the beginning of their interaction with a new client , gm staff ask client businesses to define their needs and what they want to achieve with the support of the program .

this shift toward a more consultative approach was designed to encourage staff to work with clients to establish goals and expectations , design solutions responsive to those goals , and identify potential challenges .

commerce has also instituted new data collection procedures for the goal .

it now collects data on apg performance on a regular , weekly basis , and in a way that will allow for comparisons across regions .

the gm office of strategic planning sends out weekly updates to leaders and managers with data on how each region , and gm as a whole , is performing against weekly and annual targets for a number of key metrics .

these data on performance are also being reviewed by gm leaders in monthly review meetings to see how regions are performing in relation to one another and against established targets , and to identify challenges that offices are experiencing and effective practices that could be more widely shared .

according to commerce officials , progress on the apg is discussed in regular review meetings at the bureau and departmental levels .

for example , the deputy under secretary for international trade , who oversees the daily operations of the international trade administration ( ita ) , meets regularly with the apg goal leader to discuss performance on the goal .

progress on the goal is also discussed in monthly meetings of the ita management council , which consists of the senior leaders of each of ita's three business units .

finally , progress toward , and the management of , the apg is also discussed in department - level review meetings .

here , commerce leaders provide apg goal representatives with specific feedback and guidance , as well as information on other agency - wide initiatives that could impact the ability of gm staff and leadership to appropriately manage the program .

according to a commerce official , the priority goal and associated measures , data collection process , reviews at multiple levels , and a system that holds staff accountable for identifying what clients want to achieve and working to deliver on those expectations have provided an increased sense of accountability and strong incentives for behavioral change amongst staff within the program .

commerce reported that in the first quarter of fiscal year 2015 , 73 percent of gm clients reported that they have achieved their export objectives , exceeding the current target goal of 71 percent .

seventeen of 22 agencies reported that their data - driven reviews have had a positive effect on the efficiency of agency or program operations.some of our selected agencies are using their review meetings as an opportunity to review the status of management improvement initiatives and to improve the efficiency of business operations .

for example , dot's review meetings have been used to uncover and correct inefficiencies in its hiring process , which involves multiple offices throughout dot .

according to a federal railroad administration ( fra ) official we spoke with , staff were able to calculate how many days it took to complete each step in its hiring process and identify which offices were responsible for each step .

the increased scrutiny this issue received through dot's review meetings led to improvements in the average number of days it now takes to hire a new employee .

for example , for fra time - to - hire decreased from approximately 160 days in fiscal year 2012 to 77 days in the third quarter of fiscal year 2014 .

agency officials also emphasized that review meetings that bring together leaders and officials from across an agency can increase the overall efficiency of an agency's decision making processes .

these meetings allow leaders and managers to discuss and respond to ideas , ask questions , voice concerns , and immediately make decisions on how to move forward .

if these review meetings did not exist , officials explained they would likely need to hold separate meetings , with questions and answers moving up and down the agency's lines of communication .

having all key players present in one meeting makes this entire process more efficient and allows for more timely action .

as reported through our survey and discussions with agency officials , data - driven review meetings can improve agency performance and results by increasing leadership oversight and management capacity to use performance information , focusing attention on goals and priorities , identifying areas where targeted improvements are needed , and improving communication and collaboration across an agency .

our survey data suggested , however , that sustaining a data - driven review process over time and across leadership transitions can be a challenge for agencies .

ten agencies reported that it has been a challenge to continue holding reviews despite turnover of agency or priority goal leadership , with 2 reporting it has been a great challenge , 2 a moderate challenge , and 6 a small challenge .

through our discussions with agency officials , we learned that sustaining review meetings and their positive effects requires ongoing leadership commitment and involvement , the institutionalization of review practices , and the development of a broad base of support for the reviews through a shared appreciation for the positive effects that review meetings produce .

these factors build on one another , as agency leaders , participants , and those supporting the reviews engage in a cycle of ongoing actions , as shown in figure 12 .

first , agency officials emphasized that agency leaders' commitment to lead , support , and remain involved in the reviews is a key element that must be in place for reviews and their positive effects to be sustained over time .

the experiences of our selected agencies show that active involvement by agency leaders in review meetings is critical to establish the importance of the meetings and the clear expectation that other agency officials participate in them when called to do so .

furthermore , through their ongoing involvement in the reviews and their communication with other participants , leaders must reinforce that data - driven review meetings remain a priority and are seen as a valuable tool to achieve key agency objectives .

second , agency officials indicated that reviews in which agency leaders critically assess data on performance , and where follow - up actions are identified and tracked , should be institutionalized and made a routine part of the agency's operations .

through gprama , congress took a critical step to help ensure agencies do this by requiring that agency leaders agencies have also taken conduct reviews frequently and regularly.several specific steps to institutionalize their review processes .

for instance , agency officials gave existing or newly established offices responsibility for supporting the review process and ensuring that meetings are carried out with regularity and consistency over time .

staff in these offices generally manage data collection , meeting preparation , and follow - up , offer training for staff , and provide analytical expertise that helps support and inform the discussion in review meetings .

agencies also established clear expectations and procedures for how reviews would be carried out , including processes for preparation and follow - up .

institutionalizing review processes in this way , and providing sufficient institutional and staff support to manage them , can also facilitate the maintenance of review processes across leadership changes , as the process is made less dependent on the management style of a particular leader .

as the cycle continues , officials also noted that agencies need to continuously assess their review processes and address any identified weaknesses by incorporating improvements that respond to the needs of leaders and participants .

while it is important to have a basic approach that persists over time , the ongoing assessment and improvement of review processes will help ensure reviews are adapted to meet the needs of new leaders and participants and continue to be used in a way that sustains their positive effects .

for example , commerce and ssa have recently changed their review meetings and processes to strengthen the focus on the agencies' respective strategic goals .

officials with dot and gsa also indicated that they are considering and instituting changes to their reviews .

according to dot officials , the agency formed a working group that examined ways to more effectively structure dot's review meetings .

the group has also made recommendations to the deputy secretary regarding the format of the meetings , participation , the presentation of data , and the best ways to follow up on identified action items .

the recommendations are currently under review by dot senior leadership .

at gsa , under a new acting administrator and acting deputy administrator , additional monthly and quarterly reviews of priority initiatives and apgs are being instituted , while weekly review meetings have been refocused on a subset of gsa's key performance measures and initiatives that are most ambitious or most in need of assistance and focus .

third , agency officials emphasized that it is critical for those leading , managing , and participating in the reviews to assess , understand , and communicate the results that review meetings produce to help develop a broad base of support throughout an agency for sustaining review processes over time .

according to a number of agency officials we spoke with , for agency leaders and managers — both political appointees and career agency officials — to maintain their commitment to review processes , the reviews must demonstrate that the benefits they provide outweigh the costs in time and resources spent .

if the reviews show positive results , add value for participants , and have the support of senior political and career leaders who are able to articulate their merits , this will help sustain organization - wide commitment and increase the likelihood that reviews will be continued following leadership transitions .

most federal agencies reported conducting data - driven reviews frequently and regularly , involving agency leaders and other key personnel , and using the process to assess progress on agency goals and identify strategies to address challenges or improve performance .

these practices are consistent with requirements , guidance , and leading practices .

our findings also underscore the value of conducting frequent , in - person , data - driven reviews as a leadership strategy and management practice that can promote the use of performance information by agency officials and produce improved results .

our survey results indicated that agency data - driven review meetings enhanced their progress toward the achievement of agency goals , the engagement of agency leaders in the performance management process , the level of collaboration between agency officials , the ability to hold agency officials accountable for progress on goals and milestones , and the efficiency of agency operations .

through this work , however , we found that dhs was not holding in - person , data - driven reviews of its apgs , and that four other agencies were conducting reviews in a manner that was not consistent with requirements , guidance , or leading practices .

we also found that data - driven review meetings should be held on a regular , frequent schedule , actively involve senior agency leaders or other key officials , involve in - depth reviews of progress on agency goals , and be supported by rigorous methods for identifying and tracking follow - up actions .

otherwise , there could be missed opportunities for these agencies' leaders to hold officials accountable for progress toward identified goals and milestones , to take timely and better informed action to address identified challenges , and to encourage continuous improvements in agency performance and operations .

to help ensure that agency review processes provide frequent , regular opportunities to assess progress on agency priority goals ( apg ) , and are conducted in a manner consistent with gpra modernization act of 2010 ( gprama ) requirements , omb guidance , and leading practices , we recommend the following actions: that the secretary of agriculture work with the coo and pio to modify the department's review processes to ensure that review meetings: ( 1 ) are held at least quarterly ; ( 2 ) are led by the agency head or coo ; ( 3 ) involve apg leaders ; ( 4 ) and involve , as appropriate , agency officials with functional management responsibilities .

that the secretary of defense work with the coo and pio to modify the department's review processes to ensure that review meetings: ( 1 ) are led by the agency head or coo ; ( 2 ) are used to review progress on all apgs at least once a quarter , discuss at - risk goals and improvement strategies , and assess whether specific program activities , policies , or other activities are contributing to goals as planned ; and ( 3 ) are used by participants to identify , agree upon , document and track follow - up actions .

that the secretary of health and human services work with the coo and pio to modify the department's review process to ensure that progress on each apg is reviewed in an in - person review meeting at least quarterly .

that the secretary of homeland security work with the coo and pio to reestablish regular , in - person , data - driven review meetings conducted in a manner consistent with the requirements of gprama , omb guidance , and leading practices outlined in this report .

that the secretary of state work with the coo and pio to modify the department's review processes to ensure: ( 1 ) that progress on each apg is reviewed in an in - person review meeting at least quarterly ; ( 2 ) and that the reviews are led by the agency head or coo ; and ( 3 ) involve , as appropriate , agency officials with functional management responsibilities .

we provided a draft of this report for review and comment to the secretaries of agriculture , commerce , defense , education , energy , health and human services , homeland security , housing and urban development , interior , labor , state , transportation , treasury , and veterans affairs ; the attorney general of the united states ; the directors of the office of management and budget , the office of personnel management , and the national science foundation ( nsf ) ; the administrators of the environmental protection agency , national aeronautics and space administration ( nasa ) , small business administration ( sba ) ; the acting administrators of the general services administration ( gsa ) and the u.s. agency for international development ( usaid ) ; and the acting commissioner of social security .

we received written comments from the departments of defense ( dod ) , health and human services ( hhs ) , homeland security ( dhs ) , and state , and the social security administration ( ssa ) .

these responses are reproduced in appendixes iv through viii .

the department of agriculture ( usda ) provided its response in an email transmitted on june 17 , 2015 .

dhs , hhs , and usda concurred with our recommendations .

dod and state concurred with all but one recommendation — to ensure the coo leads the reviews — with which they partially concurred .

in its response , dod concurred with our recommendations that agency leaders modify the agency's review process to ensure that reviews are held to assess progress on apgs at least once a quarter , to identify goals at risk , to assess contributions , and to identify and track follow - up actions .

dod's response said that the agency plans to comply with the recommendations by november 30 , 2015 .

dod partially concurred with our recommendation that the agency ensure the reviews are led by at least the coo .

dod , in its response , outlined a more specific role for its coo in future reviews .

however , we do not believe this role is sufficient to bring dod's practices in line with the requirements of gprama or with omb guidance that the agency head and / or the coo conduct the reviews .

in its response , state concurred with our recommendations that reviews should be held on a quarterly basis and that the reviews involve applicable functional management officials .

state did not explicitly agree or disagree with our recommendation that the reviews be led by the agency head or coo .

however , the agency's response indicated that the agency would continue to have the pio lead their review meetings .

as outlined in the report , this is not consistent with the requirements of gprama or with omb guidance that the agency head and / or the coo conduct the reviews .

we believe that dod and state have both interpreted the language of omb's circular a - 11 in a way that provides them with the flexibility to delegate responsibility for conducting data - driven performance reviews to the pio .

as dod notes , a - 11 provides agencies with flexibility at key points to design a performance management system that best meets the agency's needs .

however , it is also important to emphasize that omb's circular a - 11 guidance clearly and unambiguously states in six separate sections that the coo is responsible for running agency reviews , and that these reviews should be held quarterly .

the guidance also specifies that reviews must be held in person .

as omb has also noted , the personal engagement of agency leaders demonstrates commitment to improvement across the organization , ensures coordination across agency silos , and enables rapid decision making .

the personal engagement of the coo in the data - driven reviews of progress on apgs is also critical given that , under gprama , apgs are to reflect the agency's highest priorities , and coos are responsible for improving the management and performance of the agency through the regular assessment of progress and the use of performance information .

for these reasons , we believe that these recommendations to follow requirements and guidance remain valid .

the following agencies provided technical comments that were incorporated into the draft as appropriate: department of energy , nasa , nsf , sba , ssa , usaid , and usda .

the following agencies had no comments on the draft report: the departments of commerce , labor , education , housing and urban development , interior , justice , treasury , and veterans affairs ; the environmental protection agency ; the general services administration ; the office of management and budget ; and the office of personnel management .

we are sending copies of this report to the director of omb as well as appropriate congressional committees and other interested parties .

the report is also available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions concerning this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix xi .

this report is part of our response to a statutory requirement that we evaluate how the implementation of the gpra modernization act of 2010 ( gprama ) is affecting performance management in federal agencies , and whether performance management is being used by agencies to improve the efficiency and effectiveness of agency programs .

specifically , this report examines ( 1 ) the extent to which agencies are conducting data - driven performance reviews in a manner consistent with gprama requirements , office of management and budget ( omb ) guidance , and leading practices ; and ( 2 ) how agency data - driven performance reviews have affected performance , collaboration , accountability , and efficiency within agencies , and how positive effects can be sustained .

to address these objectives , we assessed review practices at the 23 executive agencies that fall under the purview of the sections of gprama that relate to agency performance reviews .

gprama states that the 24 agencies identified by the amended chief financial officers act of 1990 ( cfo act ) , or those agencies otherwise determined by omb , are required to develop agency priority goals ( apg ) and to review progress on these because omb did not require by conducting reviews at least quarterly .

the nuclear regulatory commission to develop apgs for 2014-2015 , we confined our study to the other 23 cfo act agencies .

to address both objectives , we surveyed performance improvement officers ( pios ) at the 23 executive agencies .

this method allowed us to collect government - wide information about the variety of practices being used by agencies to review progress on goals .

we asked pios for information about the frequency of review meetings ; leadership of , and participation in , review meetings ; preparation for , execution of , and follow - up on , review meetings ; challenges ; and perceived effects of review meetings on agency performance , collaboration , efficiency , and accountability for results .

although agencies may conduct a variety of reviews as part of their performance management processes , we asked respondents to consider only in - person , data - driven review meetings that included discussion of apgs when answering our survey questions .

gprama provisions apply directly to these types of reviews .

we administered the surveys between october and december 2014 , transmitting and receiving the surveys as attachments to e - mails .

we received responses from all 23 agency pios , signifying a 100 percent response rate .

to minimize errors related to difficulties interpreting questions , we pretested the survey with three agency performance management officials to ensure that our questions were clear , complete , and unbiased , and that answering the survey did not place an undue burden on respondents .

one of our methodology specialists assisted in developing our survey to ensure that survey questions captured the intended information .

we revised our survey questions based on feedback from the pretesters and our methodologist .

we verified our data entry and analysis programs for accuracy .

to further address both objectives , we selected five case - study agencies for a more in - depth assessment of their data - driven review processes .

to supplement government - wide data collected through our survey , we used these more in - depth reviews to gather information about specific agency practices and about agency officials' perceptions of any impacts that review meetings have had .

this allowed us to collect additional detail and illustrative examples .

we selected a sample of agencies that reflected a range of key characteristics , while excluding agencies from our selection that had been used as case studies for related recent or ongoing work , to avoid overburdening those agencies .

the key characteristics we identified were agency size , as indicated by number of civilian employees ; the extent to which agency leadership uses quarterly performance reviews to drive progress toward goals , as reported by respondents to gao's 2013 federal managers survey ; and agency compliance with basic gprama requirements to hold quarterly in - person performance review meetings , as identified by a gao team conducting related work .

using these criteria , we selected the departments of commerce ( commerce ) , health and human services ( hhs ) , and transportation ( dot ) ; the general services administration ( gsa ) ; and the social security administration ( ssa ) .

our sample of agencies is non - generalizable , and should not be considered representative of all agencies .

at each case - study agency selected for in - depth review , we also selected apgs to obtain the perspective of apg leaders and their staff on the agency's data - driven review process .

for those agencies with more than two apg leaders , we developed a selection process to determine which apg leaders we would request to interview .

when possible , we excluded apg leaders who had been selected for interviews as part of related recent or ongoing work to avoid overburdening those officials .

we selected goal leaders to produce a sample that reflected varied agency progress against apgs , where we excluded apgs for which progress was unclear .

in cases when we had to choose between two apgs with the same type of progress , we prioritized the apg with a larger number of indicators .

in cases where we selected an official who leads two apgs , we selected both apgs , resulting in three total apgs selected for the agency .

to allow us to corroborate information collected through surveys , interviews , and observations , and strengthen our confidence in the reliability of the self - reported survey responses , we requested supporting documents from 12 agencies , representing more than 50 percent of the agencies surveyed , related to review meeting frequency , leadership , participation , content , and follow - up .

the 12 agencies included the 5 agencies selected for more in - depth review , several agencies whose survey responses required clarification , and several additional agencies at random .

examples of documents submitted by agencies included review meeting attendance or invitations , agendas , presentation slides , and briefings and summary reports .

based on our findings , we determined that the survey data were sufficiently reliable for the purposes of this report .

in conducting the survey and subsequent follow - up , we learned that the department of homeland security ( dhs ) does not hold in - person review meetings , and has not done so since december 2013 .

for this reason , the summaries of survey responses in this report exclude dhs .

we also addressed both objectives by interviewing agency officials who play a central role in the review meetings , including the agency chief operating officer ( coo ) , the pio , and two apg leaders .

these interviews provided us with detailed information from individual officials' perspectives and helped us to corroborate information collected through other means .

we used a consistent set of questions for each type of official , which included the official's objectives for review meetings ; the official's role in preparing for , participating in , and conducting follow - up after review meetings ; and the official's experience of any effects of review meetings .

we also interviewed staff from omb and the performance improvement council ( pic ) to obtain information on previous surveys of agency pios administered by the pic , their perspective on the implementation and effectiveness of reviews , and to learn about the role played by the pic's internal reviews working group , which has served as a forum for agency performance staff to periodically come together to discuss performance review practices in their agencies .

we also addressed both objectives by observing agency - level review meetings at hhs , gsa , and ssa , as well as one sub - agency - level review meeting at gsa .

observing review meetings allowed us to gain firsthand knowledge of review meeting processes .

this served to provide context , increase our familiarity with the process , and corroborate information gained through other means .

while we requested to observe a review meeting at both commerce and dot , we were not allowed to do so due to agency concerns that our presence could inhibit open discussion .

to address the first objective , we compared what we learned about the review processes at all 23 agencies with requirements for review meetings established in gprama , as well as standards set forth in guidance in omb's circular a - 11 and leading practices for data - driven to address the second objective , we reviews we previously identified.analyzed our survey data to determine how agencies characterized the effects of their review meetings .

we also used interviews with officials and documentation from our selected agencies to identify illustrative examples of the effects review meetings have produced , and to identify actions they have taken to sustain the benefits of the reviews .

because the scope of our review was to examine the implementation of data - driven review processes in agencies , we did not evaluate whether apgs were appropriate indicators of performance , sufficiently ambitious , or met other dimensions of quality .

although agency performance information was reported in illustrative examples of data - driven review meeting materials , as well as illustrative examples of the effects of review meetings , we did not independently assess the accuracy of the agency performance information cited in these examples .

we conducted our work from july 2014 to july 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusion based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our finding and conclusions based on our audit objectives .

description in 2014 and 2015 , the administrator of gsa convened weekly review meetings .

at least once a month , the public buildings service and the federal acquisition service , which had responsibility for the agency's two agency priority goals ( apg ) , presented on their key performance measures , including apgs .

other offices within the agency also presented on their goals once a month .

the administrator also held additional weekly meetings with the heads of pbs and fas to discuss key performance measures , including apgs , in greater detail .

in 2014 , the deputy secretary of commerce held biweekly review meetings , with each meeting focusing on one to three of the agency's 27 strategic objectives , and including a review of progress on related apgs .

in 2015 , commerce changed the frequency of review meetings from biweekly to monthly .

beginning in 2015 , the deputy secretary held monthly meetings , with each meeting focusing on reviewing progress on one of the agency's five strategic goals , including related apgs .

the deputy secretary also held monthly check - in meetings with each of the five strategic goal teams to discuss progress on the goals and related apgs .

in 2014 , the acting commissioner of ssa held quarterly review meetings that covered all four of ssa's apgs , as well as selected additional performance measures .

in 2015 , in addition to these quarterly reviews , ssa began to convene additional “theme - based” reviews to discuss cross - cutting issues , like human resource management , which , according to ssa officials , are also intended to include a discussion of relevant apgs .

ssa plans to hold three of these “theme - based” reviews in 2015 .

in 2014 and 2015 , the deputy secretary held four to five “management review meetings” a year with representatives of each of dot's 10 operating administrations .

these meetings focused primarily on the status of each operating administration's pending and proposed regulations , but also included a discussion of relevant performance measures , including apgs .

in 2014 and 2015 , twice a year the deputy secretary and performance improvement officer held in - person review meetings to review progress on each of the agency's five apgs .

according to hhs officials , agency leaders reviewed written progress updates for each apg during quarters without in - person review meetings .

to address our research questions related to agency data - driven review practices and effects we distributed a survey to the performance improvement officer ( pio ) at each of the 23 agencies with agency priority goals ( apgs ) .

we received responses from all 23 pios .

through our survey and subsequent follow - up , however , we learned that the department of homeland security ( dhs ) is not currently holding in - person data - driven review meetings , so its responses are not included in the aggregated results presented below .

therefore , unless otherwise indicated , the number of responses to each question is 22 .

there were nine questions in the survey , six of which contained multiple subquestions .

tables 1 through 9 below show our survey questions and aggregated responses .

for more information about our methodology for designing and administering the survey , see appendix i .

in addition to the contact named above , elizabeth curda ( acting director ) and adam miles supervised the development of this report .

linda collins , shelby kain , kathleen padulchick , steven putansu , and a.j .

stephens made significant contributions to this report .

dierdre duffy and robert robinson also made key contributions .

