collaboration across agencies is fundamental to effectively addressing many complex , high - risk challenges facing the federal government , including protecting the nation's critical information systems , reducing billions of dollars lost through improper payments , and better managing the risks of climate change .

effectively managing these challenges is essential to national and economic security , public health and safety , and the integrity of federal programs .

however , because these challenges are complex , responsibility for addressing them often rests with multiple agencies .

both congress and the executive branch have recognized the need for improved collaboration across the federal government , and the gpra modernization act of 2010 ( gprama ) established a new framework and processes aimed at encouraging a more crosscutting and integrated approach to focusing on results and improving government performance and management .

among other things , gprama requires the office of management and budget ( omb ) to coordinate with agencies to develop federal government priority goals ( now known as cross - agency priority , or cap , goals ) , which include outcome - oriented goals covering a limited number of crosscutting policy areas , as well as goals to improve management across the federal government .

these goals are intended to cover areas where increased cross - agency collaboration is needed to improve progress towards shared , complex policy or management objectives , such as improving our nation's cybersecurity posture and addressing critical skills gaps in the federal workforce .

in addition , in an effort to drive improved performance , transparency , and accountability across the federal government , gprama requires the use of specific processes to encourage the application of leading management practices .

one gprama requirement is that leaders regularly review progress towards established agency and cross - agency goals .

specifically , the director of omb , with the support of the performance improvement council ( pic ) , must review progress towards the cap goals with the appropriate lead government official at least quarterly to increase management's collection and use of performance information to achieve crosscutting goals .

under gprama , most large federal agencies have been holding quarterly reviews to assess progress in achieving their individual agency priority goals .

the emphasis on the creation of shared performance goals , the collection and analysis of data , and the regular review of these data with involved officials is designed to foster collaboration among agencies , ensure that interagency efforts are informed by data on performance , identify successful practices , and reinforce agency and collective accountability for the achievement of shared outcomes .

this report is our response to a mandate that we evaluate the implementation of gprama .

due to the timing of our work , we focused on efforts surrounding the 14 interim goals established in february 2012.this report assesses ( 1 ) what is known about progress made towards the interim cap goals ; and ( 2 ) how , if at all , quarterly progress reviews reflected gprama requirements and leading practices for reviews , as well as how they contributed to improved cross - agency performance and collaboration .

the experience with the interim goals can provide lessons for the implementation and management of the new cap goals released with the fiscal year 2015 budget .

to address these objectives , we interviewed representatives of 13 of the 14 interim cap goals .

for 8 of the 13 goals , we spoke directly with the goal leader or deputy goal leader , along with , in some cases , other staff from agencies and omb involved in supporting efforts related to the goals .

for the other five goals , we met with agency officials or omb staff playing a key role in the management of interagency efforts related to the goal .

we also interviewed the goal leaders for 11 agency priority goals aligned with , or contributing to , a cap goal .

to address the first objective , and assess what is known about progress made towards the interim cap goals , we analyzed performance information included in the quarterly status updates for each cap goal published on performance.gov , as well as relevant information collected through our interviews with cap goal representatives .

we compared the data and information made available through these updates with the gprama requirement that performance.gov includes the results achieved during the most recent quarter and overall trend data .

to assess the reliability of performance data and information available through performance.gov , we collected information from omb and pic staff and cap goal representatives about data quality control procedures .

we determined that the data and information were sufficiently reliable for our analysis of what was reported on performance.gov about progress towards identified goals and milestones .

to address the second objective and assess how omb cap goal progress reviews were conducted , we reviewed quarterly review memorandums developed by omb staff .

we also conducted interviews with staff from omb and the pic .

we compared the contents of these memorandums with information required to be reviewed by the director of omb on a quarterly basis .

to further address the second objective , we reviewed documents created for interagency meetings ( where available ) , such as meeting agendas , presentation materials , notes , and attendee lists .

we also interviewed sub - goal leaders from the closing skills gaps and entrepreneurship and small business cap goals to learn more about their involvement in quarterly review processes for their cap goals , and to learn of any impact the cap goal designation or review processes had at the sub - goal level .

we selected these goals to reflect one review process that involved regular meetings between the goal leader and contributors dedicated to reviewing progress ( closing skills gaps ) , and one that did not rely on regular meetings at the cap - goal level to review progress ( entrepreneurship and small business ) .

our findings from these interviews cannot be generalized to the other cap goals .

we then compared what we learned about the processes instituted by each goal leader to review progress to leading practices for performance reviews previously identified by gao .

because the scope of our review was to examine the implementation of quarterly reporting and reviews , we did not evaluate whether these goals were appropriate indicators of performance , were sufficiently ambitious , or met other dimensions of quality .

we conducted our work from may 2013 to june 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

more detailed information on our scope and methodology appears in appendix i .

gprama requires omb to coordinate with agencies to develop long - term , outcome - oriented federal government priority goals for a limited number of crosscutting policy areas and management improvement areas every 4 years .

furthermore , with the submission of the fiscal year 2013 budget , gprama required omb to identify a set of interim priority goals.interim cap goals , 9 of which were related to crosscutting policy areas and 5 of which were management improvement goals .

the president's 2013 budget submission included a list of 14 the cap goal leader .

as required by gprama , each of the interim cap goals had a goal leader responsible for coordinating efforts to achieve each goal .

cap goal leaders were given flexibility in how to manage these efforts , and were encouraged by omb to engage officials from contributing agencies by leveraging existing inter - agency working groups , policy committees , and councils .

for information on the position of the goal leader and the interagency groups used to engage officials from agencies contributing to each interim cap goal , see figure 1 .

for more information on the interagency groups used to engage agency officials in efforts related to each goal , see appendix iii .

according to omb and pic staff , because cap goal leaders were responsible for managing efforts related to the achievement of the goals as part of a larger portfolio of responsibilities , staff from the pic , omb , and — in some cases — from agencies with project management responsibilities , provided additional capacity for coordinating interagency efforts and overseeing the process of collecting , analyzing , and reporting data .

specifically , pic staff provided logistical support , assisting with the regular collection of data , updates to performance.gov , and the development of cap goal governance structures and working groups .

they also provided support in the area of performance measurement and analysis .

for example , pic staff supported the exports goal leader by informing discussions of how to measure the success and impact of export promotion efforts , providing expertise in the development and selection of appropriate performance measures , and assisting in the collection and analysis of relevant data .

progress reviews .

gprama also requires that the director of omb , with the support of the pic , review progress towards each cap goal with the appropriate lead government official at least quarterly .

specifically , the law requires that these should include a review of progress during the most recent quarter , overall trends , and the likelihood of meeting the planned level of performance .

as part of these reviews , omb is to assess whether relevant agencies , organizations , program activities , regulations , tax expenditures , policies , and other activities are contributing as planned to the goal .

the law also requires that omb categorize the goals by risk of not achieving the planned level of performance and , for those at greatest risk of not meeting the planned level of performance , identify strategies for performance improvement .

in an earlier evaluation of the implementation of quarterly performance reviews at the agency level , we found that regular , in - person review meetings provide a critical opportunity for leaders to use current data and information to analyze performance , provide feedback to managers and staff , follow up on previous decisions or commitments , learn from efforts to improve performance , and identify and solve performance problems .

as part of this work we also identified nine leading practices that can be used to promote successful performance reviews at the federal level .

to identify these practices , we conducted a review of relevant academic and policy literature , including our previous reports .

we refined these practices with additional information obtained from practitioners at the local , state , and federal level who shared their experiences and lessons learned .

nine leading practices that can be used to promote successful performance reviews leaders use data - driven reviews as a leadership strategy to drive performance improvement .

key players attend reviews to facilitate problem solving .

reviews ensure alignment between goals , program activities , and resources .

leaders hold managers accountable for diagnosing performance problems and identifying strategies for improvement .

there is capacity to collect accurate , useful , and timely performance data .

staff have skills to analyze and clearly communicate complex data for decision making .

rigorous preparations enable meaningful performance discussions .

reviews are conducted on a frequent and regularly scheduled basis .

participants engage in rigorous and sustained follow - up on issues identified during reviews .

reporting requirements .

in addition to requiring quarterly reviews , gprama requires that omb make information available on “a single website” ( now known as performance.gov ) for each cap goal on the results achieved during the most recent quarter , and overall trend data compared to the planned level of performance .

in addition , information on performance.gov is to include an assessment of whether relevant federal organizations , programs , and activities are contributing as planned , and , for those cap goals at risk of not achieving the planned level of performance , information on strategies for performance improvement .

new cap goals .

as required by gprama , in march 2014 , omb announced the creation of a new set of cap goals in the fiscal year 2015 budget .

it then identified 15 cap goals with 4-year time frames on performance.gov — 7 mission - oriented goals and 8 management - focused goals .

five goal areas — cybersecurity ; open data ; science , technology , engineering , and mathematics ( stem ) education ; strategic sourcing ; and sustainability ( renamed climate change ( federal actions ) ) — were carried over from the set of interim cap goals , while the other 10 are new goal areas .

omb stated on performance.gov that more detailed action plans for each of the goals , including specific metrics and milestones that will be used to gauge progress , will subsequently be released .

the new cap goals will also have co - leaders ; one from an office within the executive office of the president ( eop ) and one or more from federal agencies .

according to omb staff , this change was made to ensure that cap goal leaders can leverage the convening authority of officials from the eop while also drawing upon expertise and resources from the agency level .

gprama requirements for establishing planned performance for cap goals gprama requires the director of omb to establish , in the annual federal government performance plan , a planned level of performance for each cap goal for the year in which the plan is submitted and the next fiscal year , as well as quarterly performance targets for the goals .

gprama requirements for reporting cap goal performance information gprama requires the director of omb to publish on performance.gov information about the results achieved during the most recent quarter and trend data compared to the planned level of performance for each cap goal .

omb released the federal government performance plan on performance.gov concurrently with the fiscal year 2013 budget submission that identified the 14 interim cap goals .

the information on performance.gov included a goal statement for each of the interim goals that established an overall planned level of performance .

during the two - year interim goal period , omb addressed the requirement to report on results achieved during the most recent quarter for each of the cap goals by publishing 5 sets of quarterly updates to the interim cap goals on performance.gov .

the first set of updates , for the fourth quarter of fiscal year 2012 , was published in december 2012 and the final set of updates , for the fourth quarter of fiscal year 2013 , was published in february 2014 .

these documents described general accomplishments made to date , specific actions completed , or both .

the updates to the broadband cap goal , for instance , included short descriptions of general progress made towards each of the five strategies identified for achieving the goal , as well as specific milestones accomplished .

the quarterly updates did not , however , consistently identify required interim planned levels of performance and data necessary to indicate progress being made toward the cap goals .

updates to eight of the goals included quarterly , biannual , or annual data that indicated performance achieved to date toward the target identified in the goal statement .

three of the eight goals ( cybersecurity , energy efficiency , and strategic sourcing ) also contained the required annual or quarterly targets that defined planned levels of performance , which allowed for an assessment of interim progress .

for example , the cybersecurity goal's updates stated that the goal would not be met within its established time frame , and provided quarterly performance data compared to quarterly targets for the entirety of the goal period to support the statement .

in contrast , the updates for the other five goals did not contain annual or quarterly targets , which made it difficult to determine whether interim progress towards the goals' overall planned levels of performance was being made .

for example , updates to the exports goal included data on the total amount of u.s. exports by quarter for calendar years 2012 and 2013 but did not include a target level of performance for those years or quarters .

therefore , it was unclear whether the goal's overall planned level of performance of doubling u.s. exports by the end of 2014 is on track to be met .

furthermore , updates to six interim cap goals did not include trend data to indicate progress being made towards the goals' overall planned levels of performance .

figure 2 below identifies the frequency with which data on cap goal performance were reported , as well as the overall performance cap goal leaders reported making compared to the goal's planned level of performance through the fourth quarter of fiscal year 2013 .

through our review of information on performance.gov and interviews with managers of the six interim cap goals that did not report any data on progress towards the stated goal , we identified reasons that included: lack of quantitative planned level of performance ( targets ) .

the entrepreneurship and small business cap goal lacked a quantitative performance target .

the quarterly updates to the goal explained that efforts were focused on the goal's 10 sub - goals .

most of these sub - goals , however , also lacked quantitative performance targets .

the deputy goal leader told us that some of the sub - goals did not have quantitative targets by design , as goal managers thought it more appropriate to use qualitative milestones to track progress towards them .

the quarterly updates to the “streamline immigration pathways for immigrant entrepreneurs” sub - goal lacked a quantitative target but had a range of qualitative milestones .

for example , the department of homeland security and the department of state established a milestone to identify reforms needed to ease the application and adjudication processes for visas available to certain immigrant entrepreneurs .

unavailable data .

some cap goal managers told us that the data needed to assess and report progress toward their goals' performance targets were unavailable or not yet being collected .

for example , a manager of the job training cap goal told us that staff had not established a baseline number of participants served by federal job training programs against which progress towards the goal could be tracked .

in addition , managers of the real property cap goal told us that they did not have data available for tracking progress toward the goal of holding the federal real property footprint at its fiscal year 2012 baseline level .

where key data were not reported , some goal managers took actions to obtain previously unavailable data or developed an alternative approach for assessing progress .

job training cap goal .

the first quarterly update for the job training cap goal , published on performance.gov in december 2012 , stated that federal agencies were surveyed to compile a list of all job training programs in the federal government , including the number of participants served by those programs , and that a working group was developing a baseline for measuring progress towards the goal of preparing 2 million workers with skills training by 2015 .

a goal manager told us that the deputy goal leader and staff from the pic gathered baseline information for most of the programs within the scope of the cap goal , but that they were unable to complete the efforts by the end of the goal period .

real property cap goal .

managers of the real property cap goal told us that they worked to establish a baseline and metrics for measuring future performance and would be able to report on progress after the goal period ended .

closing skills gaps cap goal .

a manager of the closing skills gaps goal told us that the goal's managers decided early on that it did not make sense for each of the goal's identified mission - critical occupations to have the same skills gaps reduction target .

instead , managers of the goal's sub - goals identified efforts to reduce skills gaps in their specific occupations .

they identified an individual targeted level of performance for that effort and collected and reported data on progress made towards the target .

for instance , managers of the acquisitions sub - goal established a target for increasing the certification rate of gs - 1102 contract specialists to 80 percent .

the final quarterly status update to the closing skills gaps cap goal reported that the target was met and the certification rate increased to 81 percent .

veteran career readiness cap goal .

the leader of the veteran career readiness cap goal told us that efforts were made to collect data to assess the veteran employment situation .

for instance , she said that an interagency data - gathering working group reviewed sources of available data , integrated those data – such as the unemployment rate for various sub - populations of veterans – into dashboards for senior leadership review , and made proposals to improve data availability .

in addition , the army led a working group to develop a more complete picture of veterans receiving unemployment compensation .

she said that these and other efforts led to a concerted effort to improve the availability of data , and to develop and implement metrics measuring career readiness and attendance in a veteran career transition assistance program .

however , no data to track progress towards the overall goal were reported during the interim goal period .

as we have previously reported , no picture of what the federal government is accomplishing can be complete without adequate performance information .

however , omb and cap goal leaders did not identify interim planned levels of performance or targets for most of the interim cap goals .

furthermore , they established a number of cap goals for which data necessary to indicate progress towards the goal could not be reported .

in so doing , they limited their ability to demonstrate progress being made towards most of the cap goals and ensure accountability for results from those who helped to manage the goals .

gprama requirement for establishing milestones gprama requires the director of omb to establish , in the federal government performance plan , clearly defined quarterly milestones for the cap goals .

gprama requirement for reporting on contributions towards cross - agency priority goals gprama requires that omb identify the agencies , organizations , program activities , regulations , tax expenditures , policies , and other activities that contribute to each cap goal on performance.gov .

it also requires omb to make available on the website an assessment of whether relevant agencies , organizations , program activities , regulations , tax expenditures , policies , and other activities are contributing as planned .

in the status updates that were published on performance.gov , managers of each of the cap goals reported the general approaches , strategies , or specific initiatives being employed to make progress towards the achievement of the goal , as well as the departments , agencies , and programs that were expected to contribute to goal achievement .

for example , the leader of the science , technology , engineering , and mathematics ( stem ) education cap goal identified a number of general strategies for making progress towards the achievement of its goal of increasing the number of graduates in stem subjects by 1 million over the next 10 years , such as “address the mathematics preparation gap that students face when they arrive at college” and “identifying and supporting the role of technology and innovation in higher education.” in addition , the goal leader identified a number of programs and goals within four departments and agencies that were likely to contribute in part or in whole to the goal .

figure 3 below illustrates how this information was presented in the update to the stem education cap goal for the fourth quarter of fiscal year 2013 .

in a may 2012 report on our work related to the cap goals , we noted that information on performance.gov indicated additional programs with the potential to contribute to each of the cap goals may be identified over time .

we then recommended that omb review and consider adding to the list of cap goal contributors the additional departments , agencies , and programs that we identified , as appropriate .

omb agreed with the recommendation , and in the quarterly updates to the cap goals published in december 2012 and march 2013 , omb added some of the departments , agencies , and programs we identified in our work to some cap goals' lists of contributors .

for example , we had noted that 12 member agencies of the trade promotion coordinating committee had not been identified as contributors to the exports cap goal .

omb added additional information about contributors to the exports goal in the update published in december 2012 .

during our review , in some cases cap goal managers told us about additional organizations and program types that contributed to their goals , but which were not identified on performance.gov or in our previous report .

for example , the leader of the stem education cap goal told us that representatives from the smithsonian institution led an interagency working group that contributed to key efforts towards achieving the goal .

although the cap goal updates indicate that the smithsonian institution is involved in federal stem education efforts , it was not identified in a dedicated list of contributors to the goal .

we have previously found that federal stem education programs are fragmented across a number of agencies .

we continue to believe that the federal government's efforts to ensure stem education programs are effectively coordinated must include all relevant efforts .

furthermore , the leader of the broadband cap goal told us that he is aware that tax deductions available to businesses making capital investments contributed to the goal by incentivizing investments in broadband .

we have long referred to such deductions , along with other reductions in a taxpayer's liability that result from special exemptions and exclusions from taxation , credits , deferrals of tax liability , or preferential tax rates , as tax expenditures .

as we have previously reported , as with spending programs , tax expenditures represent a substantial federal commitment to a wide range of mission areas .

we have recommended greater scrutiny of tax expenditures .

periodic reviews could help determine how well specific tax expenditures work to achieve their goals and how their benefits and costs compare to those of programs with similar goals .

as previously mentioned , gprama also requires omb to identify tax expenditures that contribute to cap goals .

however , tax expenditures were not reported as contributors to the broadband cap goal in the quarterly status updates published on performance.gov .

leading practices state that a clear connection between goals and day - to - day activities can help organizations better articulate how they plan to accomplish their goals .

in addition , a clear connection between goals and the programs that contribute to them helps to reinforce accountability and ensure that managers keep in mind the results their organizations are striving to achieve .

milestones — scheduled events signifying the completion of a major deliverable or a set of related deliverables or a phase of work — can help organizations demonstrate the connection between their goals and day - to - day activities and that they are tracking progress to accomplish their goals .

organizations , by describing the strategies to be used to achieve results , including clearly defined milestones , can provide information that would help key stakeholders better understand the relationship between resources and results .

gao - 13-174 ; gao - 13-228 ; and gao , managing for results: critical issues for improving federal agencies' strategic plans , gao / ggd - 97-180 ( washington , d.c.: sept. 16 , 1997 ) .

actions , however , lacked clear time frames for completion .

figure 4 below illustrates the “next steps” identified for the strategic sourcing cap goal in the update for the third quarter of fiscal year 2013 .

completion status: the real property cap goal update for the second quarter of fiscal year 2013 identified two planned actions as “next steps.” “after agencies submit their revised cost savings and innovation plans to omb , omb will evaluate agency plans to maintain their square footage baselines , while balancing mission requirements,” and “updates on agency square footage baselines and projects are forthcoming and will be posted on performance.gov.” these two actions were again identified as “next steps” in the update for the third quarter of fiscal year 2013 , but no update was provided on the status of the actions .

by establishing planned activities that , in many of the cap goal updates , did not have information about their alignment with the strategies they supported , their time frames for completion , or their completion status , cap goal leaders did not fully demonstrate that they had effectively planned to support goal achievement or were tracking progress toward the goal or identified milestones .

omb did not issue formal guidance to cap goal leaders on the types of information that were to be included in the cap goal updates , including information about contributors and milestones .

standards for internal control in the federal government emphasize the importance of documenting policies and procedures to provide a reasonable assurance that activities comply with applicable laws and regulations , and that managers review performance and compare actual performance to planned or expected results and analyze significant differences.staff told us they provided an implementation plan template to goal leaders , which outlined the data elements to be reported in the quarterly status updates .

the template was also used to collect information for internal and public reporting .

some cap goal managers told us that omb or pic staff , in their role supporting the collection , analysis , and presentation of data on cap goal performance , occasionally provided feedback on the information that the individuals submitted in draft updates that omb reviewed before they were published on performance.gov .

for example , one cap goal manager told us that during a review of an update submission pic staff told him that he should develop additional milestones to be completed during a specific future fiscal year quarter .

this is in contrast to the detailed guidance that omb issued on the types of information that agencies must provide for the updates for agency priority goals ( apg ) , which are also published quarterly on performance.gov .

the apg guidance includes explicit instructions for agencies to identify , as appropriate , the organizations , regulations , tax expenditures , policies , and other activities within and external to the agency that contribute to each apg , as well as key milestones with planned completion dates for the remainder of the goal period .

because guidance for the types of information that should have been included in the cap goal updates was never formally established , cap goal leaders were at a heightened risk of failing to take into account important contributors to the goal and providing incomplete information about milestones that could help demonstrate progress being made .

gprama requirement for omb progress reviews gprama requires that , not less than quarterly , the director of omb , with the support of the pic , shall review progress on the cap goals , including progress during the most recent quarter , overall trends , and the likelihood of meeting the planned level of performance .

gprama also requires that , as part of these reviews , omb categorize goals by their risk of not achieving the planned level of performance and , for those goals most at risk of not meeting the planned level of performance , identify strategies for performance improvement .

as required by gprama , omb reviewed progress on cap goals each quarter , beginning with the quarter ending june 30 , 2012 .

this review process consisted of the collection of updated information for each cap goal by omb or pic staff , and the development of a memorandum for the director of omb with information on the status of the cap goals .

to develop these memorandums , omb staff told us that approximately 6 weeks after the end of each quarter , omb and pic staff worked with cap goal leaders to collect updated data and information on goal metrics and milestones , and to update the narratives supporting the data .

cap goal leaders , or staff assisting leaders with the management of efforts related to the goal , would provide this information to omb using a template for the status updates ultimately published on performance.gov .

in addition to the memorandums developed for the director of omb , omb published more detailed information through the quarterly status updates available on performance.gov .

omb and pic staff told us that to support omb's quarterly review efforts , pic staff were to conduct assessments rating the overall health of implementation efforts and goal leader engagement .

they were also to assess the execution status of each goal , including the quality and trend of performance indicators .

one purpose of these assessments was to identify areas where risks , such as goal leader turnover , could affect the ability to achieve the planned level of performance .

consistent with this intent , several of the quarterly omb review memorandums we examined highlighted turnover in goal leader or deputy goal leader positions as risks , and suggested the need to find or approve replacements .

although pic staff have been tasked with assessing these elements of cap goal implementation , and said that there was a shared understanding between involved staff as to how these assessments would be carried out , the pic has not documented its procedures or criteria for conducting these assessments .

standards for internal control in the federal government emphasize the importance of documenting procedures , including those for assessing performance .

without clearly established criteria and procedures , pic staff lack a means to: consistently assess implementation efforts and execution across all goals ; bring any deficiencies , risks , and recommended improvements identified to the attention of leadership ; and ensure consistent application of criteria over time .

while these quarterly review memorandums identified one goal as being at risk of not achieving the planned level of performance , and identified other instances where progress on goals had been slower than planned , the memorandums did not consistently outline the strategies that were being used to improve performance or address identified risks .

for example , the cybersecurity cap goal was the one goal specifically described as being at risk of not achieving the planned level of performance , both in these memorandums and in the status updates on performance.gov .

specifically , the memorandum for the third quarter of fiscal year 2012 identified the risk of not achieving the planned level of performance , and outlined seven specific risks facing the goal and the steps being taken to mitigate them .

similarly , the memorandum for the second quarter of fiscal year 2013 also acknowledged that some agencies were at risk of not meeting their cybersecurity cap goal targets .

however , in contrast to the earlier memorandums , no information was included about the specific steps that were being taken to mitigate these risks , although information on planned and ongoing actions to improve government - wide implementation was included in the milestones section of the status update for that quarter on performance.gov .

the memorandum for the fourth quarter of fiscal year 2012 also acknowledged that the pace of progress on the stem education and closing skills gaps goals had been slower than expected .

while the memorandum stated that additional omb attention was needed to support implementation and assure sufficient progress , no information on the specific strategies being employed to improve performance was mentioned .

according to omb staff , however , these memorandums were used to inform subsequent conversations with omb leadership , which would build on the information presented in the memorandums .

furthermore , because the data necessary to track progress for some goals were unavailable , the director of omb would not have been able to consistently review progress for all cap goals , or make a determination about whether some cap goals were at risk of meeting their planned levels of performance .

this fact was acknowledged in the quarterly review memorandums for quarters one and two of fiscal year 2013 , which acknowledged that progress on three goals ( entrepreneurship and small business , job training , and stem education ) was difficult to track , and that additional work was needed on data collection .

however , no information on the specific steps that were being taken to address these shortcomings was included .

a lack of specific information about the steps being taken to mitigate identified risk areas and improve performance could hinder the ability of omb leadership — and others — to adequately track the status of efforts to address identified deficiencies or risks and to hold officials accountable for taking necessary actions .

gprama requirement for goal leader and agency involvement in progress reviews as part of the quarterly review process , gprama requires that the director of omb review each priority goal with the appropriate lead government official , and include in these reviews officials from the agencies , organizations , and program activities that contribute to the achievement of the goal .

according to omb staff , to encourage goal leaders and contributing agencies to take ownership of efforts to achieve the goals , omb gave goal leaders flexibility to use different approaches to engage agency officials and review progress at the cap - goal level .

while guidance released by omb in august 2012 encouraged goal leaders to leverage existing interagency working groups , committees , and councils in the management of the goals as much as practicable , it did not include information on the purpose of reviews , expectations for how reviews should be conducted to maximize their effectiveness as a tool for performance management and accountability , or the roles that cap goal leaders and agency officials should play in the review process .

again , standards for internal control in the federal government emphasize the importance of documenting procedures for reviewing performance against established goals and objectives .

this is in contrast to the detailed guidance that omb released for agency priority goal and agency strategic objective reviews , which outlined the specific purposes of the reviews , how frequently they should be conducted , the roles and responsibilities of agency leaders involved in the review process , and how the reviews should be conducted .

we also found that this guidance for reviews at the agency level was broadly consistent with the leading practices for performance reviews that we previously identified .

while no official guidance was published to guide how reviews involving goal leaders and staff from contributing agencies could be conducted for the cap goals , omb staff said the principles of the guidance released for agency reviews , which reflected many of the leading practices , was referenced in conversations with cap goal leaders and teams .

omb has emphasized that flexibility is needed to ensure that goal leaders can use review processes that are appropriate given the scope of interagency efforts , the number of people involved , and the maturity of existing reporting and review processes .

the guidance for agency reviews gave agencies flexibility to design their performance review processes in a way that would fit the agency's mission , leadership preferences , organizational structure , culture , and existing decision - making processes .

in our previous work , we detailed how several federal agencies had implemented quarterly performance reviews in a manner consistent with leading practices , but which were also tailored to the structures , processes , and needs of each agency .

in this way , flexible implementation of review processes is possible within a framework that encourages the application of leading practices .

a lack of clear expectations for how progress should be reviewed at the cap - goal level resulted in a number of different approaches being used by goal leaders to engage officials from contributing agencies to review progress on identified goals and milestones , ranging from regular in - person review meetings led by the cap goal leader to the review of written updates provided to the goal leader by officials from contributing agencies .

see appendix iv for more detailed information on the various processes used by goal leaders to collect data on , and review progress towards , identified goals .

instituting review processes consistent with the leading practices we previously identified can help ensure that reviews include meaningful performance discussions , provide opportunities for oversight and accountability , and drive performance improvement .

taken together , these leading practices emphasize the importance of leadership involvement in the review process , data collection and review meeting preparation , participation by key officials , and rigorous follow - up .

through our evaluation of how goal leaders and contributing agency officials reviewed progress towards the interim goals , we identified two cap goals — cybersecurity and closing skills gaps — and one sub - goal — the entrepreneurship and small business sub - goal on improving access to government services and information ( businessusa sub - goal ) — where goal managers instituted in - person review processes with officials from contributing agencies that were broadly consistent with the full range of leading practices for reviews , which we have summarized in four categories below .

the processes used by other cap goal leaders to engage agency officials in the review of progress did not reflect the full range of leading practices .

leadership involvement .

leading practices indicate that leaders should use frequent and regular progress reviews as a leadership strategy to drive performance improvement and as an opportunity to hold people accountable for diagnosing performance problems and identifying strategies for improvement .

the direct and visible engagement of leadership is vital to the success of such reviews .

leadership involvement helps ensure that participants take the review process seriously and that decisions and commitments can be made .

the goal leaders managing the cybersecurity and closing skills gaps goals , as well as the businessusa sub - goal , were directly involved in leading in - person reviews for these goals , and in using them as opportunities to review progress , identify and address performance problems , and hold agency officials accountable for progress on identified goals and milestones , as detailed in table 1 .

data collection and review meeting preparation .

leading practices also indicate that those managing review processes should have the capacity to collect , analyze , and communicate accurate , useful , and timely performance data , and should rigorously prepare for reviews to enable meaningful performance discussions .

the collection of current , reliable data on the status of activities and progress towards goals and milestones is critical so that those involved can determine whether performance is improving , identify performance problems , ensure accountability for fulfilling commitments , and learn from efforts to improve performance .

the ability to assess data to identify key trends and areas of strong or weak performance , and to communicate this to managers and staff effectively through materials prepared for reviews , is also critical .

as detailed in table 2 , those supporting the cybersecurity and skills gap goals , and the businessusa sub - goal , instituted processes to regularly collect and analyze data on progress towards identified goals and milestones , and to ensure these data would be communicated through materials prepared for review meetings .

participation by key officials .

leading practices indicate that key players involved in efforts to achieve a goal should attend reviews to facilitate problem solving .

this is critical as their participation enables those involved to break down information silos , and to use the forum provided by the review to communicate with each other , identify improvement strategies , and agree on specific next steps .

reviews for both the cybersecurity and closing skills gaps cap goals , and the businessusa sub - goal , were structured so that relevant agency officials playing a key role in efforts to carry out the goal were included , as detailed in table 3. review follow - up .

leading practices indicate that participants should engage in sustained follow - up on issues identified during reviews , which is critical to ensure the success of the reviews as a performance improvement tool .

important follow - up activities include identifying and documenting specific follow - up actions stemming from reviews , those responsible for each action item , as well as who will be responsible for monitoring and follow - up .

follow - up actions should also be included as agenda items for subsequent reviews to hold responsible officials accountable for addressing issues raised and communicating what was done .

goal managers for the cybersecurity and closing skills gap cap goals , as well as the businessusa sub - goal , took steps to follow up on action items identified in these meetings , and to ensure that steps were taken towards their completion , as detailed in table 4. review effects .

goal leaders and managers we interviewed said that these review processes were valuable in driving improved performance , establishing a greater sense of accountability for progress on the part of contributors , and in providing a forum for interagency communication and collaboration .

for example , according to dhs staff involved in the management of the cybersecurity cap goal , implementation of personal identity verification ( piv ) requirements across the federal government had been stagnant for several years prior to the introduction of cybersecurity as a cap goal .

the review process was used to hold agencies accountable for improved piv implementation , which helped bring an increased focus on the issue and drive recent progress .

since the reviews were instituted in 2012 , dhs has reported improved piv adoption in civilian agencies , which has increased from 1.24 percent in fiscal year 2010 , to 7.45 percent in fiscal year 2012 , to 19.67 percent in the fourth quarter of fiscal year 2013 .

according to data from dhs , while still falling short of the target , this has contributed to the overall increase in piv adoption across the federal government — including both civilian agencies and the department of defense — from 57.26 percent in fiscal dhs year 2012 to 66.61 percent in fourth quarter of fiscal year 2013.staff also added that agencies generally had not previously collaborated on cybersecurity issues or worked to identify best practices .

according to dhs staff , the reviews have created an important point of collaboration between dhs , omb , national security staff , and agencies , and provided an opportunity to inform agencies of best practices and connect them with other agencies that are meeting their targets to learn from them .

similarly , opm officials and sub - goal leaders involved in the management of the closing skills gap cap goal said that the quarterly review meetings were a critical means to ensure sub - goal leaders and staff were demonstrating progress .

having sub - goal leaders report out on progress , and hear about the progress made in other sub - goal areas , provided additional pressure for continuous improvement and the need to remain focused on driving progress towards their goals .

having the goal leader lead the review was also a way to demonstrate leadership commitment to the achievement of each sub - goal .

according to opm officials and sub - goal leaders , the review meetings also served as an important forum for discussing innovative approaches being taken to address skills gaps in different areas , opportunities for collaboration to address challenges shared by different sub - goals , and how leaders could leverage the efforts of other sub - goals to drive progress on their own .

the businessusa sub - goal leader said that having it as the basis for a cap sub - goal elevated the cross cutting nature of the initiative .

in addition to reviewing performance information and the status of deliverables , discussions at inter - agency steering committee meetings were used to discuss how contributors could work together to meet the initiative's performance goals .

this communication and coordination led to connections between agencies and to discussions about how programs could be working in a more integrated way .

for example , these discussions were used to identify ways that programs could more effectively integrate program information on the businessusa website to increase customer satisfaction .

we found that the processes used by other cap goal leaders to engage agency officials in the review of progress , which are summarized in appendix iv , did not reflect the full range of leading practices .

for example , the process for reviewing progress on the job training cap goal involved staff from the pic collecting updates on recent milestones from agencies , which were then compiled in the quarterly status update and reviewed by the goal leader .

this approach was used by the goal leaders for the broadband and stem education cap goals to review progress as well .

while goal leaders and managers for these goals indicated that they used the collection and review of information as an opportunity to communicate with officials from contributing agencies , this approach contrasts with omb guidance for reviews of agency priority goals , which states explicitly that performance reviews should not be conducted solely through the sharing of written communications .

as omb noted in its guidance , in - person engagement of leaders in performance reviews greatly accelerates learning and performance improvement , and personal engagement can demonstrate commitment to improvement , ensure coordination across agency silos , and enable rapid decision making .

while not employing the full range , goal leaders for a number of goals did use processes that reflected one or more leading practices .

for example , many cap goal leaders led or participated in interagency meetings with representatives of contributing agencies .

while these were used to facilitate interagency communication and collaboration on the development of plans and policies , it was unclear whether many of these meetings were consistently used to review progress on identified cap goals and milestones .

the goal leader for the strategic sourcing cap goal used processes that reflected leadership involvement , participation by key officials , and the collection and analysis of relevant data .

specifically , according to goal managers , the goal leader led regular meetings of the strategic sourcing leadership council ( sslc ) , which were attended by senior procurement officials from eight agencies that combine to make up almost all of the federal government's total procurement spending .

to prepare for each sslc meeting , staff from omb's office of federal procurement policy ( ofpp ) held a meeting for supporting staff from each agency , who would then prepare the sslc member from their agency for the issues to be discussed in the sslc meeting .

ofpp also established a regular data collection process where each agency would report on its adoption and spending rates for two strategic sourcing options , which would then be used for the purposes of reporting on the cap goal .

however , it was unclear how regularly , if at all , sslc meetings were used to engage agency officials in the review of data on agency adoption of , and spending on , strategic sourcing options , or how regularly meetings were used to review progress that was being made towards the cap goal .

it was also unclear what mechanisms , if any , were used to ensure rigorous follow - up on issues raised in these meetings , a key leading practice , as there were no official meeting minutes maintained .

the lack of an official record could hinder follow - up and accountability for any identified actions that need to be taken .

representatives of some goals stated that it was difficult to isolate the impact of the cap goal designation , and its associated reporting and review requirements , on performance and collaboration .

according to some goal managers , because their interim goals were based on initiatives that had been previously established in executive orders or presidential memorandums , much of the interagency activity supporting their efforts would have happened without the cap goal designation and its reporting and review requirements .

for example , a manager for the data center consolidation cap goal told us that the previously established federal data center consolidation initiative was used to drive progress and that the cap goal designation and quarterly reporting and review requirements had little impact .

similarly , job training cap goal managers said that interagency collaboration on job training issues had been established prior to the creation of the cap goal , that the goal's reporting and review requirements were incidental to the contributors' ongoing work , and that it did not add an additional level of accountability for the completion of job training initiatives .

however , this is a goal where no data were reported to demonstrate its impact on federal job training programs , and which was identified in multiple omb reviews as having slower than anticipated progress due , in part , to extended periods of time in which there was no deputy cap goal leader to provide support necessary to improve coordination and collaboration .

while many cap goal leaders and staff we interviewed noted the progress they had made with their existing interagency meetings and approaches , a lack of clear expectations or guidance for how review processes at the cap goal level should be carried out can lead to a situation where reviews are implemented in a manner that is not informed by , or fully consistent with , leading practices .

this could result in missed opportunities to realize the positive effects on performance and accountability that can stem from the implementation of review processes that regularly and consistently involve leaders and agency officials in the analysis of performance data to identify and address performance deficiencies , and use rigorous follow - up to ensure accountability for commitments .

many of the meaningful results that the federal government seeks to achieve require the coordinated efforts of more than one federal agency .

gprama's requirement that omb establish cap goals offers a unique opportunity to coordinate cross - agency efforts to drive progress in priority areas .

that opportunity will not be realized , however , if the cap goal reporting and review requirements and leading review practices are not followed .

the reporting and review requirements for the cap goals , and leading practices for the reviews , are designed to ensure that relevant performance information is used to improve performance and results , and that omb and goal leaders actively lead efforts to engage all relevant participants in collaborative performance improvement initiatives and hold them accountable for progress on identified goals and milestones .

omb reported performance information in the quarterly cap goal status updates it published on performance.gov .

while updates for most goals reported data on performance towards the identified planned level of performance , the information in the updates did not always present a complete picture of progress towards identified goals and milestones .

for example , while updates for 8 of the 14 goals included data that indicated performance towards the identified overall planned level of performance , only 3 also contained annual or quarterly targets that allowed for an assessment of interim progress .

updates for the other 6 of the 14 goals did not report on performance towards the goal's primary performance target because the goal was established without a quantitative target or because goal managers were unable to collect the data needed to track performance .

in other cases , planned activities that were identified as contributing to the goal were sometimes missing important elements , including alignment with the strategies for goal achievement they supported , a time frame for completion , or information on their implementation status .

the incomplete picture of progress that many of the updates gave limited the ability of goal leaders and others to ensure accountability for the achievement of targets and milestones .

holding regular progress reviews that are consistent with gprama requirements and the full range of leading practices can produce positive effects on performance and collaboration .

engaging contributors in regular reviews of data on performance can help ensure interagency efforts are informed by information on progress towards identified goals and milestones , which can be used to identify and address areas where goal or milestone achievement is at risk .

reviews can also be used to reinforce agency and collective accountability for the achievement of individual and shared outcomes , helping to ensure that efforts to improve performance or address identified risks are implemented .

lastly , reviews can be used to foster greater collaboration , ensuring opportunities for communication and coordination between officials involved in efforts to achieve shared outcomes .

while omb and cap goal leaders instituted processes for reviewing progress on the interim cap goals , if gprama requirements and leading practices for reviews are not consistently followed , it may result in missed opportunities to improve performance , hold officials accountable for achieving identified goals and milestones , and ensure agency officials are coordinating their activities in a way that is directed towards the achievement of shared goals and milestones .

we recommend that the director of omb take the following three actions: include the following in the quarterly reviews of cap goal progress , as required by gprama: a consistent set of information on progress made during the most recent quarter , overall trends , and the likelihood of meeting the planned level of performance ; goals at risk of not achieving the planned level of performance ; and the strategies being employed to improve performance .

work with the pic to establish and document procedures and criteria to assess cap goal implementation efforts and the status of goal execution , to ensure that the pic can conduct these assessments consistently across all goals and over time .

develop guidance similar to what exists for agency priority goal and strategic objective reviews , outlining the purposes of cap goal progress reviews , expectations for how the reviews should be carried out , and the roles and responsibilities of cap goal leaders , agency officials , and omb and pic staff in the review process .

to ensure that omb and cap goal leaders include all key contributors and can track and report fully on progress being made towards cap goals overall and each quarter , we recommend that the director of omb direct cap goal leaders to take the following four actions: identify all key contributors to the achievement of their goals ; identify annual planned levels of performance and quarterly targets for each cap goal ; develop plans to identify , collect , and report data necessary to demonstrate progress being made towards each cap goal or develop an alternative approach for tracking and reporting on progress quarterly ; and report the time frames for the completion of milestones ; the status of milestones ; and how milestones are aligned with strategies or initiatives that support the achievement of the goal .

we provided a draft of this report for review and comment to the director of omb , the secretaries of commerce and homeland security , the director of the office of personnel management , the administrator of the small business administration , as well as the officials we interviewed to collect information on the interim cap goals from the council on environmental quality , department of education , department of labor , department of veterans affairs , national science foundation , and the office of science and technology policy .

omb and pic staff provided oral comments on the draft , and we made technical changes as appropriate .

omb staff generally agreed to consider our recommendations .

for example , while they said that omb and pic staff will continue to work directly with cap goal leaders to convey suggested practices for reviewing performance , they will consider referencing principles and practices for data - driven performance reviews in future circular a - 11 guidance related to the management of cap goals .

furthermore , while they noted that quantitative performance data for some key measures may not available on a quarterly basis , they said that they will continue to work to develop more robust quarterly targets .

officials or staff from the departments of commerce and veterans affairs , and the office of science and technology policy provided technical comments , which we incorporated as appropriate .

we are sending copies of this report to the director of omb as well as appropriate congressional committees and other interested parties .

the report is also available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions concerning this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix vi .

this report is part of our response to a mandate that we evaluate the implementation of the federal government priority goals under the gpra modernization act of 2010 ( gprama ) .

due to the timing of our work , we focused on the implementation of the reporting and review requirements for the 14 interim cross - agency priority ( cap ) goals established in february 2012.about progress made towards the interim cap goals ; and ( 2 ) how , if at all , quarterly progress reflected gprama requirements and leading practices for data - driven reviews , as well as how they contributed to improved cross - agency performance and collaboration .

specifically , this report assesses ( 1 ) what is known to address these objectives , we interviewed representatives of 13 of the 14 interim goals .

for 8 of the 13 goals we spoke directly with the goal leader or deputy goal leader , along with , in some cases , staff from office of management and budget ( omb ) and agencies involved in supporting efforts related to the goals .

for the other five goals ( closing skills gaps , cybersecurity , data center consolidation , exports , and job training ) we met with agency officials or omb staff playing a key role in the management of interagency efforts related to the cap goal .

during these interviews , we asked officials questions concerning how the goal leader and officials from contributing agencies reviewed progress on the goal ; the interagency groups used to engage agency officials and manage efforts related to the goal ; the role that staff from omb and the performance improvement council ( pic ) played in the review process ; and any impact the cap goal designation and review processes had on performance , collaboration , and accountability .

we also participated in interviews with the goal leaders of 11 agency priority goals that were aligned with , or identified as a contributor to , a cap goal .

to further address the first objective , and assess what is known about progress made toward the interim cap goals , we analyzed information on identified performance metrics and milestones included in the quarterly status updates for each cap goal published on performance.gov .

we also analyzed relevant information collected through our interviews with cap goal leaders , deputies , and supporting staff .

we compared the data and information made available through the quarterly status updates with requirements in gprama that performance.gov include information for each goal on results achieved during the most recent quarter and overall trend data .

to assess the reliability of performance data and information available through performance.gov we collected information from omb and pic staff , and cap goal representatives , about data quality control procedures .

we determined that the data and information were sufficiently reliable for our analysis of what was reported on performance.gov about progress towards identified goals and milestones .

to address the second objective , we reviewed quarterly review memorandums developed for omb leadership for five quarters , from the third quarter of fiscal year 2012 to the third quarter of fiscal year 2013.we compared the contents of these review memorandums with requirements for the omb quarterly reviews established in gprama .

we also interviewed staff from omb and the pic to discuss the various approaches being used to review progress at the cap - goal level , the data collection and review process , and the role of the pic in supporting the quarterly review process .

to further address the second objective we reviewed ( where available ) documents created for interagency meetings , such as meeting agendas , presentation materials , meeting notes , and attendee lists .

we also observed one quarterly review meeting held for the closing skills gap goals , and conducted interviews with sub - goal leaders from the closing skills gaps and entrepreneurship and small business cap goals .

these interviews were used to learn more about the involvement of officials from contributing agencies in the quarterly review process for each cap goal , the processes that had been established to review progress at the sub - goal level , and to gain a more complete picture of participating agency officials' perceptions of the impact of the cap goals and review processes .

we selected these sub - goals through a two - part process .

of the eight cap goals for which we had completed interviews through the end of 2013 , the team selected one goal for which the goal leader held quarterly meetings dedicated to reviewing progress toward the cap goal with the goal's contributors ( closing skills gaps ) .

the team also selected a second goal for which the goal leader used a review process that did not rely on quarterly meetings between the goal leader and contributing agencies ( entrepreneurship and small business ) .

to ensure that the team would have at least one goal representing each type of goal , the team also ensured that one goal would be an outcome - oriented policy goal and one goal would be a management goal .

for both the closing skills gaps and entrepreneurship and small business cap goals the team then selected four sub - goals for interviews .

for the closing skills gaps cap goal the team interviewed the sub - goal leaders for the economist ; information technology / cybersecurity ; science , technology , engineering , and mathematics ( stem ) education ; and human resources sub - goals .

for the entrepreneurship and small business cap goal the team held interviews with the sub - goal leaders for the sub - goals to “accelerate commercialization of federal research grants,” “advance federal small business procurement goals,” “improve access to government services and information,” and “streamline immigration pathways for immigrant entrepreneurs.” these were selected to ensure that the team would capture sub - goals in which a range of approaches for measuring and reviewing progress were being used .

specifically , sub - goals were selected to ensure the team would have some that did , and did not , hold regular meetings , and some that did , and did not , track quantitative measures of performance or milestones with time frames .

our selection of these sub - goals was nonstatistical and therefore our findings from these interviews are not generalizable to the other cap goals .

we compared what we learned about review processes at the cap goal and sub - goal levels , through interviews and the collection of documentation , used by leaders from each goal against leading practices for performance reviews previously identified by gao .

because the scope of our review was to examine the implementation of quarterly progress reviews , we did not evaluate whether these goals were appropriate indicators of performance , sufficiently ambitious , or met other dimensions of quality .

we conducted our work from may 2013 to june 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

goal statement as part of expanding all broadband capabilities , ensure 4g wireless broadband coverage for 98 percent of americans by 2016 .

close critical skills gaps in the federal workforce to improve mission performance .

by september 30 , 2013 , close the skills gaps by 50 percent for three to five critical federal government occupations or competencies , and close additional agency - specific high risk occupation and competency gaps .

executive branch departments and agencies will achieve 95 percent implementation of the administration's priority cybersecurity capabilities by the end of fy 2014 .

these capabilities include strong authentication , trusted internet connections , and continuous monitoring .

improve information technology service delivery , reduce waste , and save $3 billion in taxpayer dollars by closing at least 2,500 data centers by fiscal year 2015 .

increase energy productivity ( amount of real gross domestic product in dollars / energy demand ) 50 percent by 2030 .

increase federal services to entrepreneurs and small businesses with an emphasis on 1 ) startups and growing firms and 2 ) underserved markets .

double u.s. exports by the end of 2014 .

the federal government will achieve a payment accuracy rate of 97 percent by the end of 2016 .

ensure our country has one of the most skilled workforces in the world by preparing 2 million workers with skills training by 2015 and improving the coordination and delivery of job training services .

the federal government will maintain the fiscal year 2012 square footage baseline of its office and warehouse inventory .

in support of the president's goal that the u.s. have the highest proportion of college graduates in the world by 2020 , the federal government will work with education partners to improve the quality of stem education at all levels to help increase the number of well - prepared graduates with stem degrees by one - third over the next 10 years , resulting in an additional 1 million graduates with degrees in stem subjects .

reduce the costs of acquiring common products and services by agencies' strategic sourcing of at least two new commodities or services in both 2013 and 2014 , that yield at least a 10 percent savings .

in addition , agencies must increase their use of federal strategic sourcing initiative vehicles by at least 10 percent in both fiscal years 2013 and 2014 .

by 2020 , the federal government will reduce its direct greenhouse gas emissions by 28 percent and will reduce its indirect greenhouse gas emissions by 13 percent by 2020 ( from 2008 baseline ) .

by september 30 , 2013 , increase the percent of eligible service members who will be served by career readiness and preparedness programs from 50 percent to 90 percent in order to improve their competitiveness in the job market .

goal leaders for 13 of 14 cross - agency priority ( cap ) goals leveraged interagency groups for the purposes of coordinating efforts designed to contribute to progress on the cross - agency priority goal .

this appendix includes information on the membership of these interagency groups , the frequency with which they met , and the purposes of those meetings .

membership fourteen agencies with federal property management or transportation funding responsibilities , and broadband or other related expertise .

to discuss best practices on broadband - related land management issues , and actions to implement an executive order on accelerating broadband infrastructure deployment .

senior officials from agencies considered major spectrum stakeholders and users of spectrum , including the departments of defense , justice , homeland security ( dhs ) , commerce , and the national aeronautics and space administration ( nasa ) .

to provide advice on spectrum policy and strategic plans , discuss commercial transfer of federal agency spectrum , and resolve issues affecting federal / non - federal users .

to review progress on performance metrics and actions taken to close skills gaps in each of the six sub - goal areas .

officials from national institute of standards and technology , general services administration ( gsa ) , dhs , national security staff , office of management and budget ( omb ) and performance improvement council .

twice each quarter beginning in 2013 , a meeting was held each quarter prior to the collection of data on agency progress on cybersecurity metrics .

another was held after data had been collected and analyzed to review and discuss agency progress .

data center consolidation program managers from 24 federal agencies .

to identify and disseminate key information about solutions and processes to help agencies make progress towards data center consolidation goals .

interagency group no interagency groups were used to manage efforts related to this goal .

interagency groups were used to manage efforts at the sub - goal level .

senior - level representatives from 24 participating agencies .

to oversee strategy , resources and timetables for the development of the businessusa website , resolve interagency issues and ensure department / agency viewpoints are represented .

mid - to - senior level program , technology and customer service managers from 24 participating agencies .

to assist the businessusa program management office coordinate the design , development , and operation of the businessusa website , and to track and monitor performance metrics on customer service and outcomes .

sbir / sttr program managers from11 agencies , and coordinating officials from small business administration ( sba ) and office of science and technology policy ( ostp ) to discuss the development of sbir / sttr program policy directives , the implementation of requirements , outreach and access to the programs , and program best practices .

to provide updates on relevant agency activities and identify opportunities for interagency collaboration .

to share best practices for expanding contracting to small and disadvantaged businesses , and reviewing progress on agency simplified - acquisition threshold goals .

to provide officials from the white house , sba , commerce , and omb with an opportunity to meet with senior agency leaders and discuss the steps agencies are taking to increase small business contracting .

membership principals ( cabinet secretaries and deputies ) and staff from 20 agencies involved in export policy , service , finance , and oversight .

to review progress on deliverables supporting the national export strategy , communications , and the status of individual export promotion initiatives .

bi - weekly to monthly to review the status of agency implementation of do not pay requirements and milestones , and guidance for implementation .

officials from agencies with “high - priority” programs , as designated by omb .

to discuss the government - wide improper payment initiative and overall strategy .

to discuss expanding access to job training performance data , and opportunities to promote its use at the local , state , and federal levels .

among other policy discussions , to discuss the development of agency “freeze the footprint” plans .

to discuss policy to guide the federal government on sustainability issues , and to discuss sustainability goals .

every 4-6 weeks , during the development of the 5-year strategic plan .

to develop a 5-year strategic plan for federal support for stem education .

representatives from departments of defense , energy , and veterans affairs ( va ) , dhs , hhs , gsa , nasa , and sba .

to discuss the development and adoption of strategic sourcing options .

to review ongoing policy initiatives and opportunities for collaboration between agencies .

to develop and implement a redesigned veterans transition program .

meetings with officials from each agency .

according to omb staff , during these reviews participants reviewed metrics from across the agency's information technology portfolio , which included , in some cases , those related to data center consolidation .

each quarter staff supporting the goal leader would collect updated information on contributing agency priority goals for the purposes of updating the quarterly status update .

each quarter the deputy goal leader would collect updated information on goals and milestones from the leaders of each of 10 sub - goals for the purposes of developing the quarterly status update .

the deputy goal leader would follow - up with sub - goal leaders or agency officials , as necessary , to address issues or questions about the status of efforts .

the goal leader would then review and approve the quarterly status update .

some sub - goal leaders would hold in - person meetings with officials from contributing agencies to , among other things , review progress on identified goals and milestones .

see appendix iii for information on interagency groups that were used to manage efforts for four of the sub - goals .

each quarter the goal leader , with the assistance of staff from commerce and the pic , would collect updated information on relevant agency metrics and activities for the purposes of updating the quarterly status update .

periodic meetings of the export promotion cabinet / trade promotion coordinating committee , and its small business and other working groups , were also used to discuss the status of export promotion efforts and progress on specific deliverables .

each year omb would collect and report data on agency improper payment rates .

staff from the omb office of federal financial management led monthly meetings with agency representatives to discuss the implementation of the do not pay initiative , which was designed to contribute to the reduction of improper payments .

the department of treasury , as the agency leading implementation of the do not pay initiative , would track agency progress on implementation milestones .

each quarter staff from the pic would collect updated information on progress towards agency milestones , and work with the goal leader on the development of the quarterly status update .

after this goal was revised in the second quarter of 2013 , a new review process to track agency adherence to the goal was under development by omb .

twice a year the council on environmental quality ( ceq ) would collect and review quantitative and qualitative data on agency progress towards established sustainability goals , including the reduction of agency greenhouse gas emissions .

following the collection of these data , the goal leader hosted meetings of the steering committee on federal sustainability , which were used to discuss federal sustainability policy and progress on sustainability goals .

according to ceq staff , the goal leader and ceq staff would meet with representatives from agencies about sustainability issues on an ad hoc basis .

in instances where there was a gap between an agency's actual performance and the target established in that area , the goal leader , or other staff from ceq , would meet with officials from that agency to discuss ways to address the performance gap .

each quarter the goal leader would collect updated information on agency milestones for inclusion in the quarterly status updates .

progress on some identified strategies to achieve the goal , such as the national science foundation's efforts to improve undergraduate stem education , were reviewed at the agency level .

after progress was reviewed at the agency level the information was passed onto the goal leader and reported publicly in the quarterly status update .

each quarter the general services administration would collect data on agency adoption and spending rates for the federal strategic sourcing initiative ( fssi ) solutions for domestic delivery and office supplies .

the strategic sourcing leadership council met bi - monthly to guide the creation and adoption of new fssi options , and , as part of that effort , might review quarterly data on agency adoption and spending rates .

according to the goal leader , each month staff from the departments of defense and veterans affairs , and the pic , would provide data for “one - pagers” and other status update documents with key pieces of relevant information , such as the veterans' unemployment rate and the number of active employers on the veteran's job bank .

these one - pagers would be used to inform regular interagency policy council ( ipc ) discussions , along with more specific briefing memorandums , which were used to cover the latest issues , keep stakeholders focused on overall outcomes , and to inform discussion around specific outliers .

some of the data in these one - pagers would also be incorporated into the quarterly status updates .

more frequently , issue papers and data analysis were provided to veterans employment initiative ( vei ) task force and ipc members as needed to address topical issues .

ongoing milestone reviews held by the vei task force and its associated working groups on education , employment , transition , and entrepreneurship , provided an opportunity to discuss strategies being employed to improve performance .

this appendix includes the print version of the text and rollover graphics contained in interactive figure 2 .

overall planned level of performance …achieve 95 percent implementation of the administration's priority cybersecurity capabilities by the end of fiscal year 2014 .

data reported for primary performance goal …save $3 billion in taxpayer dollars by closing at least 2500 data centers by fiscal year 2015 .

“agencies have already closed 640 data centers… .

overall planned level of performance double u.s. exports by the end of 2014 .

frequency of data reporting for overall goal quarterly …agencies' strategic sourcing of at least two new commodities or services in both 2013 and 2014 , that yield at least a 10 percent savings… in addition , agencies must increase their use of federal strategic sourcing initiative vehicles by at least 10 percent in both fiscal years 2013 and 2014 .

…ensure 4g wireless broadband coverage for 98 percent of americans by 2016 .

overall planned level of performance …achieve a payment accuracy rate of 97 percent by the end of 2016 .

“data not reported .

increase federal services to entrepreneurs and small businesses with an emphasis on 1 ) startups and growing firms and 2 ) underserved markets .

“data not reported .

“data not reported .

the federal government will maintain the fiscal year 2012 square footage baseline of its office and warehouse inventory .

“data not reported .

…increase the number of well - prepared graduates with stem degrees by one - third over the next 10 years , resulting in an additional 1 million graduates with degrees in stem subjects .

frequency of data reporting for overall goal “data not reported .

“data not reported .

in addition to the contact named above , elizabeth curda ( assistant director ) and adam miles supervised the development of this report .

virginia chanley , jehan chase , steven putansu , stacy ann spence , and dan webb made significant contributions to this report .

deirdre duffy and robert robinson also made key contributions .

