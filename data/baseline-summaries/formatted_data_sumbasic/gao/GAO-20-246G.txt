gao provides congress , federal agencies , and the public with objective , reliable information to help the government save money and work more efficiently .

science and technology ( s&t ) issues figure prominently in problems that congress confronts , and one component of the assistance gao provides to congress is the production of technology assessments ( tas ) .

this ta design handbook provides gao staff and others with tools to consider for supporting robust and rigorous assessments .

this handbook is particularly important given the need for gao to provide insight and foresight on the effects of technologies and corresponding policy implications related to a wide range of s&t issues .

while other organizations — including , previously , the office of technology assessment ( ota ) and a number of ta organizations elsewhere , such as in europe — conduct tas , each has different relationships with its stakeholders and government bodies .

while their ta approaches and considerations may vary , some may still find portions of this handbook useful .

we are seeking comments on this draft of the handbook .

this handbook elaborates on gao's approach to ta design and outlines the importance of ta design ( chapter 1 ) , describes the process of developing ta design ( chapter 2 ) , and provides approaches to select ta design and implementation challenges ( chapter 3 ) .

the handbook generally follows the format of the 2012 gao methodology transfer paper , designing evaluations .

given that gao is likely to learn from its current expansion of ta work , gao will review and update this draft handbook as needed , based on experience gained through ongoing ta activities and external feedback .

gao has defined ta as the thorough and balanced analysis of significant primary , secondary , indirect , and delayed interactions of a technological innovation with society , the environment , and the economy and the present and foreseen consequences and impacts of those interactions .

the effects of those interactions can have implications .

recognizing this , gao has in some of its products included policy options , which policymakers could consider in the context of a given technology and policy goal .

in this context , policy goals serve to guide the development of policy options by stating the overall aim of the policy options , and helping to identify the landscape and scope of policy options .

policy options can be defined as a set of alternatives or menu of options ( including the status quo ) that policymakers , such as legislative bodies , government agencies , and other groups , could consider taking .

gao is exploring approaches to making policy options a more standard feature or component of tas .

in this handbook , we include considerations related to the development of policy options that ta teams may wish to consider at each phase of ta design .

in the united states , the technology assessment act of 1972 established ota , which was an analytical support agency of the congress , but was defunded in 1995 .

in 2002 , congress asked gao to begin conducting tas , and in 2008 , a permanent ta function was established at gao .

in 2019 , the science , technology assessment , and analytics ( staa ) team was created at gao .

staa has taken a number of steps to account for the unique requirements of tas and related s&t work to meet the needs of congress .

gao tas share some common design principles with gao's general audit engagement process , which is centered around intentional and purpose - driven design .

while general design principles are shared across gao's product lines , tas are distinct from other gao product lines , such as performance audits , financial audits , and other routine non - audit products .

the specialized content of tas , their scope , and their purpose , warrant some different considerations .

table 1 highlights some similarities and differences between tas and other gao product lines , including where tas follow aspects of gao's general audit engagement process , and where tas may further emphasize certain steps or require additional steps during the engagement process .

not all steps have been included in table 1 .

we expect to continue to regularly seek input and advice from external experts related to the ta design handbook initiative , as well as throughout the conduct of gao tas .

while the primary audience of this handbook is gao staff , we expect that other organizations engaged or interested in tas will find portions of this handbook useful .

for example , these organizations could use the handbook to gain insight into gao's ta design approaches , as well as use aspects of gao's ta design approaches that they deem helpful .

we will accept comments on this handbook at tahandbook@gao.gov for approximately 1 year after publication .

the handbook seeks to affirm and document gao's approach , and we expect to modify and refine this handbook , as needed , based both on comments received and further experience in conducting tas that include policy options .

we anticipate that the final handbook will contain additional information and details related to ta design , such as elaborating on specific methodologies that could be applied within this general design framework , including those designed to identify policy options .

below is a summary of the approach we used to identify and document ta design steps and considerations for this handbook .

for more information , please refer to appendix i: objectives , scope , and methodology .

reviewed select gao documents , including designing evaluations ( gao - 12-208g ) , published gao tas , select gao products utilizing policy analysis approaches to present policy options , and other gao reports reviewed select office of technology assessment reports reviewed select congressional research service reports reviewed select literature regarding tas and related to development and analysis of policy options held an expert forum to gather experts' input regarding ta design considered experiences of gao teams that have successfully assessed and incorporated policy options into gao products , as well as gao teams that are incorporating policy options into their ta design collected input from gao staff who provided key contributions to gao tas , regarding challenges to ta design and implementation and possible solutions we conducted our work to develop this handbook from april 2019 to december 2019 in accordance with all sections of gao's quality assurance framework that are relevant to our objectives .

the framework requires that we plan and perform the engagement to obtain sufficient and appropriate evidence to meet our stated objectives and to discuss any limitations in our work .

we believe that the information and data obtained , and the analysis conducted , provide a reasonable basis for any findings and conclusions in this product .

this chapter underscores the importance of technology assessment ( ta ) design , outlining reasons for performing tas and for spending time on the design of tas .

the information presented in this chapter is based on review of results of a literature search , an expert forum , select gao reports , and experiences of gao teams and technical specialists .

for more information , please refer to appendix i: objectives , scope , and methodology .

tas are significant given their increasing importance to policymakers , and the growing effects of s&t on society , economy , and other areas .

while technological changes can be positive , they can also be disruptive .

therefore , it is critical for congress to be able to understand and evaluate these changes , to ensure , for example , national security and global competitiveness .

examples of potential uses of tas related to enhancing knowledge and awareness to assist decision - making include: highlight potential short , medium , and long - term impacts of a elaborate on and communicate the risks and benefits associated with a technology , including early insights into the potential impacts of technology highlight the status , viability , and relative maturity of a technology plan and evaluate federal investments in s&t gao tas are most commonly requested by congressional committees , which may use them to , among other things , make decisions regarding allocating or reallocating resources to address research gaps , support updated rulemaking for a regulatory agency , or inform a legislative agenda or the development of a national strategy .

technologies present opportunities and challenges that may vary , depending in part on the policy context in which they are evaluated .

therefore , part of a ta is considering the policy context surrounding a given technology .

gao may , where appropriate , identify and analyze policy options as part of its tas , which may also include: clarifying and summarizing policy - related issues and challenges , and providing information that can be used for decision - making .

in this situation , policy options can be defined as a set of alternatives or menu of options ( including the status quo ) that policymakers , such as legislative bodies , government agencies , and other groups , could consider taking .

policy options can be used to articulate a range of possible actions a policymaker could consider in the context of a given technology and policy goal .

policy options do not state what policymakers should do in a given circumstance with a certain technology .

policy options do not endorse or recommend a particular course of action ; they are not recommendations or matters for congressional consideration , which gao makes in its audits .

in addition , policy options are addressed to policymakers more broadly , and are not addressed to a specific federal agency or entity .

developing a written ta design helps ta teams agree on and communicate a clear plan of action to the project team and the team's advisers , requesters , and other stakeholders .

written ta designs also help guide and coordinate the project team's activities and facilitate documentation of decisions and procedures in the final report .

in addition , focusing the ta on answering specific researchable questions can assist teams to define and select the appropriate scope , approach , and type of product , ensuring usefulness of the product to the intended users .

more specific reasons for spending time on systematically designing a ta include: enhance its quality , credibility , and usefulness ensure independence of the analysis ensure effective use of resources , including time data collection and quality assurance of data can be costly and time - consuming .

a thorough consideration of design options can ensure that collection and analysis of the data are relevant , sufficient , and appropriate to answer the researchable question ( s ) , and helps to mitigate the risk of collecting unnecessary evidence and incurring additional costs .

this chapter highlights design phases , cross - cutting considerations , and gao ta design examples for sound technology assessment ( ta ) design .

to ensure that the information and analyses in tas meet policymakers' needs , it is particularly useful to outline the phases and considerations involved in sound ta design , while remaining aware of the iterative and nonlinear process of designing a ta .

the information presented in this chapter is based on review of results of a literature search , an expert forum , select gao reports , and experiences of gao teams and technical specialists .

for more information , please refer to appendix i: objectives , scope , and methodology .

below are questions to consider for a sound ta design .

reflecting on these questions may help teams make important decisions ( like selecting an appropriate design ) and ensure quality tas .

does the design address the needs of the congressional requester ? .

will the design yield a quality , independent , balanced , thorough , and objective product ? .

will the design likely yield information that will be useful to stakeholders ? .

will the design likely yield valid conclusions on the basis of sufficient and credible evidence ? .

will the design yield results in the desired time frame ? .

will the design likely yield results within the constraints of the resources available ? .

how will policy options be identified and assessed , if applicable ? .

figure 1 outlines three phases and seven considerations for ta design .

while figure 1 presents ta design as a series of phases , actual execution is highly iterative and nonlinear .

teams may need to be prepared to re - visit design decisions as information is gathered or circumstances change .

below are some considerations for the team to think about while designing a ta and throughout the process of performing the ta .

this list is not exhaustive , and some of the considerations may not be unique to tas .

of the technology ) and context of the technology ( such as social , political , legal , and economic factors ) circumstances change and new information comes to light , it may be necessary to revisit scope and design .

the initial situational analysis may also be used to: inform the goal ( s ) , purpose , and objectives ( also known as researchable questions ) challenges to design and implementation of the ta , such as: ( 1 ) possible changes in operating environment ; ( 2 ) characterizing or quantifying anticipatory factors , uncertainty , and future condition ( s ) ; and ( 3 ) lack of or limitations with data .

see chapter 3 for more specific examples .

communication strategy: consider potential users of the product ( s ) and how information regarding the ta will be communicated .

how results are communicated can affect how they are used , so it is important for ta teams to discuss communication options .

statement .

ta teams will need to think about whether the initial policy options are appropriate to the size and scope of the ta , as well as whether they are in line with the policy goal and the overall ta purpose and objectives .

in keeping with the iterative nature of ta design and execution , any initial policy option list will be revisited , modified , or refined , as needed , as the work progresses and more information is gained .

ta teams may also need to plan to include policy analysis and exploration of the ramifications of each policy option during subsequent design and implementation phases .

during this phase , ta teams continue to build on the situational analysis work and gather more background information .

in addition , ta teams: confirm and validate the scope from phase 1 reach agreement with stakeholders on the initial design may perform an “environmental scan” to further highlight limitations , assumptions , divergent points of view , potential bias , and other factors that may help the team select a design other specific activities that take place during this phase include: identify and select appropriate design , methodologies , and analytical approaches ( refer to the next section of this chapter for example ta design approaches and app .

iii for examples of ta methods ) examples of data collection and analytical techniques used in gao tas to date include: interviews , literature review , expert forums , site visits , technology readiness assessments , surveys , conceptual models , small group discussion , content analysis such as delphi , among others .

ota reported using similar methodologies for its tas ( ota , policy analysis at ota: a staff assessment , 1983 ) .

identify and select appropriate data sources , or the need to gather data identify , select , and possibly develop appropriate dimensions of analysis , if applicable develop possible policy goal ( s ) clarify the possible initial policy options that will be considered and describe how they may be analyzed , if applicable identify and consult with external experts to inform design and implementation , and assist with external review , as appropriate if policy options are being considered , it is important to determine the relevant dimensions along which to analyze the options .

the dimensions will be highly context - specific , vary from ta to ta , and depend on the scope and policy goal statement of the ta .

during this phase , the design and project plan are being implemented , potentially while aspects of phase 2 are still underway .

it is important to consider changes in the operating context — such as changes in the operating environment , understanding of the issues , and access to information — and review and make changes to the design and project plan accordingly .

we reviewed select gao products that used policy analysis to present policy options .

we found that these products used a variety of data collection and analytical approaches , such as: interviews , literature review , survey , expert forum , site visits , case studies , analysis of secondary data , content analysis , among others .

if an initial policy options list was developed earlier in design , it may be necessary to revisit the list as work progresses .

during this phase , ta teams may gather additional information regarding the policy options , further analyze policy options , and present the results of the analysis .

policy options are to be presented in a balanced way , including presentation of opportunities and considerations , and not resulting in a single overall ranking of policy options .

we found that gao tas used a variety of design approaches and methodologies to answer various categories of design objectives ( researchable questions ) .

gao tas generally include one or more of the following categories of design objectives , which are not mutually exclusive: ( 1 ) describe status of and challenges to development of a technology ; ( 2 ) assess opportunities and challenges arising from the use of a technology ; and ( 3 ) identify and assess cost - effectiveness , other policy considerations , or options related to the use of a technology .

provided below are example questions , design approaches , and gao tas , for each of these categories of objectives .

gao ta examples were used given our familiarity with gao products , though numerous non - gao ta design examples exist .

this is not intended to be a comprehensive list of design examples .

for more examples of methodologies , please refer to app .

iii .

describing the status and challenges to the development of a technology .

table 2 provides example questions , design approaches , and gao tas , for design objectives related to describing the status and challenges to the development of a technology .

questions may address , for example , what the current state of the technology is , and may involve identifying and describing the status of the technology , which gao tas have done using a variety of methods .

assessing opportunities and challenges that may result from the use of a technology .

table 3 provides example questions , design approaches , and gao tas , for design objectives related to assessing opportunities and challenges that may result from the use of a technology .

questions may address , for example , what are the expected or realized benefits of the technology , and may involve gathering and assessing evidence on the results from using the technology , which gao tas have done using a variety of methods .

assessing cost - effectiveness , policy considerations , or policy options related to the use of a technology .

table 4 provides example questions , design approaches , and gao tas , for design objectives related to assessing cost - effectiveness , policy considerations , or policy options related to the use of a technology .

questions may address , for example , what are the economic trade - offs of a technology , and may involve gathering and analyzing evidence related to cost , which gao tas have done using a variety of methods .

this chapter describes select challenges regarding technology assessment ( ta ) design and implementation , as well as possible strategies to mitigate those challenges .

the information in this chapter is based on review of results of a literature search , an expert forum , select gao reports , and experiences of gao teams and technical specialists .

the tables provided below are not intended to be a comprehensive list of challenges or strategies .

for more information , please refer to appendix i: objectives , scope , and methodology .

to be useful , ta assessment products must be readable and timely , among other things , which may present a challenge for numerous reasons .

table 5 provides examples of potential mitigation strategies to address these challenges .

another challenge in ta design arises from determining policy goals and policy options , and estimating their potential impacts .

many of the effects of policy decisions may be distant , and policy outcomes may be uncertain at the time of the ta .

table 6 provides examples of potential mitigation strategies to address these challenges .

tas are complex and interdisciplinary , and emerging technologies are inherently difficult to assess .

table 7 provides examples of potential mitigation strategies to address these challenges .

an additional challenge in conducting tas is engaging all relevant internal and external stakeholders , ensuring none are overlooked .

table 8 provides examples of potential mitigation strategies to address this challenge .

this handbook identifies key steps and considerations in designing technology assessments ( tas ) .

below is a summary of methodologies used for all chapters of the handbook .

we reviewed gao documents , including: designing evaluations ( gao - 12-208g ) select gao products utilizing policy analysis approaches to identify and assess policy options we reviewed and analyzed 14 gao tas , including their designs and considerations , using a data collection instrument that contained fields regarding each report's purpose , methodologies , and key considerations for each methodology used ( such as strengths and weaknesses ) .

the data collection instrument also contained fields regarding whether policy considerations were presented or if specific policy options were identified and assessed in each ta report , what methodologies were used to identify and assess policy options , and key considerations associated with the methodologies used .

we also reviewed gao reports from non - ta product lines that utilized policy analysis approaches to assess policy options .

an initial pool of 56 gao reports was generated based on a keyword search of gao's reports database .

of the 56 gao reports , 12 were selected for review based on the following criteria: ( 1 ) the reports were publicly released after january 1 , 2013 and ( 2 ) the reports included identification and assessment of policy options ( not solely a presentation of agency actions related to policy options or general policy considerations ) .

testimonies and correspondence were excluded .

we analyzed each of these selected gao reports according to a data collection instrument that contained the following fields regarding policy options in the report: purpose , methodologies , and key considerations for each methodology used ( such as strengths and weaknesses ) .

a list of gao documents reviewed is provided below .

retirement security: some parental and spousal caregivers face financial risks .

gao - 19-382 .

washington , d.c.: may 1 , 2019 .

gao science technology assessment , and analytics team: initial plan and considerations moving forward .

washington , d.c.: april 10 , 2019 .

retirement savings: additional data and analysis could provide insight into early withdrawals .

gao - 19-179 .

washington , d.c.: march 28 , 2019 .

critical infrastructure protection: protecting the electric grid from geomagnetic disturbances .

gao - 19-98 .

washington , d.c.: december 19 , 2018 .

postal retiree health benefits: unsustainable finances need to be addressed .

gao - 18-602 .

washington , d.c.: august 31 , 2018 .

data collection seminar participant manual .

washington , d.c.: march 2018 .

artificial intelligence: emerging opportunities , challenges and implications .

gao - 18-142sp .

washington , d.c.: march 28 , 2018 .

chemical innovation: technologies to make processes and products more sustainable .

gao - 18-307 .

washington , d.c.: february 8 , 2018 .

federal regulations: key considerations for agency design and enforcement decisions .

gao - 18-22 .

washington , d.c.: october 19 , 2017 .

medical devices: capabilities and challenges of technologies to enable rapid diagnoses of infectious diseases .

gao - 17-347 .

washington , d.c.: august 14 , 2017 .

u.s .

postal service: key considerations for potential changes to usps's monopolies .

gao - 17-543 .

washington , d.c.: june 22 , 2017 .

internet of things: status and implications of an increasingly connected world .

gao - 17-75 .

washington , d.c.: may 15 , 2017 .

flood insurance: comprehensive reform could improve solvency and enhance resilience .

gao - 17-425 .

washington , d.c.: april 27 , 2017 .

flood insurance: review of fema study and report on community - based options .

gao - 16-766 .

washington , d.c.: august 24 , 2016 .

medicaid: key policy and data considerations for designing a per capita cap on federal funding .

gao - 16-726 .

washington , d.c.: august 10 , 2016 .

municipal freshwater scarcity: using technology to improve distribution system efficiency and tap nontraditional water sources .

gao - 16-474 .

washington , d.c.: april 29 , 2016 .

gao memorandum: quality assurance framework requirements for technology assessments .

washington , d.c.: april 6 , 2016 .

biosurveillance: ongoing challenges and future considerations for dhs biosurveillance efforts .

gao - 16-413t .

washington , d.c.: february 11 , 2016 .

social security's future: answers to key questions .

gao - 16-75sp .

washington , d.c.: october 2015 .

water in the energy sector: reducing freshwater use in hydraulic fracturing and thermoelectric power plant cooling .

gao - 15-545 .

washington , d.c.: august 7 , 2015 .

nuclear reactors: status and challenges in development and deployment of new commercial concepts .

gao - 15-652 .

washington , d.c.: july 28 , 2015 .

veterans' disability benefits: improvements needed to better ensure va unemployability decisions are well supported .

gao - 15-735t .

washington , d.c.: july 15 , 2015 .

debt limit: market response to recent impasses underscores need to consider alternative approaches .

gao - 15-476 .

washington , d.c.: july 9 , 2015 .

temporary assistance for needy families: potential options to improve performance and oversight .

gao - 13-431 .

washington , d.c.: may 15 , 2013 .

private pensions: timely action needed to address impending multiemployer plan insolvencies .

gao - 13-240 .

washington , d.c.: march 28 , 2013 .

designing evaluations: 2012 revision .

gao - 12-208g .

washington , d.c.: january 2012 .

neutron detectors: alternatives to using helium - 3 .

gao - 11-753 .

washington , d.c.: september 3 , 2011 .

climate engineering: technical status , future directions , and potential responses .

gao - 11-71 .

washington , d.c.: july 28 , 2011 .

technology assessment: explosives detection technologies to protect passenger rail .

gao - 10-898 .

washington , d.c.: july 28 , 2010 .

technology assessment: protecting structures and improving communications during wildland fires .

gao - 05-380 .

washington , d.c.: april 26 , 2005 .

technology assessment: cybersecurity for critical infrastructure protection .

gao - 04-321 .

washington , d.c.: may 28 , 2004 .

technology assessment: using biometrics for border security .

gao - 03-174 .

washington , d.c: november 15 , 2002 .

we spoke with and gathered input from gao teams that are in the process of or have successfully assessed and incorporated policy options into gao products .

in addition , to augment our understanding of ta design and implementation challenges , we collected input from gao staff who had provided key contributions to gao tas .

specifically , we asked for their thoughts regarding: ( 1 ) the strengths and limitations of ta methodologies and ( 2 ) challenges they faced , and strategies to address those challenges .

a gao librarian performed a search for relevant office of technology assessment ( ota ) reports , using keyword searches .

from this initial list of ota reports , we selected 17 reports to review that were frameworks , guides , models , or other compilations .

we also reviewed the methodologies of the ota reports selected for review .

a list of ota reports reviewed is included below .

office of technology assessment .

insider's guide to ota .

washington , d.c.: january 1995 .

office of technology assessment .

policy analysis at ota: a staff assessment .

washington , d.c.: may 1993 .

office of technology assessment .

research assistants handbook .

washington , d.c.: june 1992 .

office of technology assessment .

strengths and weaknesses of ota policy analysis .

washington , d.c.: 1992 .

office of technology assessment .

the ota orange book: policies and procedures of the office of technology assessment: communication with congress and the public .

washington , d.c.: february 1986 .

office of technology assessment .

what ota is , what ota does , how ota works .

washington , d.c.: march 1983 .

office of technology assessment .

draft: an ota handbook .

washington , d.c.: june 7 , 1982 .

office of technology assessment .

draft: a management overview methodology for technology assessment .

washington , d.c.: february 2 , 1981 .

* office of technology assessment .

draft: technology assessment in industry: a counterproductive myth .

washington , d.c.: january 30 , 1981 .

* office of technology assessment .

draft: technology assessment methodology and management practices .

washington , d.c.: january 12 , 1981 .

* office of technology assessment .

draft: technology assessment in the private sector .

washington , d.c.: january 9 , 1981 .

* office of technology assessment .

draft: a process for technology assessment based on decision analysis .

washington , d.c.: january 1981 .

* office of technology assessment .

draft: technology as social organization .

washington , d.c.: january 1981 .

* office of technology assessment .

a summary of the doctoral dissertation: a decision theoretic model of congressional technology assessment .

washington , d.c.: january 1981 .

* office of technology assessment .

report on task force findings and recommendations: prepared by the ota task force on ta methodology and management .

washington , d.c.: august 13 , 1980 .

office of technology assessment .

phase i survey results: draft papers prepared for the task force on ta methodology and management .

washington , d.c.: april 10 , 1980 .

we identified a pool of 29 congressional research service ( crs ) reports to consider reviewing that were technology assessments or included an analysis of policy options , based on a keyword search of crs's website .

we also interviewed crs officials .

of the initial 29 crs reports we identified , we selected six crs reports to review , based on the following criteria: ( 1 ) published within the past 15 years ( 2004-2019 ) and ( 2 ) if a review of technology ( technology assessment ) and / or policy options was included .

reports were excluded based on the following criteria: ( 1 ) for technology assessment related reports — if they represented a summary of a technology assessment that was included in our review or ( 2 ) for policy options related reports — the report did not indicate how crs arrived at the policy options ( no methodology to review or analyze ) .

a list of crs reports reviewed is included below .

congressional research service .

advanced nuclear reactors: technology overview and current issues .

washington , d.c.: april 18 , 2019 .

congressional research service .

drug shortages: causes , fda authority , and policy options .

washington , d.c.: december 27 , 2018 .

congressional research service .

policy options for multiemployer defined benefit pension plans .

washington , d.c.: september 12 , 2018 .

congressional research service .

shale energy technology assessment: current and emerging water practices .

washington , d.c.: july 14 , 2014 .

congressional research service .

carbon capture: a technology assessment .

washington , d.c.: november 5 , 2013 .

congressional research service .

energy storage for power grids and electric transportation: a technology assessment .

washington , d.c.: march 27 , 2012 .

a gao librarian performed a literature search based on keyword searches for two areas — ta and policy options .

for ta literature , the team selected 29 documents to review that were frameworks , guides , models , or other compilations , based on a review of the literature titles and abstracts .

in general , we excluded specialized types of tas , such as health - related tas , as we focused on ta design more broadly .

for policy options literature , the team selected 14 documents to review that were frameworks , guides , models , or other compilations and focused on policy options related to science and technology .

we also asked experts we consulted to suggest literature for our review ; these suggestions confirmed the literature list noted below .

a list of literature reviewed is included below .

grunwald , armin .

technology assessment in practice and theory .

london and new york: routledge , 2019 .

armstrong , joe e. , and willis w. harman .

strategies for conducting technology assessments .

london and new york: routledge , 2019 .

noh , heeyong , ju - hwan seo , hyoung sun yoo , and sungjoo lee .

“how to improve a technology evaluation model: a data - driven approach.” technovation , vol .

72 / 73 ( 2018 ) : p. 1-12 .

larsson , a. , t. fasth , m. wärnhjelm , l. ekenberg , and m. danielson .

“policy analysis on the fly with an online multicriteria cardinal ranking tool.” journal of multi - criteria decision analysis , vol .

25 ( 2018 ) : p. 55-66 .

nooren , p. , n. van gorp , n. van eijk , and r. o. fathaigh .

“should we regulate digital platforms ? .

a new framework for evaluating policy options.” policy and internet , vol .

10 , no .

3 ( 2018 ) : p. 264-301 .

smith , a. , k. collins , and d. mavris .

“survey of technology forecasting techniques for complex systems.” paper presented at 58th aiaa / asce / ahs / asc structures , structural dynamics , and materials conference , grapevine , tx ( 2017 ) .

ibrahim , o. , and a. larsson .

“a systems tool for structuring public policy problems and design of policy options.” int .

j .

electronic governance , vol .

9 , nos .

1 / 2 ( 2017 ) : p. 4-26 .

christopher , a. simon .

public policy preferences and outcomes .

3rd ed .

new york: routledge , 2017 .

weimer , david l. , and r. aidan vining .

policy analysis concepts and practice .

6th ed .

london and new york: routledge , 2017 .

mulder , k. “technology assessment.” in foresight in organizations: methods and tools , edited by van der duin , patrick , 109-124 , 2016 .

coates , joseph f. “a 21st century agenda for technology assessment.” technological forecasting and social change , vol .

113 part a ( 2016 ) : p. 107-109 .

coates , joseph f. “next stages in technology assessment: topics and tools.” technological forecasting and social change , vol .

113 ( 2016 ) : p. 112-114 .

mazurkiewicz , a. , b. belina , b. poteralska , t. giesko , and w. karsznia .

“universal methodology for the innovative technologies assessment.” proceedings of the european conference on innovation and entrepreneurship ( 2015 ) : p. 458-467 .

sadowski , j .

“office of technology assessment: history , implementation , and participatory critique.” technology in society , vol .

42 ( 2015 ) : p. 9-20 .

larsson , a. , o. ibrahim .

“modeling for policy formulation: causal mapping , scenario generation , and decision evaluation.” in electronic participation: 7th ifip 8.5 international conference , 135-146 , springer , 2015 .

moseley , c. , h. kleinert , k. sheppard - jones , and s. hall .

“using research evidence to inform public policy decisions.” intellectual and developmental disabilities , vol .

51 ( 2013 ) : p. 412-422 .

calof , j. , r. miller , and m. jackson .

“towards impactful foresight: viewpoints from foresight consultants and academics.” foresight , vol .

14 ( 2012 ) : p. 82-97 .

parliaments and civil society in technology assessment , collaborative project on mobilization and mutual learning actions in european parliamentary technology assessment .

the netherlands: rathenau instituut , 2012 .

blair , p. d. “scientific advice for policy in the united states: lessons from the national academies and the former congressional office of technology assessment.” in the politics of scientific advice: institutional design for quality assurance , ed .

lentsch , justus , 297-333 , 2011 .

paracchini , m.l. , c. pacini , m.l.m .

jones , and m. pérez - soba .

“an aggregation framework to link indicators associated with multifunctional land use to the stakeholder evaluation of policy options.” ecological indicators , vol .

11 ( 2011 ) : p 71-80 .

roper , a. t. , s. w. cunningham , a. l. porter , t. w. mason , f. a. rossini , and j .

banks .

forecasting and management of technology , 2nd ed .

new jersey: wiley , 2011 .

lepori , b. , e. reale , and r. tijssen .

“designing indicators for policy decisions: challenges , tensions and good practices: introduction to a special issue.” research evaluation , vol .

20 , no .

1 ( 2011 ) : p. 3-5 .

russel , a. w. , f. m. vanclay , and h. j. aslin h.j .

“technology assessment in social context: the case for a new framework for assessing and shaping technological developments.” impact assessment and project appraisal , vol .

28 , no .

2 ( 2010 ) : p. 109-116 .

shiroyama , h. , g. yoshizawa , g. , m. matsuo , and t. suzuki .

“institutional options and operational issues in technology assessment: lessons from experiences in the united states and europe.” paper presented at atlanta conference on science and innovation policy , atlanta , 2009 .

tran , t.a. , and t. daim t. “a taxonomic review of methods and tools applied in technology assessment.” technological forecasting and social change , vol .

75 ( 2008 ) : p. 1396-1405 .

brun , g. , and g. hirsch hadorn .

“ranking policy options for sustainable development.” poiesis prax , vol .

5 ( 2008 ) : p. 15-31 .

tran , t.a .

“review of methods and tools applied in technology assessment literature.” paper presented at portland international conference on management of engineering and technology , portland oregon , 2007 .

burgess , j. , a. stirling , j. clark , g. davies , m. eames , k. staley , and s. williamson .

“deliberative mapping: a novel analytic - deliberative methodology to support contested science - policy decisions.” public understanding of science , vol .

16 ( 2007 ) : p. 299-322 .

decker , m. , and m. ladikas .

bridges between science , society and policy: technology assessment — methods and impacts .

berlin: springer - verlag , 2004 .

guston , d. h. , and d. sarewitz .

“real - time technology assessment.” technology in society , vol .

24 ( 2002 ) : p. 93-109 .

rip , a .

“technology assessment.” in international encyclopedia of the social & behavioral science , vol .

23 , edited by smelster , n. j. and b. p. baltes , 15512-15515 .

amsterdam: elsevier , 2001 .

van den ende , j. , k. mulder , m. knot , e. moors , and p. vergragt .

“traditional and modern technology assessment: toward a toolkit.” technological forecasting and social change , vol .

58 ( 1998 ) : p. 5-21 .

wood , f. b .

“lessons in technology assessment: methodology and management at ota.” technological forecasting and social change , vol .

54 ( 1997 ) : p. 145-162 .

janes , m. c. “a review of the development of technology assessment.” international journal of technology management , vol .

11 , no .

5-6 ( 1996 ) : p. 507-522 .

hastbacka , m. a. , and c. g. greenwald .

“technology assessment - are you doing it right ? ” arthur d. little – prism , no .

4 ( 1994 ) .

rivera , w. m. , d. j. gustafson , and s. l. corning .

“policy options in developing agricultural extension systems: a framework for analysis.” international journal of lifelong education , vol .

10 , no .

1 ( 1991 ) : p. 61-74 .

lee , a. m. , and p. l. bereano .

“developing technology assessment methodology: some insights and experiences.” technological forecasting and social change , vol .

19 ( 1981 ) : p. 15-31 .

porter , a. l. , f. a. rossini , s. r. carpenter , and a. t. roper .

a guidebook for technology assessment and impact analysis , vol .

4 .

new york and oxford: north holland , 1980 .

pulver , g.c .

“a theoretical framework for the analysis of community economic development policy options.” in nonmetropolitan industrial growth and community change , edited by summers , g. and a. selvik , 105-117 .

massachusetts and toronto: lexington books , 1979 .

ascher , w. “problems of forecasting and technology assessment.” technological forecasting and social change , vol .

13 , no .

2 ( 1979 ) : p. 149-156 .

majone , g. “technology assessment and policy analysis.” policy sciences , vol .

8 , no .

2 ( 1977 ) : p. 173-175 .

berg , m. , k. chen , and g. zissis .

“a value - oriented policy generation methodology for technology assessment.” technological forecasting and social change , vol .

4 , no .

4 ( 1976 ) : p. 401-420 .

lasswell , harold d. a pre - view of policy sciences .

policy sciences book series .

new york: elsevier , 1971 .

we held a forum to gather experts' opinions regarding ta design .

an initial list of experts was prepared based on a review of gao ta reports , literature , and referral by other experts .

experts were selected based on their knowledge and expertise in the subject , including: ( 1 ) prior participation on a national academy of sciences panel or other similar meeting ; ( 2 ) leadership position in one or more organizations or sectors relevant to technology research and development implementation or policy ; and ( 3 ) relevant publications or sponsorship of reports .

care was also taken to ensure a balance of sectors , backgrounds , and specific areas of expertise ( eg , science , technology , policy , information technology , and law ) .

we also asked the experts to suggest literature for our review ; these suggestions confirmed the literature list noted above .

a list of external experts consulted is included below .

dr. jeffrey m. alexander , senior manager , innovation policy , rti international dr. robert d. atkinson , president , information technology and innovation foundation mr. david bancroft , executive director , international association for impact assessment mr. duane blackburn , s&t policy analyst , office of the cto , mitre dr. peter d. blair , executive director , division of engineering and physical sciences , national academies of sciences , engineering , and medicine ms. marjory blumenthal , acting associate director , acquisition and technology policy center ; senior policy researcher , rand corporation mr. chris j. brantley , managing director , institute of electrical and electronics engineers , inc. , usa dr. jonathan p. caulkins , h. guyford stever university professor of operations research and public policy , carnegie mellon university mr. dan chenok , executive director , center for the business of government , ibm dr. gerald epstein , distinguished research fellow , center for the study of weapons of mass destruction , national defense university dr. robert m. friedman , vice president for policy and university relations , j. craig venter institute mr. zach graves , head of policy , lincoln network ms. allison c. lerner , inspector general , national science foundation mr. mike molnar , director of office of advanced manufacturing , national institute of standards and technology dr. michael h. moloney , ceo , american institute of physics dr. ali nouri , president , federation of american scientists dr. jon m. peha , professor , engineering and public policy ; courtesy professor , electrical and computer engineering , carnegie mellon university dr. stephanie s. shipp , deputy director and professor , university of virginia , biocomplexity institute and initiative , social and decision analytics division dr. daniel sarewitz , co - director , consortium for science , policy & outcomes professor of science and society , school for the future of innovation in society , arizona state university ms. rosemarie truman , founder and ceo , center for advancing innovation dr. chris tyler , director of research and policy , department of science , technology , engineering and public policy ( steapp ) , university college london ( ucl ) .

as part of gao's quality assurance framework , gao's general design and project plan templates contain five phases that are followed in sequential order , with modifications or changes as needed .

gao technology assessments ( tas ) use these templates , as applicable .

throughout the phases , the status of the work , including decisions , is communicated to stakeholders and congressional committees that requested the work .

provided below is a summary of the activities gao staff undertake during each of the phases , and is based on a review of gao documentation related to engagement phases .

phase i: acceptance engagement characteristics such as risk level or internal stakeholders are determined at a high - level engagement acceptance meeting .

engagement teams obtain a copy of and review the congressional request letter ( s ) , as applicable .

phase ii: planning and proposed design staff are assigned to the engagement and set up the electronic engagement documentation set folders .

staff enter standard information regarding the engagement in gao's engagement management system ( ems ) , which is used to monitor the status of the engagement throughout the engagement process and regularly updated .

engagement teams hold an initiation meeting with engagement stakeholders to discuss potential research questions , design options , and stakeholder involvement .

engagement teams clarify engagement objectives and approach through discussions with the congressional requesters , as applicable .

engagement teams obtain background information .

for example , to gather information about the topic and any work already performed , teams may conduct a literature review , search prior and ongoing gao work related to the topic , or consult with external stakeholders , outside experts , and agency officials , including the congressional research service , congressional budget office , and inspectors general of federal agencies .

engagement teams formally notify agencies of the engagement through a notification letter , and hold an entrance conference , as applicable .

engagement teams prepare a design matrix , project plan , risk assessment tool , data reliability assessment , and all participants on engagements , including stakeholders , affirm their independence .

the design matrix is a tool that describes: researchable questions ; criteria ; information required and sources ; scope and methodology ; and limitations .

the project plan identifies key activities and tasks , dates for completing them , and staff assigned .

engagement teams secure approval to move forward with engagement approach at a high - level engagement review meeting .

phase iii: evidence gathering , finalizing design , and analysis engagement teams finalize design: teams work with internal stakeholders to confirm soundness and reach agreement on proposed initial design .

if engagement teams and stakeholders conclude that additional work is needed or the design faces significant implementation challenges , design is reviewed and modified , as needed .

engagement teams collect and analyze evidence: teams may collect and analyze evidence using a variety of methodologies including document review , interviews , surveys , focus groups , and various forms of data analysis .

for example , engagement teams may meet with agency officials and outside experts , as applicable , to gather evidence .

engagement teams assess evidence and agree on conclusions: teams assess whether the evidence collected is sufficient and appropriate to support findings and conclusions reached for each objective .

once sufficient evidence is collected and analyzed , the team discusses how the evidence supports potential findings and shares these findings with stakeholders , generally in the form of a formal message agreement meeting .

engagement teams update congressional requesters , as applicable , on the engagement status and potential findings .

phase iv: product development engagement teams draft product: after drafting the product , teams send draft to internal stakeholders for review .

teams also send draft to relevant external parties , including relevant agencies , to confirm facts and obtain their views .

teams identify sources of all information in the draft and an independent analyst ( not on the team ) verifies the sources through a process called indexing and referencing .

engagement teams perform exit conferences with agencies , as applicable , to discuss findings and potential recommendations .

agencies and external parties are given the opportunity to comment on the draft , as applicable .

engagement teams communicate findings and potential recommendations , as well as timeframes for issuing the product , to congressional requesters , as applicable .

the draft product is copy - edited , prepared for issuance , and publicly released on gao's website , as applicable .

phase v: results engagement documentation is closed out .

engagement teams conduct follow - up , track the results , and prepare reports on the status of recommendations and financial and non - financial benefits , as applicable , using gao's results tracking system .

this appendix provides examples of methods and analytical approaches that gao technology assessment ( ta ) teams can use to examine different types of evidence .

also included in this appendix are considerations of the strengths , limitations , and synergies among evidence types and methods , which can be useful to consider throughout design to ensure that evidence is sufficient and appropriate to answer the researchable questions .

examples from gao tas were used given our familiarity with gao products , though numerous other ( non - gao ) examples of ta methods exist .

this appendix included a review of gao reports and select literature , and is not intended to be comprehensive .

this is a simplified presentation of methods , and there is variation in the levels of structure of the example methods .

this appendix is divided into several sections , including by evidentiary types: testimonial , documentary , and physical .

for each of these types of evidence , example methods are presented with low and high levels of structure , and include examples of considerations ( such as general benefits and limitations ) that analysts may consider .

in general , more highly structured approaches generate increased consistency and comparability of results that allows for stronger quantification .

less structured approaches tend to provide more flexibility and context , and richer illustrative evidence .

testimonial evidence is elicited from respondents to understand their experience , opinions , knowledge , and behavior , and it can be obtained through a variety of methods , including inquiries , interviews , focus groups , expert forums , or questionnaires .

testimonial evidence can be gathered from individuals who may be responding personally based on their own experience in an official capacity to represent agencies or other entities , or groups , who may share individual level responses , or may present a single group response .

group testimony enables interactions that can be used to explore similarities and differences among participants , to identify tensions or consensus in a group , or to explore ideas for subsequent research and collaboration .

it is important to evaluate the objectivity , credibility , and reliability of testimonial evidence .

analysts may use a combination of approaches to gather testimonial evidence , depending on the relevant population ( s ) of respondents , intended analytical approach ( es ) , likely respondent burden , and resource considerations .

table 9 provides more examples .

documentary evidence is existing information , such as letters , contracts , accounting records , invoices , spreadsheets , database extracts , electronically stored information , and management information on performance .

it is important to evaluate the objectivity , credibility , and reliability of documentary evidence .

analysts may use a combination of approaches to gather documentary evidence , depending on the relevant sources and types of documents , intended analytical approach ( es ) , and resource considerations .

table 10 provides more examples .

physical evidence is obtained by direct inspection or observation of people , property , or events .

the appropriateness of physical evidence depends on when , where , and how the inspection or observation was made and whether it was recorded in a manner that fairly represents the facts observed .

common considerations for physical evidence include the reliability of site selection , intended analytical approaches , and resource considerations .

table 11 provides more examples .

gao may also rely on agency and other secondary data .

considerations for those secondary data are dependent on the type , source , and collection method , and could include all of the considerations above .

use of secondary data is usually more efficient than collecting new data on a topic , and administrative records ( a form of documentary evidence ) are generally not as prone to self - reporting biases that may be present in testimonial evidence .

however , when secondary data are used , more work may be required to assess whether data are reliable and appropriate for a given purpose .

for example , analysts will gather all appropriate documentation , including record layout , data element dictionaries , user's guides , and data maintenance procedures .

depending on the database , procedures and analysis can be very complex — and it would be important to note assumptions , limitations , and caveats pertaining to the data , which may affect the conclusions that can be drawn based on the analyses .

examples of analytical approaches found in the literature to analyze data include: interpretive structural modeling: shows a graphical relationship among all elements to aid in structuring a complex issue area , and may be helpful in delineating scope .

trend extrapolation: is a family of techniques to project time - series data using specific rules , and may be helpful in forecasting technology .

scenarios: is a composite description of possible future states incorporating a number of characteristics , and may be helpful in policy analysis .

scanning methods , such as checklists: is listing factors to consider in a particular area of inquiry , and may be helpful in identifying potential impacts .

tracing methods , such as relevance trees: includes identifying sequential chains of cause and effect or other relationships , and may be helpful in identifying potential impacts .

cross - effect matrices: are two - dimensional matrix representations to show the interaction between two sets of elements , and may be helpful in analyzing consequences of policy options .

simulation models: are a simplified representation of a real system that is used to explain dynamic relationships of the system , and may be helpful in identifying impacts and forecasting technology .

benefit - cost analysis: is a systematic quantitative method of assessing the desirability of government projects or policies when it is important to take a long view of future effects and a broad view of possible side effects .

decision analysis: is an aid to compare alternatives by weighing the probabilities of occurrences and the magnitudes of their impacts , and may be helpful in determining impacts and assessing policy options .

scaling: is an aid that may include developing a matrix that identifies potential impact related to an activity and stakeholder group , and qualitatively or quantitatively assesses the potential impact , and may be helpful analyzing potential impacts , including of policy options .

in addition to the contacts named above , key contributors to this report were r. scott fletcher ( assistant director ) , diantha garms ( analyst - in - charge ) , nora adkins , colleen candrl , virginia chanley , robert cramer , david dornisch , john de ferrari , dennis mayo , anika mcmillon , saraann moessbauer , amanda postiglione , steven putansu , oliver richard , meg tulloch , ronald schwenn , ben shouse , amber sinclair , ardith spence , andrew stavisky , david c. trimble , and edith yuh .

the government accountability office , the audit , evaluation , and investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents at no cost is through our website .

each weekday afternoon , gao posts on its website newly released reports , testimony , and correspondence .

you can also subscribe to gao's email updates to receive notification of newly posted products .

the price of each gao publication reflects gao's actual cost of production and distribution and depends on the number of pages in the publication and whether the publication is printed in color or black and white .

pricing and ordering information is posted on gao's website , https: / / www.gao.gov / ordering.htm .

place orders by calling ( 202 ) 512-6000 , toll free ( 866 ) 801-7077 , or tdd ( 202 ) 512-2537 .

orders may be paid for using american express , discover card , mastercard , visa , check , or money order .

call for additional information .

connect with gao on facebook , flickr , twitter , and youtube .

subscribe to our rss feeds or email updates .

listen to our podcasts .

visit gao on the web at https: / / www.gao.gov .

james - christian blockwood , managing director , spel@gao.gov , ( 202 ) 512-4707 u.s. government accountability office , 441 g street nw , room 7814 , washington , dc 20548 please print on recycled paper .

