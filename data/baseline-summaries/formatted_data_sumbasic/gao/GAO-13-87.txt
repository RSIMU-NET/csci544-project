in fiscal year 2011 , 26 key federal agencies reported spending approximately $79 billion on information technology ( it ) systems to the office of management and budget ( omb ) .

of the $79 billion , $54 billion was reported by the agencies to be spent on operations and maintenance ( o&m ) , which consists of existing legacy systems ( i.e. , steady state ) and systems that are in both development and o&m ( known as mixed life cycle ) .

given the size and magnitude of these investments , it is important that agencies effectively manage the operations and maintenance of existing investments to ensure they ( 1 ) continue to meet agency needs , ( 2 ) deliver value , and ( 3 ) do not unnecessarily duplicate or overlap with other investments .

omb directs agencies to periodically examine the performance of these investments against , among other things , established cost , schedule , and performance goals .

specifically , omb calls for agencies to perform annual operational analyses ( oa ) , which is a key method for examining the performance of such investments in o&m .

as requested , our objective was to determine the extent to which federal agencies assess the performance of steady state it investments in accordance with this omb guidance .

to do so , we selected five agencies , the departments of defense ( dod ) , health and human services ( hhs ) , homeland security ( dhs ) , the treasury ( treasury ) , and veterans affairs ( va ) , which have the largest budgets for major steady state it investments , accounting for approximately $37 billion annually or about 70 percent of all reported o&m spending in fiscal year 2011 .

in doing this we focused on these agencies' 75 major it investments valued at $4.6 billion annually that were strictly in the o&m phase ( i.e. , excluded mixed cycle investments ) .

we determined whether the agencies developed oa policies in accordance with omb guidance .

we also determined whether these agencies were conducting oas to manage these investments .

more specifically , we reviewed all of these agencies' oas performed during fiscal year 2011 and compared them to omb and related criteria .

we conducted this performance audit from october 2011 to september 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

details on our objective , scope , and methodology are contained in appendix i .

in fiscal year 2011 , the 26 key federal agencies that report to omb on their it investments reported spending approximately $79 billion on a wide variety of it systems .

of this amount , agencies reported spending $54 billion on o&m for existing steady state investments ; they plan on spending about $53 billion in fiscal year 2012 .

as shown in figure 1 , these amounts represent a significant majority ( i.e. , 69 and 71 percent ) of the overall reported it spending in 2011 ( $79 billion ) and that planned for 2012 ( $75 billion ) , respectively .

although o&m spending governmentwide is about 70 percent of total it spending , the amount spent by each agency varies from a high of 98 percent to a low of 45 percent ( as shown in the table below ) .

the national aeronautics and space administration ( nasa ) reported spending approximately 90 percent of its total it spending on o&m with the remaining 10 percent going to new investments .

the reason for this mix of spending , according to nasa officials , is due to nasa's mission ( i.e. , the space shuttle mission ) which relies heavily on legacy systems .

by contrast , the department of transportation reported spending approximately 45 percent of its total it costs on o&m with the other 55 percent going to new investments .

according to department officials , this mix of spending is largely due to the fact that the department has a number of it development investments underway that involve large financial commitments relative to o&m investments .

to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a further , the act places responsibility for result of these investments.managing investments with the heads of agencies and establishes chief information officers to advise and assist agency heads in carrying out this responsibility .

in carrying out its responsibilities , omb uses several data collection mechanisms to oversee federal it spending during the annual budget formulation process .

specifically , omb requires 26 key federal departments and agencies to provide information to it related to their it investments ( called exhibit 53s ) and capital asset plans and business cases ( called exhibit 300s ) .

exhibit 53 .

the purpose of the exhibit 53 is to identify all it investments — both major and nonmajor organization .

information included on agency exhibit 53s is designed , in part , to help omb better understand what agencies are spending on it investments .

the information also supports cost analyses prescribed by the clinger - cohen act .

as part of the annual budget , omb publishes a report on it spending for the federal government representing a compilation of exhibit 53 data submitted by the 26 agencies .

 — and their associated costs within a federal exhibit 300 .

the purpose of the exhibit 300 is to provide a business case for each major it investment and to allow omb to monitor it investments once they are funded .

agencies are required to provide information on each major investment's cost , schedule , and performance .

according to omb guidance , a major investment is a system or acquisition requiring special management attention because of its importance to the mission or function of the agency , a component of the agency , or another organization ; is for financial management and obligates more than $500,000 annually ; has significant program or policy implications ; has high executive visibility ; has high development , operating , or maintenance costs ; is funded through other than direct appropriations ; or is defined as major by the agency's capital planning and investment control process .

display of these data is intended to allow omb , other oversight bodies , and the general public to hold the government agencies accountable for results and progress .

since the dashboard has been implemented , we have reported and made recommendations to improve the data accuracy and reliability .

in 2010 and 2011 , we reported on the progress of the dashboard and made recommendations to further improve how it rates investments relative to current performance .

capital programming guide , supplement to omb circular a - 11 , part 7 ( july 2012 ) ; omb memorandum m - 10-27 ( june 2010 ) , requires agencies to establish a policy for performing oas on steady state investments as a part of managing and monitoring investment baselines .

a comparison of current performance with a pre - established cost areas for innovation in the areas of customer satisfaction , strategic and business results , and financial performance ; indication if the agency revisited alternative methods for achieving the same mission needs and strategic goals ; consideration of issues , such as greater utilization of technology or consolidation of investments to better meet organizational goals ; an ongoing review of the status of the risks identified in the investment's planning and acquisition phases ; identification of whether there is a need to redesign , modify , or terminate the investment ; an analysis on the need for improved methodology ( i.e. , better ways for the investment to meet cost and performance goals ) ; lessons learned ; cost or schedule variances ; recommendations to redesign or modify an asset in advance of potential problems ; and overlap with other investments .

with regard to overseeing the agencies' development of policies and annual performance , omb officials responsible for governmentwide oa policy stated that they expect agencies to perform all the steps specified in the guidance and to be prepared to show documentation as evidence of compliance with the guidance should omb decide to check .

although omb guidance calls for agencies to develop an oa policy and perform such analyses annually , the extent to which the five federal agencies we reviewed carried out these tasks varied significantly .

specifically , dhs and hhs developed a policy and conducted oas , but in doing so , they excluded key investments and assessment factors .

dod , treasury , and va did not develop a policy or conduct oas .

the following table shows the total number of steady state investments for each agency , and provides the number and budgeted amount for those investments that underwent an assessment and those that did not .

until agencies more completely address their policy and performance shortcomings , there is increased risk that existing multibillion dollar investments will continue to be funded although it is not fully known whether they meet their intended objectives .

dhs and hhs had developed policies , which contained all performance factors identified in omb's guidance .

specifically , in 2008 , dhs issued its policy called “operational analysis guidance.” the guidance states that oas should be performed on an annual basis to evaluate the operational results of agency steady state investments .

in addition , the guidance provides a report template which includes sections that should be contained and reported on in it .

dhs's policy addressed all of the key factors in the omb guidance , including , for example , assessing current costs against life - cycle costs and a detailed schedule assessment .

in 2008 , hhs issued its policy called “practices guide: annual operational analysis.” the guide states oas are required to be performed on an annual basis .

further , the guide includes a template and a checklist for conducting them .

in addition , agencies within the department have issued their own policy .

for example , in 2011 , the centers for disease control and prevention issued its “operational analysis guide” and in 2010 , the national institutes of health issued these policies contained all of the its framework , “a how - to guide.”key factors identified in the omb policy , such as measuring the effect the investment has on the performing organization itself and identifying any areas for innovation .

further , dhs and hhs performed oas on some of their steady state investments , but not for all .

specifically , of their 52 total steady state investments , dhs and hhs conducted analyses on 23 with total budgets of $1.4 billion and did not conduct analyses on 29 investments with total budgets of $1.1 billion .

more specifically , of dhs's 44 steady state investments , the department conducted oas on16 of them , which have an annual budget of $1.2 billion ; it did not perform analyses on the other 28 , which have an annual budget of almost $1 billion .

of hhs's 8 steady state investments , the department conducted analyses on 7 of them , which have an annual budget of $207 million ; it did not perform an oa on the remaining investment , which has an annual budget of $77 million .

tables 3 and 4 show dhs's and hhs's steady state investments by component agency and whether oas were performed on these investments in fiscal year 2011 .

 ( details of our analysis of all the analyses and a brief description of the investments are included in app .

ii. ) .

in addition , although dhs and hhs performed analyses , the agencies did not address all key factors in conducting them .

specifically , of dhs's 16 oas , which were to include a total 272 key factors , dhs: addressed 145 ( or 53 percent ) , partially addressed 20 ( or 7 percent ) , and did not address 107 ( or 39 percent ) ; and of hhs's 7 oas , which were to include a total of 119 key factors , hhs: addressed 66 ( or 55 percent ) , partially addressed 6 ( or 5 percent ) , and did not address 47 ( or 39 percent ) factors .

the following provides key examples by component agency to illustrate how factors were fully addressed , partially addressed , or not addressed at all .

in its operational analysis of its u.s. coast guard business intelligence investment , the u.s. coast guard fully addressed five key factors ( see table 16 in app .

ii ) .

for example , on the factor regarding whether the investment supports customer processes as designed and is delivering the goods and services it was designed to deliver , the component measured ( via surveys ) customer satisfaction , usage trends , system trends , and feedback , and used this information to implement system improvements .

u.s. coast guard partially addressed three factors .

for example , in assessing performance goals , the component identified two major goals of the investment , but did not include how or when these goals were to be achieved .

u.s. coast guard did not address nine key factors , including those on identifying lessons learned and reviewing the status of risk versus cost , schedule , and performance .

these factors are important because they provide management with key information on why problems occurred and how they can be avoided in the future , as well as whether the investment is worth pursing given anticipated costs , benefits , and associated risks .

in assessing the information technology infrastructure program , transportation security administration addressed eight key factors ( see table 14 in app .

ii ) .

for example , on the factor calling for performance of a structured schedule assessment , the component analyzed a detailed list of task descriptions , start and end dates , and planned versus actual costs to ensure the investment is performing against an established schedule which can minimize costs over the life cycle of an investment .

the component partially addressed one key factor ; specifically , the factor calling for identifying whether the investment supports customer processes and is delivering the goods and services intended .

in assessing this factor , transportation security administration conducted surveys to measure customer satisfaction , but in doing so did not include measures to assess whether the investment was delivering the goods and services it was designed to deliver .

the component did not address eight key factors .

for example , it did not identify any areas for innovation or whether the investment overlapped with other systems .

these latter steps are essential to identifying investment improvements , increasing value and reducing costs , and eliminating duplicate systems and the costs associated with them .

for the u.s. immigration and customs enforcement's intelligence fusion system , the component fully addressed nine key factors ( see table 9 in app .

ii ) .

these factors included analyzing current costs against life - cycle costs and whether the investment supports customer processes as designed and is delivering the goods and services it was designed to deliver , through measures such as customer surveys and help desk metrics .

the component partially addressed the factor on identifying areas ( eg , business results and customer satisfaction , financial performance ) for innovation .

specifically , it identified two areas for innovation , namely strategic and business results and customer satisfaction , but did not address financial performance .

u.s. immigration and customs enforcement did not address seven key factors ; for example , it did not identify lessons learned or assess whether to modify or terminate the investment .

fully addressing these factors is crucial to agencies in determining whether to continue an investment that is not performing as required .

for its infrastructure , office automation , and telecommunications investment , indian health service fully addressed 14 key factors ( see table 26 in app .

ii ) .

for example , in addressing the factor on assessing performance goals , it analyzed the investment's performance goals against the results to date for each goal .

the component partially addressed the factor on the status of risks versus cost , schedule , and performance .

specifically , it analyzed cost and schedule progress , but did not include an assessment of risks .

indian health service did not address two key factors .

for example , it did not identify lessons learned and whether the investment overlapped with other systems .

addressing these factors is important because they help agencies to , among other things , identify where cost - effective improvements can be made .

hhs's health resources and services administration fully addressed 15 key factors in its operational analysis of its electronic handbooks program management office ( see table 24 in app .

ii ) .

for example , it conducted a structured assessment of performance goals , including a detailed list of goals , and how and when they were addressed .

health resources and services administration only partially addressed the key factor on providing a structured schedule assessment .

specifically , the component identified certain parts of the investment schedule , such as standard and unscheduled maintenance efforts , but other schedule elements , such as completion dates and goals , were not identified .

health resources and services administration did not address one factor .

for example , it did not assess current costs against life - cycle costs .

this factor is important because it can , among other things , provide information to agency decision makers with answers to whether annual operating and maintenance costs are comparable to the estimated costs developed during the development phase .

in its analysis of the business intelligence system , national institutes of health fully assessed six key factors ( see table 27 in app .

ii ) .

for example , on the factor calling for identifying whether the investment supports customer processes as designed and is delivering the goods and services it was designed to deliver , the component analyzed user and customer assessments that showed improvement in this area .

the component partially addressed the key factor on measuring the effect an investment has on the performing organization itself .

for example , national institutes of health identified that the investment was in line with the appropriate component enterprise architecture , but did not identify the effect the investment had on other aspects of the department such as its mission and business processes .

national institutes of health did not address 10 factors , including lessons learned and determining whether to modify or terminate the investment .

these factors are critical to whether to continue an investment that is not performing as required .

with regard to why analyses were not performed on all investments and why those that were conducted did not fully address all factors , dhs and hhs attributed these shortfalls to the following: officials from dhs's office of the cio who are responsible for overseeing the performance of oas department - wide told us the components only performed 16 of the 44 analyses and did not address all key factors ( in the 16 oas that were performed ) because they were not consistently implementing department and omb policy as they should have because it was not a priority .

to illustrate their point , the officials told us that while most components strive to perform annual analyses , other components do not require them to be conducted on an annual basis citing other tasks as taking precedence .

to address these shortfalls , the department recently took steps to make oas a priority and to ensure consistent application of department and omb policy .

specifically , in may 2012 , dhs's cio issued a memorandum stating that all steady state it investments are required to have an annual oa completed no later than june of each fiscal year and that component cios are to work with program managers to implement and ensure compliance with dhs oa requirements .

further , as part of this initiative , dhs's cio plans to assign resources and responsibility to cio office staff to review and ensure compliance with dhs's policy .

these are steps in the right direction ; however , the dhs cio officials told us that these initiatives will not be fully implemented until sometime in fiscal year 2013 .

hhs officials from the office of the cio said their shortfalls ( i.e. , one component did not perform an oa and those that were performed did not address all factors ) were due , in part , to inconsistent implementation of department and omb policy across the components due to analyses not being a priority .

as an example of this , officials from the office of the hhs cio who are responsible for overseeing the department oa program cited how they had planned to implement an initiative to annually review all analyses performed by the components to ensure consistency and quality but have not been able to do so due to limited cio staff being assigned to other initiatives .

although dhs and hhs had 23 investments — with collective annual budgets of $1.4 billion — that underwent oas , these investments were not thoroughly assessed against all key factors .

until these agencies assess all steady state investments and ensure that they are fully assessed against factors , there is increased risk that these agencies will not know whether the multibillion dollar investments fully meet their intended objectives .

dod , treasury , and va had not developed a policy for performing oas and did not conduct oas for their 23 steady state investments that have combined annual budgets of $2.1 billion .

specifically , dod did not conduct analyses for its 4 major investments that have annual budgets totaling $381 million , treasury did not conduct such analyses for its16 major investments that have annual budgets totaling $152 million , and va did not conduct oas for its 3 major investments that have annual budgets totaling $1.6 billion .

regarding why dod and va had not developed policies and are not performing analyses , officials from those agencies stated that in lieu of conducting oas , they assess the performance of steady state investments as part of developing their annual exhibit 300 submissions to omb .

while we have previously reported that using the exhibit 300 process can be a tool to manage investment performance , our analysis shows that the process does not fully address 11 of the 17 factors .

for example , the exhibit 300 process does not fully provide for addressing the following factors: identifying alternative methods for achieving the same mission needs and strategic goals .

doing this is important because it helps agencies assess whether they are using the most cost effective solution to achieving agency goals ; addressing greater utilization of technology or consolidation of investments will better meet organizational goals .

it is also critical to helping agencies ensure that their investments are meeting performance goals in the most cost - effective manner ; identifying lessons learned , why problems occurred , or how savings were realized , which is essential to avoid repeating the same mistakes , which helps saving resources ; and identifying where the agency needs to redesign , modify , or terminate the investment , which is a means to achieving solutions that return the greatest benefit for funds invested .

further , omb officials told us that the exhibit 300 process is not a substitute for conducting oas .

although omb requires oas for all steady state systems , its guidance does not provide a mechanism for ensuring they are completed and submitted to omb for review .

in particular , it does not have a reporting mechanism that provides for public transparency into the results of these assessments , which the it dashboard could provide .

having such a mechanism for the performance of steady state systems is consistent with clinger - cohen act requirements that call for omb to analyze and report on the performance of it capital investments .

moreover , such public disclosure promotes increased transparency which is one of omb's goals in establishing the it dashboard .

treasury officials from the department's office of the cio told us they decided not to perform oas in 2011 and instead decided to use the time to develop a policy .

however , the officials stated that they did not anticipate the policy to be completed until the end of this calendar year .

until these agencies establish policies and begin performing oa assessments for their steady state investments , there is increased risk that these agencies will not know whether the multibillion dollar investments fully meet their intended objectives , therefore increasing the potential for waste and duplication .

the federal government has made a multibillion dollar commitment to operating and maintaining its it investments .

omb has established guidance for federal agencies to use to evaluate the performance of such investments , including whether a sound basis exists for agencies to continue funding them .

dhs and hhs had developed policies in accordance with omb guidance and performed analyses , but did not do so for all of their investments and their analyses did not address all key factors .

during the course of this review , dhs reiterated the importance of performing oas and issued a memorandum with initiatives to address the department's shortcomings .

further , dod , treasury , and va had not developed a policy nor had they performed oas .

taken together , these five agencies continue to invest billions of dollars each year on it steady state investments without ensuring that they are continuing to meet agency needs and are delivering value .

these shortcomings are due in part to a number of factors , including agencies relying on budget submission processes through their annual exhibit 300 submissions , which are not intended as a substitute for oas , and not viewing performance of these assessments as a priority .

although omb's current guidance does not require agencies to report on oas to omb , using existing oversight and transparency tools like the it dashboard could help ensure that these important performance assessments are completed and available for public viewing .

nonetheless , until the agencies address these shortcomings and ensure all their steady state investments are fully assessed , there is increased potential for these multibillion dollar investments to result in unnecessary waste and duplication .

to ensure that major steady state it investments are being adequately analyzed , we recommend that the secretaries of defense , veterans affairs , and the treasury direct appropriate officials to develop an oa policy , annually perform oas on all investments , and ensure the assessments include all key factors .

in addition , we recommend that the secretaries of homeland security and health and human services direct their chief information officers to ensure oas are performed annually on all major steady state investments and the assessments include all key factors .

further , to ensure that oa policies are developed and that annual analyses are conducted and to promote transparency into the results of these analyses , we recommend that the director of omb revise existing guidance to include directing agencies to report on the it dashboard the results from the oas of their steady state investments .

in commenting on a draft of this report , omb and the five agencies agreed with our findings and recommendations .

their comments are discussed in more detail below .

in oral comments , staff from omb's office of e - government and information technology concurred with our recommendations and stated that omb had recently initiated an effort to address the specific recommendation directed to it .

specifically , the staff stated that omb's fiscal year 2014 budget guidance ( dated august 3 , 2012 ) directs agencies to include oas as part of their exhibit 300 submissions to omb .

in written comments — signed by dod's deputy chief information officer for information enterprise and reprinted in appendix iii — dod concurred with our recommendation and said it plans to establish an oa policy in coordination with omb .

in written comments — signed by the director of the departmental gao - oig liaison office and reprinted in appendix iv — dhs concurred with our recommendation .

the department , after receiving our draft report , identified and provided to us oas that it had performed for 3 additional investments in fiscal year 2011 .

dhs also provided technical comments which we incorporated in the report as appropriate .

in comments provided via e - mail from its gao intake coordinator within the office of the assistant secretary for legislation , hhs stated that it did not have any general comments on the report .

the department did provide technical comments which we incorporated in the report as appropriate .

in written comments — signed by the deputy assistant secretary for information systems and chief information officer and reprinted in appendix v — treasury agreed with the report's recommendations .

in addition , after receiving our draft report , the department identified and provided to us oas that it had performed on 9 of its 17 investments in fiscal year 2011 .

in written comments — signed by its chief of staff and reprinted in appendix vi — va generally agreed with our conclusions and concurred with the recommendation to it .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to interested congressional committees ; the secretaries of defense , homeland security , health and human services , the treasury , veterans affairs ; and other interested parties .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions on the matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vii .

our objective was to determine the extent to which selected federal agencies assess the performance of steady state information technology ( it ) investments in accordance with office of management and budget ( omb ) guidance .

to accomplish our objective , we selected the five agencies ( departments of defense ( dod ) , health and human services ( hhs ) , homeland security ( dhs ) , the treasury ( treasury ) , and veterans affairs ( va ) ) that have the largest budgets for major steady state it investments ; collectively , these investments accounted for approximately $37 billion annually or about 70 percent of all reported it operations and maintenance ( o&m ) spending .

in particular , we focused on these agencies' 75 major it investments valued at $4.6 billion annually that were strictly in the steady state phase as opposed to the agencies' other o&m investments — called mixed life - cycle investments by the agencies and omb — which are not solely in o&m ; these mixed life - cycle investments have projects under development as well as projects being placed into o&m .

we analyzed omb's guidance and identified a key practice called operational analysis ( oa ) that agencies are to use to assess the performance of existing o&m investments .

we also interviewed omb officials to corroborate our understanding of the key practice .

we then determined whether the five agencies developed oa policies as called for by the omb guidance .

specifically , we compared each agency's policy , if they had one , to the omb criteria to determine the extent of compliance and where there were variances .

we further determined whether the agencies were conducting analyses .

specifically , for the 75 major investments , we determined whether the agencies had performed an oa on each of them .

in those cases where one had been performed , we analyzed the agencies' efforts to address the omb criteria in the analysis and categorized the extent to which the omb key factors had been addressed using the following criteria: yes: if all aspects of the key factor specified in the omb criteria were fully addressed .

no: if none of the aspects of the key factor were addressed .

partial: if some , but not the entire key factor was addressed .

in cases where agencies did not fully address factors ( i.e. , partially or not all ) , we analyzed documentation and interviewed agency officials , including staff from the offices of the chief information officers responsible for overseeing these investments , to assist in identifying causes for shortfalls and any actions planned or underway to address the causes .

we conducted this performance audit from october 2011 to september 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

tables 5-20 show our analysis for dhs's investments with oas in fiscal year 2011 .

tables 21-27 show our analysis for hhs's investments with oas in fiscal year 2011 .

in addition to the contact name above , individuals making contributions to this report included gary mountjoy ( assistant director ) , gerard aflague , rebecca eyler , lori martinez , and teresa smith .

