i am pleased to be here today to discuss the highlights and recommendations of our selected reports that focused on key aspects of the federal government's acquisition and management of information technology ( it ) investments .

as reported to the office of management and budget ( omb ) , federal agencies plan to spend more than $74 billion on it investments in fiscal year 2013 .

given the size of these investments and the criticality of many of these systems to the health , economy , and security of the nation , it is important that omb and federal agencies provide appropriate oversight of and adequate transparency into these programs .

as we have previously reported , federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

during the past several years , we have issued multiple reports and testimonies on federal initiatives to acquire and improve the management of it investments .

we made numerous recommendations to federal agencies and omb to further enhance the management and oversight of it programs .

as part of its response to our prior work , omb deployed a public website in june 2009 , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 700 major federal it investments .

in addition , omb has initiated other significant efforts following the creation of the dashboard .

for example , it developed a 25- point plan for reforming federal it ( it reform plan ) , launched an initiative to reduce the number of federal data centers ( the federal data center consolidation initiative ( fdcci ) ) , implemented a cloud computing policy , and recently initiated its portfoliostat effort .

you asked us to testify on the results and recommendations from our selected reports that focused on key aspects of the federal government's acquisition and management of it investments .

accordingly , my testimony specifically discusses our recent reports on omb's it dashboard , it acquisition best practices , management of it operations and maintenance ( o&m ) investments , cloud computing , the it reform plan , and data center consolidation .

all work on which this testimony is based was performed in accordance with generally accepted government auditing standards or all sections of gao's quality assurance framework that were relevant to our objectives .

those standards and the framework require that we plan and perform our audits and engagements to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives ; the framework also requires that we discuss any limitations in our work .

we believe that the information , data , and evidence obtained and the analysis conducted provide a reasonable basis for our findings and conclusions based on our objectives .

omb assists the president in overseeing the preparation of the federal budget submission and supervising budget administration in executive branch agencies .

in helping to formulate the president's spending plans , omb is responsible for evaluating the effectiveness of agency programs , policies , and procedures ; assessing competing funding demands among agencies ; and setting funding priorities .

further , the agency ensures that the budget submission is consistent with relevant statutes and presidential objectives .

each year , omb and federal agencies work together to determine how much the government plans to spend on it projects and how these funds are to be allocated .

as reported to omb , federal agencies plan to spend more than $74 billion on it investments in fiscal year 2013 , which is the total expended for not only acquiring such investments , but also the funding to operate and maintain them .

of the reported amount , agencies plan to spend about $20 billion on development and acquisition , and $54 billion on o&m .

figure 1 shows the percentages of total planned spending for 2013 .

however , this $74 billion does not reflect the spending of the entire federal government .

we have previously reported that omb's figure understates the total amount spent in it investments .

specifically , it does not include it investments by 58 independent executive branch agencies , including the central intelligence agency , or by the legislative or judicial branches .

further , agencies differed on what they considered an it investment ; for example , some have considered research and development systems as it investments , while others have not .

as a result , not all it investments are included in the federal government's estimate of annual it spending .

omb provided guidance to agencies on how to report on their it investments , but this guidance did not ensure complete reporting or facilitate the identification of duplicative investments .

consequently , we recommended , among other things , that omb improve its guidance to agencies on identifying and categorizing it investments .

to assist agencies in managing their it investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cio ) to advise and assist agency heads in carrying out this responsibility .

many of these investments are critical to our nation .

for example , they include systems to process tax returns , secure our nation , and control aircraft .

however , the federal government has spent billions of dollars on poorly performing it investments , as the following examples illustrate: in july 2010 , omb directed the national archives and records administration ( nara ) to halt development of its electronic records archive system at the end of fiscal year 2011 ( 1 year earlier than planned ) .

omb cited concerns about the system's cost , schedule , and performance and directed nara to better define system functionality and improve strategic planning .

through fiscal year 2010 , nara had spent about $375 million on the system .

we issued several reports and made recommendations to improve this system .

these findings and recommendations contributed to the decision to halt the system .

in january 2011 , the secretary of homeland security ended the secure border initiative network program after obligating more than $1 billion to the program because it did not meet cost - effectiveness and viability standards .

since 2007 , we have identified a range of issues and made several recommendations to improve this program .

for example , in may 2010 we reported that the final acceptance of the first two deployments had slipped from november 2009 and march 2010 to september 2010 and november 2010 , respectively , and that the cost - effectiveness of the system had not been justified .

we concluded that the department of homeland security ( dhs ) had not demonstrated that the considerable time and money being invested to acquire and deploy the program was a wise and prudent use of limited resources .

as a result , we recommended that the department ( 1 ) limit near - term investment in the first incremental block of the program , ( 2 ) economically justify any longer - term investment in it , and ( 3 ) improve key program management disciplines .

this work contributed to the department's decision to cancel the program .

in february 2011 , the office of personnel management canceled its retirement systems modernization program after several years of trying to improve the implementation of this investment .

according to the office of personnel management , it spent approximately $231 million on this investment .

we issued a series of reports on the agency's efforts to modernize its retirement system and found that the office of personnel management was hindered by weaknesses in several important management disciplines that are essential to successful it modernization efforts.recommendations in areas such as project management , organizational change management , testing , cost estimating , and earned value management .

in may 2008 , an office of personnel management official cited the issues that we identified as justification for issuing a stop work order to the system contractor , and the agency subsequently terminated the contract .

in march 2011 , we reported that while the department of defense's ( dod ) navy next generation enterprise network investment's first increment was estimated to cost $50 billion , the program was not well - positioned to meet its cost and schedule estimates .

accordingly , we recommended dod limit further investment until it conducts an interim review to reconsider the selected acquisition approach and addresses its investment management issues .

dod stated that it did not concur with the recommendation to reconsider its acquisition approach , but we maintained that without doing so , dod could not be sure it was pursuing the most cost - effective approach .

in december 2012 , dod canceled the air force's expeditionary combat support system after having spent more than a billion dollars and missing multiple milestones .

we issued several reports on this system and found that , among other things , the program was not fully following best practices for developing reliable schedules and cost estimates .

in addition to these poorly performing investments , the it dashboard identifies other at - risk investments .

specifically , as of august 2012 , according to the it dashboard , 190 of the federal government's approximately 700 major it investments — totaling almost $12.5 billion — were in need of management attention ( rated “yellow” to indicate the need for attention or “red” to indicate significant concerns ) .

 ( see fig .

2. ) .

as previously mentioned , in june 2009 , to further improve the transparency into and oversight of agencies' it investments , omb publicly deployed the it dashboard .

as part of this effort , omb issued guidance directing federal agencies to report , via the dashboard , the performance of their it investments .

currently , the dashboard publicly displays information on the cost , schedule , and performance of over 700 major federal it investments at 26 federal agencies .

further , the public display of these data is intended to allow omb , other oversight bodies , and the general public to hold the government agencies accountable for results and progress .

in december 2010 omb released its 25-point plan to reform federal it .

among other things , the plan noted the goal of turning around or terminating at least one - third of underperforming projects by june 2012 .

to its credit , omb's it reform plan provided specific actions to agencies so they could ( 1 ) more effectively manage it acquisitions and ( 2 ) achieve operational efficiencies .

to effectively manage it acquisitions , the plan identified key actions such as improving accountability and governance and aligning acquisition processes with the technology cycle .

to achieve operational efficiencies , the plan outlined actions required to adopt cloud solutions and leverage shared services .

one of these actions was the consolidation of data centers as described in omb's fdcci , which was announced in february 2010 and included a high - level goal to reduce the cost of data center hardware , software , and operations .

another action that was identified was related to cloud computing .

omb developed a “cloud first” policy that required each agency cio to fully migrate three services to a cloud solution by june 2012 , and implement cloud - based solutions whenever a secure , reliable , and cost - effective cloud option exists .

as part of the it reform plan , in 2011 the federal cio council launched an initial best practices platform on http: / / www.cio.gov to provide agency case studies that demonstrate best practices in managing federal it systems .

according to omb , agencies have been encouraged to develop practices that focus on early , frequent , and constructive communication during the acquisition process so that the government clearly understands the marketplace and can obtain an effective solution at a reasonable price .

further , since june 2010 , omb has required agencies to develop and carry out an operational analysis ( oa ) policy for examining the ongoing performance of existing operational it investments to measure , among other things , whether the investment is continuing to meet business and customer needs and is contributing to meeting the agency's strategic goals .

omb's guidance calls for the policy to provide for an annual oa of each investment that addresses the following: cost , schedule , customer satisfaction , strategic and business results , financial goals , and innovation .

more recently , the federal cio initiated the portfoliostat effort for commodity it in march 2012 .

omb requires agency deputy secretaries or chief operating officers to lead portfoliostats — it portfolio reviews — working in coordination with cios , chief financial officers , and chief acquisition officers .

such an effort , as planned , is appropriate given the numerous investments performing the same function , as we reported in february 2012 .

for example , 27 major federal agencies planned to spend $2.7 billion on 580 financial management systems in 2011 .

see figure 3 for the total number of investments within the 27 federal agencies , by function .

omb believes that the portfoliostat effort has the potential to save the government $2.5 billion over the next 3 years by , for example , consolidating duplicative systems .

we previously reported and testified on the issue of duplicative it investments at dod and the department of energy .

specifically , we found 37 potentially duplicative investments , accounting for about $1.2 billion in total it spending for fiscal years 2007 through 2012 .

we made recommendations to those agencies to report on the progress of efforts to identify and eliminate duplication , where appropriate .

over the past several years , we have highlighted omb efforts to enhance oversight of it acquisition .

most notably , we issued a series of reports on the it dashboard .

in addition , we identified common factors critical to successful it investments .

it dashboard omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , and by improving the accuracy of investment ratings .

however , there were issues with the accuracy and reliability of cost and schedule data , and we recommended steps that omb should take to improve these data .

our july 2010 report found that the cost and schedule ratings on omb's dashboard were not always accurate for the investments we reviewed , because these ratings did not take into consideration current performance .

as a result , the ratings were based on outdated information .

we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting .

omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and completed activities .

similarly , in march 2011 , omb had initiated several efforts to increase the dashboard's value as an oversight tool , and had used its data to improve federal it management.calculations contributed to inaccuracies in the reported investment performance data .

these included , for instance , missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data .

as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance .

most agencies generally concurred with our recommendations ; omb agreed with our recommendation for improving ratings for schedule variance .

it disagreed with our recommendation to improve how it reflects current performance in cost and schedule ratings , but more recently made changes to dashboard calculations to address this while also noting challenges in comprehensively evaluating cost and schedule data for these investments .

however , agency practices and the dashboard's noted that that the accuracy of investment cost and schedule ratings had improved since our july 2010 report because omb had refined the dashboard's cost and schedule calculations .

most of the ratings for the eight investments we reviewed were accurate , although more could be done to inform oversight and decision making by emphasizing recent performance in the ratings .

we recommended that the general services administration comply with omb's guidance for updating its ratings when new information becomes available ( including when investments are rebaselined ) and the agency concurred .

since we previously recommended that omb improve how it rates investments , we did not make any further recommendations .

more recently , in october 2012 we found that opportunities existed to improve transparency and oversight of investment risk at our selected agencies .

specifically , cios at six federal agencies consistently rated the majority of their it investments as low risk .

these agencies rated no more than 12 percent of their investments as high or moderately high risk , and two agencies ( dod and the national science foundation ) rated no investments at these risk levels .

over time , about 47 percent of the agencies' dashboard investments received the same rating in every rating period .

for ratings that changed , dhs and office of personnel management reported more investments with reduced risk when initial ratings were compared with those in march 2012 ; the other four agencies reported more investments with increased risk .

in the past , omb reported trends for risky it investments needing management attention as part of its annual budget submission , but discontinued this reporting in fiscal year 2010 .

accordingly , we recommended omb analyze agencies' investment risk over time as reflected in the dashboard's cio ratings and present its analysis with the president's annual budget submission , with which omb concurred .

further , agencies generally followed omb's instructions for assigning cio ratings , which included considering stakeholder input , updating ratings when new data become available , and applying omb's six evaluation factors .

dod's ratings were unique in reflecting additional considerations , such as the likelihood of omb review , and consequently dod did not rate any of its investments as high risk .

however , in selected cases , these ratings did not appropriately reflect significant cost , schedule , and performance issues reported by gao and others .

although three dod investments experienced significant performance problems and were part of a gao high - risk area ( business systems modernization ) , they were all rated low risk or moderately low risk by the dod cio .

for example , in early 2012 , we reported that air force's defense enterprise accounting and management system ( deams ) faced a 2-year deployment delay and an estimated cost increase of about $500 million from an original life - cycle cost estimate of $1.1 billion ( an increase of approximately 45 percent ) , and that assessments by dod users had identified operational problems with the system , such as data accuracy issues , an inability to generate auditable financial reports , and the need for manual workarounds .

in july 2012 , the dod inspector general reported that the deams's schedule delays were likely to diminish the cost savings it was to provide , and would jeopardize the department's goals for attaining an auditable financial statement .

dod's cio rated deams low risk or moderately low risk from july 2009 through march 2012 .

moreover , dod did not apply its own risk management guidance to the ratings , which reduces their value for investment management and oversight .

therefore , we recommended that dod ensure that its cio ratings reflect available investment performance assessments and its risk management guidance .

dod concurred with our recommendation .

critical factors underlying successful major acquisitions to help the federal agencies address the well - documented acquisition challenges they face , we identified seven successful investment acquisitions and nine common factors critical to their success in 2011 .

specifically , we reported that department officials identified seven successful investment acquisitions , in that they best achieved their respective cost , schedule , scope , and performance goals.common factors critical to the success of three or more of the seven investments were: ( 1 ) program officials were actively engaged with stakeholders ; ( 2 ) program staff had the necessary knowledge and skills ; ( 3 ) senior department and agency executives supported the programs ; ( 4 ) end users and stakeholders were involved in the development of requirements ; ( 5 ) end users participated in testing of system functionality prior to formal end user acceptance testing ; ( 6 ) government and contractor staff were stable and consistent ; ( 7 ) program staff prioritized requirements ; ( 8 ) program officials maintained regular communication with the prime contractor ; and ( 9 ) programs received sufficient funding .

further , officials from all seven investments cited active engagement with program stakeholders as a critical factor to the success of those investments .

these critical factors support omb's objective of improving the management of large - scale it acquisitions across the federal government , and wide dissemination of these factors could complement omb's efforts .

the nine in addition to efficiently acquiring it investments , it is also important for the federal government to efficiently manage operational investments , especially since agencies are planning to spend about $54 billion in fiscal year 2013 on operational systems .

accordingly , we issued key reports on the federal government's oversight of it investments in o&m , progress toward meeting omb data center consolidation goals , and progress toward cloud computing as specified in the “cloud first” policy .

oversight of investments in o&m while agencies spend billions on operational investments , they have not always provided adequate oversight of these investments .

specifically , assessments of the performance of such investments — commonly referred to as oas — varied significantly , as we reported in october 2012.perform such analyses annually to ensure o&m investments continue to meet agency needs .

the guidance also includes 17 key factors ( addressing areas such as cost , schedule , customer satisfaction , and innovation ) that are to be assessed .

the agencies in our review were dhs , dod , the departments of health and human services ( hhs ) , the treasury , and veterans affairs ( va ) .

all investments and that all factors are fully assessed ; and omb revise its guidance to include directing agencies to report on the it dashboard the results from the oas .

the five agencies and omb agreed with our recommendations .

data center consolidation agencies have developed plans to consolidate data centers ; however , these plans were incomplete and did not include best practices .

we issued two reports on the federal government's effort to consolidate data centers and made several recommendations for improvements .

our july 2011 report found that agency consolidation plans indicated that agencies anticipated closing about 650 data centers by fiscal year 2015 and saving about $700 million in doing so .

however , only one of the 24 agencies submitted a complete inventory and no agency submitted complete plans .

further , omb did not require agencies to document the steps they took , if any , to verify the inventory data .

we noted the importance of having assurance as to the accuracy of collected data and , specifically , the need for agencies to provide omb with complete and accurate data and the possible negative impact of that data being missing or incomplete .

we concluded that until these inventories and plans were completed , agencies would not be able to implement their consolidation activities and realize expected cost savings .

moreover , without an understanding of the validity of agencies' consolidation data , omb could not be assured that agencies were providing a sound baseline for estimating consolidation savings and measuring progress against those goals .

accordingly , we made several recommendations to omb , including that the federal cio require that agencies , when updating their data center inventories , state what actions have been taken to verify the inventories and to identify any associated limitations on the data .

in a subsequent report we noted that agencies updated their inventories and plans , but key elements were still missing .

specifically , as of september 2011 , 24 agencies identified almost 2,900 total centers , established plans to close 1,186 of them by 2015 , and estimated they would realize over $2.4 billion in cost savings in doing so .

omb noted that the savings would be even greater and estimated that fdcci would realize $3 billion in savings by 2015.however , while omb required agencies to complete missing elements in their data center inventories and plans by the end of september 2011 , only 3 agencies submitted complete inventories , and only 1 agency submitted a complete plan .

further , in their consolidation plans , 13 agencies did not provide a full master program schedule , and 21 agencies did not fully report their expected cost savings .

our report noted that until these inventories and plans were complete , agencies would continue to be at risk of not realizing anticipated savings , improved infrastructure utilization , or energy efficiency .

we also reiterated our recommendation that the agencies complete the missing elements of their inventories and plans .

in addition , while omb required a master program schedule and a cost - benefit analysis ( a type of cost estimate ) as key requirements of agencies' consolidation plans , none of the five agencies we reviewed had a schedule or cost estimate that was fully consistent with the four selected attributes of a properly sequenced schedule ( such as having identified dependencies ) or the four characteristics that form the basis of a reliable cost estimate ( such as being comprehensive and well - documented ) .

omb had established a standardized cost model to aid agencies in their consolidation planning efforts , but use of the model was voluntary .

as a result , we recommended that the five selected agencies should implement recognized best practices when establishing schedules and cost estimates for their consolidation efforts and that omb ensure agencies utilize its standardized cost model across the consolidation initiative .

omb and three agencies agreed with our recommendation , and two did not agree or disagree with it .

finally , we highlighted consolidation successes , such as the benefits of focusing on key technologies and the benefits of working with other agencies and components to identify consolidation opportunities .

however , agencies continued to report a number of the same challenges that we first described in 2011 , while other challenges were evolving .

for example , 15 agencies reported continued issues with obtaining power usage information , and 9 agencies reported that their organization continued to struggle with acquiring the funding required for consolidation .

in light of these successes and challenges , we noted that it was important for omb to continue to provide leadership and guidance , such as — as we previously recommended — using the consolidation task force to monitor agencies' consolidation efforts .

therefore , we recommended that omb ensure that all future revisions to the guidance on data center consolidation inventories and plans are defined in an omb memorandum and posted to the fdcci public website in a manner consistent with the guidance published in 2010 .

omb agreed with our recommendation .

cloud computing implementing cloud computing has security implications , which federal agencies began addressing .

further , agencies made progress implementing omb's “cloud first” policy .

we reported on these two issues and made recommendations .

in may 2010 , we reported that cloud computing can both increase and decrease the security of information systems in federal agencies .

risks included dependence on the security practices and assurances of a vendor , dependency on the vendor , and concerns related to sharing of computing resources .

federal agencies had begun efforts to address information security issues for cloud computing , but key guidance was lacking and efforts remained incomplete .

although individual agencies had identified security measures needed when using cloud computing , they had not always developed corresponding guidance .

for example , only nine agencies reported having approved and documented policies and procedures for writing comprehensive agreements with vendors when using cloud computing .

agencies had also identified challenges in implementing existing federal information security guidance and the need to streamline and automate the process of implementing this guidance .

these concerns included having a process to assess vendor compliance with government information security requirements and the division of information security responsibilities between the customer and vendor .

among other things , we recommended that omb establish milestones for completing a strategy for implementing the federal cloud computing initiative .

federal agencies made progress in implementing omb's “cloud first” consistent with this policy , each policy , as we reported in july 2012.of the seven agencies in our review incorporated cloud computing requirements into their policies and processes .

further , each of the seven agencies met the omb deadlines to identify three cloud implementations by february 2011 and to implement at least one service by december 2011 .

however , two agencies did not plan to meet omb's deadline to implement three services by june 2012 , but planned to do so by calendar year's end .

each of the seven agencies also identified opportunities for future cloud implementations , such as moving storage and help desk services to a cloud environment .

while each of the seven agencies submitted plans to omb for implementing the cloud solutions , all but one plan were missing key required elements .

as a result , we recommended that the seven agencies develop key planning information , such as estimated costs and legacy it systems' retirement plans , for existing and planned services .

the agencies generally agreed with these recommendations .

it reform plan omb's it reform plan acknowledged many of these acquisition and operation issues by requiring 25 actions to be completed by 2012 .

for example , it required agencies to launch a best practices collaboration platform and shift to a “cloud first” policy .

we reported on the federal government's progress toward implementing these actions in april 2012.omb and key federal agencies had made progress on action items in the it reform plan , but there were several areas where more remained to be done .

specifically , we reviewed 10 actions and found that 3 were complete: design a cadre of specialized it acquisition professionals .

stand - up contract vehicle for infrastructure , reform and strengthen investment review boards , and additionally , 7 items were partially completed: work with congress to create budget models for modular complete plans for data center consolidation , issue guidance on modular development , shift to a “cloud first” policy , work with congress to consolidate routine it purchases under agency cio , launch a best practices platform , and redefine the role of agency cio and cio council .

omb reported greater progress than we determined .

while omb officials acknowledged that there is more to do in each of the topic areas , they considered the key action items to be completed because the it reform plan has served its purpose as a catalyst for a set of broader initiatives .

they explained that work will continue on all of the initiatives even after omb declares that the related action items are completed under the it reform plan .

we disagreed with this approach and noted that in prematurely declaring the action items to be completed , omb risked losing momentum on the progress it has made to date .

we recommended that three agencies complete key it reform action items .

we also recommended that omb accurately characterize the status of the it reform plan action items in the upcoming progress report in order to keep momentum going on action items that are not yet completed .

further , omb and key agencies planned to continue efforts to address the seven items that we identified as behind schedule , but lacked time frames for completing most of them .

for example , omb had planned to work with congressional committees during the fiscal year 2013 budget process to assist in exploring legislative proposals to establish flexible budget models and to consolidate certain routine it purchases under agency cios .

however , omb had not established time frames for completing five of the seven it reform plan action items that were behind schedule .

accordingly , we recommended that omb ensure that the action items called for in the it reform plan be completed by the responsible parties prior to the completion of the it reform plan's 18-month deadline of june 2012 , or if the june 2012 deadline could not be met , by another clearly defined deadline ; and provide clear time frames for addressing the shortfalls associated with the action items .

last , omb had not established performance measures for evaluating the results of most of the it reform initiatives we reviewed .

specifically , omb established performance measures for 4 of the 10 action items , including data center consolidation and cloud computing .

however , no performance measures existed for 6 other action items , including establishing the best practices collaboration platform and developing a cadre of it acquisition professionals .

thus , we recommended that omb establish outcome - oriented measures for each applicable action item .

in summary , omb's and agencies' recent efforts have resulted in greater transparency and oversight of federal spending , but continued leadership and attention is necessary to build on the progress that has been made .

for example , federal agencies need to continue to improve the accuracy of information on the dashboard to provide greater transparency and even more attention to the billions of dollars invested in troubled projects .

further , the expanded use of the common factors critical to the successful management of large - scale it acquisitions should result in the more effective delivery of mission - critical systems .

in addition , the federal government can more efficiently manage operational systems by ensuring the $54 billion in o&m is continuing to improve mission performance , in particular the $3 billion which had not undergone required analyses .

the federal government can also build on the momentum of the $2.4 billion in estimated savings as a result of data center consolidation efforts .

overall , implementation of outstanding gao recommendations can help further reduce wasteful spending on poorly managed , unnecessary , and duplicative investments .

chairman issa , ranking member cummings , and members of the committee , this concludes my statement .

i would be pleased to answer any questions at this time .

if you should have any questions about this testimony , please contact me at ( 202 ) 512-9286 or by e - mail at pownerd@gao.gov .

individuals who made key contributions to this testimony are dave hinchman , assistant director ; gary mountjoy , assistant director ; rebecca eyler ; kevin walsh ; and shawn ward .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

