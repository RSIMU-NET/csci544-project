a decennial census must be as accurate as possible , because census results are used to , among other purposes , apportion congressional seats , redraw congressional districts , and allocate federal aid to state and local governments .

however , given the nation's size and demographic complexity , some amount of error is inevitable .

unfortunately , evaluations of past censuses have shown that certain groups , for example african - americans and hispanics , have been undercounted in comparison to other groups .

to estimate the extent that some groups were over - or undercounted in 2000 — what the bureau refers to as “coverage error” — the census bureau ( bureau ) planned and implemented the accuracy and coverage evaluation ( a.c.e. ) .

program .

the primary goals of a.c.e .

were to more accurately estimate the rate of coverage error via a sample survey of select areas nationwide , and if warranted , to use the results of this survey to adjust census estimates of the population for nonapportionment purposes .

in march 2003 , after much deliberation and research , the bureau decided not to use any a.c.e .

estimates of coverage error to adjust the 2000 census , because it judged these estimates no more accurate than the official census data .

the bureau found that a.c.e .

did not account for at least 3 million erroneously counted persons ( mostly duplicates , or people counted twice ) in the census , which raised questions about the reliability of the coverage error estimates .

furthermore , because of the difficulties the bureau experienced in trying to produce reliable coverage error estimates , it announced officially in january 2004 that it did not plan to develop a procedure for adjusting the 2010 census results for redistricting purposes .

agency officials said that in light of their past experiences , they do not think they can produce reliable coverage error estimates in time to meet deadlines for adjusting the census .

this report responds to your request that we review why a.c.e .

coverage error estimates were reportedly not sufficiently reliable to adjust or validate the 2000 census .

specifically , our review examined ( 1 ) factors contributing to a.c.e.'s reported failure to accurately estimate census coverage error , and ( 2 ) the reliability of the revised coverage error estimates the bureau subsequently produced .

to meet the objectives of this report , we reviewed and analyzed the bureau's publicly available research data and reports on the 2000 census .

we also reviewed methodology documents and other available information such as the minutes and supporting documents of the executive steering committees for adjustment policy .

finally , we discussed the results of our analysis with senior bureau officials and interviewed bureau officials and committee members to obtain their views on the process .

since our focus was on the process and decisions that led to the results rather than on determining the underlying numbers themselves , we did not audit the bureau's research , the underlying data , or its conclusions .

our work was performed in washington d.c. and at the u.s. census bureau headquarters in suitland , maryland , from december 2002 through july 2004 in accordance with generally accepted government auditing standards .

since 1980 , the bureau has used statistical methods to generate detailed estimates of census undercounts and overcounts , including those of particular ethnic , racial , and other groups .

to carry out the 2000 census's accuracy and coverage evaluation program ( a.c.e .

 ) , the bureau conducted a separate and independent sample survey that , when matched to the census data , was to enable the bureau to use statistical estimates of net coverage errors to adjust final census tabulations according to the measured undercounts , if necessary .

the bureau obligated about $207 million to its coverage evaluation program from fiscal years 1996 through 2001 , which was about 3 percent of the $6.5 billion total estimated cost of the 2000 census .

while the a.c.e .

sample survey of people was conducted several weeks after census day , april 1 , the “as of” date on which the total population is to be counted , many of the processes were the same as the 2000 census .

for the census , the bureau tried to count everybody in the nation , regardless of their dwelling , and certain kinds of dwellings , including single - family homes , apartments , and mobile homes , along with demographic information on the inhabitants .

for a.c.e. , the bureau surveyed about 314,000 housing units in a representative sample of “clusters” — geographic areas each with about 30 housing units .

the sample comprised roughly 12,000 of the about 3 million “clusters” nationwide .

as illustrated in figure 1 , the bureau used a similar process to develop address lists , collect response data , and tabulate and disseminate data — one for the decennial census and one for a.c.e .

sample areas .

for the census , the bureau mailed out forms for mail - back to most of the housing units in the country ; hand - delivered mail - back forms to most of the rest of the country ; and then carried out a number of follow - up operations designed to count nonrespondents and improve data quality .

a.c.e .

collected response data through interviewing from april 24 through september 11 , 2000 .

after the census and a.c.e .

data collection operations were completed , the bureau attempted to match each person counted on the a.c.e .

list to the list of persons counted by the 2000 census in the a.c.e .

sample areas to determine exactly which persons had been missed or counted more than once by either a.c.e .

or the census .

the results of the matching process , along with data on the racial / ethnic and other characteristics of persons compared , were to provide the basis for a.c.e .

to estimate the extent of coverage error in the census and population subgroups and enable the bureau to adjust the final decennial census tabulations accordingly .

the matching process needed to be as precise and complete as possible , since a.c.e .

collected data on only a sample of the nation's population , and small percentages of matching errors could significantly affect the estimates of under - and overcounts generalized to the entire nation .

since the 2000 census , we have issued four other reports on a.c.e. , addressing its cost and implementation as part of our ongoing series on the results of the 2000 census , as well as a report on the lessons learned for planning a more cost - effective census in 2010 .

 ( see the related gao products section at the end of this report for the assessments issued to date. ) .

these reports concluded , among other things , that while ( 1 ) the address list the bureau used for the a.c.e .

program appeared much more accurate than the preliminary lists developed for the 2000 census and ( 2 ) quality assurance procedures were used in the matching process , certain implementation problems had the potential to affect subsequent matching results and thus estimates of total census coverage error .

in the end , the bureau decided not to use a.c.e.'s matching process results to adjust the 2000 census .

in march 2001 , a committee of senior career bureau officials recommended against using a.c.e .

estimates of census coverage error to adjust final census tabulations for purposes of redistricting congress .

in october 2001 , the committee also recommended against adjusting census data used for allocating federal funds and other purposes , largely because bureau research indicated that a.c.e .

did not account for at least 3 million erroneously counted persons ( mostly duplicates ) in the census , raising questions about the reliability of coverage error estimates .

in march 2003 , after considerable additional research , the bureau published revised coverage error estimates and again decided not to adjust official census data , this time for the purpose of estimating the population between the 2000 and 2010 censuses .

in light of its 2000 experience , the bureau officially announced in january 2004 that while it plans to fully evaluate the accuracy of the 2010 census , it will not develop plans for using these coverage error estimates to adjust the 2010 decennial census .

bureau officials have told us that there is insufficient time to carry out the necessary evaluations of the coverage estimates between census field data collection and the bureau's legally mandated deadline ( within 12 months of census day ) for releasing redistricting data to the states .

furthermore , the bureau does not believe adjustment is possible .

in responding to an earlier gao report recommending that the bureau “determine the feasibility” of adjusting the 2010 census , the bureau wrote that the 2000 census and a.c.e .

was “a definitive test of this approach” which “provided more than ample evidence that this goal cannot be achieved.” however , in march , the national academy of sciences ( nas ) published a report that recommended that the bureau and the administration request and congress provide funding for an improved coverage evaluation program that could be used as a basis for adjusting the census , if warranted .

the academy agrees with the bureau that 12 months is insufficient time for evaluation and possible adjustment ; in the same publication , nas recommended congress consider extending the statutory deadline of 12 months for providing data for redistricting purposes , a suggestion which , if appropriate , could make adjustment possible .

to identify the factors that may have contributed to a.c.e .

missing coverage errors in the census , we reviewed evaluations of a.c.e .

and the bureau's subsequent revisions to its estimation methodology , as well as changes made to the design from its 1990 attempts to estimate coverage .

we interviewed bureau officials responsible for a.c.e .

decision making to obtain further context and clarification .

we did not attempt to identify all factors contributing to the success or failure of a.c.e in estimating coverage error .

since our focus was on the process and decisions that led to the results rather than on determining the underlying numbers themselves , we did not audit the bureau's research , the underlying data , or its conclusions .

we relied on the bureau's own reporting quality assurance processes to assure the validity and accuracy of its technical reporting , and thus we did not independently test or verify individual bureau evaluations of their methodologies .

to identify the extent of the census errors not accounted for by a.c.e. , we reviewed the descriptive coverage error estimates and the limitations and context of these data as described in the bureau reports published by the bureau in march 2001 , october 2001 , and march 2003 .

on , august 9 , 2004 , we requested comments on the draft of this report from the secretary of commerce .

on september 10 , 2004 , the under secretary for economic affairs , department of commerce forwarded written comments from the department ( see app .

i ) , which we address in the “agency comments and our evaluation” section at the end of this report .

the following bureau decisions concerning the design of the census and the a.c.e .

program created difficulties and blind spots for the coverage evaluation , possibly preventing a.c.e .

from reliably measuring coverage error: ( 1 ) using residence rules that were unable to capture the complexity of american society , ( 2 ) excluding the group quarters population from the a.c.e .

sample survey , ( 3 ) making various decisions that led to an increase in the number of “imputed” records in the census , ( 4 ) removing 2.4 million suspected duplicate persons from the census but not the a.c.e .

sample , and ( 5 ) reducing the sample area wherein a.c.e .

searched for duplicates during matching .

however , the bureau has not accounted for how these design decisions have affected coverage error estimates , which has prevented it from pinpointing what went wrong with a.c.e. , and this in turn could hamper its efforts to craft a more successful coverage measurement program for the next national head count .

bureau officials attribute a.c.e.'s inaccuracy primarily to the fact that it used residence rules that do not fully capture the complexity of american society .

according to senior bureau officials , increasingly complicated social factors , such as extended families and population mobility , presented challenges for a.c.e. , making it difficult to determine exactly where certain individuals should have been counted .

specifically , in developing a.c.e .

methodology , bureau officials assumed that each person in its sample could be definitively recorded at one known residence that the bureau could determine via a set of rules .

however , individuals' residency situations are often complicated: bureau officials cite the example of children in custody disputes whose separated parents both may have strong incentives to claim the children as members of their household , despite census residence rules that attempt to resolve which parent should report the child ( ren ) .

in such situations , wherein the residence rules are either not understood , are not followed , or do not otherwise provide resolution , the bureau has difficulty determining the correct location to count the children .

bureau officials cite similar difficulties counting college students living away from home , as well as people who live at multiple locations throughout the year , such as seasonal workers or retirees .

a.c.e .

design also assumed that follow - up interviews would clarify and improve residence data for people for whom vague , incomplete , or ambiguous data were provided and whose cases remained unresolved .

however , the bureau found it could not always rely on individuals to provide more accurate or complete information .

in fact , in our earlier reporting on a.c.e .

matching , we described several situations wherein conflicting information had been provided to the bureau during follow - up interviews with individuals , and bureau staff had to decide which information to use .

more recently , the associate director for decennial census told us that returning to an a.c.e .

household to try and resolve conflicting data sometimes yielded new or more information but not necessarily better information or information that would resolve the conflict .

the bureau plans to review and revise its census residence rules for 2010 , which may clarify some of this confusion .

while the bureau emphasizes residence rules as the primary cause of a.c.e .

failure , our research indicates some of the bureau's other design decisions created blind spots that also undermined the program's ability to accurately estimate census error .

for example , the bureau decided to leave people living in group quarters — such as dormitories and nursing homes — out of the a.c.e .

sample survey , which effectively meant they were left out of the scope of a.c.e .

coverage evaluation ( see fig .

2 ) .

as a result , the matching results could not provide coverage error information for the national group quarters population of 7.8 million .

in addition , the bureau did not design a.c.e .

matching to search for duplicate records within the subset of this population counted by the census , though later bureau research estimated that if it had , it would have measured over 600,000 additional duplicates there .

in response to our draft report the department wrote that coverage evaluation was designed to measure some of these duplicates since during its follow - up interviews at households during a.c.e .

matching , the bureau included questions intended to identify college students living away at college .

while coverage evaluation in 1990 included some group quarters , such as college dormitories and nursing homes , within its sample , the bureau reported that the high mobility of these people made it more difficult to count them , thus the 1990 estimates of coverage for this population were weak .

the bureau decided not to gather these data during 2000 a.c.e .

data collection based in part on the difficulty of collecting and matching this information in the past , and in part as a result of a separate design decision to change the way to treat information for people who moved during the time period between the census and the coverage evaluation interviews .

by excluding group quarters from the coverage evaluation sample , the bureau had less information collected on a sample of this population that included some duplication , and the missing information may have enabled it to better detect and account for such duplication .

in addition , by developing coverage error estimates that were not applicable to the group quarters population , the bureau made the task of assessing the quality of the census as a whole more difficult .

figure 2 also shows that another blind spot emerged as the bureau increased the number of “imputed” records in the final census , though they could not be included in the a.c.e .

sample survey .

the bureau estimates a certain number of individuals — called imputations — that they have reason to believe exist , despite the fact that they have no personal information on them , and adds records to the census ( along with certain characteristics such as age and race / ethnicity ) to account for them .

for example , when the bureau believes a household is occupied but does not have any information on the number of people living there , it will impute the number of people as well as their personal characteristics .

the bureau increased imputations in 2000 from about 2 million in 1990 to about 5.8 million records .

changes in census and coverage evaluation design from 1990 likely contributed to this increase .

specifically , the bureau reduced the number of persons who could have their personal information recorded on the standard census form in 2000 .

in addition , the bureau changed the way coverage evaluation accounted for people who moved between census day and the day of the coverage evaluation interview .

these design changes resulted in less complete information on people and likely contributed to an increase in imputations .

 ( these and other changes are explained in more detail in app .

ii. ) .

because imputed records are simply added to the census totals and do not have names attached to them , it was impossible for a.c.e .

to either count imputed individuals using the a.c.e .

sample survey or incorporate them into the matching process .

at any rate , since the true number and characteristics of these persons are unknown , matching these records via a.c.e .

would not have provided meaningful information on coverage error .

a.c.e .

was designed to measure the net census coverage error , in essence the net effect of people counted more than once minus people missed , and included measurement of the effects of imputation on coverage error .

the bureau generalizes its estimates of coverage error to cover imputations and also maintains that its imputation methods do not introduce any statistical bias in population counts .

but the formulas used by the bureau to calculate its estimates of coverage error account for imputations by subtracting them from the census count being evaluated , not by measuring the error in them .

and the bureau did not attempt to determine the accuracy of the individual imputations , that is although the bureau imputed persons they have reason to believe existed , the bureau does not know whether it over - or underestimated such persons .

as more imputations are included in the census total , the generalization of coverage error estimates to that total population becomes less reliable .

similarly , the bureau created an additional coverage error blind spot by including in the census 2.4 million records that it previously suspected were duplicates and thus were not included in the coverage evaluation .

prior to a.c.e .

matching , the bureau removed about 6 million persons from the census population , identifying them as likely duplicates .

then , after completing additional research on these possible duplicates , the bureau decided to reinstate the records for 2.4 million of these persons it no longer suspected were duplicates .

however , it did so after a.c.e .

had completed matching and evaluating the census records from which the 2.4 million persons had been removed and for which coverage error estimation had begun ( see fig .

3 ) .

the bureau documented in a memorandum that the exclusion of the records from a.c.e .

processing was not statistically expected to affect a.c.e .

results .

however , later bureau research concluded that over 1 million of these 2.4 million records were likely duplicates , none of which could have been detected by a.c.e .

while the bureau maintains that the reinstatement of the over 1 million reinstated likely duplicates did not affect the a.c.e .

estimate in a statistically significant way , this suggests that the resulting a.c.e. - based estimate of national population itself is blind to the presence in the census of the over 1 million duplicates the bureau reintroduced .

for 2010 , bureau officials have chartered a planning group responsible for , among other things , proposing improvements to reduce duplication in the census , which may address some of the problem .

in addition to excluding populations from the scope of evaluation , the bureau further curtailed its ability to measure coverage error by reducing a.c.e.'s search area to only one geographic ring around selected a.c.e .

sample areas during the matching process .

for the 1990 census , the bureau's coverage evaluation program always searched at least one surrounding ring and an even larger ring in rural areas .

however , in 1999 , before a panel of the national academy of sciences , bureau officials announced and defended the decision to not expand the search area except in targeted instances , saying that bureau research indicated that the additional matches found in 1990 from the expanded search areas did not justify the additional effort .

in its comments on our draft report , the department writes that more important than the size of the search area is maintaining “balance” — i.e. , search areas must be used consistently both to identify people who have been missed and to identify people who have been counted in error ( including duplicates ) .

the department also justified the decision to reduce the search area in 2000 from 1990 in part by stating , “in an expected value sense , the reduced search area would have affected” the extra missed people and the extra miscounted people equally , or been balanced .

however , later research discovered large numbers of the missed duplicates in the actual census by matching a.c.e .

persons to census persons nationwide — far beyond the areas searched during a.c.e .

matching .

a 2001 bureau report presenting the results of computer rematching of the a.c.e .

sample concluded , “our analysis found an additional 1.2 million duplicate enumerations in units that were out - of - scope for the a.c.e .

but would have been in - scope for [1990 coverage evaluation].” in other words , if the bureau had continued its 1990 practice of searching in housing units in larger geographic areas in 2000 , the a.c.e .

process might have identified more duplicates and yielded better results .

the bureau research cited above appears to question the decision to reduce the search areas .

in fact , after the 2000 census was completed , again before the national academy of sciences , a bureau official suggested that any coverage evaluation methods for 2010 should conduct a more thorough search , perhaps expanding the search area to two or more geographic rings everywhere .

this review has identified only some of the decisions that could have created problems in a.c.e .

estimates .

because the bureau has not attempted to account for how all of its design decisions relating to a.c.e .

and the census affected the outcome of the program , the full range of reasons that a.c.e .

estimates were not reliable remains obscure .

bureau research documented and this report describes the magnitude of the direct effects of most of these design decisions in terms of the size of the census population affected , and the bureau's final reporting on the revised a.c.e .

estimates mentions many design changes , but not together or comprehensively , and they do not explain how the changes might have affected the estimates of coverage error .

without clear documentation of how significant changes in the design of the census and a.c.e .

might have affected the measurements of census accuracy , it is not apparent how problems that have arisen as a result of the bureau's own decisions can be distinguished from problems that are less under the bureau's control , i.e. , difficulties inherent to conducting coverage evaluation .

thus the bureau's plans to measure the coverage error for the 2010 census are not based on a full understanding of the relationship between the separate decisions it makes about how to conduct a.c.e .

and the census and the resulting performance of its coverage measurement program .

this in turn could hamper the bureau's efforts to craft a more successful coverage measurement program for the next national head count .

while the bureau produced a vast body of research regarding the census and a.c.e. , including multiple reassessments and revisions of earlier work , the revised estimates are not reliable .

the initial a.c.e .

estimates of coverage error suggested that while historical patterns of differences in undercounts between demographic groups persisted , the bureau had succeeded in 2000 in reducing the population undercounts of most minorities , and the subsequent revised estimates showed even greater success in reducing population undercounts in the census .

however , the large number of limitations described in the bureau's documentation of the methodology used to generate the revised estimates of coverage error suggest that these estimates are less reliable than reported and may not describe the true rate of coverage error .

the bureau , however , has not made the full impact of these methodological limitations on the data clear .

moreover , the final revised estimates of coverage error for the count of housing units and the count of people , which the bureau expected to be similar if the estimates were reliable , differed , further raising questions about the reliability of the revised estimates .

the bureau undertook an extensive review of a.c.e.'s results over a 3-year period .

in doing so , the bureau revised its estimates of a.c.e .

coverage error twice — first in october 2001 and again in march 2003 .

these revisions suggest that the original a.c.e .

estimates were unreliable .

figure 4 illustrates how each of the revised a.c.e .

estimates of coverage error reduced the undercount for most of the three major race / origin groups from the initial a.c.e .

estimates .

note that the final revised estimate indicates that the total population was actually overcounted by one - half of 1 percent .

the differences in the revised estimates presumably provide a measure of the error in the original a.c.e .

estimates .

 ( the estimated net population undercounts — and their standard errors — for these groups are provided in app .

iii. ) .

however , the revised estimates of coverage error may not be reliable enough themselves to provide an adequate basis for such a comparison to measure error in the original a.c.e .

estimates .

first , the number of bureau - documented limitations with respect to the methodologies used in generating a.c.e.'s revised estimates raises questions about the accuracy of the revised estimates .

within voluminous technical documentation of its process , the bureau identified several methodological decisions wherein if the decisions had been made differently , they may have led to appreciably different results .

thus the methods the bureau chose may have affected the estimates of census coverage error themselves and / or the measures of uncertainty associated with the estimates , limiting the general reliability of the revisions .

the limitations in the methodologies for the revised estimates included the following: errors in the demographic data used to revise estimates may have contributed to additional error in the estimates .

errors stemming from the bureau's choice of model to resolve uncertain match cases were accounted for in initial march 2001 a.c.e .

estimates but were not accounted for in the revised estimates in march 2003 .

alternative possible adjustments for known inefficiencies in computer matching algorithms would directly affect revised estimates .

the bureau's evaluations of the quality of clerical matching were used to revise the initial a.c.e .

coverage estimates , leaving the bureau less reliable means to measure the uncertainty in the revised estimates .

for the earlier revision of coverage error estimates , the bureau provided the range of impacts that could result from some different methodological decisions , enabling more informed judgments regarding the reliability of the data .

for example , in support of its october 2001 decision to not adjust census data , the bureau showed that different assumptions about how to treat coverage evaluation cases that the bureau could not resolve could result in variations in the census count of about 6 million people .

the bureau also had reported previously the range of impacts on the estimates resulting from different assumptions and associated calculations to account for the inefficiency of computer matching .

they found that different assumptions could result in estimates of census error differing by about 3.5 million people .

however , with the final revision of the a.c.e .

coverage error estimates , the bureau did not clearly provide the ranges of impact resulting from different methodological decisions .

while the bureau did discuss major limitations and indicated their uncertain impact on the revised estimates of coverage error , the bureau's primary document for reporting the latest estimates of coverage error did not report the possible quantitative impacts of all these limitations — either separately or together — on the estimates .

thus readers of the reported estimates do not have the information needed to accurately judge the overall reliability of the estimates , namely , the extent of the possible ranges of the estimates had different methodological decisions been made .

sampling errors were reported alongside estimates of census error , but these do not adequately convey the extent of uncertainty associated with either the reported quantitative estimates themselves or the conclusions to be drawn from them .

for example , the bureau decided to make no adjustment to account for the limitation of computer matching efficiency when calculating its latest revision of estimates of coverage error , unlike the adjustment it made when calculating its earlier revised estimates .

when justifying its adjustment made in its earlier revised estimates , the bureau demonstrated that the choice of adjustment mattered to the calculated results .

but the potential significance to the calculated results of the bureau's having made a different assumption was not reflected in the bureau's primary presentation of its estimates and their errors .

the absence of clear documentation on the possible significant impacts of such assumptions could lead readers of the bureau's reporting to believe erroneously that all assumptions have been accounted for in the published statistics , or that the estimates of coverage error are more reliable than they are .

according to bureau reporting , when it examined the validity of the revised coverage error estimates the bureau expected to see across demographic groups similar patterns between the coverage error for the count of the population and the count of housing .

that is , if a population was overcounted or undercounted , then the count of housing units for that population was expected to be overcounted or undercounted as well .

the bureau developed estimates of coverage error in the count of housing units from a.c.e .

data .

but the comparisons of non - hispanic blacks and hispanics to non - hispanic whites in figure 5 shows that the relative housing undercounts are opposite of what was expected by the bureau .

for example , the estimated population undercount for non - hispanic blacks is almost 3 percent greater than that of the majority group — non - hispanic white or other — but the estimated housing unit undercount for non - hispanic blacks is about 0.8 percent less than that of the majority group .

in addition , while the bureau estimated that the non - hispanic white majority group had a net population overcount of over 1 percent , the bureau estimated the majority group as having its housing units undercounted by about one - third of a percent .

 ( the estimated net housing unit undercounts — and their standard errors — for these groups are provided in app .

iii. ) .

bureau officials told us that the problems a.c.e .

and the census experienced with identifying coverage error in the population do not seem likely to have affected housing counts .

however , when estimating coverage error for housing units for specific populations ( eg , by gender or race / ethnicity ) errors in the population count can affect the reliability of housing tabulations .

this is because when the bureau tabulates housing data by characteristics like gender or race , it uses the personal characteristics of the person recorded as the head of the households living in each housing unit .

so if there are problems with the bureau's count of population for demographic groups , for example by gender or sex , they will affect the count of housing units for demographic groups .

while the unexpected patterns in population and housing unit coverage error may be reconcilable , bureau officials do admit that problems with the estimations of population coverage error may also adversely affect the reliability of other measures of housing count accuracy they rely upon , such as vacancy rates .

bureau officials have indicated the need to review this carefully for 2010 .

while the multiple reassessments and revisions of earlier work did not result in reliable estimates , these efforts were not without value , according to the bureau .

bureau officials stated that the revision process and results helped the bureau focus for 2010 on detecting duplicates , revising residence rules , and improving the quality of enumeration data collected from sources outside the household , such as neighbors , as well as providing invaluable insights for its program of updating census population estimates throughout the decade .

the volume and accessibility over the internet of the bureau's research may have made this the most transparent coverage evaluation exercise of a decennial census .

however , as the bureau has closed the book on census 2000 and turned toward 2010 , the reliability of the bureau's coverage estimates remains unknown .

the bureau made extensive efforts to evaluate the census and its coverage error estimates resulting from a.c.e. , but these efforts have not been sufficient to provide reliable revised estimates of coverage error .

so while much is known about operational performance of the 2000 census , one of the key performance measures for the 2000 census remains unknown .

moreover , neither congress nor the public know why the coverage evaluation program did not work as intended , because the bureau has not provided a clear accounting of how census and a.c.e .

design decisions and / or limitations in the a.c.e .

revision methodology discussed in this report accounted for the apparent weakness — or strengths — of a.c.e .

without such an accounting , the causes of problems and whether they can be addressed will remain obscure .

and as the bureau makes plans for coverage evaluation for the 2010 census , whether that program approximates a.c.e.'s design or not , the bureau will be missing valuable data that could help officials make better decisions about how to improve coverage evaluation .

finally , this lack of information calls into question the bureau's claim ( made in response to a prior gao recommendation that the bureau determine the feasibility of adjustment ) that it has already established that using coverage evaluation for adjustment purposes is not feasible .

without clearly demonstrating what went wrong with its most recent coverage evaluation and why , the bureau has not shown that coverage evaluation for the purpose of adjustment is not feasible .

in fact , this report mentions two census improvements — to residence rules and to efforts to identify and reduce duplicates — that the bureau is already considering that could make a.c.e .

estimates more reliable , and perhaps even feasible .

furthermore , although the bureau reports that its experience with revising a.c.e .

estimates has provided lessons , it remains unclear how the bureau will use its published coverage error estimates to make decisions leading to a more reliable measure of coverage error in 2010 , or how the unreliable estimates can be of value to policymakers or the public .

as the bureau plans for its coverage evaluation of the next national head count in 2010 , we recommend that the secretary of commerce direct that the bureau take the following three actions to ensure that coverage evaluation results the bureau disseminates are as useful as possible to congress and other census stakeholders: to avoid creating any unnecessary blind spots in the 2010 coverage evaluation , as the bureau plans for its coverage evaluation in 2010 , it should take into account how any significant future design decisions relating to census ( for example , residence rules , efforts to detect and reduce duplicates , or other procedures ) or a.c.e .

 ( for example , scope of coverage , and changes in search areas , if applicable ) , or their interactions , could affect the accuracy of the program .

furthermore , in the future , the bureau should clearly report in its evaluation of a.c.e .

how any significant changes in the design of census and / or a.c.e .

might have affected the accuracy of the coverage error estimates .

in addition gao recommends that in the future the bureau plan to not only identify but report , where feasible , the potential range of impact of any significant methodological limitation on published census coverage error estimates .

when the impact on accuracy is not readily quantifiable , the bureau should include clear statements disclosing how it could potentially affect how people interpret the accuracy of the census or a.c.e .

the under secretary for economic affairs at the department of commerce provided us written comments from the department on a draft of this report on september 10 , 2004 ( see appendix i ) .

the department concurred with our recommendations , but took exception to some of our analyses and conclusions and provided additional related context and technical information .

in several places , we have revised the final report to reflect the additional information and provided further clarification on our analyses .

the department was concerned that our draft report implied that a.c.e .

was inaccurate because it should have measured gross coverage error components , and that this was misleading because the bureau designed a.c.e .

to measure net coverage errors .

while we have previously testified that the bureau should measure gross error components , and the department in its response states that this is now a bureau goal for 2010 , we clarified our report to reflect the fact that the bureau designed a.c.e .

to measure net coverage error .

further , although the department agreed with our finding that the bureau used residence rules that were unable to capture the complexity of american society thus creating difficulty for coverage evaluation , the department disagreed with our characterization of the role four other census and a.c.e .

design decisions played in affecting coverage evaluation .

specifically , the bureau does not believe that any of the following four design decisions contributed significantly to the inaccuracy of the a.c.e .

results: 1 .

the treatment of the group quarters population — the department commented that we correctly noted that group quarter residents were excluded from the a.c.e .

universe who would have been within the scope of a.c.e .

under the 1990 coverage evaluation design , and that a large number of these people were counted more than once in 2000 .

the department maintains that the bureau designed a.c.e .

to measure such duplicates .

we believe this is misleading .

as the department noted , during its follow - up at housing units the bureau included questions intended to identify the possible duplication of college students living away at college , and we have now included this in our final report .

but as we stated in our draft report , a.c.e .

did not provide coverage error information for the national group quarters population .

moreover , during a.c.e .

the bureau did not search for duplicate people within the group quarters population counted by the census , as it did within housing units counted by the census .

in fact , later bureau research estimated that if it had done so , the bureau would have identified over 600,000 additional duplicates there .

as such , our finding that this may have contributed to the unreliability of coverage error estimates still stands .

2 .

the treatment of census imputations — the department stated that a.c.e .

was designed to include the effects of imputations on its measurement of coverage error and that there was no basis for our draft report stating that as more imputations were included in the census then coverage error estimates became less reliable .

while we agree that the bureau's estimates of coverage error accounted for the number of imputations , as we report , and as the department's response reiterated , no attempt was made to determine the accuracy of the imputations included in the census .

thus any errors in either the number or demographic characteristics of the population imputed by the bureau were not known within the coverage error processes or estimation .

as a result , in generalizing the coverage error estimates to the imputed segment of the population , the bureau assumed that the imputed population had coverage error identical to the population for which coverage error was actually measured .

furthermore , the larger the imputed segment of the population became the more this assumption had to be relied upon .

since the real people underlying any imputations are not observed by the census , the assumption is , in its strictest sense , untestable , thus we maintain that increasing the number of imputations included in the census may have made generalizing the coverage error estimates to the total census population less reliable .

3 .

the treatment of duplicate enumerations in the reinstated housing units — the department wrote that our draft report incorrectly characterized the effects of reinstating duplicates into the census .

the department indicated that a.c.e. , having been designed to measure net coverage error , treated the over 1 million likely duplicates “exactly correctly” and that including them in the census had no effect on the mathematical estimates of coverage error produced by a.c.e .

we reported that , according to bureau research , introducing the additional duplicates into the census appeared to have no impact on the a.c.e .

estimates .

but we view this fact as evidence of a limitation , or blind spot , in the bureau's coverage evaluation .

the fact that 2.4 million records , containing disproportionately over 1 million duplicate people could be added to the census without affecting the a.c.e .

estimates demonstrates a practical limitation of those coverage error estimates .

we maintain that the resultant measure of coverage error cannot be reliably generalized to the entire population count of which those 1 million duplicates are a part .

4 .

size of the search area — the department wrote that a search area like that used in 1990 would have done little to better measure the number of students and people with vacation homes who may have been duplicated in 2000 .

it described our conclusion regarding the reduction in search area from 1990 as not supported by the relative magnitudes of these situations .

and finally , the department offered additional support for the decision to reduce the search area by describing the reduced search area as balanced , or “in an expected value sense” [emphasis added] affecting the number of extra missed people and the extra miscounted people equally .

in our final report we added a statement about the department's concern over the importance of balance in its use of search areas .

but we disagree that our conclusion is unsupported , since in our draft report we explicitly cited bureau research that found an additional 1.2 million duplicate enumerations in units that were out - of - scope for 2000 a.c.e .

but that would have been in - scope for 1990's coverage evaluation .

in addition , the department offered several other comments .

regarding our finding that the bureau has not produced reliable revised estimates of coverage error for the 2000 census , and , specifically , that the full impact of the bureau's methodological limitations on the revised estimates has not been made clear , the department wrote that the census bureau feels that further evaluations would not be a wise use of resources .

we concur , which is why our recommendations look forward to the bureau's preparation for 2010 .

the department commented that it did not see how we could draw conclusions about the reliability of the bureau's coverage evaluation estimates if we did not audit the underlying research , data , or conclusions .

we maintain that the objectives and scope of our review did not require such an audit .

as we described , and at times cited , throughout our draft report , we used the results of the bureau's own assessment of the 2000 census and its coverage evaluation .

that information was sufficient to draw conclusions about the reliability of the a.c.e .

estimates .

as a result , there was no need to verify individual bureau evaluations and methodologies .

the department expressed concern that our draft report implied that the unexpected differences in patterns of coverage error between the housing and the population count were irreconcilable .

that was not our intent , and we have clarified that in the report .

the department expressed concern over the report's characterization of the 1990 coverage error estimates for group quarters as weak in part due to the high mobility of this population .

however , the 1990 group quarters estimates are described as “weak” in a bureau memorandum proposing that group quarters be excluded from the 2000 coverage evaluation .

the memorandum also explains how the mobility within the group quarters population contributes to the resulting estimates .

we have not revised the characterization of the group quarters coverage error estimates or the causal link due to the mobility of that population , but we have revised our text to state more clearly that the 1990 estimates being discussed are those for group quarters .

as agreed with your offices , unless you release its contents earlier , we plan no further distribution of this report until 30 days from its issue date .

at that time we will send copies to other interested congressional committees , the secretary of commerce , and the director of the u.s. census bureau .

copies will be made available to others upon request .

this report will also be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have any questions concerning this report , please contact me on ( 202 ) 512-6806 or by e - mail at daltonp@gao.gov or robert goldenkoff , assistant director , at ( 202 ) 512-2757 or goldenkoffr@gao.gov .

key contributors to this report were ty mitchell , amy rosewarne , and elan rohde .

the bureau made various design decisions that resulted in an increase in the number of “imputations” — or people guessed to exist — included in the census population that could not be included within the a.c.e .

sample survey .

the bureau believes certain numbers of people exist despite the fact that the census records no personal information on them ; thus it projects , via computer - executed algorithms , numbers and characteristics of people and includes them in the census .

such records are simply added to the census totals , and do not have names attached to them .

thus it was impossible for a.c.e .

to either count imputed individuals using the a.c.e .

sample survey or incorporate them into the matching process .

since the true number and the characteristics of these persons are unknown , matching nameless records via a.c.e .

would not have provided any meaningful information on coverage evaluation .

the number of people the bureau imputed grew rapidly in 2000 , from about 2 million in 1990 to about 5 .

one of the reasons for the large increase in imputations may be the decision by the bureau to eliminate field edits — the last - minute follow - up operation to collect additional information from mail - back forms that had too little information on them to continue processing — from field follow - up in 2000 .

while acknowledging that this decision may have increased imputations for 2000 , a senior bureau official justified the decision by describing the field edits in 1990 as providing at times a “clerical imputation” that introduced a subjective source of error , which computer - based imputation in 2000 lacked .

the bureau also reduced the number of household members for whom personal information could be provided on standard census forms , and this also contributed to the increase in imputations .

households reporting a household size greater than 6 in 2000 — the number for whom personal characteristics could be provided — were to be automatically contacted by the bureau to collect the additional information .

yet not all large households could be reached for the additional information , and the personal characteristics of the remaining household members needed to be imputed .

again , a.c.e .

would have been unable to match people counted by its sample survey to imputations , so imputed people were excluded from a.c.e .

calculations of coverage errors .

an a.c.e .

design choice by the bureau that likely increased the amount of data imputed within the a.c.e .

sample survey was how the bureau decided to account for people who moved between census day and the day of the a.c.e .

interview .

departing from how movers were dealt with in 1990 , and partly to accommodate the initial design for the 2000 census , which relied on sampling nonrespondents to the census , for 2000 the bureau relied on the counts of the people moving into a.c.e .

sample areas to estimate the number of matched people who had actually lived in the a.c.e .

areas on census day but moved out .

this decision resulted in the bureau having less complete information about the census day residents in a.c.e .

sample areas who had moved out , and likely increased the number of imputations that were later required , making it more difficult to match these moving persons to the census .

a bureau official also cited this decision as partial justification for not including group quarters in a.c.e .

search areas .

the extent that imputation affected the accuracy of the census is unknown .

the national academy of sciences discussed in an interim report on the 2000 census the possibility of a subset of about 1.2 million of these imputations being duplicates .

that report stated that , for example , “it is possible that some of these cases — perhaps a large proportion — were erroneous or duplicates,” and described another subset of about 2.3 million that could include duplicates .

however , this academy report did not include any data to suggest the extent of duplicates within these groups , and it may similarly have been possible for the number of persons in this group to have been underestimated .

the bureau maintains that the imputations were necessary to account for the people its field operations led it to believe had been missed , and that its imputation methods do not introduce statistical bias .

as shown in table 1 , the initial a.c.e .

results suggested that the differential population undercounts of non - hispanic blacks and hispanics — the difference between their undercount estimate and that of the majority groups — persisted from bureau estimates from its coverage evaluation in 1990 .

yet they also demonstrated that the bureau had apparently succeeded in reducing the magnitude of those differences since its evaluation of the 1990 census .

subsequent revised results published in october 2001 for three race / origin groups indicated that differential undercounts were generally lower than the initial a.c.e .

estimates , but that only the undercount estimate for hispanics was still statistically different from zero .

finally , the latest revised estimates of undercount reported in march 2003 that of these three major race / origin groups , only the non - hispanic black and non - hispanic white percentage undercounts were significantly different from zero , in addition to the national net overcount .

unlike the estimates of census population accuracy , which were revised twice since initial estimates , the census housing count accuracy estimates have not been revised and are based on the initial a.c.e .

data .

a subset of those results , including those provided here , were also published in october 2001 .

this glossary is provided for reader convenience , not to provide authoritative or complete definitions .

the bureau's accuracy and coverage evaluation ( a.c.e. ) .

program was intended to measure coverage error ( see below ) for the 2000 decennial census .

the program was to enable the bureau to more accurately estimate the rate of coverage error via a sample survey of select areas nationwide , and if warranted , to use the results of this survey to adjust census estimates of the population for nonapportionment purposes .

the use of statistical information to adjust official census data .

a tally of certain kinds of dwellings , including single - family homes , apartments , and mobile homes , along with demographic information on the inhabitants .

the headcount of everybody in the nation , regardless of their dwelling .

the extent that minority groups are over - or undercounted in comparison to other groups in the census .

statistical studies to evaluate the level and sources of coverage error in censuses and surveys .

when the census erroneously counts a person more than once .

the rules the bureau uses to determine where people should be counted .

2010 census: cost and design issues need to be addressed soon .

gao - 04-37 .

 ( washington , d.c.: january 15 , 2004 ) .

2000 census: coverage measurement programs' results , costs , and lessons learned .

gao - 03-287 .

 ( washington , d.c.: january 29 , 2003 ) .

2000 census: complete costs of coverage evaluation programs are not available .

gao - 03-41 .

 ( washington , d.c.: october 31 , 2002 ) .

2000 census: coverage evaluation matching implemented as planned , but census bureau should evaluate lessons learned .

gao - 02-297 .

 ( washington , d.c.: march 14 , 2002 ) .

2000 census: coverage evaluation interviewing overcame challenges , but further research needed .

gao - 02-26 .

 ( washington , d.c.: december 31 , 2001 ) .

the government accountability office , the audit , evaluation and investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents at no cost is through gao's web site ( www.gao.gov ) .

each weekday , gao posts newly released reports , testimony , and correspondence on its web site .

to have gao e - mail you a list of newly posted products every afternoon , go to www.gao.gov and select “subscribe to updates. .

