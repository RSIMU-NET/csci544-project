the u.s. census bureau ( bureau ) conducts various field tests over the course of the decade to prepare for the decennial census .

these tests must be designed well to produce useful information about how to implement the 2020 census .

for the 2010 census , the bureau conducted relatively large , infrequent , and costly field tests , which were time consuming to design , implement , and analyze .

when , as a result of these tests , the bureau needed to modify its plans or conduct additional testing , it had little time to do so .

in preparation for the 2020 census , the bureau is designing smaller , more targeted , and less costly tests to support its decision - making process .

with a total cost of about $13 billion , the 2010 census was the costliest u.s. census in history .

the bureau is committed to limiting its per - household cost for the 2020 census to not more than that of the estimated $94 per household cost of the 2010 census , adjusted for inflation .

it believes less costly tests can help .

you asked us to monitor the bureau's testing for the 2020 decennial census .

for this report , we ( 1 ) determined to what extent the bureau followed key practices for a sound study plan in designing the earliest 2020 decennial census field tests , and ( 2 ) identified what lessons were learned from the design process that may help improve future tests .

to determine to what extent the bureau followed practices for a sound study plan , we reviewed its design documents for the initial 2020 decennial census field tests .

we also interviewed bureau officials about the field test design process .

the three tests included ( 1 ) assessing methods of providing an internet response option , ( 2 ) measuring the accuracy of private vendor - supplied contact information , such as phone numbers and email addresses , and ( 3 ) determining how to improve quality control on mobile computing devices .

we compared the bureau's field test design documents for the three initial tests to 25 key practices from our prior work on designing evaluations , an audit of a prior bureau census test design on overseas enumeration , and other management practices .

we assessed a practice as “generally followed” if the evidence showed that the bureau followed more than 75 percent of the practice .

we considered it “partially followed” if the evidence showed the bureau followed between 25 and 75 percent of the practice , and “not followed” if the bureau followed less than 25 percent .

our assessment provides a measure of the general rigor of the test designs .

we shared these practices with the bureau and it considered the practices reasonable .

to identify lessons learned , we examined where practices had not been followed and identified corrective actions .

we then discussed with bureau officials what lessons they had learned and how the bureau could implement them for future field tests .

more information on our scope and methodology can be found in appendix i .

we conducted this performance audit from january 2013 to october 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the bureau's testing program for the 2010 census relied principally on a small number of large - scale census - like tests .

specifically , the 2010 testing program included two national tests on the content of questionnaires in 2003 and 2005 , two site tests focused on data - collection methods and systems in 2004 and 2006 , and a final “dress rehearsal” at two sites in 2008 .

the dress rehearsal , considered to be the final step of a decade of research and testing , had the primary focus of testing automated field operations and their interfaces .

the bureau previously reported that implementing the census tests , including the dress rehearsal , cost about $108 million .

as part of the bureau's effort to conduct the 2020 census at a cost lower than the 2010 census , it plans to invest in early research and conduct smaller more frequent tests to inform its 2020 census design decisions .

the lifecycle for 2020 census preparation is divided into five phases , as illustrated in figure 1 .

the bureau is attempting to frontload critical research and testing to an earlier part of the decade than it had in prior decennials .

it intends to use the early research and testing phase through fiscal year 2015 to develop a preliminary design and to evaluate the possible impact that changes would have on the census' cost and quality .

by the end of the early research and testing phase , the bureau plans to decide on preliminary operational designs .

in august 2012 , as part of the 2020 census testing program , the bureau issued a research and testing management plan .

the plan defines eight phases of the life cycle for a census field test , as shown in figure 2 .

according to the plan , the first three phases culminate in the approval of a field test design by a group of senior bureau managers that provides decision - making support to the 2020 census program .

to facilitate the field test design process , the bureau developed templates as guidance for developing test designs .

the bureau also developed management plans in specific functional areas .

one , a communications and stakeholder engagement plan , identified stakeholder groups involved in 2020 census planning .

another , a governance plan , identified decision - making bodies for the 2020 research and testing program .

the plans outline processes the bureau will implement as it prepares for the 2020 census .

prior to this , the bureau used 2010 census program guidance and standards to govern some of its earliest design discussions .

the bureau plans to conduct 10 field tests in preparation for the 2020 census during the early research and testing phase .

according to the bureau , not all field tests are alike , and they vary in scope and capability .

some will be designed to encompass more exploratory questions .

others will be designed to more rigorously test the implementation of specific operations .

when we initiated this review in early 2013 , the bureau had designed its initial three tests as summarized in table 1 .

other planned field tests will cover topics such as building the address list and narrowing possible approaches for self - response , non - response follow - up , and workload management .

the field tests will culminate in a larger test late in fiscal year 2015 to further narrow possible design options .

based on our prior work , we identified 25 key practices for a sound study plan .

following these practices before test designs are completed can help ensure that test designs are appropriate , feasible , and produce useful results .

we organized the 25 practices into the following six themes .

general research design , data collection plan , data analysis plan , design process management .

sample and survey , stakeholders and resources , and as demonstrated in figure 3 , the bureau generally followed most of the 25 key practices for two of the three field test designs and at least partially for the third field test design .

research questions frame the scope of a test , drive the design , and help ensure that findings are useful and answer the research objectives .

the objectives should be relevant , creating a clear line of sight to the bureau's goals for the 2020 census .

in addition , clearly articulating the test design in advance of conducting a test aids researchers in discussing methodological choices with stakeholders .

across the three field tests , the bureau generally followed three of the general research design practices , followed one practice to a varying degree , but did not follow the practice of identifying potential biases ( see table 2 ) .

the bureau defined concepts , considered relevant prior research in each test , and included objectives that were relevant .

however , the bureau omitted research questions from the design of the 2012 national census test ( 2012 nct ) .

identifying specific research questions linked to the research objectives helps ensure that answers resulting from the field test will address the needs of the 2020 census research program reflected in the research objectives .

for example , the 2013 quality control test ( 2013 qct ) design includes an objective to investigate how the bureau can modernize and increase the efficiency and utility of its field infrastructure .

the corresponding research question states that the test will research the feasibility of using global positioning system ( gps ) data .

the test would determine , among other things , if field staff appropriately visited housing units for address listing and enumeration and if gps data can be used to reduce or eliminate field quality control checks .

none of the three test designs addressed potential biases , such as cultural bias .

if a test design does not address potential biases , systematic errors could be introduced .

such errors could affect the accuracy of the test and thus potential design decisions for the 2020 census .

identifying data sources and data collection procedures is key to obtaining relevant and credible information .

the bureau generally followed two of the data collection practices for all three tests and followed the others to varying degrees ( see table 3 ) .

in its initial three field test designs , the bureau generally followed the practices of clearly presenting how data will be collected , and describing a method to encourage responses , as applicable .

the three other practices were followed less consistently .

these practices help researchers ensure that the data collected for a test will be sufficient and appropriate .

first , only the 2013 national census contact test ( 2013 ncct ) design included a plan for administering and monitoring data collection .

the test designs should include data collection procedures that will obtain relevant and credible information to ensure that the resulting information meets the decision maker's needs .

in its design documents , bureau officials explained they would collect data using telephone interviews from census bureau contact centers , outbound interviewing , and telephone questionnaire assistance .

the bureau further explained that survey data and the information on the outcomes of calls would be provided to the survey team .

second , the bureau discussed the level of difficulty in two of the test designs , but did not explain why it would be difficult to obtain the data .

bureau officials stated that with limited resources and based on the importance of the objectives of a given test , they will not be able to apply all the practices equally to every test design .

for example , the 2013 qct relied on tests of software by bureau staff — not a traditional field test involving contact with households .

so , although the bureau identified that it may be difficult to reliably identify deviations in procedures using gps , it did not include an explanation or mitigation of the possible difficulty .

third , the bureau only identified factors that may interfere with data collection in the 2013 ncct design .

pre - specifying a data analysis plan as part of a test design can help researchers select the most appropriate data to measure and the most accurate and reliable ways to collect them .

although the bureau generally followed two of the four practices related to a data analysis plan in the test designs that we reviewed , its discussion of the possible limitations of findings or test results varied ( see table 4 ) .

for all of the test designs , the bureau generally identified a basis for comparing the test results and included a proposed design or research plan that was directly related to the objectives and / or questions .

in addition , analytical techniques were proposed for two of three of the test designs .

while in the 2012 nct design the bureau documented that the information from re - interviewing respondents will be used to validate their initial responses , officials did not discuss how they would match this information .

lastly , only the 2013 ncct design included a discussion of possible limitations .

for example , in its design the bureau noted that one of the data sources used in the test might not be representative .

discussing possible limitations is important so that it is clear what the test design can and cannot address , and so that test results are not overly generalized .

a survey with an appropriate sample design can provide descriptive information about a population and its subgroups , as well as information about relationships among variables being measured .

in addition , researchers should consult prior relevant research and test any new questions so that the survey questions will elicit appropriate information from respondents to address the bureau's data needs .

for all three tests , the bureau generally followed two of the sample and survey practices , while use of the third practice varied across the field tests ( see table 5 ) .

across the test designs , the bureau included both a discussion of how to reach the intended sample and the status of the survey instrument or questionnaire , as applicable .

while all of the test designs included a rationale for the type of sample , the 2013 ncct design also included a rationale for the size of its sample .

bureau documents showed this sample size was selected due to the absence of any documentation of a prior study and the test team's conservative estimation of the response rate .

further , the bureau noted the estimated response rate and selected sample size needed to enable the team to determine the quality and comprehensiveness of the data in its analysis .

test designs that explain how their sampling methodology will yield information of sufficient quality for its intended purpose provide a better justification for their cost .

managing stakeholders , identifying team member responsibilities , and identifying resources are key to a test's success , as people are the primary resource of a high - performing organization .

for the 2013 test designs , the bureau followed all of the stakeholder and resource practices .

the 2012 test design did not follow three of the four practices ( see table 6 ) .

the bureau included a timeline and required resources in each field test design , including how much each field test would cost .

the bureau also generally followed the other practices related to stakeholders and resources for two of the test designs .

however , for the 2012 nct , officials did not identify stakeholders , their respective roles in the test , or their involvement in developing the test design .

the latter two tests were designed with management plans for communication , stakeholder engagement , and governance , which for example , states that stakeholders' roles should be defined and that their feedback should be gathered .

the 2013 quality census test design documented the role that a stakeholder had in outlining how the bureau can increase data accuracy .

by including these practices in guidance , the bureau has better ensured that its people and resources are being effectively and efficiently leveraged during the development of future 2020 census tests .

good management of the design process can help managers identify factors that can affect test quality , such as potential risk of delays and challenges to the lines of communication .

across the three tests , the bureau's governance process for developing test designs varied in following the four practices ( see table 7 ) .

first , the bureau identified clear reporting relationships for only the 2013 qct .

it partially followed this practice for the 2013 ncct design , and did not follow it for the 2012 nct design .

for the two latter tests , the bureau utilized membership lists and responsibilities matrices to identify test and project teams , and the assigned tasks and deliverables .

second , the bureau identified review and approval roles for two of the test designs , but not for the 2012 nct .

for example , for the 2013 qct design , the bureau identified which individuals were supposed to review and approve certain design documents .

when authority is clearly assigned and communicated , individuals can be held responsible accordingly .

third , the bureau's documentation of performance measures and timelines with associated milestones for all three test designs did not identify how the measures would be monitored .

for example , for the 2013 qct design the bureau included a list of deliverables with associated dates , such as sending an initial study plan to senior bureau officials for their review .

however , it did not indicate how the bureau would know whether these deliverables were implemented by the indicated dates .

measuring and monitoring performance allows bureau managers to track progress toward their goals and have crucial information on which to base their organizational and management decisions .

fourth , the bureau did not follow its guidance for approving its 2013 ncct test design and only partially followed it for the 2013 qct design .

according to the bureau's august 2012 research and testing management plan , the test designs should be approved at four different stages .

the test design phase is complete after the fourth approval .

the bureau partially followed this practice for the 2013 qct by documenting approval of its design at only one of the stages .

according to bureau meeting records , senior bureau officials discussed the 2013 ncct design after its implementation .

further , bureau records indicate that senior bureau officials discussed the 2012 nct design , before the august 2012 management plan was issued , but did not document approval .

in july 2013 , the bureau began using a table that includes test - design approval dates .

this practice helps ensure that management's approval of a plan maintains its relevance and value to management in control over operations .

further , documenting that management has approved a design provides accountability and offers transparency as to when decisions were made .

the bureau's design templates outline the information that should be included in two of its key design documents , the field test overview and the field test plan .

the templates list topics to be discussed in the overview and plan , and , in some cases , provide examples of what staff should include for a topic .

we found that the templates did not address some of the practices we identified for a sound study plan .

for example , the templates did not require a test design to include ( 1 ) discussion of potential biases , ( 2 ) identification of factors that could interfere with obtaining data , ( 3 ) identification of difficulties in collecting data , and ( 4 ) specification of stakeholder's roles .

in response to this audit , the bureau subsequently revised its field test template to include these four practices as topics to be discussed .

as the bureau works to develop field tests to inform decisions about the 2020 census , bureau officials are learning lessons that can strengthen the design of future tests .

according to bureau officials , our audit helped to reinforce the bureau's need to draw on early lessons learned from initial tests .

these lessons were derived from examining where the bureau did not follow best practices for study designs and identifying corrective actions .

the bureau has adopted some of these test design lessons and is taking steps to adopt others .

table 8 lists six lessons learned from the initial three field tests .

one lesson the bureau identified is the importance of obtaining buy - in from management early in the test development process .

while designing the three initial tests , bureau field test designers did not brief senior bureau management on the development of the designs or involve them in the planning or review of data collection methods .

in addition , managers of various bureau divisions responsible for methodology and other subject matter areas requested to be involved in the process earlier .

according to bureau officials , without early involvement , it may be difficult to obtain upper management approval of test designs quickly , which can lead to unexpected late changes or delays in testing .

early managerial involvement can help ensure early agreement on goals , objectives , timing , and capabilities needed to support a test .

this lesson complements the practices of identifying stakeholders benefiting from the field test as well as stakeholders involved in the preparation of the design .

to involve management earlier , bureau officials began briefing upper management about the planning of tests during other regularly scheduled agency - wide executive meetings early in the test planning stages .

officials also started conducting one - day planning sessions beginning with tests planned for fiscal year 2014 .

since beginning these sessions , bureau officials said they have improved at communicating input from external experts at the national academy of the sciences to upper bureau management .

further , officials said they have found the sessions to be effective in identifying issues early .

bureau officials said they now intend to hold these planning sessions for each test .

in january 2013 , the bureau began convening a bureau - wide test strategy review body .

this panel of experts first met after the 2013 national census contact test was implemented .

in february 2013 , bureau officials decided that prior to conducting future tests , design teams would present the plans , sample design , and objectives to the panel .

according to the bureau , the panel will now look at the bureau's research strategy and goals , design decisions , and how the field tests will affect design decisions in fiscal years 2014 and 2015 , and clarify operational milestones .

the first pre - implementation presentation was conducted in february 2013 for the 2013 quality control test .

this allowed the test team to clarify the 2013 qct's purpose and verify its testing methodology with bureau - wide experts .

bureau research managers believe the test's design is better because of these meetings , and expect future test designs to benefit similarly .

during our review , we discussed with bureau officials whether the bureau took steps to evaluate the test development process after the three initial tests .

the officials told us they recently started conducting staff reviews of tests they have implemented .

such post - test reviews allow the bureau the opportunity to identify any further lessons learned from developing tests to improve either the design or management of remaining tests for the 2020 census .

the bureau conducted its first post - test review following the 2013 national census contact test .

the review documented , for example , that involving stakeholders such as methodologists , in test planning and identifying their roles and responsibilities helps improve communication during the design process .

further , the review documented that test designs should not only identify responsible parties , but have information on what deliverables are expected of these parties .

in addition , the bureau also conducted a review of the 2013 quality control test .

going forward , these reviews will provide the bureau with additional opportunities to build its knowledge base on conducting small , targeted field tests .

the bureau has taken steps to improve how it monitors the status of field test design deliverables .

bureau officials said that they previously reported on the status of some test deliverables in a biweekly report .

however , these reports did not track the status of all deliverables across all the tests .

as a result , senior decennial managers had to contact individual test team leaders to obtain the status for each of the initial test designs .

bureau officials acknowledged that our review led them to realize that with additional field test designs being created , monitoring across all field tests would improve their test status reporting process , and increase their efficiency in collating status information for managers .

in july 2013 , a bureau official informed us that they began using a new tracking sheet to monitor the progress of field test deliverables .

the new tracking tool provides a more comprehensive and global perspective on the status of deliverables across all tests .

bureau officials described this as an evolving process and said that they plan to take additional steps to develop a process for monitoring the status of field tests as well .

the bureau has also recognized the importance of keeping team leaders informed about key design elements .

according to the bureau , design teams are required to submit certain documents for field test design reviews and approvals .

testing guidance is available electronically .

newly assigned team leaders are individually emailed links to baseline documents .

new team leaders are also provided a team leader handbook .

however , the handbook does not identify which documents are required for field test design development , nor does it indicate which documents were required for submission for test design reviews or approvals .

without having a listing of required documents , the bureau risks duplicating its efforts to keep team leaders informed of key design elements .

to ensure that team leaders are consistently informed of field test development guidance and the documents that should be prepared to support test design reviews and approvals , bureau officials said they plan to include a listing of such documents in the team leader handbook .

bureau officials acknowledged that our work offered a way of improving how some information is disseminated to team leaders .

the effort to revise the team leader handbook is in progress , but bureau officials could not provide a timeline for completing it .

achieving a consistent understanding among team leaders of documents required for field test design approvals could help reduce possible delays in the test design review process .

in response to this audit and as part of its effort to adapt its management structures to oversee multiple census field tests being developed concurrently , bureau officials say that they are realigning field test governance processes to improve communication and accountability .

further , they said that the bureau has already taken steps to identify one point of contact for each future test .

previously , a field test coordinator had to track input from various project team leaders involved with a particular census field test .

this lesson complements the practice of identifying clear internal reporting relationships , including who reports to whom and points of contact for different types of information for a sound research plan .

in addition to identifying reporting relationships , the bureau acknowledged that taking steps like establishing one point of contact will help it to more effectively maintain clear lines of communication , and establish accountability when it develops a field test .

while the bureau has taken some initial steps to implement its proposed restructuring , such as conducting a field test management group meeting to further integrate the 2020 field tests across projects , it has not formalized other proposed field test management restructuring and guidance revisions .

for example , to improve the coordination of field test planning , the bureau has proposed creating a field test management team that would provide centralized coordination and streamline the test processes .

further , bureau officials said that the research and testing plan is under review and being updated to reflect the current process for approving field test designs and plans .

internal controls require that agencies complete , within established time frames , all actions that correct or otherwise resolve the matters brought to management's attention .

the controls also require management to periodically evaluate the organizational structure and make changes as necessary in response to changing conditions .

but , the proposed restructuring and guidance revisions have not yet been formalized .

the bureau has many other competing priorities that may need attention more urgently and officials could not provide us a timeline or milestones for formalizing the changes .

meanwhile , the bureau is continuing work on the design of tests .

without a timeline and milestones for this restructuring , the bureau risks uncoordinated management for its field tests .

this puts the field test's effectiveness and efficiency regarding possible overlap and duplication at risk .

lastly , it is important for the bureau to document the lessons it has learned from designing its initial tests .

in conducting post - test reviews , the bureau documented some lessons learned from the 2013 national census contact test and 2013 quality control test .

however , bureau officials acknowledged that the bureau did not consistently document the lessons learned from the design phases for the initial tests .

internal controls require that managers use control activities , such as the production of records or documentation , to aid in managing risk .

documenting these lessons can help reduce the risk of repeating prior deficiencies or inconsistencies that may lead to test development delays .

given the long time frames involved in planning the census , documentation is essential to ensure lessons are incorporated into future tests .

in its effort to design smaller and more targeted tests for the 2020 census , the bureau has taken important steps that could make its testing strategy more effective .

the bureau's investments in early testing are intended to validate the use of strategies and innovations geared toward reducing cost .

the three test designs we reviewed generally or partially followed most key practices for designing a sound research plan .

the bureau has already begun taking corrective actions on others , in part by adding additional requirements for designs to its standard guidance .

finalizing planned revisions that focus on field test management in the team leader handbook can help improve how team leaders learn about test design elements .

formalizing its proposed field test management restructuring and guidance revisions will enable the bureau to ensure that there is improved accountability , communication , and monitoring of its test management design processes .

further , documenting lessons learned while designing the initial field tests can increase the bureau's ability to take advantage of the prior experiences .

by ensuring these practices are consistently used in field tests , the bureau will increase the soundness of the tests in areas such as design management and stakeholder involvement .

this , in turn , will enhance the likelihood that the bureau achieves its goal of conducting a cost effective census .

we recommend that the secretary of commerce require the under secretary for economic affairs who oversees the economics and statistics administration , as well as the director of the u.s. census bureau , to take the following three steps to improve the bureau's process of designing its census field tests for the 2020 census: finalize planned revisions that focus on field test management in the team leader handbook .

set a timeline and milestones for formalizing proposed field test management restructuring and guidance revisions .

document lessons learned from designing initial field tests .

we provided a draft of this report to the department of commerce for comment .

in its written comments , reproduced in appendix ii , the department of commerce concurred with our recommendations .

the department of commerce also provided minor technical comments that were incorporated , as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to the secretary of commerce , the under secretary of economic affairs , the director of the u.s. census bureau , and interested congressional committees .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions about this report please contact me at ( 202 ) 512-2757 or goldenkoffr@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

the gao staff that made major contributions to this report is listed in appendix iii .

the objectives of our review were to determine to what extent the bureau followed key practices for a sound study plan in designing the earliest 2020 decennial census field tests , and to identify any lessons learned from the design process that may help improve future tests .

to identify key practices for a sound study plan , we reviewed program evaluation literature , including our design evaluation guide , our 2004 review of census bureau overseas field tests , our 2012 review of the planning of the 2020 census , and our guide to internal controls .

we selected 25 key practices from these sources .

we shared these practices with the bureau and it agreed that they were reasonable .

using these criteria , we evaluated whether the bureau's three initial field test designs followed key practices for a sound research plan .

additionally , since the program evaluation literature noted the importance of program management to developing a sound study plan , we also interviewed bureau officials on the management processes used for developing the designs .

we did not evaluate the outcomes of the field tests .

to determine to what extent the designs for the initial 2020 decennial census field tests were consistent with key practices for a sound study plan , we reviewed bureau design documents and interviewed bureau officials about the field test design process .

we compared each of the 25 practices to the bureau's field test design documents for the three initial tests to answer the question of whether the respective practice was followed .

our determinations provide a measure of the general rigor of the test designs , although they do not recognize the extent to which the bureau may have considered the key practices later in the life cycle of the designs .

after comparing documents provided by the bureau for each field test to the key practices and determining the extent each practice was followed for each test , we verified the determinations by having different auditors independently determine the extent practices were followed for 25 percent of each others' initial determinations for each test .

we rated each practice as being either “generally followed,” “partially followed,” or “not followed.” we also discussed our preliminary findings with bureau officials to learn of additional context or documents that we might have missed .

table 9 describes how we made our determinations .

for each test , we limited our scope to the “design” of the tests , which is the first three of eight phases of the field test life cycle and includes initiation , concept development , and planning , analysis and design .

our reviews of test designs were designed as snapshots “as of” the approval of the designs by senior management , or at an equivalent stage of their life cycle , intended to benchmark or baseline the preparation of future test designs .

as such , our determination that a given test design did not follow a given key practice does not mean that the bureau did not consider that key practice later in the test's life cycle .

to identify lessons learned from how the tests were designed , we examined where the bureau had not followed key practices and identified corrective actions needed .

we determined the extent to which the key practice criteria were followed and then considered whether there was a pattern or an underlying cause , such as a lack of guidance .

we then discussed with bureau officials what lessons they had learned and what lessons they could implement for future field tests .

we conducted this performance audit from january 2013 to october 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , ty mitchell , assistant director ; maya chakko ; robert gebhart ; ellen grady , wilfred holloway ; andrea levine ; donna miller ; and aku pappoe made key contributions to this report .

