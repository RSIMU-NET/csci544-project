threats to commercial aviation persist and continue to evolve .

in march 2017 , more than 15 years after the terrorist attacks of september 11 , 2001 , the transportation security administration ( tsa ) imposed new screening measures to enhance security after intelligence agencies confirmed that terrorist organizations had the capability to plant explosives in personal electronic devices , such as laptops .

further , in november 2017 , the acting secretary of homeland security reported that the aviation sector remains a primary target for terrorist activity .

to help thwart possible attacks , tsa uses covert testing as a key method to identify possible vulnerabilities in the checkpoint and checked baggage screening systems at tsa - regulated ( i.e. , commercial ) airports across the united states .

during covert tests , undercover personnel ( testers ) attempt to pass threat items ( i.e. , guns , simulated improvised explosive devices , etc. ) .

through checkpoint and checked baggage screening equipment undetected .

tsa's covert tests are intended to help officials identify vulnerabilities and then address or mitigate them through various means , such as conducting additional training , changing existing screening procedures , or adopting new ones .

recent investigations identified vulnerabilities both in tsa's checkpoint and checked baggage screening and with its covert testing of these processes .

for example , in 2017 , the department of homeland security ( dhs ) inspector general identified deficiencies in tsa screener performance .

in addition , in 2016 , we reported that tsa's detection rates for the aviation screening assessment program ( its prior covert testing program ) were unreliable .

in 2016 tsa redesigned its covert test processes to strengthen test procedures and enhance the quality of covert test data and analysis , as well as improve its use of test results to address vulnerabilities .

within tsa , two offices carry out covert tests of checkpoint and checked baggage screening operations at airports: inspection and security operations .

inspection's tests identify vulnerabilities related to any aspect of tsa's checkpoint and checked baggage screening systems , to include the procedures for screening and whether the system is vulnerable to threats identified in intelligence reporting .

security operations' tests focus entirely on transportation security officers' ( tso ) performance against standard operating procedures for checkpoint and checked baggage screening .

in july 2018 , tsa began a transfer of existing covert test programs managed by security operations to inspection for the purposes of improving covert testing and increasing the validity of data collection and reporting .

until this transfer is complete , both inspection and security operations continue to perform covert tests at the nation's commercial airports using distinct processes .

given that tsa continues to refine its processes for conducting covert tests and using the results , you asked us to review tsa's current covert test program , including how the results are used to address identified vulnerabilities .

this report ( 1 ) describes how tsa has changed its covert test processes since 2016 and analyzes the extent to which these processes are risk - informed ; ( 2 ) analyzes the extent to which tsa covert tests for fiscal years 2016 through march 2018 produced quality information ; and ( 3 ) analyzes the extent to which tsa has used the results of covert tests to address any identified security vulnerabilities .

to understand how both security operations and inspection changed their respective covert test processes since 2016 , we reviewed agency documentation , interviewed agency officials , and observed 22 security operations and four inspection covert tests at five airports .

see appendix i for more information on how we selected airports for observations .

for all these observations , we were able to observe tsos performing checkpoint or checked baggage screening activities during tests .

to determine the extent to which security operations and inspection testing is risk - informed , we reviewed program documentation and spoke with agency officials .

we compared the results of tsa risk assessments to the threat items and locations that inspection and security operations selected for tests in fiscal years 2016 and 2017 .

we evaluated each office's process for making risk - informed decisions against dhs risk management policies , which require that agencies use risk information and analysis to inform decision making and document risk management methodologies .

to assess the quality of security operations' test information , we observed security operations tests and reviewed its efforts to assess the quality of airport - run testing by comparing results for the same covert tests carried out by two different groups â€” tsa airport staff and tsa headquarters staff .

specifically , we calculated detection rates for 12,000 covert tests conducted in fiscal year 2017 and about 3,600 covert tests conducted during the first half of 2018 , and compared the results against security operations' internal criterion for determining quality test information .

we assessed security operations' quality assurance methods for covert testing against program criteria and federal internal control criteria for documenting processes .

to assess the quality of inspection's test information , we observed inspection's tests , reviewed completed reports based on fiscal year 2016 and 2017 testing , and conducted interviews with program managers and technical experts to identify the extent to which inspection followed its documented requirements for quality assurance .

to assess the extent to which inspection and security operations address security vulnerabilities , we reviewed their efforts separately because each office used a different approach .

to assess inspection's efforts , we focused on its use of a new , agency - wide vulnerability management process that inspection designated in 2016 as the principal means by which it addresses its identified vulnerabilities .

to obtain a more complete understanding of the extent to which tsa's vulnerability management process has addressed vulnerabilities identified by inspection , we reviewed documentation related to the process and other information pertaining to all vulnerabilities inspection submitted to the process , including those that were unrelated to checkpoint and checked baggage screening .

we assessed the new vulnerability management process against standards for program management issued by the project management institute , a not - for - profit association that provides global standards for , among other things , project and program management .

to determine how security operations headquarters officials address vulnerabilities involving screener performance , we reviewed program documentation and interviewed program managers .

to understand how the results of covert testing are used at the airport level to improve tso performance , we conducted semi - structured interviews with 10 federal security directors ( fsd ) at airports across the united states , and with three tsa regional directors .

we selected fsds for interviews to reflect a range of airport performance on fiscal year 2017 covert tests , among other factors ( see appendix i ) .

we assessed security operations' and tsa officials at airports' efforts against federal internal control standards and criteria in the national infrastructure protection plan for improving program outcomes through information sharing .

this is the public version of a classified report that we issued on january 10 , 2019 .

the classified report included an objective related to identifying the results of covert testing for fiscal years 2016 and 2017 and assessing the quality of this test information .

dhs deemed covert testing results ( including detection rates and identified vulnerabilities ) to be classified information , which must be protected from loss , compromise , or inadvertent disclosure .

consequently , this report omits part of an objective identifying the results of covert testing .

dhs also deemed some of the information in our january report to be sensitive security information , which must be protected from unauthorized release .

therefore , this report omits information describing tsa screening procedures , specific information related to agency risk assessments , and airport - level covert test results .

the performance audit upon which this report is based was conducted from september 2017 to january 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient and appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained from this work provides a reasonable basis for our findings and conclusions based on our audit objectives .

we worked with dhs from february 2019 through april 2019 to prepare this unclassified , non - sensitive version of the original classified report for public release .

this public version was also prepared in accordance with these standards .

tsa is the primary federal agency responsible for implementing and overseeing the security of the nation's civil aviation system and is responsible for ensuring that all passengers and property transported by commercial passenger aircraft to , from , within , or overflying the united states are adequately screened .

specifically , tsa performs , or oversees the performance of , screening operations at about 440 tsa - regulated ( i.e. , commercial ) airports nationwide .

these airports range in size from smaller airports ( category iii and iv airports ) to larger airports ( categories x , i , and ii airports ) .

according to tsa policies and procedures in effect at these airports , all passengers , their accessible property , and their checked baggage are to be screened prior to entering the airport sterile area â€” the portion of an airport beyond the security screening checkpoint that provides passengers access to boarding aircraft .

among other things , these policies and procedures generally provide that passengers must pass through security checkpoints where their person , identification documents , and accessible property are to be screened by tsos , and that all checked baggage must be screened by tsos .

checkpoint screening .

the checkpoint screening process , as set forth in tsa's procedures , is intended to deter and prevent passengers from carrying any unauthorized or prohibited items into the airport's sterile area and onboard an aircraft .

upon entering the airport terminal security checkpoint , passengers provide travel document checkers their boarding passes for review .

based on the printed boarding pass result , travel document checkers are to direct passengers to designated areas for standard , enhanced , or expedited screening .

standard screening is generally applied to all passengers with boarding passes that are not marked for enhanced or expedited screening .

this screening typically includes passing through either a walk - through metal detector or advanced imaging technology ( the latter of which identifies objects or anomalies concealed on the person ) and using x - ray equipment to screen the passenger's accessible property .

in the event that any of these screening devices identify a potential item of concern , additional security measures are to result as part of the alarm resolution process .

these measures may include pat downs , explosives trace detection searches ( which involve a device to detect explosive particles ) , and colorimetric testing to identify the concentration of certain chemical elements .

enhanced screening is generally required for passengers tsa identifies as high risk , such as passengers that have been matched to federal government lists of known or suspected terrorists .

enhanced screening involves the same procedures applied during a typical standard screening experience , as well as a pat down and an explosives trace detection search or physical search of the interior of the passenger's accessible property , electronics , and footwear .

expedited screening is allowed for passengers tsa believes to be low risk .

one group of passengers who routinely receive expedited screening are those enrolled in tsa's preâœ“Â® â€” a program through which individuals vetted and approved by tsa are eligible for this level of screening .

at airports with dedicated tsa preâœ“Â® lanes , expedited screening includes walk - through metal detector screening and x - ray screening of the passenger's accessible property , and travelers do not have to remove their belts , shoes , or light outerwear , or remove items such as laptops from carry - on baggage .

checked baggage screening .

tsa procedures for checked baggage screening establish a process intended to deter , detect , and prevent the transport of any unauthorized explosive , incendiary , or weapon aboard an aircraft .

checked baggage screening generally entails the use of explosives detection systems â€” which use x - rays and other technology to automatically measure the physical characteristics of objects in baggage and trigger an alarm when objects that exhibit the physical characteristics of explosives are detected .

inspection's tests are intended to identify vulnerabilities related to any aspect of tsa's checkpoint and checked baggage screening systems , to include the procedures for screening , the tsos who implement these procedures , and the technology for screening ( eg , x - ray machines and advanced imaging technology ) .

security operations' testing focuses entirely on tso performance of existing standard operating procedures for checkpoint and checked baggage screening , and unlike inspection's testing , does not test other aspects of screening , such as the performance of screening equipment .

to carry out covert testing , both inspection and security operations create test scenarios that describe the overall intent of the test , the threat item , the method of execution ( eg , an explosive device concealed in a shoe carried through the checkpoint ) , and other pertinent details .

generally , security operations' scenarios have tested tsos' performance of procedures pertaining to one of three different paths travelers must follow to have either their persons or property screened ( i.e. , screening paths ) : checkpoint on - person â€” the tester travels through the checkpoint with the threat item concealed on his or her person ; checkpoint in - property â€” the tester travels through the checkpoint with the threat item concealed in a carry - on bag ; and checked baggage â€” the threat item is concealed in checked baggage .

for both offices , covert tests begin when program managers notify an airport's fsd and local law enforcement agency that testing is scheduled to begin .

testers typically pose as passengers and attempt to smuggle a threat object , concealed either on their person or in their property , through one or more layers of the checkpoint or checked baggage screening process ( see fig .

1 ) .

these layers of screening include the travel document checker and the walk - through metal detector or the advanced imaging technology machine , among others .

in general , tsa's covert tests conclude with a meeting between either inspection or security operations staff and the tsos and their supervisors who were tested to discuss the results .

these meetings , known as post - test reviews , allow officials to reinforce actions resulting in test successes , review the correct procedures for any failures , and collect additional data relating to factors contributing to success and failure .

in addition , documented test results are reported to local tsa airport officials , so that they may schedule and track tso participation in the remedial training that is required by law when screeners fail a test .

more broadly , inspection and security operations report test results to certain internal and external stakeholders .

historically , inspection has reported its test results directly to tsa management to inform executive leadership about the aviation screening system's potential vulnerabilities to new and evolving threats .

in addition , security operations has reported test results for its prior testing program to the office of management and budget quarterly and has also briefed tsa senior leadership on results periodically .

dhs policy requires that its components , including tsa , use risk information and analysis to inform decision making .

a risk - informed approach helps decision makers identify and evaluate potential risks so that actions can be taken to mitigate those risks .

dhs defines risk as a calculation of threat , vulnerability , and consequence .

these elements are defined as follows: threat likelihood is estimated based on intent and capability of an adversary .

vulnerability is a physical feature or operational attribute that renders an entity open to exploitation or susceptible to a given hazard .

in calculating risk , vulnerability is based on the likelihood that an attack is successful , given that it is attempted .

consequence refers to the negative effect of an event , incident , or occurrence .

according to the 2010 dhs risk lexicon , which defines key risk - management terms for dhs agencies and components , risk - based decision making uses the assessment of risk as the primary decision driver , while risk - informed decision making may consider other relevant factors in addition to risk assessment information , for decision making .

to guide agency efforts to make risk - based decisions , tsa issues annually its transportation sector security risk assessment â€” a report on transportation security that assesses risk by establishing risk scores for various attack scenarios within different transportation sectors , including domestic aviation .

these scenarios are continuously refined to reflect evolving threats to the various transportation modes and feedback from subject matter experts .

in scoring risk scenarios for the transportation sector security risk assessment , tsa considers the three elements of risk ( threat likelihood , vulnerability , and consequence ) .

in 2016 , inspection redesigned its process to conduct covert tests more consistently across airports , and began using quantitative methods to design tests and analyze results so that its findings might be applied more broadly across airports nationwide .

inspection officials explained that , prior to redesigning their process , inspection's findings could not be applied more broadly because of how tests were designed and executed .

in addition , officials noted that some prior test practices risked diminishing the quality of testing .

for example , some testers consistently ran tests at the same airports , increasing the likelihood that they might be recognized by tsos and compromise the covertness of tests .

as part of its new testing effort , inspection recruited a technical team of employees with expertise in statistics and engineering to enhance the design , execution , analysis , and reporting of its covert tests .

inspection also documented its new covert test process and rationales for key program decisions , including its approach to performing quantitative analysis of test results , in overarching guidance issued in october 2016 .

these documents set forth a framework for conducting tests that includes the creation of detailed scenarios that specify inspection's covert test objectives and scope of testing .

for example , for one inspection test scenario conducted in fiscal year 2016 , inspection conducted 280 tests at larger airports to assess whether certain types of assembled explosive devices contained in carry - on luggage could evade detection at the checkpoint .

under new guidance , inspection's testers may not conduct tests at the same airport within a predetermined period , to limit the potential of being recognized by airport staff .

in addition , under its new process , inspection selects airports for testing so that it may apply its findings more broadly across airports nationwide .

once inspection testers complete all tests for a given scenario , inspection develops classified reports containing results of its quantitative analysis ( including detection rates for specific threat items ) and suggested actions aimed at addressing any identified vulnerabilities .

inspection uses a risk - informed approach to select locations and scenarios for covert tests , but has not fully documented this approach .

according to inspection officials , to select airport locations for tests , they use a tool to randomly select airports from various regions and of various sizes to ensure appropriate representation .

according to our review of the locations inspection tested in fiscal years 2016 and 2017 , inspection predominantly conducted testing at the larger airports .

as previously discussed , this is consistent with a risk - informed approach , as tsa's analysis has shown that larger airports face an increased threat of a terrorist attack .

in addition , inspection officials said that they use a risk - informed approach to select scenarios for their covert tests that takes into consideration all three aspects of a comprehensive risk assessment â€” threat , vulnerability , and consequence .

according to officials , inspection's approach to each of the three components of risk is described below .

efforts to consider threats .

according to inspection leadership officials , inspection has developed close working relationships with key intelligence community agencies to obtain current and specific intelligence information about threats to commercial aviation .

inspection uses this information to create test scenarios involving threat items and attack methods that correspond with the most current threat intelligence .

inspection officials explained that they also consult risk assessments such as the transportation sector security risk assessment to help determine which scenarios to test , but do not rely solely on this information .

officials said this is because such assessments can lack specificity about the type and placement of threat items along different screening paths .

for example , the transportation sector security risk assessment may not convey the specific type of device or the mechanism by which an explosive device will be presented at the checkpoint ( eg , in a laptop ) .

inspection's approach , which uses both current intelligence and risk assessments , is consistent with a risk - informed approach , which allows agencies to utilize resources beyond risk assessments to inform decision making .

efforts to consider vulnerability .

inspection officials told us they have considered vulnerability as a factor for making risk - informed decisions , and have found that it is not useful when deciding which scenarios to test for two reasons .

first , their covert testing is intended to identify the existence of vulnerabilities in the aviation security system .

second , officials explained that vulnerabilities at some airports are well - documented and understood ; therefore , they would generally not use their limited resources to test a vulnerability that is well - known .

efforts to consider consequence .

inspection officials explained that when selecting among possible scenarios to test , considering the consequences that might result from a scenario is less important than the likelihood of a given threat .

however , inspection officials explained that they require that any scenario tested is one that would result in the loss of life if the attack were actually to occur .

although inspection program officials could articulate the risk - informed approach used to select scenarios for testing , they had not sufficiently documented this approach .

specifically , we found that inspection documents its process for making risk - informed selections of scenarios in formal work plans .

this documentation includes general criteria that inspection leadership is to consider when developing threat scenarios , one of which is threat likelihood .

however , the work plans we reviewed did not identify selection criteria that address the vulnerability or consequence components of risk .

dhs's risk management fundamentals ( 2011 ) requires that agency documentation include transparent assumptions about the rationale behind risk management decisions .

in addition , according to standards for internal control in the federal government , agencies should document key decisions in a way that is complete and accurate .

according to inspection officials , they have not fully documented their risk - based process for selecting scenarios because their decision making is often informed by unforeseen events associated with the most exigent threats .

nevertheless , without documenting in its work plans how consequence and vulnerability are considered when determining which scenarios to test , current inspection program managers may not be able to ensure that their scenario selection decisions are appropriately accounting for risk as called for by dhs and tsa guidance .

furthermore , although vulnerability and consequence are less important criteria for inspection's current risk - informed selections , documentation of its approach toward each would serve as a baseline for how inspection makes risk - informed decisions for selecting scenarios to test .

this baseline could inform future program managers and agency leadership seeking to make changes .

in 2016 , security operations replaced its aviation screening assessment program with a new covert test program .

security operations issued guidance for this new program that , among other things , established a parallel test process carried out by headquarters staff to validate ( i.e. , determine the quality of ) local covert test results from airports .

in conjunction with this process , security operations also developed and launched a new web - based tool to collect more detailed information on covert tests .

according to security operations officials , the new program is intended to address problems with its covert testing process identified by an independent contractor in 2015 .

specifically , the contractor performed the same covert tests that tsa personnel at local airports conducted , and the contractor's test results showed that screeners performed more poorly on its tests .

in september 2016 , we reported that , based on the results of the contractor's study , tsa had determined that prior - year tests conducted by tsa officials at airports likely showed a higher level of performance than was actually the case .

further , tsa attributed these higher detection rates , in part , to local airport difficulties in successfully maintaining the covert nature of their tests .

to address deficiencies identified by the tsa - contracted study , security operations issued test guidance in december 2016 and january 2017 that provides more structure to the planning and execution of tests and is intended to help ensure the quality of test results , among other things .

for example , the guidance directs local test coordinators to schedule covert tests at varying times of day and varying days of the month , to prevent tsos from becoming accustomed to testing at particular times .

also , to help ensure that testers are not recognizable by tsos , the guidance states that airports must not recruit testers from the airport in which the test is to be conducted .

additionally , security operations' guidance expands opportunities for recruiting testers at airports .

security operations' new covert test program also features a headquarters - based covert test effort , known as headquarters evaluation team ( het ) testing , to help validate the results of covert tests conducted by tsa officials at airports , known as field evaluation team ( fet ) testing .

under the new process , fet teams , which are composed of tsa staff at airports and locally recruited testers , oversee testing at airports where fsds are located and at any smaller airports under the fsd's authority .

fet teams perform tests of three different screening paths â€” checkpoint in - property , checkpoint on - person , and checked baggage â€” using a variety of scenarios assigned by security operations program managers every 6 months .

fet teams test scenarios for a designated number of times over the 6-month period , after which , program managers are to select and assign a new set of scenarios for testing for the next 6-month period .

for its het tests , security operations is to select , on a quarterly basis , three scenarios to test from among the current set of scenarios assigned for fet testing .

het teams are to travel to airports quarterly to conduct these tests and help validate the fet testing results .

security operations' validation process involves comparing detection rates â€” the percentage of tests in which tsa screening recognized and prohibited a threat item from entering the sterile area of an airport â€” for similar scenarios from both groups of testers .

to assist het and fet teams in collecting more detailed information from its new test program , in april 2016 , security operations developed a web - based data collection instrument called the task process factor ( tpf ) tool that tsa officials use to record more detailed information on covert tests .

according to program officials , collecting more detailed information about test failures was part of the agency's effort to improve screener performance following the dhs inspector general's 2015 covert test findings that identified vulnerabilities in tsa's checkpoint screening .

the tool defines the key tso activities for conducting checkpoint and checked baggage screening as tasks ( eg , interpret the x - ray image ) .

the tool also identifies the various processes associated with a given task ( eg , move property into the x - ray scanner and stop when a full image appears ) .

for any task in which a tso fails , testers are to use the tpf tool to record the task and process associated with the failure â€” so that security operations may identify points of failure for tests with greater specificity .

furthermore , for all test failures , the tool requires het and fet testers to identify the factor , or root cause , for failure .

although security operations considers some tsa risk information when selecting airport locations to test , we found that security operations does not fully consider this information when determining which scenarios to use for its covert tests , and also does not document its rationale for choosing the scenarios it selects .

according to its planning documents for conducting het and fet tests , security operations conducts more tests at larger airports than smaller airports .

according to tsa officials , this is because larger airports generally have more tsos who are subject to covert testing .

tsa's decision to allocate more testing resources to larger airports is based on its own risk analysis and , therefore , is consistent with a risk - informed approach .

however , security operations has not taken steps to incorporate known risks â€” such as those documented in tsa's annual transportation sector security risk assessment , tsa's primary risk assessment of threats for all transportation modes â€” into its process for selecting covert test scenarios .

as our prior work has shown , implementing a risk - informed approach involves using risk assessments or other risk information to determine the most pressing security needs and developing strategies to address them .

in reviewing tsa's 2016 transportation sector security risk assessment â€” the version that would have informed security operations' selection of tests for fiscal year 2017 â€” we identified numerous attack scenarios that could have been incorporated into security operations' selection of scenarios to test .

specifically , the 2016 risk assessment included 20 scenarios that involved attacks that could be carried out through expedited screening conducted in dedicated tsa preâœ“Â® screening lanes .

we reviewed all scenarios security operations selected to test in fiscal year 2017 , but found that only one involved a test of the tsa preâœ“Â® lane .

more generally , we also found that tsa's selection of threat items to test at the checkpoint in fiscal year 2017 did not reflect threats identified in tsa's 2016 transportation sector security risk assessment .

security operations officials acknowledged that they do not use formal tsa risk assessments to determine what threat scenarios or items to test .

they also do not work with intelligence agencies or review classified information when developing covert test scenarios .

instead , security operations officials said they rely mainly on professional judgment regarding which areas of checkpoint and checked baggage procedures tsos frequently overlook or may not perform correctly ( eg , pat downs ) .

officials explained that their judgment is informed by monitoring covert test results ; unclassified media reports on threats ; and requests from agency leadership , such as from tsa's administrator .

security operations' program managers further explained that because their tests are intended to assess tso performance of screening procedures and identify any gaps , their selection of scenarios for testing is intended to cover the breadth of checkpoint and checked baggage screening procedures .

however , as previously discussed , using a risk - informed approach would allow program managers to balance other goals of testing , such as the need to test a variety of screening procedures , with risk information , when making decisions on what to test .

dhs's policy for integrated risk management ( 2010 ) states that dhs components should use risk information and analysis to inform decision making .

additionally , the tsa strategy 2018 â€“ 2026 prioritizes structuring programs to manage risk and optimize resource allocation .

formal risk assessments such as the transportation sector security risk assessment identify the most significant risks to checkpoint and checked baggage screening , and accordingly identify some of the most critical skills tsos need to detect or prevent possible attack scenarios .

using a risk - informed approach to select scenarios that more fully account for known risks â€” such as those identified in the transportation sector security risk assessment or a similar risk assessment â€” could better ensure that tsa is using its finite testing resources to target screening activities that will counter the most likely threats .

additionally , dhs's risk management fundamentals ( 2011 ) requires that agency documentation include transparent assumptions about the rationale behind risk management decisions .

however , security operations has not documented its rationales for selecting covert test scenarios in any of its overarching guidance or planning documentation .

such rationales would delineate security operations' framework for determining what screening activities to test , and specify how security operations officials balance a risk - informed selection of scenarios with their need to test scenarios that cover the breadth of requirements within existing screening procedures .

security operations officials said they do not document their scenario selection process because they review covert test data on a frequent enough basis to identify which processes have low detection rates and , thus , are in need of testing .

however , documenting a risk - informed rationale for its selection of scenarios would better enable security operations or an external party to assess tsa's covert test programs and ensure that decisions are appropriately accounting for risk as called for by dhs and tsa guidance .

it would also allow security operations to demonstrate how it balances its goal of promoting a risk - informed culture , as required by dhs , with program goals to ensure that tsos are following all required screening procedures correctly .

inspection has established a new process and principles for conducting covert tests , as well as collecting and analyzing test data , intended to result in quality information on screening vulnerabilities .

we reviewed two reports on results of inspection's covert testing that were completed using its new processes , and found they resulted in quality information on screening vulnerabilities .

with respect to its new processes inspection has implemented guidance to ensure a standardized process for developing and executing tests .

specifically , inspection guidance requires that headquarters staff with expertise in relevant fields ( including physical security , explosives , and intelligence analysis ) develop all threat items used for testing and conceal these items within test bags or on testers in the same manner across tests .

in addition , inspection program managers require that testers have detailed background stories to explain the purpose ( s ) of their travel .

inspection now employs multiple standard practices to ensure test covertness .

we observed several of these practices during four inspection tests conducted at one airport .

these four tests consisted of two scenarios that were each tested at two different checkpoints within the airport .

first , we observed that inspection teams notified the fsd of their presence only immediately prior to beginning tests , to limit the potential for local airport staff to be forewarned .

we also observed that inspection conducted tests simultaneously across checkpoints , and concluded testing at the airport after an initial round of testing .

according to inspection program managers , conducting tests simultaneously and leaving after the initial round of testing are necessary because once tsos at a tested checkpoint become aware of testing , there is no reliable way to prevent this knowledge from spreading to other checkpoints .

inspection now integrates its technical operations team ( technical team ) into all aspects of test design and data collection and analysis .

inspection officials recruited staff with expertise in research and test design , statistics , and systems engineering , among other relevant fields , to analyze this information .

inspection has integrated these staff into all aspects of its test process to ensure the quality of test information collected and analyses performed .

for example , according to tsa documentation , inspection technical team members are to oversee the selection of airports for testing by first conducting an analysis to determine the number of airports to be tested , and then ensuring the selection of airports for testing is made using a random process â€” a requirement , given that inspection intends to use test results to understand and describe screening activities at airports nationwide .

inspection now identifies data to be collected for each scenario and monitors this data as it is being collected for quality assurance .

according to tsa documentation , inspection's technical team develops the data collection forms used to record test information for every scenario .

such data elements are specific to each scenario and can include , for example , the time when the tester entered the checkpoint , whether the tso running the x - ray machine stopped the belt to review the tester's bag , and the brand of x - ray machine .

according to tsa documentation , the technical team is also to monitor incoming data from scenarios on a regular basis to address any problems as they arise .

inspection now uses guidance to ensure consistency in analysis and reporting .

this includes requirements for reviewing all test data and applying rules about which data should be excluded .

inspection also developed guidance to specify the types of statistical analyses that may be used to draw conclusions about test results and how to report on the results to ensure that its analysis of test results is appropriate and transparent .

for example , inspection guidance identifies what technical information should be included in the report to help readers interpret inspection's conclusions that are based on statistical analysis of results .

we reviewed the two full reports that inspection issued using this new guidance and found that inspection generally followed the guidance for using statistical analysis and reporting final results in these reports .

as previously discussed , the primary method by which security operations tries to ensure that quality covert test results are generated at airports is by having het and fet testers conduct the same test scenarios at airports , and then comparing detection rates identified by the two teams .

security operations program managers explained that this method presupposes that test results collected by het and fet ( following security operations' overarching guidance for conducting tests and using the same test scenarios ) should produce similar detection rates at the national level .

security operations program managers further explained that , because het testers are unaffiliated with the airports they test , they can more easily maintain test covertness .

according to program managers , this aspect of het testing , along with additional training het testers receive in conducting covert tests , gives them greater assurance that het tests accurately reflect screener performance at airports .

therefore , program managers generally consider large disparities between het and fet detection rates to indicate problems with the quality of local airport covert test results .

according to our analysis of security operations national covert test data for fiscal years 2017 and 2018 , checked baggage tests consistently met the security operations criterion for quality test results , but checkpoint tests did not .

in fiscal year 2018 , tsa included a new criterion for quality test results for regional director and fsd annual performance evaluations .

the criterion requires that het and fet covert test detection rates at airports under their supervision be within a designated percentage point difference for the three types of tests ( checkpoint in - property , checkpoint on - person , and checked baggage ) .

according to our analysis of security operations national covert test data for fiscal year 2017 and the first half of fiscal year 2018 , checked baggage tests consistently met the criterion for quality test results , however , checkpoint on - person and in - property tests did not .

specifically , we calculated het and fet detection rates for the three kinds of security operations tests ( checkpoint on - person , checkpoint in - property , and checked baggage tests ) for three 6-month periods from fiscal year 2017 through the first half of fiscal year 2018 .

we found that , for each 6-month period , het detection rates for checkpoint tests were lower than fet detection rates , and the differences exceeded tsa's established criterion for quality test information .

security operations officials acknowledged the differences between het and fet rates , but noted that the differences generally decreased from the last 6-month cycle of testing for fiscal year 2017 through the first 6-month cycle of 2018 , and program managers are working to address them further .

nevertheless , our analysis showed that for the first half of fiscal year 2018 ( the most recent cycle's data available for our analysis ) differences between het and fet test detection rates for checkpoint on - person and checkpoint in - property remained greater than security operations' criterion for quality test information .

in our observations of fet tests , we identified practices in local airport testing that impact the covertness of tests , and thus may contribute to differences between het and fet detection rates .

first , in our observations of local airport fet tests in which tsos correctly identified the threat items , at one airport the tsa airport official in charge of fet testing was present at the checkpoint , and his presence may have provided advance notice to the tsos that testing was in progress .

further , we learned from airport testing officials that having the fet test coordinator present at the checkpoint was a routine practice when testing was in progress .

at another airport visit , one tso told us that tsos often know a fet test is in progress because tsa airport officials use the same test bag to conceal threat items across all tests performed at the airport .

according to tsa documentation , potential lapses in the covertness of covert tests , similar to those we observed and were told about , can make tsos aware that they are being tested and lead to results on tests that overstate actual tso performance .

in addition , we found that the level of potential variability in how tsa airport officials build threat items and test bags for fet tests may affect the quality of the test results used for comparison purposes .

security operations requires that fet personnel build the threat items , such as explosive devices , that are used for scenarios according to specifications included within tsa headquarters - disseminated scenarios .

these scenarios provide a description of the test scenario , a list of materials needed for the threat item , assembly instructions , and directions on how to conceal the threat item within checked or carry - on baggage .

tsa provides standard kits to local airports that contain some of the materials fet teams need to build threat items ( eg , an explosive simulant ) , but tsa staff at the airport must independently procure a number of items needed for each scenario .

given that approximately 80 different teams of fet testers use non - standardized items to build and conceal threat items for tests , the test bags used by teams of fet testers vary to a certain extent across test programs nationwide .

according to tsa officials , variations in the construction of test bags ( including the simulated explosive devices and test bag assembly ) can affect how easy or difficult it is to detect a threat item .

the program manager for the het - fet testing program agreed there is a need for greater assurance of the quality of covert test results , but stated that security operations has not taken action on this issue due to resource constraints .

however , quality assurance is critical to ensure that the resources tsa has invested in covert testing will yield valid and usable information .

moreover , given its resource constraints , security operations' actions to improve local airport test results could encompass less resource - intensive undertakings , such as providing more standardized items for fet tests or improving guidance to address issues that impact the covertness and consistency of tests .

standards for internal control in the federal government states that management should use quality information to achieve an entity's objectives , and that reliable internal sources should provide data that are reasonably free from error and bias and faithfully represent what they purport to represent .

by assessing its current fet testing processes â€” including factors that may compromise the covertness and consistency of tests â€” security operations could identify opportunities to improve the quality of its testing .

further , making changes to its testing process based on its assessment of the current fet testing process could help improve the quality of test results .

this , in turn , would better position those who use these results ( including agency leadership and tsa airport officials ) to reliably identify and address vulnerabilities based on tso performance .

in addition , we found that issues we identified with the quality of fet test results also affect security operations' reporting to external stakeholders .

as previously discussed , officials internal and external to tsa use security operations test results to assess the effectiveness of tso performance .

currently , security operations reports quarterly fet detection rates as a performance measure to the office of management and budget .

the measure identifies the percent of time that tsos correctly detect threat items at the checkpoint ( concealed in carry - on baggage and on the passenger's body ) and within checked baggage .

however , as previously discussed , we found that airport testers were not generating quality covert test information on checkpoint screening because their fet detection rates were higher than the het rates used for comparison , and the difference between the rates exceeded the criterion tsa established for quality covert test information .

tsa management officials acknowledged that the agency needs to use more reliable covert test results for measures reported to the office of management and budget .

in october 2018 , tsa notified the office of management budget that it is in the process of assessing the quality of covert test results it uses to report on tso performance , and expects to develop new measures by fiscal year 2020 .

in addition to issues with the overall quality of airport test results , we found that security operations faced challenges with the quality of information it collected on the root cause of tests failures .

for each test failure , het and fet testers are to use the tpf tool to identify and record the factor , or root cause , leading to a covert test failure .

the tpf tool groups test failure factors into three main categories â€” ( 1 ) failures characterized by the screener's lack of knowing what is required to effectively accomplish a task or job ( a knowledge deficiency ) ; ( 2 ) failures caused by incorrectly performing a procedure ( a skill deficiency ) ; or ( 3 ) failures due to the tso not assigning the correct level of importance to performing a specific screening procedure ( a value deficiency ) .

although security operations has provided some guidance on when to apply a particular factor as a root cause for a covert test failure , this guidance may not be adequate and some testers may not be selecting factors appropriately as a root cause .

in our analysis of the factors assigned by both security operations het and fet testers for all covert test failures in fiscal year 2017 , we found that testers assigned one factor more than the other two .

to assist het and fet testers in conducting root cause analyses for test failures , security operations provides definitions of the three root causes ( knowledge , skills , and value ) .

it also requires that all testers ( het or fet ) complete three online exercises for using the tpf tool to record results , but the exercises do not provide additional guidance on how to appropriately select root causes .

in addition , security operations provides in - person training to all het testers that includes a practice case on selecting from among the factors , and the training course material indicates that the process can be subjective .

in our observation of het tests , we observed numerous failures in which het testers had to assign a root cause .

in a majority of these failures , the tester attributed the same factor as the root cause .

het testers who completed the root cause analyses for these failures all told us they assigned this particular factor by default , once they ruled out the other two causes .

our observations were consistent with a 2017 independent evaluation of the tpf tool performed by the dhs science and technology directorate .

among other things , subject matter experts conducting the 2017 evaluation found that testers they spoke with were not clear on the meaning of the three root causes , and the evaluation recommended that security operations provide better guidance to testers on how to select the root cause of a test failure .

security operations' program managers concurred with the dhs science and technology directorate's recommendation that testers need better guidance on how to select among the factors as the root cause for test failures .

they also stated they are working on guidance to assist testers in selecting the appropriate root cause for failures .

however , in september 2018 , program managers told us they had suspended these efforts to address the recommendation as a result of tsa efforts to transfer program operations to inspection and in anticipation of broader changes to the security operations testing program .

inspection officials , who will assume responsibility for het and fet testing once the transfer of the program to inspection is complete , stated that they were unsure what changes they would make to security operations' legacy testing process with respect to het and fet tests at local airports , but stated both types of testing will continue to use their respective legacy testing processes in fiscal year 2019 until final decisions are made .

standards for internal control in the federal government states that management should use quality information to achieve an entity's objectives , and that reliable internal sources should provide data that are reasonably free from error and bias and faithfully represent what they purport to represent .

as long as security operations' legacy testing process is in use , testers will continue to inconsistently and potentially incorrectly identify the root cause for test failures , and in doing so , will diminish the usefulness of root cause information for addressing tso performance problems .

reviewing existing guidance and training and providing , where appropriate , additional clarification on applying the factors as a root cause would allow tsa to collect more reliable information on the factors leading to test failures .

this , in turn , would better position those who use this information ( including agency leadership and tsa airport officials ) to address root causes of screener failures at individual airports and across the entire system .

security operations has not fully documented its methodology for using het testing as a quality assurance process for fet test results .

while security operations has documented some aspects of the het test process , such as training for het testers on how to conduct tests and post - test reviews with tsos , we found that security operations has not documented its methodology for using het tests to ensure the quality of fet test results in either its program guidance or other internal documentation .

for example , security operations has no documentation on how program managers should select airports ( eg , by airport category ) and scenarios for het testing , as well as how they should analyze , compare , and report on het test results against fet test results .

security operations officials described some aspects of how they calculate het and fet test detection rates for comparison purposes , but they did not have a documented methodology for this quality assurance process .

for example , security operations officials said that they only use data from the largest airports that receive both het and fet tests ( approximately 120 of the about 440 commercial airports ) for comparison purposes .

security operations officials also explained they exclude all het and fet tests involving enhanced screening from the rates used for comparison purposes because enhanced screening involves a more detailed inspection of the subject that tends to result in the screeners identifying threat items at a higher rate .

in addition to these explanations , program managers provided a document explaining security operations' rationale for selecting each of the het test scenarios used for the last half of fiscal year 2017 .

while these explanations and the accompanying documentation helped clarify aspects of security operations' process , security operations has not developed a policy that provides a comprehensive description ( and therefore understanding ) of the quality assurance process that its program managers are to use for program planning purposes .

such a policy would describe security operations' approach to selecting het test scenarios used for ongoing covert testing , how it calculates and compares test results , and how it reports and uses the results .

security operations program managers agreed that more transparent information regarding the use of het test results to assess fet test results would be beneficial , but , given that the program was established in late 2016 , they acknowledged that they have not had time to document this process .

standards for internal control in the federal government states that all transactions and other significant events need to be clearly documented , and this documentation should be readily available for examination .

the documentation should appear in management directives , administrative policies , or operating manuals .

by fully describing its methodology for comparing the results of het testing with fet test results as a quality assurance process within its program guidance , security operations can better ensure that all aspects of this process are clear and available for assessment and validation by third party users of het and fet test information , such as tsa senior leadership officials .

doing so can also ensure that future program managers for the het - fet test program can continue to use this quality assurance method appropriately by following the guidance .

inspection submits its covert test findings that it determines to be security vulnerabilities to tsa's security vulnerability management process .

tsa established this agency - wide process in 2015 to review and address any systemic vulnerability facing tsa ( including those related to checkpoint and checked baggage screening ) .

however , it is unclear if vulnerabilities reviewed through this process are being addressed in a timely manner because the process lacks clear timeframes and milestones for mitigation steps , as well as an established method for monitoring the achievement of such timeframes and milestones .

in 2015 , before establishing the security vulnerability management process , tsa conducted a review of then - existing processes for evaluating and managing identified vulnerabilities , and found that they were not centralized and did not ensure the level of visibility and accountability needed to adequately mitigate and resolve ( or close ) the vulnerabilities .

consequently , tsa determined that its processes for tracking and managing the closure of identified security vulnerabilities represented an organizational deficiency that should be addressed .

in addition , inspection officials stated that , under the prior processes , they lacked complete knowledge of all agency resources that could be leveraged to develop mitigation strategies , as well as the necessary authority to compel offices to share these resources , which made it difficult to ensure identified vulnerabilities were addressed .

as a result , tsa created the security vulnerability management process to better ensure the cooperation of various program offices within tsa that had the expertise needed to address vulnerabilities identified by inspection or other offices within tsa .

this process is intended to centralize agency efforts to mitigate vulnerabilities by ensuring that they receive agency - wide visibility and are evaluated , resourced , and managed by appropriate tsa program offices until fully addressed .

tsa's strategy , policy coordination , and innovation office is responsible for managing and overseeing the security vulnerability management process , as well as enforcing deadlines for vulnerability mitigation .

the strategy , policy coordination , and innovation office submits vulnerabilities for review by one of two groups of tsa stakeholders â€” the executive risk steering committee or the risk assessment integrated project team .

these two groups are responsible for identifying all tsa program offices affected by the vulnerability in question and working with those program offices to determine whether and how vulnerabilities can be mitigated and formally closed ( see fig .

2 ) .

according to tsa strategy , policy coordination , and innovation office officials , to close a given vulnerability , one of the two groups will assess whether the risk posed by the vulnerability aligns to the identified amount of risk that tsa is willing to accept .

tsa officials told us that the agency is risk averse to any vulnerability that could cause catastrophic consequences , such as the loss of an airplane .

the strategy , policy coordination , and innovation office has responsibility for enforcing deadlines for mitigating identified vulnerabilities , but our review of tsa documentation found that the office does not establish timeframes and milestones to ensure measured progress toward mitigation of those vulnerabilities .

moreover , we found that although the security vulnerability management process charter establishes a broad framework for developing and implementing mitigation strategies , it does not establish a method for how the strategy , policy coordination , and innovation office is to monitor mitigation activities to ensure that tsa program offices are meeting identified timeframes and milestones , such as by identifying a person or entity responsible for escalating cases when these requirements are not being met .

specifically , we found that inspection has submitted nine vulnerabilities for consideration .

with one exception , as of september 2018 , none of the vulnerabilities have been formally closed as a result of mitigation steps taken via the vulnerability management process .

under the process , a vulnerability owner has responsibility for developing and leading mitigation efforts for a specific vulnerability .

tsa closed one of the nine vulnerabilities 2 years after submission to this process because the relevant program office made policy changes that addressed inspection's interim findings .

the remaining vulnerabilities have been in progress from 4 months to 2.5 years .

of these eight vulnerabilities , five have had tsa offices assigned as vulnerability owners , and three of these five have mitigation efforts in progress .

the three remaining open vulnerabilities that did not yet have vulnerability owners assigned at the time of our review had been waiting for vulnerability owners for a period of 4 , 5 , and 7 months , respectively ; however , tsa officials told us that these three open vulnerabilities had owners assigned in september 2018 .

tsa officials told us that timeframes for vulnerability mitigation can vary due to the number of stakeholders required to address the situation .

they also explained that the complexity of certain threats affect the timeliness of final mitigation solutions ( eg , those requiring technology solutions can involve multiple tsa offices ) ; and before such solutions are developed , inspection works with program offices to help them develop interim mitigation procedures .

additionally , they cited factors beyond tsa's control that can delay mitigation efforts , such as changes to agency leadership or in staff within a particular office .

for example , mitigation has been delayed for one of the vulnerabilities under review for over 2 years , due to changes in agency leadership in 2016 , among other things .

in another example , tsa officials told us that mitigation for a vulnerability under review had been delayed for over two years due to personnel changes within the office tasked with developing and leading mitigation efforts .

inspection officials told us that while officials are working on mitigation solutions for identified vulnerabilities , inspection will assist tsa program offices with implementing interim mitigation procedures before formal mitigation plans are developed .

for example , inspection officials stated that they worked with security operations to provide interim guidance to tsa airport officials to address an identified vulnerability that involved transportation security specialists for explosives using screening equipment incorrectly to clear passengers through the checkpoint .

although tsa has implemented interim mitigation steps for some vulnerabilities while its program offices develop long - term solutions , in some cases inspection's findings represent system - wide vulnerabilities to commercial aviation that could result in potentially serious consequences for tsa and the traveling public .

for this reason , it is important that tsa make timely progress on formal mitigation solutions .

moreover , tracking progress for a given vulnerability against timeframes and milestones would not necessarily preclude tsa program managers from accounting for complex mitigation efforts .

program managers could , for example , establish longer timeframes at a mitigation effort's onset and adjust these as needed , should challenges arise .

the standard for program management states that the governance of programs includes establishing minimum acceptable criteria for success and the standards by which they are measured and communicated to achieve desired outcomes .

additionally , programs should include the concept of time and incorporate schedules through which specific milestone achievements are measured to ensure that appropriate progress is made toward achieving a defined set of outcomes .

in tsa's case , this would mean the mitigation of identified vulnerabilities .

the standard for program management further states that program governance plans are to describe the systems and methods to be used to monitor a given program , and the responsibilities of specific roles for ensuring the timely and effective use of those systems and methods .

tsa officials agreed that their vulnerability management process lacks a clear set of deadlines for the timely completion of mitigation steps , as well as a method for monitoring completion of these steps to ensure vulnerabilities are closed .

by establishing timeframes and milestones for vulnerability mitigation , tsa would better ensure that progress toward addressing vulnerabilities continues , despite internal challenges , such as personnel changes , or external factors .

in addition , by establishing the methods by which tsa's strategy , policy coordination , and innovation office will monitor milestones for completion , and the steps it will take when mitigation is not progressing as planned , tsa will be better positioned to ensure that the agency is making measured progress toward addressing the vulnerabilities managed through this process .

security operations program managers said that they continuously monitor covert test results to identify potential vulnerabilities and to assess progress at airports in addressing vulnerabilities identified through covert tests .

security operations primarily monitors tso performance by reviewing information within its tpf tool .

specifically , program officials said that they monitor the database each month to identify gaps between het and fet detection rates at an individual airport and regional level .

security operations officials said that they will alert tsa officials at airports if they detect anomalies or large disparities between their het and fet test rates , and suggest strategies for conducting tests .

while reviewing the data , security operations officials told us they may also identify specific test scenarios that tsos are experiencing difficulties with , and sometimes develop strategies to improve performance .

for example , officials said that when tsos demonstrated difficulty with a scenario involving colorimetric testing , security operations developed a pamphlet for tsos to clarify those procedures .

security operations' monitoring has also resulted in changes to processes and procedures .

for example , according to tsa documentation , in early 2016 security operations officials conducted an ad hoc analysis of relevant covert test data .

this analysis led to the implementation of enhanced accessible property screening procedures for personal property screened at airport checkpoints .

according to tsa documentation , these new procedures are intended to help tsa officers obtain a clearer x - ray image to enhance screening effectiveness .

among other things , they involve advising passengers to remove organic materials from carry - on bags for x - ray screening , requiring that electronics larger than a cell phone be removed from carry - on bags and placed in bins for x - ray screening , and more targeted property search protocols .

in addition to periodic monitoring of test data within the tpf tool's database , security operations officials also told us they monitor threat detection improvement plans , which are based on recommended actions stemming from each airport's covert testing results .

tsa officials told us that these plans can include test - specific action plans and high - level improvement strategies .

security operations now monitors airport progress against these plans in order to ensure that airports are taking the necessary actions to improve tso performance deficiencies identified in covert testing .

security operations officials told us they use covert test results as the basis for feedback and periodic reporting on tso performance and the quality of covert test programs or results to headquarters , regional , and local tsa officials and other stakeholders .

according to security operations officials , this feedback and reporting includes the following .

het reports and feedback: security operations directly communicates with tsa officials at airports on het test performance .

for example , in our observations of het tests at airports , testers conducted an equal number of post - test reviews , during which they reviewed with tsos and their supervisors the intent and results of the het tests , reinforced actions resulting in test successes , and reviewed the correct procedures for any failures .

in addition to post - test reviews , at the conclusion of each het test at an airport , security operations program managers provide tsa management at the airport a report compiling the results of the recent het test and statistics on the quality of the covert test program at the airport .

according to tsa documentation , these reports include a comparison of local fet test results against the results of het tests that were conducted during that visit .

tpf report: on a monthly basis , according to tsa documentation , security operations also provides a classified spreadsheet report to fsds that contains a high - level analysis of het and fet covert test data collected for the fiscal year to date , as well as a copy of the most current test results in the tpf tool's database .

security operations program managers stated that allowing airports access to the entire database allows fsds to compare their airport's performance against counterparts in other regions and address any areas in which they are lagging .

in our interviews with fsds , we found that officials from all of the airports we spoke with used the tpf data to help manage tsos .

for example five fsds told us they download the raw test data into local systems for use in their local processes for monitoring tso performance .

classified monthly conference calls: according to tsa officials , security operations hosts monthly classified conference calls with local and regional tsa officials to discuss issues related to covert testing .

security operations officials told us these discussions typically include the results of specific covert test rounds , methods for using covert tests results , and fsds' beneficial practices for carrying out covert testing at their airports .

reporting to senior leadership and other stakeholders: security operations officials said they continue to use covert test results for monthly briefings to fsds and tsa senior leadership .

according to tsa documentation , these briefings include high - level analysis of regional covert test performance , as well as overall comparisons of detection rates for on - person , in - property , and checked baggage tests against the national averages .

as previously discussed , tsa also uses fet test results as the basis of a performance measure reported quarterly to the office of management and budget .

fsds we spoke with told us they find the feedback and reporting they receive from security operations program managers to be helpful .

in particular , all 10 fsds we spoke with told us they find both the het test reports and accessibility to tpf data in the monthly spreadsheet report to be beneficial and useful .

fsds also noted that the het reports help inform their assessments on individual and airport workforce performance and efforts to improve their airport's screening operations overall .

while security operations program officials perform some high - level analysis of tpf data for periodic reporting , they do not analyze all security operations - collected covert test data to identify potential national trends in screener performance that could constitute system - wide vulnerabilities .

for example , according to officials and tsa documentation , security operations officials use fet and het covert test data to describe broad trends in screening performance in monthly briefings to tsa management .

however , the briefings do not include a breakdown of the different screening tasks and processes that may be most often associated with tso failures nationally .

in addition , although the tpf tool's database contains information on the task , process , and factors associated with each tso test failure , security operations does not typically include a comprehensive analysis of this information within the monthly covert test reports it provides to tsa leadership at airports .

for example , based on our review of security operations' monthly tpf reports , they identify which processes have resulted in the most failures , but do not identify which factors â€” knowledge , skill , or value â€” were the root cause of these failures .

moreover , none of this reporting reflects a broader analysis to identify whether failures or causes were associated with a certain size of airport or reflected across one or more regions .

standards for internal control in the federal government states that an agency should design its information systems to respond to the entity's objectives and risks .

furthermore , agencies may use information from these systems to evaluate the agency's performance in achieving key objectives .

as discussed previously , security operations officials have performed similar types of analysis in the past with positive results .

for example , when tsa developed the enhanced accessible property screening procedures in 2017 , these actions were based ( in part ) on ad hoc analysis security operations conducted with national covert test data .

at the time , security operations' analysis showed that x - ray operators at checkpoints had problems determining the threat nature of certain categories of objects .

this led to repeated failures in detection given the time and cognitive load requirements for interpreting those types of x - ray images .

in response , tsa created or adjusted specific procedures based on the analysis of root causes of testing failures and the results of piloting new screening procedures at multiple sites to ensure effectiveness and efficiency could be sustained .

security operations officials agreed that conducting a more comprehensive , national - level analysis , and utilizing more of the covert test data currently within the tpf tool's database , would be useful in identifying system - wide vulnerabilities that could inform efforts to improve tso performance .

security operations officials told us that at present , they do not have a standard process to comprehensively analyze and report trends in tpf data across all airports .

this is because the intent of the current program has been to make test data available to tsa airport and regional officials so they can identify factors affecting screener performance and take actions to remediate and improve any deficiencies .

in addition , security operations officials cited a lack of resources available to dedicate to this activity , given that headquarters officials have been more focused on revising and improving their current covert test program .

however , security operations' tpf tool and database has enabled it to document and communicate detailed information on tso performance , such as the different screening tasks ( eg , advanced imaging technology operation ) and processes ( eg , resolving advanced imaging technology anomalies ) where screeners encounter difficulties .

given the breadth of testing conducted and information collected , more comprehensive analysis of tpf data could help tsa identify and communicate important potential trends in the vulnerabilities that tsos face across all airports .

a comprehensive analysis of tso performance at the national level beyond calculation of overall detection rates would provide security operations greater knowledge about the reasons for , and factors associated with , system - wide vulnerabilities due to tso performance of checkpoint and checked baggage screening , which would better position tsa to address these security gaps .

for example , having this information could allow security operations to provide more focused training and testing for these functions at the airport level .

the information could also position tsa to allocate resources for high - priority issues across all airports .

tsa officials at individual airports reported using different tools , techniques , and processes for conducting covert tests and using test data , but security operations does not document and disseminate this information .

in our discussions with 10 fsds and their management teams , officials identified a variety of tools , processes , and methods that were developed based on their experiences with covert tests and the resulting actions they took to utilize test data to improve tso performance .

specifically , 5 of the 10 fsds we spoke with said their teams developed some type of customized internal databases to aggregate all of their airports' covert test results , other performance - related data , and any additional inspection information .

fsds and their staff said such a tool helped present a holistic picture of tso performance for training and development purposes .

likewise , 5 of the 10 fsds we spoke with said that they use test results to develop tso performance baselines and training plans with requirements that exceed tsa's minimum standards for remediation .

additionally , 5 of 10 fsds stated that they now include supervisory tsos and / or tsa leadership officials at airports in remediation discussions with individual tsos after covert tests take place to provide leadership officials with experience on how best to coach and develop staff .

tsa officials we spoke with at airports and at the regional level said that individual airports are often a source for innovation with respect to executing covert tests and using test results , which has at times led to pilot efforts that were adopted at other airports either regionally or nationally .

for example , officials from one tsa region told us that they were the first to develop and use performance scorecards ( which incorporate covert test results ) as an additional tool for improving screener performance .

these scorecards were eventually adopted nationwide .

most of the fsds we spoke with said they communicate with their counterparts at other airports to discuss covert test practices and beneficial methods for using test results at their respective airports .

for example , officials from one airport we spoke with reported traveling to an airport in a different region to learn more about the team's tso remediation process , which involved using the results of covert testing , threat image projections , and other assessments to create tailored corrective action plans for tsos .

the officials said that this process was an improvement from the one they used previously because it incorporated a greater variety of remediation actions , such as training courses or shadowing opportunities .

as discussed previously , security operations officials communicate with tsa officials at airports on their covert test programs during a monthly classified call with all fsds and their teams .

this allows security operations program managers to provide fsds with an update on results from recent het and fet tests , among other things .

security operations program managers stated that during these calls , they encourage tsa officials not only to discuss particular issues or challenges they have faced with respect to covert testing at their airports , but also to highlight beneficial practices for conducting tests and using test results to improve tso performance that they and their teams have self - identified and implemented .

therefore , these calls also serve as a forum for fsds to discuss successful techniques for running covert tests and using test results .

in our discussions with 10 fsds , 8 out of 10 told us they have independently adopted beneficial practices used by other airports .

security operations program managers are privy to beneficial practices discussed during their teleconferences with local and regional tsa officials , but they told us that they do not regularly document or disseminate this information to tsa officials at airports .

security operations program managers explained that the call itself is adequate for tsa airport officials to share information , and that local or regional officials can follow up with one another if they want to discuss them further .

however , while a monthly conference call may be helpful for informal sharing of practices , it does not capture the breadth of methods or practices used by some tsa airport officials .

moreover , according to headquarters officials , while conference calls provide an opportunity for fsds to discuss beneficial practices , sharing is ad hoc and the level of detail provided about methods and practices can vary .

systematically documenting and disseminating these practices would provide tsa officials at airports more accurate and complete information about beneficial practices in use at airports nationwide , so that they could be more readily implemented at other airports .

the national infrastructure protection plan states that in order to ensure that situational awareness capabilities keep pace with a dynamic and evolving risk environment , officials should improve practices for sharing information and applying the knowledge gained through changes in policy , process , and culture based on shared understanding of efforts to improve security and resilience .

this plan also states that documenting and building upon beneficial practices is a key part of information sharing within a critical infrastructure risk management framework .

our interviews with fsds revealed an array of tools , techniques , and processes for covert testing that tsa officials at airports developed to address local and regional needs .

a process to systematically document and disseminate more accurate and complete information on these tools , techniques , and processes that captures the breadth of methods or practices used by some tsa airport officials could help tsa conduct better covert tests and more successfully use test results to improve tso performance , as well as inform revisions to tsa's national covert test program .

given the persistent threats to the aviation system , tsa must ensure that its covert testing program operates as effectively as possible to identify and address potential vulnerabilities in the checkpoint and checked baggage screening systems across the nation's airports .

tsa has strengthened the quality and rigor of its covert test programs since 2016 , but additional steps are needed to better ensure that tsa targets the areas of highest risk in selecting attack scenarios for testing .

without using a risk - informed approach to selecting screening activities to test , tsa cannot ensure that it is targeting those aspects of tsa screening that pose the greatest known risks .

in addition , without documenting its rationales behind how and why certain scenarios are selected for covert testing , tsa cannot demonstrate how its selections reflect identified risks in the aviation environment .

new processes for covert testing implemented by security operations and inspection have identified important vulnerabilities in checkpoint and checked baggage screening for fiscal years 2016 and 2017 .

however , these results can only be useful if they meet internal standards for quality test results .

while inspection's new process generally produced quality test results on screening vulnerabilities , security operations continues to face challenges with the quality of test results collected by tsa staff at local airports .

without taking steps to ensure that security operations collects more valid and usable information on vulnerabilities , including the root cause of test failures , tsa will not be positioned to reliably identify and address important security vulnerabilities .

in addition , without documenting its methodology for comparing the results of covert tests , tsa cannot ensure that its quality assurance process is consistently applied and transparent .

once vulnerabilities have been identified through covert testing , it is paramount that they are effectively and efficiently mitigated or addressed .

establishing the security vulnerability management process was a good step toward better tracking the vulnerabilities identified through covert tests and deploying resources to mitigate them , but key identified vulnerabilities have been stalled in the process and none have been closed using this process .

this has largely been caused by the absence of timeframes and milestones for achieving mitigation and monitoring key activities in the process .

unless tsa incorporates these aspects into its vulnerability management guidance , it cannot ensure that it is effectively addressing security vulnerabilities that could result in potentially serious consequences for the traveling public .

additionally , while tsa shares some covert test information with tsa officials at airports , more comprehensive analysis of covert test information is needed to enhance tsa's knowledge about the reasons for , and the factors associated with , tso performance vulnerabilities that exist system - wide .

furthermore , although tsa officials at individual airports informally share information about beneficial practices they use to conduct covert tests and how they use test information , without systematically documenting and disseminating these practices , tsa cannot ensure that airport officials are fully informed about the different tools , techniques , and processes used by their colleagues .

we are making the following nine recommendations to tsa: the administrator of tsa should document its rationale for key decisions related to its risk - informed approach for selecting covert test scenarios , for both the security operations' and the inspection's testing process .

 ( recommendation 1 ) the administrator of tsa should incorporate a more risk - informed approach into security operations' process for selecting the covert test scenarios that are used for tests conducted by tsa officials at airports .

 ( recommendation 2 ) the administrator of tsa should assess the current covert testing process used by tsa officials at airports â€” including factors that may affect the covertness and consistency of the tests â€” to identify opportunities to improve the quality of test data , and make changes as appropriate .

 ( recommendation 3 ) the administrator of tsa should assess security operations guidance for applying root causes for test failures , and identify opportunities to clarify how they should be applied .

 ( recommendation 4 ) the administrator of tsa should document the methodology for using the results of covert testing conducted by headquarters staff as a quality assurance process for covert testing conducted by tsa officials at airports .

 ( recommendation 5 ) the administrator of tsa should establish timeframes and milestones for key steps in its security vulnerability management process that are appropriate for the level of effort required to mitigate identified vulnerabilities .

 ( recommendation 6 ) the administrator of tsa should revise existing guidance for the security vulnerability management process to establish procedures for monitoring vulnerability owners' progress against timeframes and milestones for vulnerability mitigation , including a defined process for escalating cases when milestones are not met .

 ( recommendation 7 ) the administrator of tsa should develop processes for conducting and reporting to relevant stakeholders a comprehensive analysis of covert test results collected by tsa headquarters officials and tsa officials at airports to identify vulnerabilities in screener performance and common root causes contributing to screener test passes and failures .

 ( recommendation 8 ) the administrator of tsa should develop a standard process for systematically documenting and disseminating to airport federal security directors beneficial practices for conducting covert tests and using test results .

 ( recommendation 9 ) .

we provided a draft of this report to dhs and tsa for review and comment .

dhs provided written comments which are reprinted in appendix ii .

in its comments , dhs concurred with all 9 recommendations and described actions planned to address them .

tsa also provided technical comments , which we incorporated as appropriate .

we are sending copies of this report to the appropriate congressional committees , the secretary of the department of homeland security , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-8777 or russellw@gao.gov .

contact points for our office of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iv .

this report addresses the transportation security administration's ( tsa ) covert testing for checkpoint and checked baggage screening .

more specifically , the report ( 1 ) describes how tsa has changed its covert test processes since 2016 and analyzes the extent to which these processes are risk - informed ; ( 2 ) analyzes the extent to which tsa covert tests for fiscal years 2016 through march 2018 produced quality information ; and ( 3 ) analyzes the extent to which tsa has used the results of covert tests to address any identified security vulnerabilities .

to understand how both the security operations and inspection offices changed their respective covert test processes since 2016 , we reviewed agency documentation , interviewed agency officials , and observed 22 security operations and 4 inspection covert tests at 5 different airports .

in addition to inspection testing , our observations included two types of testing overseen by security operations â€” headquarters evaluation team ( het ) testing and field evaluation team ( fet ) testing .

to gather information on how covert tests are carried out in different airport environments , we observed tests at four category x and one category i airports .

we selected airports for observations on the basis of airport category and screener workforce ( private vs. tsa - employed screeners ) .

for all observations , we were able to observe tsos performing checkpoint or checked baggage screening activities during tests .

following all observations , we observed post - test reviews and , when appropriate , interviewed tsa airport officials , including the transportation security officers ( tso ) and private sector screeners ( collectively referred to as tsos in this report ) who were tested , about their experience with these tests .

to determine the extent to which security operations and inspection testing is risk - informed , we reviewed program documentation and spoke with agency officials .

specifically , we reviewed operational guidance and test scenarios , which describe the overall intent of the test , the threat item , and method of execution ( eg , an explosive device concealed in a shoe carried through the checkpoint ) to identify how program officials incorporated the components of risk â€” threat , vulnerability , and consequence â€” in their selection of threats and airports to test .

we also reviewed the tsa risk assessments that would have been available to inspection and security operations when planning which threats and airports to test for fiscal year 2017 , namely tsa's 2016 transportation sector security risk assessment and tsa's 2012 current airports threat assessment .

the 2016 transportation security sector risk assessment contained attack scenarios for the five transportation modes for which tsa is responsible , including domestic and international commercial aviation , as well as other mass transit systems , such highway and mass transit .

for our analysis , we used those scenarios relevant to our scope â€” domestic commercial checkpoint and checked baggage screening .

we compared the results of these assessments to the threat items and locations that security operations selected for tests in fiscal year 2017 and inspection selected for tests in fiscal years 2016 and 2017 .

we evaluated each office's process for making risk - informed decisions with department of homeland security ( dhs ) risk management policies , which require that agencies use risk information and analysis to inform decision making , and that risk management methodologies should be transparent and properly documented .

to assess the quality of security operations data , we reviewed program guidance and interviewed program officials to understand how security operations uses het test results to validate the quality of fet testing at local airports .

we also reviewed a 2016 validation study of security operations' test process conducted by the dhs office of science and technology , and spoke with subject matter experts who conducted the study about their findings and recommendations related to improving the quality of test information .

we concluded the study's findings were reasonably sufficient to use as additional support for patterns we also observed during site visits .

we were also informed by our het and fet test observations , which included observations of 19 het tests at 3 different airports , and 3 fet tests at 1 airport .

we supplemented our understanding of how airports conduct fet tests through semi - structured telephone interviews with 10 different federal security directors ( fsd ) and their staff .

to select fsds for interviews , we identified the airports at which tsa conducted more than the average number of het covert tests in fiscal year 2017 .

we focused on the number of het ( as opposed to fet ) tests because they are security operations' quality assurance method for airport covert test programs , and we wanted to ensure fsds had sufficient experience with these tests to provide us perspectives .

from this group , we identified the airports with the highest and lowest pass rates for het tests , and selected among these to reflect variation in several factors , including airport category , difference between het and fet detection rates , and whether the airport had been tested by inspection in fiscal years 2016 and 2017 .

finally , to assess the quality of security operations' testing , we calculated detection rates for its two types of testing â€” headquarters evaluations team ( het ) tests , in which security operations headquarters staff travel to airports to conduct tests , and field evaluations team ( fet ) tests , which are conducted by staff at local airports .

we assessed fet test results against security operations' criterion stating that differences in het and fet detection rates must be within a designated number of percentage points .

we made these comparisons analyzing complete test results for fiscal year 2017 and the first 6 months of fiscal year 2018 , over three 6-month periods in order to identify trends .

we used for our analysis the12,000 fiscal year 2017 security operations tpf records documenting the results of individual covert tests , and an additional 3,600 records from fiscal year 2018 .

for our analysis , we calculated het and fet detection rates ( i.e. , number of items successfully detected ) for three screening paths: a checkpoint test with the item concealed on the tester , a checkpoint test with the item concealed in a carry - on bag , and a checked baggage test with the item concealed in the checked bag .

in calculating these detection rates , we included only results for scenarios tested within the 18-month period that had both het and fet tests , and we excluded any test results for scenarios involving enhanced screening .

also , in our calculation of the fet detection rate , we included fet test results for all airports , including those from smaller ( category iii and iv ) airports , which het teams generally do not visit .

we chose to include fet results from all airports in our analysis because it better reflected the overall performance of airports on covert tests .

in addition to comparing security operations' quality assurance process against the program's criteria , we assessed it against federal internal control criteria for documenting processes .

to assess the quality of inspection testing , we reviewed program guidance to identify testing requirements , methods , and limitations .

we also observed four different tests conducted at a category x airport .

in addition , we reviewed inspection guidance to identify and assess requirements for analyzing and reporting covert test results , and reviewed completed reports to identify the extent to which inspection followed these requirements .

we met with inspection technical experts to discuss inspection processes for selecting a sample of airports for tests and for analyzing and compiling covert test findings .

to assess the extent to which inspection and security operations address security vulnerabilities , we reviewed their efforts separately because each office utilized a different approach .

to assess inspection's efforts , we focused on its use of the security vulnerability management process , an agency - wide process that inspection designated in 2016 as the principal means by which it addresses its identified vulnerabilities .

to obtain a more complete understanding of the extent to which this process has addressed inspection vulnerabilities , we reviewed documentation related to the process ( such as its charter ) and other information pertaining to all vulnerabilities inspection has submitted to the process , including those that were unrelated to checkpoint and checked baggage screening ( eg , cargo screening ) .

we analyzed timeframes associated with the vulnerabilities reviewed under the process and the progress made toward closing nine inspection - identified vulnerabilities .

we assessed the vulnerability management process against standards for program management issued by the project management institute , a not - for - profit association that provides global standards for , among other things , project and program management .

given the focus of security operations' testing on screener performance , the vulnerabilities it identified involved tso failures on tests of specific procedures .

to determine how security operations headquarters officials address vulnerabilities involving screener performance , we reviewed program documentation , including program guidance and periodic reporting of results , and interviewed program managers .

to understand how the results of covert testing are used at the airport level to improve tso performance and address other identified vulnerabilities , we conducted semi - structured interviews with 10 tsa fsds stationed at airports across the united states , and with three tsa regional directors .

we selected the latter based on whether the regional director had under his or her direction at least 1 of 10 fsds we selected for interviews , and to reflect variety in geographic location .

we assessed security operations' and tsa officials at airports' efforts to use covert test results to address vulnerabilities against federal internal control standards and criteria within the national infrastructure protection plan .

this is the public version of a classified report that we issued on january 10 , 2019 .

the classified report included an objective related to identifying the results of covert testing for fiscal years 2016 and 2017 and assessing the quality of this test information .

dhs deemed covert testing results ( including detection rates and identified vulnerabilities ) to be classified information , which must be protected from loss , compromise , or inadvertent disclosure .

consequently , this report omits part of an objective identifying the results of covert testing .

dhs also deemed some of information in our january report to be sensitive security information .

therefore , this report omits information describing tsa screening procedures , the results of agency risk assessments , and airport - level covert test results .

the performance audit upon which this report is based was conducted from september 2017 to january 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient and appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained from this work provides a reasonable basis for our findings and conclusions based on our audit objectives .

we worked with dhs from february 2019 through april 2019 to prepare this unclassified , non - sensitive version of the original classified report for public release .

this public version was also prepared in accordance with these standards .

william russell ( 202 ) 512-8777 or russellw@gao.gov .

in addition to the contact named above , ellen wolfe ( assistant director ) , mona nichols blake ( analyst in charge ) , james ashley , chuck bausell , jason blake , michele fejfar , eric hauswirth , susan hsu , tom lombardi , minette richardson , and nina thomas - diggs made significant contributions to this report .

