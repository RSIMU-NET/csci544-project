congress and the new administration crafted the american recovery and reinvestment act of 2009 ( recovery act ) with the broad purpose of stimulating the economy .

one of the express purposes of the act was to preserve and create jobs .

to help measure the progress of this effort , congress and the administration built into the act numerous provisions to increase transparency and accountability over spending that require recipients of recovery act funding to report quarterly on a number of measures .

nonfederal recipients of recovery act funded grants , contracts , or loans are required to submit reports with information on each project or activity , including the amount and use of funds and an estimate of the jobs created or retained .

neither individuals nor recipients receiving funds through entitlement programs , such as medicaid , or tax programs are required to report .

the first of these recipient reports cover cumulative activity since the recovery act's passage in february 2009 through the quarter ending september 30 , 2009 .

the recovery act requires gao to comment on the estimates of jobs created or retained in the recipient reports no later than 45 days after recipients have reported .

the final recipient reporting data for the first round of reports were first made available on october 30 , 2009 .

the transparency that is envisioned for tracking recovery act spending and results is unprecedented for the federal government .

both congress and the president have emphasized the need for accountability , efficiency , and transparency in the expenditure of recovery act funds and have made it a central principle of the act .

as congress finished work on the recovery act , the house appropriations committee released a statement saying , “a historic level of transparency , oversight and accountability will help guarantee taxpayer dollars are spent wisely and americans can see results for their investment.” in january , the new administration pledged that the recovery act would “break from conventional washington approaches to spending by ensuring that public dollars are invested effectively and that the economic recovery package is fully transparent and accountable to the american people.” however , tracking billions of dollars that are being disbursed to thousands of recipients is an enormous effort .

the administration expects that achieving this degree of visibility will be an iterative process in which the reporting process and information improve over time and , if successful , could be a model for transparency and oversight beyond the recovery act .

this report , the first in response to the recovery act's section 1512 mandate that gao comment on the estimates of jobs created or retained by direct recipients of recovery act funds , addresses the following: ( 1 ) the extent to which recipients were able to fulfill their reporting requirements and the processes in place to help ensure recipient reporting data quality and ( 2 ) how macroeconomic data and methods , and the recipient reports , can be used to assess the employment effects of the recovery act , and the limitations of the data and methods .

to meet our objectives , we performed an initial set of edit checks and basic analyses on the final recipient report data that first became available at www.recovery.gov , the federal government's official web site on recovery act spending , on october 30 , 2009 .

we calculated the overall sum , as well as sum by states , for the number of full - time equivalents ( fte ) reported , award amount , and amount received and found that they corresponded closely with the values shown for these data on recovery.gov .

we built on information collected at the state , local , and program level as part of our bimonthly reviews of selected states' and localities' uses of recovery act funds .

these bimonthly reviews focus on recovery act implementation in 16 states and the district of columbia , which contain about 65 percent of the u.s. population and are estimated to receive collectively about two - thirds of the intergovernmental federal assistance funds available through the recovery act .

a detailed description of the criteria used to select the core group of 16 states and the district is found in appendix i of our april 2009 recovery act bimonthly report .

prime recipients and delegated subrecipients had to prepare and enter their information by october 10 , 2009 .

the days following up to october 30 , 2009 , included the data review period , and as noted previously , on october 30 , 2009 , the first round of recipient reported data was made public .

over the course of three different interviews , two with prime recipients of recovery act funding and one with subrecipients , we visited the 16 selected states and the district of columbia during late september and october 2009 .

we discussed with prime recipients projects associated with 50 percent of the total funds reimbursed , as of september 4 , 2009 , for that state , in the federal - aid highway program administered by the department of transportation ( dot ) .

prior to the start of the reporting period on october 1 , we reviewed prime recipients' plans for the jobs data collection process .

after the october 10 data reporting period , we went back to see if prime recipients followed their own plans and subsequently talked with at least two vendors in each state to gauge their reactions to the reporting process and assess the documentation they were required to submit .

we gathered and examined issues raised by recipients in these jurisdictions regarding reporting and data quality and interviewed recipients on their experiences using the web site reporting mechanism .

during the interviews , we used a series of program reviews and semistructured interview guides that addressed state plans for managing , tracking , and reporting on recovery act funds and activities .

in a similar way , we examined a nonjudgmental sample of department of education ( education ) recovery act projects at the prime and subrecipient level .

we also collected information from transit agencies as part of our bimonthly recovery act reviews .

in addition , we interviewed federal agency officials who have responsibility for ensuring a reasonable degree of quality across their program's recipient reports .

we assessed the reports from the inspector generals ( ig ) on recovery act data quality review from 15 agencies .

we are also continuing to monitor and follow up on some of the major reporting issues identified in the media and by other observers .

for example , a number of press articles have discussed concerns with the jobs reporting done by head start grantees .

according to a health and human services ( hhs ) recovery act official , hhs is working with the office of management and budget ( omb ) to clarify the reporting policy as it applies to head start grantees .

we will be reviewing these efforts as they move forward .

to address our second objective , we analyzed economic and fiscal data using standard economic principles and reviewed the economic literature on the effect of monetary and fiscal policies for stimulating the economy .

we also reviewed guidance that omb developed for recovery act recipients to follow in estimating the effect of funding activities on employment , reviewed reports that the council of economic advisers ( cea ) issued on the macroeconomic effects of the recovery act , and interviewed officials from the cea , omb , and the congressional budget office ( cbo ) .

we conducted this performance audit with field work beginning in late september 2009 and began analysis of the recipient data that became available on october 30 , 2009 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in december 2007 , the united states entered what has turned out to be its deepest recession since the end of world war ii .

between the fourth quarter of 2007 and the third quarter of 2009 , gross domestic product ( gdp ) fell by about 2.8 percent , or $377 billion .

the unemployment rate rose from 4.9 percent in 2007 to 10.2 percent in october 2009 , a level not seen since april 1983 .

the cbo projects that the unemployment rate will remain above 9 percent through 2011 .

confronted with unprecedented weakness in the financial sector and the overall economy , the federal government and the federal reserve together acted to moderate the downturn and restore economic growth .

the federal reserve used monetary policy to respond to the recession by pursuing one of the most significant interest rate reductions in u.s. history .

in concert with the department of the treasury , it went on to bolster the supply of credit in the economy through measures that provide federal reserve backing for a wide variety of loan types , from mortgages to automobile loans to small business loans .

the federal government also used fiscal policy to confront the effects of the recession .

existing fiscal stabilizers , such as unemployment insurance and progressive aspects of the tax code , kicked in automatically in order to ease the pressure on household income as economic conditions deteriorated .

in addition , congress enacted a temporary tax cut in the first half of 2008 to buoy incomes and spending and created the troubled asset relief program in the second half of 2008 to give treasury authority to act to restore financial market functioning .

the federal government's largest response to the recession to date came in early 2009 with the passage of the recovery act , the broad purpose of which is to stimulate the economy's overall demand for goods and services , or aggregate demand .

the recovery act is specifically intended to preserve and create jobs and promote economic recovery ; to assist those most impacted by the recession ; to provide investments needed to increase economic efficiency by spurring technological advances in health and science ; to invest in transportation , environmental protection , and other infrastructure that will provide long - term economic benefits ; and to stabilize the budgets of state and local governments .

the cbo estimates that the net cost of the recovery act will total approximately $787 billion from 2009 to 2019 .

the recovery act uses a combination of tax relief and government spending to accomplish its goals .

the recovery act's tax cuts include reductions to individuals' taxes , payments to individuals in lieu of reductions to their taxes , adjustments to the alternative minimum tax , and business tax incentives .

tax cuts encompass approximately one - third of the recovery act's dollars .

recovery act spending includes temporary increases in entitlement programs to aid people directly affected by the recession and provide some fiscal relief to states ; this also accounts for about one third of the recovery act .

for example , the recovery act temporarily increased and extended unemployment benefits , temporarily increased the rate at which the federal government matched states medicaid expenditures , and provided additional funds for the supplemental nutrition assistance and the temporary aid to needy families programs , among other things .

other spending , also accounting for about a third of the act falls into the category of grants , loans , and contracts .

this includes government purchases of goods and services , grants to states through programs such as the state fiscal stabilization fund for education and other government services , and government investment in infrastructure , health information technology , renewable energy research , and other areas .

in interpreting recipient reporting data , it is important to recognize that the recipient reporting requirement only covers a defined subset of the recovery act's funding .

the reporting requirements apply only to nonfederal recipients of funding , including all entities receiving recovery act funds directly from the federal government such as state and local governments , private companies , educational institutions , nonprofits , and other private organizations .

omb guidance , consistent with the statutory language in the recovery act , states that these reporting requirements apply to recipients who receive funding through the recovery act's discretionary appropriations , not recipients receiving funds through entitlement programs , such as medicaid , or tax programs .

recipient reporting also does not apply to individuals .

in addition , the required reports cover only direct jobs created or retained as a result of recovery act funding ; they do not include the employment impact on materials suppliers ( indirect jobs ) or on the local community ( induced jobs ) .

figure 1 shows the division of total recovery act funds and their potential employment effects .

tracing the effects of the recovery act through the economy is a complicated task .

prospectively , before the act's passage or before funds are spent , the effects can only be projected using economic models that represent the behavior of governments , firms , and households .

while funds are being spent , some effects can be observed but often relevant data on key relationships and indicators in the economy are available only with a lag , thereby complicating real - time assessments .

when a full range of data on outcomes becomes available , economic analysts undertake retrospective analyses , where the findings are often used to guide future policy choices and to anticipate effects of similar future policies .

stimulus spending under the broad scope of the recovery act will reverberate at the national , regional , state , and local levels .

models of the national economy provide the most comprehensive view of policy effects , but they do not provide insight , except indirectly , about events at smaller geographical scales .

the diversity and complexity of the components of the national economy are not fully captured by any set of existing economic models .

some perspective can be gained by contemporaneous close observation of the actions of governments , firms , and households , but a complete and accurate picture of the recovery act's impact will emerge only slowly .

section 1512 of the recovery act requires recipients of recovery funds to report on those funds each calendar quarter .

these recipient reports are to be filed for any quarter in which a recipient receives recovery act funds directly from the federal government .

the recipient reporting requirement covers all funds made available by appropriations in division a of the recovery act .

the reports are to be submitted no later than 10 days after the end of each calendar quarter in which the recipient received recovery act funds .

each report is to include the total amount of recovery act funds received , the amount of funds expended or obligated to projects or activities , and a detailed list of those projects or activities .

for each project or activity , the detailed list must include its name and a description , an evaluation of its completion status , and an estimate of the number of jobs created or the number of jobs retained by that project or activity .

certain additional information is also required for infrastructure investments made by state and local governments .

also , the recipient reports must include detailed information on any subcontracts or subgrants as required by the federal funding accountability and transparency act of 2006 .

section 1512 ( e ) of the recovery act requires gao and cbo to comment on the estimates of jobs created or retained reported by recipients .

in its guidance to recipients for estimating employment effects , omb instructed recipients to report only the direct employment effects as “jobs created or retained” as a single number .

recipients are not expected to report on the employment impact on materials suppliers ( indirect jobs ) or on the local community ( induced jobs ) .

according to the guidance , “a job created is a new position created and filled or an existing unfilled position that is filled as a result of the recovery act ; a job retained is an existing position that would not have been continued to be filled were it not for recovery act funding .

only compensated employment .

 .

 .

should be reported .

the estimate of the number of jobs .

 .

 .

should be expressed as ‘full - time equivalents ( fte ) ,' which is calculated as total hours worked in jobs created or retained divided by the number of hours in a full - time schedule , as defined by the recipient.” consequently , the recipients are expected to report the amount of labor hired or not fired as result of having received recovery act funds .

it should be noted that one fte does not necessarily equate to the job of one person .

firms may choose to increase the hours of existing employees , for example , which can certainly be said to increase employment but not necessarily be an additional job in the sense of adding a person to the payroll .

to implement the recipient reporting data requirements , omb has worked with the recovery accountability and transparency board ( recovery board ) to deploy a nationwide data collection system at www.federalreporting.gov ( federalreporting.gov ) , while the data reported by recipients are available to the public for viewing and downloading on www.recovery.gov ( recovery.gov ) .

recovery.gov , a site designed to provide transparency of information related to spending on recovery act programs , is the official source of information related to the recovery act .

the recovery board's goals for the recovery act web site include promoting accountability by providing a platform to analyze recovery act data and serving as a means of tracking fraud , waste , and abuse allegations by providing the public with accurate , user - friendly information .

in addition , the site promotes official data in public debate , assists in providing fair and open access to recovery act opportunities , and promotes an understanding of the local impact of recovery act funding .

in an effort to address the level of risk in recipient reporting , omb's june 22 , 2009 , guidance on recipient reporting includes a requirement for data quality reviews .

omb's data quality guidance is intended to address two key data problems — material omissions and significant reporting errors .

material omissions and significant reporting errors are risks that the information is incomplete and inaccurate .

as shown in figure 2 , omb gave specific time frames for reporting that allow prime recipients and delegated subrecipients to prepare and enter their information on days 1 through 10 following the end of the quarter .

during days 11 through 21 , prime recipients will be able to review the data to ensure that complete and accurate reporting information is provided prior to a federal agency review and comment period beginning on the 22nd day .

during days 22 to 29 following the end of the quarter , federal agencies will perform data quality reviews and will notify the recipients and delegated subrecipients of any data anomalies or questions .

the original submitter must complete data corrections no later than the 29th day following the end of the quarter .

prime recipients have the ultimate responsibility for data quality checks and the final submission of the data .

since this is a cumulative reporting process , additional corrections can take place on a quarterly basis .

omb guidance does not explicitly mandate a methodology for conducting data quality reviews at the prime and delegated subrecipient level or by the federal agencies .

instead , the june 22 , 2009 , guidance provides the relevant party conducting the data quality review with discretion in determining the optimal method for detecting and correcting material omissions or significant reporting errors .

the guidance says that , at a minimum , federal agencies , recipients , and subrecipients should establish internal controls to ensure data quality , completeness , accuracy , and timely reporting of all amounts funded by the recovery act .

the recovery board published the results of the first round of recipient reporting on recovery.gov on october 30 , 2009 .

according to the web site , recipients submitted 130,362 reports indicating that 640,329 “jobs” were created or saved as a direct result of the recovery act .

these data solely reflect the direct ftes reported by recipients of recovery act grants , contracts , and loans for the period beginning when the act was signed into law on february 17 , 2009 through september 30 , 2009 .

as shown in figure 3 , grants , contracts , and loans account for about 27 percent , or $47 billion , of the approximately $173 billion in recovery act funds paid out as of september 30 , 2009 .

recipients in all 50 states reported jobs created or retained with recovery act funding provided through a wide range of federal programs and agencies .

table 1 shows the distribution of jobs created or retained across the nation as reported by recipients on recovery.gov .

not surprisingly , california , the most populous state , received the most recovery act dollars and accounted for the largest number of the reported jobs created or retained .

table 2 shows the number and share of jobs created or retained by federal program agencies as reported by recipients of recovery act funding .

the department of education accounted for nearly 400,000 or close to two - thirds of the reported jobs created or retained .

according to the department of education , this represents about 325,000 education jobs such as teachers , principals , and support staff in elementary and secondary schools , and educational , administrative , and support personnel in institutions of higher education funded primarily through the state fiscal stabilization fund ( sfsf ) .

in addition , approximately 73,000 other jobs ( including both education and noneducation positions ) were reported saved or created from the sfsf government services fund , the federal work study program , and impact aid funds .

while recipients gao contacted appear to have made good faith efforts to ensure complete and accurate reporting , gao's fieldwork and initial review and analysis of recipient data from www.recovery.gov , indicate that there are a range of significant reporting and quality issues that need to be addressed .

collecting information from such a large and varied number of entities in a compressed time frame , as required by the recovery act , is a huge task .

major challenges associated with the new recovery act reporting requirements included educating recipients about the reporting requirements and developing the systems and infrastructure for collecting and reporting the required information .

while recipients in the states we reviewed generally made good faith efforts to report accurately , there is evidence , including numerous media accounts , that the data reporting has been somewhat inconsistent .

even recipients of similar types of funds appear to have interpreted the reporting guidance in somewhat different ways and took different approaches in how they developed their jobs data .

the extent to which these reporting issues affect overall data quality is uncertain at this point .

as existing recipients become more familiar with the reporting system and requirements , these issues may become less significant although communication and training efforts will need to be maintained and in some cases expanded as new recipients of recovery act funding enter the system .

because this effort will be an ongoing process of cumulative reporting , our first review represents a snapshot in time .

we performed an initial set of edit checks and basic analyses on the recipient report data available for download from recovery.gov on october 30 , 2009 .

based on that initial review work , we identified recipient report records that showed certain data values or patterns in the data that were either erroneous or merit further review due to an unexpected or atypical data value or relationship between data values .

for the most part , the number of records identified by our edit checks was relatively small compared to the 56,986 prime recipient report records included in our review .

as part of our review , we examined the relationship between recipient reports showing the presence or absence of any fte counts with the presence or absence of funding amounts shown in either or both data fields for amount of recovery act funds received and amount of recovery act funds expended .

forty four percent of the prime recipient reports showed an fte value .

as shown in table 3 , we identified 3,978 prime recipient reports where ftes were reported but no dollar amount was reported in the data fields for amount of recovery act funds received and amount of recovery act funds expended .

these records account for 58,386 of the total 640,329 ftes reported .

as might be expected , 71 percent of those prime recipient reports shown in table 3 that did not show any ftes also showed no dollar amount in the data fields for amount of recovery act funds received and amount expended .

there were also 9,247 reports that showed no ftes but did show some funding amount in either or both of the funds received or expended data fields .

the total value of funds reported in the expenditure field on these reports was $965 million .

those recipient reports showing ftes but no funds and funds but no ftes constitute a set of records that merit closer examination to understand the basis for these patterns of reporting .

ten recipient reports accounted for close to 30 percent of the total ftes reported .

all 10 reports were grants and the majority of those reports described funding support for education - sector related positions .

for reports containing ftes , we performed a limited , automated scan of the job creation field of the report , which is to contain a narrative description of jobs created or retained .

we identified 261 records where there was only a brief description in this job creation field and that brief text showed such words or phrases as “none,” “n / a,” zero , or variants thereof .

for most of these records , the value of ftes reported is small , but there are 10 of these records with each reporting 50 or more ftes .

the total number of ftes reported for all 261 records is 1,776 .

while our scan could only identify limited instances of apparently contradictory information between the job description and the presence of an fte number , we suspect that a closer and more extensive review of the job description field in relation to the count of ftes would yield additional instances where there were problems , and greater attention to this relationship would improve data quality .

in our other analyses of the data fields showing recovery act funds , we identified 132 records where the award amount was zero or less than $10 .

there were also 133 records where the amount reported as received exceeded the reported award amount by more than $10 .

on 17 of these records , the difference between the smaller amount awarded and the larger reported amount received exceeded $1 million .

while there may be a reason for this particular relationship between the reported award amount and amount received , it may also indicate an improper keying of data or an interpretation of what amounts are to be reported in which fields that is not in accordance with the guidance .

we calculated the overall sum and sum by states for number of ftes reported , award amount , and amount received .

we found that they corresponded closely with the values shown for these data on recovery.gov .

some of the data fields we examined with known values such as the treasury account symbol ( tas ) codes and catalog of federal domestic assistance ( cfda ) numbers showed no invalid values on recipient reports .

however , our analyses show that there is reason to be concerned that the values shown for these data fields in conjunction with the data field identifying who the funding or awarding agency is may not be congruent .

both tas and cfda values are linked to specific agencies and their programs .

we matched the reported agency codes against the reported tas and cfda codes .

we identified 454 reports as having a mismatch on the cfda number — therefore , the cfda number shown on the report did not match the cfda number associated with either the funding or awarding agency shown on the report .

on tas codes , we identified 595 reports where there was no tas match .

included in the mismatches were 76 recipient reports where gao was erroneously identified as either the funding or awarding agency .

in many instances , review of these records and their tas or cfda values along with other descriptive information from the recipient report indicated the likely funding or awarding agencies .

these mismatches suggest that either the identification of the agency or the tas and cfda codes are in error on the recipient report .

another potential problem area we identified was the provision of data on the number and total amount of small subawards of less than $25,000 .

there are data fields that collect information on small subawards , small subawards to individuals , and small subawards to vendors .

there were 380 prime recipient report records where we observed the same values being reported in both small subawards and small subawards to individuals .

we also identified 1,772 other records where it could be clearly established that these values were being reported separately .

while we are able to establish that these data are not being consistently reported , it is not possible to assess from the data alone the full extent to which subaward data are being combined or reported separately across all recipient reports .

additionally , we noted 152 reports where , in either the subawards or subawards to individuals data fields , the value for the number of subawards and the total dollar value of subawards were exactly the same and , as such , most likely erroneous .

while most recipient report records were not identified as potential problems in these initial edit checks and analyses thus far , our results do indicate the need for further data quality efforts .

under omb guidance , jobs created or retained were to be expressed as ftes .

we found that data were reported inconsistently even though significant guidance and training was provided by omb and federal agencies .

while ftes should allow for the aggregation of different types of jobs — part - time , full - time or temporary — differing interpretations of the fte guidance compromise the ability to aggregate the data .

in addition to issuing guidance , omb and federal agencies provided several types of clarifying information to recipients as well as opportunities to interact and ask questions or receive help with the reporting process .

these included weekly phone calls between omb and groups representing the state budget and comptrollers offices , weekly calls between all state reporting leads , webinars , a call center , and e - mail outreach .

state officials reported they took advantage of and appreciated this outreach .

for example , ohio state officials said they were generally satisfied with the technical assistance and guidance provided by omb — specifically , the assistance it received from the federalreporting.gov help desk staff .

omb estimated that it had a better than 90 percent response rate for recipient reporting and said that they answered over 3,500 questions related to recipient reporting .

the data element on jobs created or retained expressed in ftes raised questions and concerns for some recipients .

omb staff reported that questions on ftes dominated the types of questions they fielded during the first round of recipient reporting .

although the recipient reports provide a detailed account of individual projects , as recovery.gov shows , these projects represent different types of activities and start and end at various points throughout the year , and recipients had various understandings of how to report an fte .

in section 5.2 of the june 22 guidance , omb states that “the estimate of the number of jobs required by the recovery act should be expressed as ‘full - time equivalents' ( fte ) , which is calculated as the total hours worked in jobs retained divided by the number of hours in a full time schedule , as defined by the recipient.” further , “the fte estimates must be reported cumulatively each calendar quarter.” in section 5.3 , omb states that “reporting is cumulative across the project lifecycle , and will not reset at the beginning of each calendar or fiscal year.” fte calculations varied depending on the period of performance the recipient reported on .

for example , in the case of federal highways projects , some have been ongoing for six months , while others started in september 2009 .

in attempting to address the unique nature of each project , dot's federal highway administration ( fhwa ) faced the issue of whether to report fte data based on the length of time to complete the entire project ( project period of performance ) versus a standard period of performance such as a calendar quarter across all projects .

according to fhwa guidance , which was permitted by omb , ftes reported for each highway project are expressed as an average monthly fte .

this means that for a project that started on july 1 , 2009 , the prime recipient would add up the hours worked on that project in the months of july , august , and september and divide that number by .

for a project that started on august 1 , 2009 , the prime recipient should add up the hours worked on that project in the months of august and september and divide that number by .

for a project that started on september , 1 , 2009 , the prime recipient should add up the hours worked on that project in the month of september and divide that number by .

the issue of a standard performance period is magnified when looking across programs and across states .

to consistently compare ftes , or any type of fraction , across projects , one must use a common denominator .

comparison of fte calculations across projects poses challenges when the projects have used different time periods as denominators .

tables 4 and 5 below provide more detail on the problems created by not having a standard performance period for calculating ftes .

table 4 is an application of the fhwa guidance for three projects with varying start dates .

this example illustrates the way fhwa applied the omb guidance and that the way ftes are aggregated in federalreporting.gov could overstate the employment effects .

in this example , because the 30 monthly fte data were aggregated without standardizing for the quarter , ftes would be overstated by 10 relative to the omb guidance .

a standardized quarterly measure and job - years are included as examples of a standard period of performance .

a job - year is simply one job for 1 year .

regardless of when the project begins , the total hours worked is divided by a full years worth of time ( 12 months ) , which would enable aggregation of employment effects across programs and time .

table 5 is an application of the omb guidance for two projects with varying start dates .

in this example , the omb guidance understates the employment effect relative to the standardized measure .

cumulative fte per omb guidance would result in 20 fte compared with 30 fte when standardized on a quarterly basis .

both a standardized quarterly fte measure and a job - year measure are included as examples of a standard period of performance .

regardless of when the project begins , the total hours worked is divided by a full year's worth of time ( 12 months ) , which would enable aggregation of employment effects across programs and time .

there are examples from other dot programs where the issue of a project period of performance created significant variation in the fte calculation .

for example , in pennsylvania , each of four transit entities we interviewed used a different denominator to calculate the number of full - time equivalent jobs they reported on their recipients reports for the period ending september 30 , 2009 .

southeastern pennsylvania transportation authority in philadelphia used 1,040 hours as its denominator , since it had projects underway in two previous quarters .

port authority of allegheny county prorated the hours based on the contractors' start date as well as to reflect that hours worked from september were not included due to lag time in invoice processing .

port authority used 1,127 hours for contractors starting before april , 867 hours for contractors starting in the second quarter , and 347 hours for contractors starting in the third quarter .

lehigh and northampton transportation authority in allentown used 40 hours in the 1512 report they tried to submit , but , due to some confusion about the need for corrective action , the report was not filed .

finally , the pennsylvania department of transportation in the report for nonurbanized transit systems used 1,248 hours , which was prorated by multiplying 8 hours per workday times the 156 workdays between february 17 and september 30 , 2009 .

in several other of our selected states , this variation across transit programs' period of performance for the fte calculation also occurred .

the issue of variation in the period of performance used to calculate ftes also occurred in education programs .

across a number of states we reviewed , local education agencies and higher education institutions used a different denominator to calculate the number of ftes they reported on their recipient reports for the period ending september 30 , 2009 .

for example , two higher education systems in california each calculated the fte differently .

in the case of one , officials chose to use a two - month period as the basis for the fte performance period .

the other chose to use a year as the basis of the fte .

the result is almost a three - to - one difference in the number of ftes reported for each university system in the first reporting period .

although education provides alternative methods for calculating an fte , in neither case does the guidance explicitly state the period of performance of an fte .

recipients were also confused about counting a job created or retained even though they knew the number of hours worked that were paid for with recovery act funds .

for example , the revere housing authority , in administering one recovery act project , told us that they may have underreported jobs data from an architectural firm providing design services for a recovery act window replacement project at a public housing complex .

the employees at the architecture firm that designed the window replacement project were employed before the firm received the recovery act funded contract and will continue to be employed after the contract has been completed , so from the revere housing authority's perspective there were no jobs created or retained .

as another example , officials from one housing agency reported the number of people , by trade , who worked on recovery act related projects , but did not apply the full - time equivalent calculation outlined by omb in the june 22 reporting guidance .

officials from another public housing agency told us that they based the number of jobs they reported on letters from their contractors detailing the number of positions rather than ftes .

omb staff said that thinking about the jobs created or retained as hours worked and paid for with recovery act funds was a useful way to understand the fte guidance .

while omb's guidance explains that in applying the fte calculation for measuring the number of jobs created or retained recipients will need the total number of hours worked that are funded by the recovery act , it could emphasize this relationship more thoroughly throughout its guidance .

omb's decision to convert jobs into ftes provides a consistent lens to view the amount of labor being funded by the recovery act , provided each recipient uses a standard time frame in considering the fte .

the current omb guidance , however , creates a situation where , because there is no standard starting or ending point , an fte provides an estimate for the life of the project .

without normalizing the fte , aggregate numbers should not be considered , and the issue of a standard period of performance is magnified when looking across programs and across states .

recipients we interviewed were able to report into and review data on federalreporting.gov .

particularly given the scale of the project and how quickly it was implemented , within several months , the ability of the reporting mechanisms to handle the volume of data from the range of recipients represents a solid first step in the data collection and reporting process for the fulfillment of the section 1512 mandate .

nonetheless , there were issues associated with the functional process of reporting .

for example , state officials with decentralized reporting structures reported problems downloading submitted information from recovery.gov to review top - line figures such as money spent and jobs created or retained .

the iowa department of management , which did iowa's centralized reporting into federalreporting.gov , said that , overall , the system was very slow .

in addition to the slowness , as the system was processing input from iowa's submission , every time it encountered an error , it kicked back the whole submission — but it showed only the one error .

after fixing the one errant entry , the state resubmitted its information , which would then be completely sent back the next time an error was encountered .

iowa officials believe it would have been more efficient if the system identified all errors in submission and sent back a complete list of errors to fix .

other recipient reporters we interviewed highlighted issues around duns numbers and other key identifiers , along with the inability to enter more than one congressional district for projects that span multiple districts .

the expectation is that many of these entry and processing errors were captured through the review process , but the probability that all errors were caught is low .

generally , state officials from our 17 jurisdictions reported being able to work through technical reporting and processing glitches .

for example , florida officials reported that they encountered many technical issues but were able to solve the problems by contacting the recovery board .

ohio officials noted that , although they were initially concerned , in spite of the tremendous amount of data being submitted , federalreporting.gov held up well .

while they faced some challenges , california officials reported that , overall , they were successful in reporting the numbers into federalreporting.gov .

they worked with the technical team at federalreporting.gov and performed a test on october 1 , 2009 , to see if the upload of the job data was going to work .

during the october reporting time frame , new jersey officials reported that they generally did not experience significant recipient reporting problems .

the few reporting problems new jersey experienced occurred in relation to issues uploading the data onto federalreporting.gov and issues requiring clarifying guidance from the relevant federal agency .

notwithstanding the concerns over the slowness of the reporting system and error checks , iowa officials also reported that the process worked rather well , determining that most of their state reporting problems seemed to stem from a few recipients not fully grasping all of the training the state had provided and thus not knowing or having key information like duns numbers and in some cases submitting erroneous information .

the state department of management plans to specifically address the 30 or so recipients associated with these issues — just about all of which were school districts .

as a follow - up from this first reporting cycle , several states have developed a list of lessons learned to share with omb and other federal agencies .

an example in appendix i illustrates problems public housing authorities had with both the recipient reporting processing functions and the fte calculation .

in addition to the federalreporting.gov web site , the recovery board used a revised recovery.gov web site to display reported data .

the revised site includes the ability to search spending data by state , zip code , or congressional district and display the results on a map .

the recovery board also awarded a separate contract to support its oversight responsibilities with the ability to analyze reported data and identify areas of concern for further investigation .

in addition , the board plans to enhance the capabilities of federalreporting.gov .

however , the recovery board does not yet use an adequate change management process to manage system modifications .

without such a process , the planned enhancements could become cost and schedule prohibitive .

the board has recognized this as a significant risk and has begun development of a change management process .

finally , the board has recognized the need to improve the efficiency of its help desk operation to avoid dropped calls and is working on agreements to address this risk .

recipient reporting data quality is a shared responsibility , but often state agencies have principal accountability because they are the prime recipients .

prime recipients , as owners of the recipient reporting data , have the principal responsibility for the quality of the data submitted , and subrecipients delegated to report on behalf of prime recipients share in this responsibility .

in addition , federal agencies funding recovery act projects and activities provide a layer of oversight that augments recipient data quality .

oversight authorities including omb , the recovery board , and federal agency igs also have roles to play in ensuring recipient reported data quality , while the general public and nongovernmental entities can help as well by highlighting data problems for correction .

all of the jurisdictions we reviewed had data quality checks in place for the recipient reporting data , either at the state level or a state agency level .

state agencies , as entities that receive recovery act funding as federal awards in the form of grants , loans , or cooperative agreements directly from the federal government , are often the prime recipients of recovery act funding .

our work in the 16 states and the district of columbia showed differences in the way states as prime recipients approach recipient reporting data quality review .

officials from nine states reported having chosen a centralized reporting approach meaning that state agencies submit their recipient reports to a state central office , which then submits state agency recipient reports to federalreporting.gov .

for example , colorado's department of transportation provided its recipient report to a central entity , the colorado office of information technology , for submission to federalreporting.gov .

states with centralized reporting systems maintain that they will be able to provide more oversight of recipient reporting with this approach .

advocates of centralized reporting also expect that method will increase data quality , decrease omissions and duplicate reporting , and facilitate data cleanup .

officials from the remaining eight jurisdictions reported using a decentralized reporting system .

in these cases , the state program office administering the funds is the entity submitting the recipient report .

in georgia , for example , the state department of transportation is responsible for both reviewing recipient report data and submitting it to federalreporting.gov .

illinois , as is the case for four other decentralized states , is quasidecentralized where the data are centrally reviewed and reported in a decentralized manner .

when the audit office informs the office of the governor that its review is complete and if the office of the governor is satisfied with the results , the illinois state reporting agency may upload agency data to federalreporting.gov .

appendix i provides details on california's recipient reporting experiences .

as a centralized reporting state , iowa officials told us that they developed internal controls to help ensure that the data submitted to omb , other federal entities , and the general public , as required by section 1512 of the recovery act , are accurate .

specifically , iowa inserted validation processes in its recovery act database to help reviewers identify and correct inaccurate data .

in addition , state agency and local officials were required to certify their review and approval of their agency's information prior to submission .

iowa state officials told us that they are working on data quality plans to include being able to reconcile financial information with the state's centralized accounting system .

according to iowa officials , the number of recovery act grant awards improperly submitted was relatively small .

as a decentralized reporting state , new jersey officials reported that a tiered approach to data quality checks was used for all recovery act funding streams managed by the state .

each new jersey state department or entity was responsible for formulating a strategy for data quality reviews and implementing that strategy .

the new jersey department of community affairs , for example , directed subrecipients to report data directly into an existing departmental data collection tool modified to encompass all of the data points required by the recovery act .

this system gave the department of community affairs the ability to view the data as it came in from each subrecipient .

from this data collection tool , the department uploaded prime and subrecipient data to federalreporting.gov .

all departmental strategies were reviewed by the new jersey governor's office and the new jersey recovery accountability task force .

the governor's office conducted a review of the reports as they were uploaded to federalreporting.gov on a program - by - program , department - by - department basis to identify any outliers , material omissions , or reporting errors that could have been overlooked by departments .

to help ensure the quality of recipient report data , the recovery board encouraged each federal office of inspector general overseeing an agency receiving recovery act funds to participate in a governmentwide recovery act reporting data quality review .

the recovery board requested the ig community to determine the following: ( 1 ) the existence of documentation on the agencies' processes and procedures to perform limited data quality reviews targeted at identifying material omissions and significant reporting errors , ( 2 ) the agencies' plans for ensuring prime recipients report quarterly , and ( 3 ) how the agencies intend to notify the recipient of the need to make appropriate and timely changes .

in addition , igs reviewed whether the agency had an adequate process in place to remediate systemic or chronic reporting problems and if they planned to use the reported information as a performance management and assessment tool .

we reviewed the 15 ig reports that were available as of november 12 , 2009 .

our review of these reports from a range of federal agencies found that they had drafted plans or preliminary objectives for their plans for data quality procedures .

published ig audits on agencies' recovery act data quality reviews that we examined indicated that federal agencies were using a variety of data quality checks , which included automated or manual data quality checks or a combination .

computer programs drive the automated processes by capturing records that do not align with particular indicators determined by the agency .

agencies may use a manual process where a designated office will investigate outliers that surface during the automated test .

for example , the automated process for education performs data checks to validate selected elements against data in the department's financial systems .

as part of its data quality review , education officials are to examine submitted reports against specific grant programs or contract criteria to identify outliers for particular data elements .

of the ig reports that we reviewed that mentioned systemic or chronic problems , 9 of the 11 found that their agencies had a process in place to address these problems .

although some of the igs were unable to test the implementation of their agency's procedures for reviewing the quarterly recipient reports , based on their initial audit , they were able to conclude that the draft plan or preliminary objectives for data quality review were in place .

according to omb's guidance documents , federal agencies must work with their recipients to ensure comprehensive and accurate recipient reporting data .

a september 11 , 2009 , memorandum from omb directed federal agencies to identify recovery act award recipients for each recovery act program they administer and conduct outreach actions to raise awareness of registration requirements , identify actual and potential barriers to timely registration and reporting , and provide programmatic knowledge and expertise that the recipient may need to register and enter data into federalreporting.gov .

federal agencies were also expected to provide resources to assist state and select local governments in meeting reporting requirements required by the recovery act .

in addition , federal agencies were to identify key mitigation steps to take to minimize delays in recipient registration and reporting .

omb also requires that federal agencies perform limited data quality reviews of recipient data to identify material omissions and significant reporting errors and notify the recipients of the need to make appropriate and timely changes to erroneous reports .

federal agencies are also to coordinate how to apply the definitions of material omissions and significant reporting errors in given program areas or across programs in a given agency to ensure consistency in the manner in which data quality reviews are carried out .

although prime recipients and federal agency reviewers are required to perform data quality checks , none are required to certify or approve data for publication .

however , as part of their data quality review , federal agencies must classify the submitted data as not reviewed by the agency ; reviewed by the agency with no material omissions or significant reporting errors identified ; or reviewed by the agency with material omissions or significant reporting errors identified .

if an agency fails to choose one of the aforementioned categories , the system will default to not reviewed by the agency .

the prime recipient report records we analyzed included data on whether the prime recipient and the agency reviewed the record in the omb data quality review time frames .

in addition , the report record data included a flag as to whether a correction was initiated .

a correction could be initiated by either the prime recipient or the reviewing agency .

table 6 shows the number and percentage of prime recipient records that were marked as having been reviewed by either or both parties and whether a correction was initiated .

omb's guidance provided that , a federal agency , depending on the review approach and methodology , could classify data as being reviewed by the agency even if a separate and unique review of each submitted record had not occurred .

as shown in table 6 , more than three quarters of the prime recipient reports were marked as having undergone agency review .

less than one percent was marked as having undergone review by the prime recipient .

the small percentage reviewed by the prime recipients themselves during the omb review time frame warrants further examination .

while it may be the case that the recipients' data quality review efforts prior to initial submission of their reports precluded further revision during the review time frame , it may also be indicative of problems with the process of noting and recording when and how the prime recipient reviews occur and the setting of the review flag .

overall , slightly more than a quarter of the reports were marked as having undergone a correction during the omb review time frames .

the federal - aid highway program provided a good case study of federal agency data quality reviews because the responsible federal agency , fhwa , had previous experience estimating and reporting on the employment effects of investment in highway construction .

as a result , fhwa would seem to be better positioned than some other federal agencies to fulfill the job creation or retention reporting requirements under the recovery act and may have data quality review processes that other federal agencies could replicate .

we met with officials and reviewed available documentation including federal highway reporting documents and payroll records at the selected state departments of transportation and selected vendors .

overall , we found that the state departments of transportation as prime recipients had in place plans and procedures to review and ensure data quality .

we followed up with the state departments of transportation to confirm that these procedures were followed for highway projects representing at least 50 percent of the recovery act highway reimbursements as of september 4 , 2009 in the 17 jurisdictions where we are conducting bimonthly reviews and reviewed available documentation .

appendix i illustrates recipient reporting processes and data quality checks at the florida department of transportation .

in addition to the section 1512 reporting requirements , recipients of certain transportation recovery act funds , such as state departments of transportation , are subject to the reporting requirements outlined in section 1201 ( c ) of the recovery act .

under section 1201 ( c ) , recipients of transportation funds must submit periodic reports on the amount of federal funds appropriated , allocated , obligated , and reimbursed ; the number of projects put out to bid , awarded , or work has begun or completed ; and the number of direct and indirect jobs created or sustained , among other things .

the recovery act section 1201 ( c ) requirement called for project level data to be reported twice before the first recovery act section 1512 report was due .

dot is required to collect and compile this information for congress , and it issued its first report to congress in may 2009 .

consequently , dot and its modal administrations , such as fhwa , and state departments of transportation gained experience collecting and reporting job creation and retention information before the first recovery act section 1512 report was due in october 2009 and required fhwa to have its data collection and review process in place in advance of october 1 , 2009 , the start of the section 1512 reporting .

to help fulfill these reporting requirements , fhwa implemented a reporting structure that ties together the federal and state levels of reporting , creating both a chain of evidence and redundancy in the review of the reported data .

figure 4 shows the reporting structure .

as part of t reporting structure , fhwa also created the recovery act data system ( rads ) , with the updated version of the system released in early september 2009 .

rads is primarily designed as a repository of data for states , but it also serves as an important oversight tool for fhwa becaus it links federal financial data to project data reported by the states .

the system helps ensure consistent definitions of fields and enables fhwa to auto - populate identification fields , including duns numbers , award numbers , and total award amounts , to both reduce the burden at the project level and to reduce the data entry errors .

in addition , monthly reporting requirements include payroll records , hours worked , and data quality assurances , in individual contracts for highway projects funded with recovery act funds .

fhwa may withhold payments if a recipient is found to be in noncompliance with the reporting requirements .

e e with the reporting requirements .

appendi g examples for contractors in georgia and fhwa has taken several steps to help ensure the reliability of the information contained in rads .

first , fhwa compared information states recorded in rads to the information states submitted to federalreporting.gov to identify inconsistencies or discrepancies .

second , as part of an ongoing data reliability process , fhwa monitors select fields in rads , such as number of projects , types of projects , and where projec are located , and performs data validation and reasonableness tests .

for example , it checks if a rate of payment in dollars per hour is too high or too low .

when potential issues are identified , fhwa division offices work with the state department of transportation or central office to make necessary changes .

ts for this round of recipient reporting , fhwa used an automated process to review all of the reports filed by recipients .

these automated reviews included various data validation and reasonableness checks .

for example , fhwa checked whether the range of ftes reported were within its own economic estimates .

for any reports that were out of range , fhwa would comment on these reports .

as described earlier , only recipients could make changes to the data .

in making a comment , fhwa let the recipient know there was potential concern with the record .

the recipient then had the opportunity to either change or explain the comment raised by fhwa .

according to fhwa officials , they reviewed 100 percent of more than 7,000 reports submitted by recipients of recovery act highway funds and found that the final submissions were generally consistent with department data .

although there were problems of inconsistent interpretation of the guidance , the reporting process went well for highway projects .

education has engaged in numerous efforts to facilitate jobs reporting by states and local educational agencies ( lea ) .

states and leas have also taken action to collect and report jobs data and to ensure data quality .

despite these efforts , state and local officials we spoke with raised some concerns about the quality of jobs data reported in october 2009 , such as insufficient time to incorporate updated guidance on estimating job counts .

to address these concerns , education and many state officials we interviewed said they plan to take steps to improve the reporting and data quality processes before the next reports are due in january 2010 .

our review focused on the state fiscal stabilization fund , as well as recovery act grants made for the elementary and secondary education act of 1965 , title i , part a and for the individuals with disabilities education act , part b .

to collect this information , we interviewed education officials and officials in 10 states — arizona , california , colorado , florida , georgia , illinois , massachusetts , new jersey , new york , and north carolina — the district of columbia , and 12 leas , including a mix of leas in urban and rural areas .

states were selected from the 16 states and the district of columbia in which we conduct bi - monthly reviews of the use of recovery act funds as mandated by the recovery act .

we also reviewed federal and state guidance and other documentation .

education's efforts to facilitate jobs reporting by states and leas include coordinating with omb , providing guidance and technical assistance to states and leas , and reviewing the quality of the jobs data reported .

education has coordinated its efforts regarding recipient reporting with omb in a number of ways , including participating in cross - agency workgroups and clearing its guidance materials with omb prior to disseminating them .

on august 10 , 2009 , education hosted a web - based technical assistance conference on reporting requirements that included information on omb's guidance on estimating and reporting jobs data .

on september 11 , the department issued guidance specifically related to estimating and reporting jobs created or retained by states and leas receiving recovery act grants .

education updated its jobs guidance and hosted another web - based technical assistance conference on september 21 , providing detailed instructions to states and leas on a range of topics , such as how to estimate the number of hours created or retained for a teacher who works less than 12 months in a year .

in addition , according to education officials , the department developed and implemented a draft plan to review the jobs data that states and leas reported to federalreporting.gov in october .

this plan addresses the roles and responsibilities of several education offices to assist with the data quality review throughout the 30-day reporting timeline ( for example , oct. 1 through oct. 30 , 2009 ) .

according to the plan , these responsibilities include continuous evaluation of recipient and subrecipient efforts to meet reporting requirements , as well as providing limited data quality reviews and notifying the recipient of the need to make appropriate and timely corrections .

the plan says that reviewers are to conduct two types of data quality checks – an automated and a manual review .

the automated review will validate various data elements for financial assistance against its grant management system , such as prime award numbers , recipient duns numbers , and amounts of awards .

the manual review will identify outliers in certain data elements , such as whether the reported number of jobs created is reasonable .

according to education officials , upon their initial review of recipient reported data , the most common errors were relatively small — such as mistyped award numbers or incorrect award amounts — and were easily addressed and corrected during the agency review period .

department officials told us that they provided technical assistance to states and were able to have states correct the errors such that almost all of them were corrected before the october 30 deadline .

furthermore , state officials generally provided positive feedback to the department for these efforts , according to education officials .

education's office of inspector general ( oig ) examined education's process for reviewing the quality of recipient reported data and found that education's data review process was generally adequate .

the oig's review determined that education has established a process to perform limited data quality reviews intended to identify problems , such as questionable expenditure patterns or job estimates .

oig also acknowledged that education developed a process to correct any issues that education officials find by contacting the recipients who submitted the report .

in addition , oig noted that the department plans to review quarterly data at a state level to determine whether there are systemic problems with individual recipients and that education plans to use the reported information as a management tool .

state educational agencies ( sea ) also have taken action to collect and report jobs data and to ensure data quality .

state officials in arizona , massachusetts , new jersey , and new york and officials in the district of columbia told us that they adapted their existing data systems or created new ones to track and report jobs data .

for example , massachusetts department of education officials created an online quarterly reporting web site to collect jobs data from its leas and detailed information on personnel funded by recovery act grants .

in addition , many sea and lea officials we spoke with reported taking steps to ensure data quality , such as pre - populating data fields ( that is , inserting data , such as duns numbers , into the recipient reporting template for the leas ) , checking the reasonableness of data entered , and looking for missing data .

in addition to tracking and reporting jobs data and taking steps to ensure data quality , sea officials reported providing technical assistance , such as written guidance and web - based seminars , that explain how leas should report job estimates .

for example , california state officials had leas submit their data through a new web - based data reporting system and , prior to implementing the new system , provided written guidance and offered a web - based seminar to its leas .

despite efforts to ensure data quality , state and local officials we spoke with raised some concerns about the quality of jobs data reported in october 2009 .

for example , leas were generally required to calculate a baseline number of hours worked , which is a hypothetical number of hours that would have been worked in the absence of recovery act funds .

lea officials were to use this baseline number to determine the number of hours created or retained and to subsequently derive the number of ftes for job estimates .

each lea was responsible for deriving its own estimate .

new jersey state officials we interviewed told us that it was likely that leas used different methods to develop their baseline numbers , and as a result , leas in the same state may be calculating ftes differently .

 ( see appendix ii for a complete description of the calculations used to determine baseline number of hours worked , number of hours created or retained , and ftes for jobs created or retained ) .

according to illinois state officials , some of their leas had double - counted the number of positions , attributing the positions to both state fiscal year 2009 ( which ended on june 30 , 2009 ) and fiscal year 2010 ( beginning july 1 , 2009 ) , in part because the reporting period covered both of the state's fiscal years .

also , according to illinois officials , other school districts estimated that zero positions were attributable to the recovery act .

in those cases , lea officials received recovery act funds before finalizing staff lay - offs .

since they had not officially laid off any staff , illinois officials told us that lea officials were unsure as to whether those jobs would count as “jobs saved” and believed it best to report that no jobs had been saved because of recovery act funding .

illinois officials told us that education reviewed illinois' data , but did not ask them to make any corrections , but instead asked the state to disaggregate the job estimates by type of position , such as teachers and administrators .

also , one lea official from new york reported that he did not have enough time to conduct the necessary data quality checks he wanted to perform .

education officials acknowledged that many state and local officials reported various challenges in understanding the instructions and methodology that education suggested they use to calculate job estimates .

according to education officials , when states contacted the department to report these problems , education officials provided technical assistance to resolve the state's specific issues .

states faced challenges due to the timing of guidance or changes in guidance on how to estimate jobs attributable to the recovery act , according to education officials and several state officials we interviewed .

for example , colorado officials reported that , based on june 22 , 2009 guidance from omb , they believed that subrecipient vendors' jobs would be considered “indirect jobs” and therefore leas would not have to provide estimates of their vendors' jobs in their reports .

colorado officials told us they received guidance at education's august technical assistance conference indicating that subrecipients ( in this case , leas ) are supposed to include vendor job estimates based on those jobs directly funded by recovery act grants .

however , education's guidance did not clearly distinguish between direct and in - direct vendor jobs , according to state officials , making it difficult for leas to determine which vendor jobs to include in their section 1512 reports .

state officials also reported receiving further guidance on estimating jobs from education on september 15 and attending a related technical assistance conference on september 21 .

on september 16 , the colorado sea issued guidance stating that leas would be responsible for including vendor jobs in the job estimates they would be reporting .

 ( colorado's lea reports were due to the sea on september 25 , because the sea was required to submit its data to the state controller's office on september 29 for centralized reporting. ) .

also , officials in california — where leas had to report to the sea on september 23 — said they were not notified until education's september 21 conference that all leas that received recovery act funds had to register in the central contractor registration .

they told us that this contradicted previous guidance from education and would have required leas to register within 2 days to meet their state's september 23 deadline .

california officials advised federal officials that the state would implement this requirement for the second quarterly reporting period .

education officials and officials in two states mentioned actions that might improve the reporting and data quality processes before the next reports are due in january 2010 .

education officials suggested a number of possible changes in federalreporting.gov , such as allowing education to pre - populate some basic state data , such as grant award numbers and amounts , would decrease the workload for states and help avoid some technical errors .

also , in response to problems such as leas counting jobs in two fiscal years , education plans to provide more guidance in early december 2009 to states on calculating job estimates .

at the state level , officials in georgia reported plans to make changes to the state's processes , such as adding internal edit checks so that those who enter the data will have to make corrections as part of the data entry process .

also , illinois has created an office to work with state agencies to improve their data reporting processes , according to a state official .

the state also plans to build in more checks to its review of agency data , for example , a check that would compare jobs data against existing employment data to confirm that districts are not reporting more positions than exist in the district .

as recipient reporting moves forward , we will continue to review the processes that federal agencies and recipients have in place to ensure the completeness and accuracy of data , including reviewing a sample of recipient reports across various recovery act programs to assure the quality of the reported information .

as existing recipients become more familiar with the reporting system and requirements , these issues may become less significant ; however , communication and training efforts will need to be maintained and in some cases expanded as new recipients of recovery act funding enter the system .

in addition to our oversight responsibilities specified in the recovery act , we are also reviewing how several federal agencies collect information and provide it to the public for selected recovery act programs , including any issues with the information's usefulness .

our subsequent reports will also discuss actions taken on the recommendations in this report and will provide additional recommendations , as appropriate .

we are making two recommendations to the director of omb .

to improve the consistency of fte data collected and reported , omb should continue to work with federal agencies to increase recipient understanding of the reporting requirements and application of the guidance .

specifically , omb should clarify the definition and standardize the period of measurement for ftes and work with federal agencies to align this guidance with omb's guidance and across agencies , given its reporting approach consider being more explicit that “jobs created or retained” are to be reported as hours worked and paid for with recovery act funds , and continue working with federal agencies and encourage them to provide or improve program specific guidance to assist recipients , especially as it applies to the full - time equivalent calculation for individual programs .

omb should work with the recovery board and federal agencies to reexamine review and quality assurance processes , procedures , and requirements in light of experiences and identified issues with this round of recipient reporting and consider whether additional modifications need to be made and if additional guidance is warranted .

the jobs data reported by recipients of recovery act funds provide potentially useful information about a portion of the employment effect of the act .

at this point , due to issues in reporting and data quality including uncertainty created by varying interpretations of the guidance on ftes , we cannot draw a conclusion about the validity of the data reported as a measure of the direct employment effect of spending covered by the recipient reports .

even after data quality issues are addressed , these data will represent only a portion of the employment effect .

beyond the jobs that are reported , further rounds of indirect and induced employment gains result from government spending .

the recovery act also includes entitlement spending and tax benefits , which themselves create employment .

therefore , both the data reported by recipients and other macroeconomic data and methods are necessary to understand the overall employment effects of the stimulus .

economists will use statistical models to estimate a range of potential effects of the stimulus program on the economy .

in general , the estimates are based on assumptions about the behavior of consumers , business owners , workers , and state and local governments .

against the background of these assumptions , themselves based on prior research , the effects of different policies can be estimated .

any such estimate is implicitly a comparison between alternative policies .

the reliability of any alternative scenario that is constructed depends on its underlying assumptions and the adequacy of evidence in support of those assumptions , as well as on the accuracy of the data that form the basis for what is observed and on how well the model reflects actual behavior .

in the broadest terms , economic research using macroeconomic models suggests general rules of thumb for approximating the job impact and the gdp increase for a given amount of stimulus spending .

in constructing their estimates of the employment impacts of the act , cea observed that a one percent increase in gdp has in the past been associated with an increase in employment of approximately 1 million jobs , about three quarters of 1 percent of national employment .

similarly , cbo economists have assumed that a one percent increase in output generates somewhere between 600,000 and 1.5 million jobs .

as a result , projections of the employment impact of the recovery act can be generated from macroeconomic models that estimate output , providing the basis for estimates of changes in employment .

cea estimates of the employment effects of the recovery act have been based on statistical projections and allocations using historical relationships .

in january 2009 , the incoming administration projected the anticipated effects of fiscal stimulus on output and employment in the economy , specifying a prototypical spending package of tax cuts , payments to individuals , and direct spending by federal and state government .

the effects of such additional spending on output ( gdp ) were projected using multipliers , values based on historical experience that estimate the output change per unit of different types of changes in government spending .

these output increases were translated into employment effects using a rule of thumb , again based on history , that a 1 percent rise in gdp yields 1 million jobs .

the incoming administration's january 2009 analysis of a prototypical stimulus package found that it would be expected to increase gdp by 3.7 percent and increase jobs by 3,675,000 by the fourth quarter of 2010 .

the analysis compared the unemployment rate with and without the stimulus .

at that time , the unemployment rate for 2009 was projected to be 8 percent with a stimulus and closer to 9 percent without .

in may 2009 , cea reported on the anticipated employment effects of the actual recovery act as passed by congress and signed into law by the president .

that analysis was consistent with the january projections that the recovery act ( which was deemed to closely resemble the prototypical package earlier assumed ) would result in approximately 3.5 million jobs saved or created by the end of 2010 , compared to the situation expected to exist in the absence of the act .

later , when the actual unemployment rate rose beyond 9 percent , the administration acknowledged that its earlier projections of unemployment were too low but asserted that , without the recovery act , the rate would have been even higher than observed .

in september 2009 cea reported on the effects of recovery act spending through the end of august .

it noted that statistical analysis of actual economic performance compared to that which might have been expected in the absence of the recovery act suggested that the recovery act had added “roughly” 2.3 percentage points to gdp in the second quarter and was likely to add even more in the third .

translating that output gain into employment , cea surmised that employment in august was 1 million jobs higher than it would have been without the act .

the recipient reports are not estimates of the impact of the recovery act , although they do provide a real - time window on the results of recovery act spending .

recipients are expected to report accurately on their use of funds ; what they are less able to say is what they would have done without the benefit of the program .

for any disbursement of federal funds , recipients are asked to report on the use of funds to make purchases from business and to hire workers .

these firms and workers spend money to which they would not otherwise have had access .

recipients could not be expected to report on the expansionary effects of their use of funds , which could easily be felt beyond local , state , or even national boundaries .

neither the recipients nor analysts can identify with certainty the impact of the recovery act because of the inability to compare the observed outcome with the unobserved , counterfactual scenario ( in which the stimulus does not take place ) .

at the level of the national economy , models can be used to simulate the counterfactual , as cea and others have done .

at smaller scales , comparable models of economic behavior either do not exist or cover only a very small portion of all the activity in the macroeconomy .

the effect of stimulus on employment depends on the behavior of the recipient of aid .

for consumers , it depends on the extent to which their total spending increases .

for business firms , it depends on the increase , if any , in their purchases from other business firms or their payrolls .

for state and local governments , it is the increase in their purchases of goods and services and their own employment rolls .

within any given group of recipients , choices to spend or save will vary .

for example , a consumer with a large credit card balance may use a tax cut to pay down the balance or save more rather than increasing spending .

given that the personal savings rate fell to essentially zero before the recession , households may well choose to rebuild savings rather than spend .

a business firm might not see additional capital spending or hiring as advantageous .

a state government might decide to bolster its reserves where permitted under law rather than increase its outlays or cut its taxes .

in each case , the strength of the program as immediate stimulus is weakened to the extent that all funds are not spent .

the extent to which the initial spending reverberates throughout the economy is summarized by a multiplier , a measure of the cumulative impact on gdp over time of a particular type of spending or tax cut .

the resulting change in output translates into a change in employment .

in the context of the recovery act recipient reports , the output and employment effects will likely vary with the severity of the economic downturn in a recipient's location ( as reflected by distress in labor markets and the fiscal positions of governments ) , and the amount of funds received by the recipient .

the nature of the projects or activities to which the recipient applies its funds also matters , whether the projects use labor intensively and whether those who are hired will themselves spend or save their earnings .

economists use computer models of the u.s. economy with historical data on employment , gdp , public spending , taxes , and many other factors to study the effects of monetary ( eg , changes in interest rates ) and fiscal policies ( eg , changes in government taxing and spending ) designed to affect the trajectory of the economy .

in general , a fiscal stimulus program like the recovery act is aimed at raising aggregate demand – the spending of consumers , business firms , and governments .

this may be accomplished by means of tax cuts , grants - in - aid , or direct federal spending .

in response , the recipients may purchase more goods and services than they would have otherwise .

this could lead to governments and business firms refraining from planned dismissal of employees or to hiring additional workers .

the stimulus may lead to an overall , net increase in national employment and economic output .

models of the nation's economy can provide estimates of changes in gdp and employment that result from changes in monetary or fiscal policies .

in assessing the effects of fiscal policies such as additional government spending or tax cuts on gdp , macroeconomic models can be used to estimate “multipliers,” which represent the cumulative impact on gdp over time of a particular type of spending or tax cut .

multipliers translate the consequences of a change in one variable , such as in the demand for goods and services brought about by economic stimulus , on other variables , such as the supply of those goods and services and employment , taking into account “ripple effects” that occur throughout the economy .

the size of the multiplier depends on the extent to which changes in additional government spending or revenue translates into changes in spending by recipients and beneficiaries of the additional spending .

spending increases the multiplier , and saving reduces it .

the multiplier is also larger when there is slack in the economy ( unemployed persons and idle productive capacity ) .

also , the expansionary effects of government spending are greater when stimulus funds are borrowed rather than raised by taxation .

finally , the multiplier effect in the u.s. will be greater to the extent that new spending , whether by government or individuals , is devoted to domestically - produced goods and services .

in general , macroeconomic models and estimated multipliers can provide insights on the potential effect of different types of public spending .

because of the limited historical experience with fiscal stimulus of the magnitude of the recovery act , there is uncertainty about the extent to which the multipliers estimated using historical data about the effect of previous business cycles will accurately reflect the stimulus effect this time around .

economic research , however , has developed a basis for constructing reasonable ranges of values .

in projecting the anticipated effect of the recovery act on national output , the cbo grouped the act's provisions according to the size of the multiplier — that is , the magnitude of the effect of a particular provision's spending on gdp ( see table 7 ) .

drawing on analyses based on past experience with the results of government spending , cbo has identified a range of 1.0 to 2.5 for multipliers .

for example , a multiplier of 1.0 means a dollar of stimulus financed by borrowing results in an additional dollar of gdp .

cbo assumes larger multipliers for grants to state and local governments for infrastructure spending , and lower values — 0.7 to 1.9 — for transfers not related to infrastructure investment .

figure 5 shows the distribution of recovery act funds by multiplier .

the employment effects of recovery act funds will likely vary with the strength of the labor market in a recipient's location .

recipients located in areas where labor markets are weak , that is , where unemployment is high , may find it easier to hire people and may be able to do so at lower wages than those located in areas where the recession has had little effect on labor markets .

consequently , recipients located in areas with weak labor markets may be able to employ more people than those located in areas with strong labor markets , all else being equal .

the percentage of the nation's labor force that is unemployed has reached a level not seen in decades .

for example , the unemployment rate reached 10.2 percent in october 2009 , its highest rate since april 1983 .

the national unemployment rate was 4.9 percent in december 2007 , the month that marked the end of the last business cycle and the beginning of the current recession .

in general , the unemployment rate rises and falls over the course of the business cycle , generally increasing during a recession and decreasing during an expansion .

cyclical changes in the national unemployment rate reflect changes in state unemployment rates .

state unemployment rates vary over time in much the same way that the national unemployment rate varies — increasing during recessions , decreasing during expansions , but changing direction at different times .

estimates of current labor market strength , as measured by the unemployment rate , differ across states .

figure 6 ranks states according to the most recent available unemployment data — september 2009 .

while the national unemployment rate at the time was 9.8 percent , state unemployment rates ranged from a minimum of 4.2 percent in north dakota to a maximum of 15.3 percent in michigan .

twenty - seven states had unemployment rates in september 2009 that were less than the national unemployment rate by one percentage point or more , and nine states and the district of columbia had unemployment rates that exceeded the national unemployment rate by one percentage point or more , and 14 states had unemployment rates that were within one percentage point of the national unemployment rate .

labor markets in every state weakened over the course of the recession , but the degree to which this has occurred varies widely across states .

figure 7 shows the geographic distribution of the magnitude of the recession's impact on unemployment as measured by the percent change in unemployment between december 2007 and september 2009 .

alabama's unemployment rate has grown the most over this period , increasing by about 182 percent .

other states with relatively high unemployment rate growth over this period include florida , hawaii , wyoming , idaho , and nevada , all of which have seen their unemployment rates increase by more than 120 percent .

at the other end of the spectrum are states like minnesota , mississippi , arkansas , north dakota , and alaska .

unemployment rates in these states have grown by less than 60 percent between december 2007 and september 2009 .

alaska's unemployment rate growth during this period has been the slowest , measuring only about 33 percent .

while the recession has weakened labor markets in every state , those in some states may be showing signs of recovering .

table 8 lists the states for which unemployment rates in september 2009 are less than their peak unemployment rates .

the unemployment rate peaked in some states as early as may 2009 .

in several additional states , the unemployment rate was higher in june or july than it was in september .

although unemployment rates in these states may start to increase again in the future , for the moment it seems that labor markets in these states are getting stronger .

table 9 shows the change in employment between december 2007 and september 2009 .

employment in arizona , florida , georgia , michigan , nevada , and oregon in september 2009 was over 7 percent lower than it was in december 2007 in each state .

on the other hand , employment in louisiana and south dakota fell by less than two percent over the same period , and employment in alaska , north dakota , and the district of columbia has increased during that time .

employment has declined since december 2007 , when the current recession began .

however , some signs have appeared that the losses in employment are slowing .

job losses in october 2009 numbered 190,000 .

this number is about equal to average job losses of about 188,000 per month in august , september , and october 2009 .

the rate at which employment has declined over the past three months is thus lower than the rate at which it declined in may , june , and july 2009 , when job losses averaged about 357,000 per month .

the rate at which employment has declined over the past three months is thus also lower than the rate at which it declined between november 2008 and april 2009 , when job losses averaged about 645,000 per month .

the current employment contraction has been more pronounced in the goods - producing sector , in which employment fell by about 17 percent between december 2007 and october 2009 , than the service - providing sector , in which employment fell by about three percent over the same period .

the goods - producing sector includes the construction and manufacturing industries , in which employment has fallen by about 21 percent and 15 percent , respectively , between december 2007 and october 2009 .

the goods - producing sector also includes the mining and logging industry , which lost about 6 percent of its jobs during the same time .

service - providing industries include financial activities , information , professional and business services , and trade , transportation , and utilities , all of which had employment declines of more than six percent between december 2007 and october 2009 .

employment declines in the leisure and hospitality industry were about three percent , and employment in education and health services increased by about 4 percent at the same time .

the employment effects of recovery act funds allocated to state and local governments will also likely vary with their degree of fiscal stress , as well as with the factors mentioned above .

because recessions manifest in the form of lower output , employment , and income , among other things , reductions in output , employment , and income lead state and local governments to collect less tax revenue and at the same time cause households' demand for publicly provided goods and services to increase .

state governments often operate under various constraints , such as balanced budget requirements , so they generally must react to lower tax revenues by raising tax rates , cutting publicly provided programs and services , or drawing down reserve funds , all but the last of which amplify recessionary pressure on households and businesses .

local governments must do the same unless they can borrow to make up for lost tax revenue .

by providing funds to state and local governments , the recovery act intends to forestall , or at least moderate , their program and service cuts , reserves liquidation , and tax increases .

in addition to the type of spending undertaken , the size of the multiplier and resultant employment effects will depend on the extent to which aid is not diverted to reserves .

generally speaking , states with weaker economies and finances will be more likely to spend recovery act dollars .

states that may suffer little or no harm from a national downturn are less motivated to make full use of any federal assistance .

rather than increase spending , they may choose to cut taxes or , where permitted by law , add to their reserves .

tax cuts would have some simulative effect , bu additions to reserves would reduce any multiplier effect .

the increase fmap available under the recovery act is for state expenditures for d medicaid services .

however , the receipt of this increased fmap may reduce the state share for the medicaid programs .

states are prohibited from using any funds directly or indirectly attributable to the increased but states have reported using funds fmap for state rainy day funds , made available a result of the increased fmap for a variety of purposes including offset of general fund deficits and tax revenue shortfall .

the availability of reserves and the possibility of borrowing points out the difficulties of gauging the impact of federal policy by the observed timing of aid flows .

the expectation of aid could encourage governments to draw more out of reserves or to borrow more than they would otherwise .

the rationale is that the expected aid would replace the reserves or liquidate the new debt .

in this way , the timing of aid could postdate the impact .

research on individual consumption has long wrestled with the problem of how expectations influence household decisions .

state and local governments must also look forward in making fiscal decisions .

the recession has substantially affected the states' fiscal conditions .

in recessions , state and local governments are motivated to enact “pro - cyclical” measures that aggravate the downturn .

balanced budget requirements and other constraints cause them to reduce spending and raise taxes , generating what is called “fiscal drag.” federal assistance can reduce the need for such measures .

in this way , the negative employment effects of fiscal drag can be precluded and existing jobs can be saved .

with sufficient aid , it is possible for state and local governments to go beyond saving existing jobs to creating additional ones .

however , there are likely to be limits to the abilities of governments to spend aid quickly enough to affect employment the recession has substantially reduced states' and local governments' combined tax revenues .

figure 8 indicates that tax revenue collected in the second quarter of 2009 fell from the peak in the second quarter of 2008 by more than $130 billion .

state and local revenues are not likely to return to their previous levels until well after the recession has ended .

after the 2001 recession , tax receipts did not begin to recover until after second quarter of 2003 , well after the ‘official' end of the recession in fourth quarter of 2001 .

however , the fall in receipts after the second quarter of 2008 is dramatic .

in a survey of the nation's state governments , the national governors association reported that outlays for current services provided through states' general funds decreased by 2.2 percent in fiscal year 2009 , which ended in june 2009 for most states .

spending for fiscal year 2010 is projected to fall by 2.5 percent .

in light of average annual increases of five percent for total state and local government outlays , any decrease is a significant adjustment .

most states have some sort of requirement to balance operating budgets .

however , most state governments are able to establish reserve funds .

maintenance of a baseline of five percent of annual outlays for a state's fund is regarded by state budget officers as prudent .

a lower level could increase a state's borrowing costs .

since 2006 these funds have decreased .

in the wake of the 2001 recession , according to an analyst at the rockefeller institute of government , state governments in fiscal year 2002 drew as much as 4.8 percent of their revenues from fund balances .

the national governors association reports that fund balances peaked in 2006 at $69 billion , at 11 percent of general fund expenditures .

the funds declined to 9.1 percent by 2008 and were estimated at 5.5 percent — $36.7 billion — in june 2009 .

however , by fiscal year 2010 , these funds are projected to fall to 5.3 percent of outlays .

in addition , for 2009 there is variation in state government reserves .

for example , 11 states had total reserves in excess of 10 percent of outlays , while others , such as california , had total reserves less than 1 percent of outlays .

this may be seen in figure 9 .

diversity in the economic and fiscal conditions of the states and differences in the size and composition of recovery act funds they receive suggest that the potential for employment gains varies across states .

we will continue work in this area , along with our other work on federal - state fiscal interactions .

in commenting on a draft of our report , omb staff told us that omb generally accepts the report's recommendations .

it has undertaken a lessons learned process for the first round of recipient reporting and will generally address the report's recommendations through that process .

we are sending copies of this report to the office of management and budget and to the departments of education , housing and urban development , and transportation .

the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staffs have any questions about this report , please contact j. christopher mihm or susan offutt at ( 202 ) 512-5500 .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

the office of management and budget ( omb ) and federal agencies have provided wide - ranging guidance to states on how to report full - time equivalent ( fte ) data — that is , jobs created or retained .

omb staff reported that questions on ftes dominated the inquiries they fielded during the first round of recipient reporting , and recipients had various understandings of how to report an fte .

following are selected examples of the challenges of reporting and calculating ftes , as seen through public housing agencies and four states — california , florida , georgia , and massachusetts .

as we reported in september 2009 , the department of housing and urban development ( hud ) is using two methods to satisfy reporting requirements for public housing agencies under the recovery act .

first , omb and the recovery act board have created and manage www.federalreporting.gov ( federalreporting.gov ) , a web site where all recovery act recipients can report on the nature of projects undertaken with recovery act funds and on jobs created or retained .

second , hud developed the recovery act management and performance system ( ramps ) in response to reporting requirements outlined in section 1609 of the recovery act .

hud officials said approximately 96 percent of housing agencies had successfully reported into federalreporting.gov .

initial reports suggested a lower reporting rate , but this was due to a substantial number of housing agencies incorrectly entering values into certain identification fields , such as the award id number , the awarding agency , or the type of funding received .

hud officials said that the system did not have validation measures in place to ensure the correct award id numbers were entered .

in addition , housing agencies could not edit the award id number without submitting a new report .

according to a hud official , omb initially classified reports that could not be matched with a federal agency as “orphaned.” the hud official told us hud program and recovery team staff reviewed reports submitted with nonmatching award id numbers and omb's list of reports that could not be matched to determine if they matched hud awards .

according to hud officials , public housing agencies encountered challenges related to registration and system accessibility .

for example , a hud official said the registration process for federalreporting.gov requires several steps such as obtaining a duns number , registering with the central contractor registration ( ccr ) and obtaining a federal reporting personal identification number ( frpin ) .

the hud official told us these steps are necessary for validating the recipient reports because they ensure the appropriate points of contact at the appropriate organizations — in this case , public housing agencies — are reporting for each program .

the federalreporting.gov web site states that each recipient's point of contact information is taken directly from the ccr and if an organization changes its point of contact information it will take 48 hours for federalreporting.gov to receive the change and e - mail the frpin and temporary password to the new point of contact .

according to the hud official , a housing agency's contact information in ccr is sometimes outdated and the systems are often not updated in time for access to be correctly transferred .

additionally , one housing agency official reported he saved his data entry as a draft before being timed out of the system , but was unable to retrieve the data when he reentered the reporting web site .

a hud official said in the future , hud and omb will need to improve the function of the system and the official said that they are working to ensure all housing agencies have access to the reporting systems .

according to a hud official , there was widespread misunderstanding by public housing agencies about omb's methodology for calculating the number of jobs created or retained by the recovery act , in part because housing agencies are not familiar with reporting jobs information .

in a few cases , we found that public housing agencies had reported the number of jobs created or retained into federalreporting.gov without converting the number into full - time equivalents .

for example , officials from one housing agency reported the number of people , by trade , who worked on recovery act related projects , but did not apply the full - time equivalent calculation outlined by omb in the june 22 reporting guidance .

additionally , officials from another public housing agency told us that they based the number of jobs they reported into federalreporting.gov on letters from their contractors detailing the number of positions rather than full - time equivalents created as a result of their recovery act - funded projects .

in another case , a housing agency official reported having difficulty locating guidance on calculating job creation .

as a result , the housing agency may have underreported jobs data from an architectural firm providing design services for a recovery act window replacement project at a public housing complex .

hud officials cited the fact that omb and hud provided additional clarification and guidance close to the deadline for recipient reporting as a factor in housing agencies' confusion about the methodology for counting jobs .

according to a hud official , hud was in discussions with omb about finalizing and clarifying portions of the june 22 , 2009 , job guidance right up to the end of september .

in early september , hud posted the omb guidance to its web site and provided information by e - mail to housing agencies on registration for federalreporting.gov , as well as links to web seminars and training provided by omb .

hud issued further guidance to public housing agencies by e - mail on september 25 , 2009 , approximately 2 weeks before the october 10 , 2009 , deadline for recipient reporting , providing templates and data dictionaries tailored to the public housing capital fund .

the guidance also reiterated the recipient reporting responsibilities for public housing agencies .

hud officials told us they did not have enough time to translate some of the terminology into concrete terms that would be clearer to housing agency officials .

for example , hud posted a jobs calculator spreadsheet to its web site , and hud field staff would direct housing agencies to this guidance when they asked specific questions about how to calculate jobs .

nonetheless , greater instruction may be needed beyond what was provided to housing agencies on the job calculator's instructions page .

a hud official said it seemed like some housing agencies may have pulled information for the recipient reports from the wrong fields in the job calculator , which produced errors .

a hud official stated they will work with omb to improve housing agencies' understanding of the methodology for reporting in full - time equivalents prior to the next round of recipient reporting in january 2010 .

state officials from the california recovery task force and the california office of the state chief information officer ( cio ) explained that while the centralized reporting structure had several benefits , challenges with changing reporting requirements from federal agencies and technological glitches still occurred .

as a centralized reporting state , each state agency reported directly to the cio through the california arra accountability tool .

the task force is responsible for uploading the data to federalreporting.gov .

however , according to state officials , local government agencies that received direct recovery act dollars from the federal government are not under the task force's purview of the state officials and report to federalreporting.gov on their own .

state officials stated that a centralized reporting structure allows the cio to act as a liaison between omb and the state for faster reconciliation of issues .

the cio , on behalf of the task force , was responsible for collecting , validating , and uploading data from state agencies to federalreporting.gov .

the state officials believed the process went well overall and commended their state team for successfully reporting into federalreporting.gov .

the task force officials believed the reporting process could be improved if omb provided a comprehensive list of awards to better crosscheck reporting .

california officials stated that many of the challenges in reporting did not come from the additional information requested during october 11 to 20 , but from changes immediately prior to the september 30 cut - off date .

these changes included issues such as the department of education's request to include central contract registration numbers on september 21 , and fhwa's changes to four of the data elements , including the award amounts .

california officials have a greater appreciation of what to expect during the reporting process .

they believe that the continuous communication with the state agencies , including weekly data group meetings at which as many as 60 people attended , contributed to the overall success of the reporting process .

they also have been developing their own internal logic checks to assist with data validation .

california officials continue to be concerned that problems at federalreporting.gov and changing agency requirements will cause subrecipient data , initially correctly collected in accordance with federal guidance , to be rejected , which will result in penalties for late submissions .

the florida department of transportation ( fdot ) has reporting requirements under both sections 1512 and 1201 of the recovery act .

although the state had an existing system in place that could be used for section 1201 reporting , officials decided to develop two additional systems for 1512 reporting .

one system was created to assist fdot in reporting information to the state recovery czar and a second system for employment reporting was created to allow subrecipients to enter total number of employees , payroll , and employee hours for recovery act - funded highway projects .

according to state officials , the system was launched on may 29 , 2009 , and is currently in use .

fdot officials experienced no significant reporting problems while submitting more than 400 reports .

florida began preparing for reporting early and conducted extensive training to assist contractors , consultants , and local agencies in the collection of employment data required by the recovery act .

for example , fdot's office of inspector general ( oig ) developed five computer - based training modules to assist department staff and external partners in the use of the electronic reporting system .

fdot also partnered with its oig and the florida division of the federal highway administration ( fhwa ) to conduct town hall presentations for its seven district offices and florida's turnpike enterprise .

the presentations were designed to ensure consistent use of the electronic employment data application .

in september , oig followed up with a survey to local agencies to determine the levels of proficiency for using the department's electronic employment reporting system and to solicit feedback .

fdot's electronic employment data reporting system provides for several levels of data review and approval .

for example , once the subrecipient enters their monthly employment data into the electronic system , the data is available for review and subsequent approval by the local agency project manager .

once approved , the data is available for review and approval by the department's district office project manager .

the district office project manager performs a reasonableness check of the submitted data prior to electronically approving the same .

the electronic employment data is then available for review by oig where two types of analyses are performed .

first , oig identifies whether the subrecipient should be reporting job data by comparing submitted data ( and subrecipient identifiers ) against the master list of awarded recovery act transportation projects .

second , oig compares previously submitted subrecipient information against information contained in its current submission to determine any data anomalies or variances .

should any significant data anomalies or variances occur , oig will contact the appropriate district and local agency .

fdot did not require subrecipients to submit verification of their job data but subrecipients were advised by fdot to maintain documentation for review .

for two subrecipients we visited , we found the extent to which documentation was being maintained varied .

for example , one subrecipient kept time - sheets for all employees associated with recovery act projects , while another had documentation for its hourly employees but not its management employees .

reporting process: in georgia , one of the highway contractors we visited noted that it was responsible for reporting on about 30 recovery act - funded projects with approximately 10 subrecipients for each project .

the contractor stated that they are required to fill out a monthly report ( fhwa form 1589 ) indicating the number of employees , the hours worked , and the dollars charged to the job through a direct portal created by georgia department of transportation ( gdot ) .

according to the contractor , this reporting requirement is in the contract , and gdot will withhold payment if this report is not completed .

as the general contractor , the firm is also responsible for collecting the 1589 information from its subcontractors on each job .

officials with the firm stated that they would withhold payment from the subcontractors if they fail to provide the information .

we examined these contracts and confirmed these requirements .

in addition to the 1589 report , the contractor also submits certified payroll to gdot on a monthly basis .

guidance and challenges: in terms of guidance , the contractor noted that there was not a lot of training provided but that they did not necessarily need much training .

the main challenges raised were issues with making changes within the gdot system and the duns number field .

for example , officials explained once a report was submitted into gdot's system , it could not be edited , which made errors in entry or reporting difficult to correct .

the contractor has discussed this issue with gdot and hopes a solution will be reached for the next reporting cycle .

the duns number requirement was an issue for several subrecipients since they did not have a number and they were under the impression that a cost was involved in obtaining a number , which there was not .

after discussions with gdot , it was determined that subrecipients did not need a duns number , but the field could not be blank .

therefore , gdot advised the contractor to have its subrecipients complete the file by entering “not applicable.” the contractor suggested that improvements in reporting could be achieved by delaying the reporting date to gdot to allow more time to handle delays in payroll and obtaining supporting information .

overall , the contractor felt that the september report was the most accurate month reported to date and believed greater accuracy will be achieved over time .

data quality: officials of the highway contractor told us they think they have a handle on the process and were confident in the data submitted .

in their words , “if it's inaccurate , we paid somebody wrong” since the report comes out of their payroll system .

in terms of data from subcontractors , the officials noted that their confidence varied somewhat across subcontractors .

officials explained that information varied , based on the capacity and expertise of the subcontractor ( that is , experience in reporting and if a certified payroll is in place ) .

officials explained they had greater confidence in subcontractors that had certified payroll .

they provided several examples of subrecipients who were truckers or haulers who are not familiar with reporting and often are a small operation of one employee .

officials noted that the number of truckers or haulers on a project is often large in order to meet disadvantaged business requirements .

officials questioned if truckers and haulers should be part of the job creation or retained count since similar positions may not be counted for subcontractors that provide materials such as pipe .

officials believed over time , subcontractors would become more comfortable and familiar with the process .

reporting process: an official at a major highway contractor we interviewed in massachusetts explained that one of his primary responsibilities as the construction cost accountant is to certify payroll records and ensure compliance with federal labor standards .

this company is the general contractor ( or prime contractor ) on six recovery act highway construction projects .

a company official stated that that there was no additional burden associated with filing the quarterly recipient reports because they routinely report employment data to the massachusetts department of transportation ( massdot ) , highway division for federal - highway funded projects through the massdot highway division's equitable business opportunities ( ebo ) system .

although there were additional data elements required for recovery act projects , the company official noted that fhwa form 1589 specifies these additional reporting elements , and they have been added to the ebo system to make it easier for contractors and subcontractors to report on a monthly basis .

according to the company official , the process was very straightforward .

contractors and subcontractors log into the ebo system and can see detailed information on all the projects they are working on for the massdot highway division .

typically , by the 15th day of each month , contractors and subcontractors upload their certified payroll files into the ebo system .

however , for the september submission , massdot's highway division required contractors to submit their employment reports early by october 9 , so that they could meet the state's october 10 deadline of submitting the quarterly recovery act report .

guidance and challenges: the official noted that the only guidance he received came from the massdot highway division in the form of training on the ebo system , which he said helped contractors and subcontractors transition from the old employment reporting system to the ebo system .

he noted that for contractors that were used to working with complex accounting systems , this training was adequate , but for smaller contractors with little computer experience , the training could have been better .

in general , the official observed that most contractors and subcontractors are very pleased with the new system because it interfaces so well with their existing accounting and certified payroll databases and because the cost is low .

data quality: there are several steps for ensuring data quality .

first , a company official explained that most large contractors and many subcontractors have accounting and payroll data systems that interface with the ebo database well , so they are able to upload data from these systems directly into the ebo system , eliminating the need to reenter employment data .

however , some smaller contractors don't have these systems and thus must enter the data by hand each month .

the company official stated that he is not concerned with the quality of data because it is verified both internally and by the massdot highway division .

the official explained that the massdot highway division puts the responsibility for ensuring that subcontractors file monthly reports with the general contractor , and his company ensures subcontractor compliance by withholding their reimbursements .

although it is rarely needed , the official noted that withholding payments to subcontractors is a very effective tool for getting subcontractors to submit their monthly reports .

furthermore , all subcontractor employment reports are verified against the daily duty log that is kept by the project supervisor , who is an employee of the company .

the massdot highway division also posts resident engineers at each job site on a daily basis , and they keep a daily diary of employment and work status that is used to verify the data submitted by general contractors in the massdot highway division project management system .

this is the same system that is used to generate contractor invoices for reimbursement .

according to education's clarifying guidance on jobs estimation , local educational agencies ( lea ) are generally required to calculate a baseline number of hours worked , consisting of a hypothetical number of hours that would have been worked in the absence of recovery act funds .

once lea officials derive this number , they then deduct the number from actual hours worked by individuals whose employment is attributable to recovery act funding to determine the number of hours created or retained .

they then derive the number of full - time equivalents ( fte ) for jobs created or retained , as shown in table 10 .

then , they divide the resulting number of hours created or retained by the number of fte hours in the quarter or reporting period to determine the number of ftes to report .

for example , in the table above , employees 3 and 6 went from being unemployed ( 0 hours of employment ) in the hypothetical situation where no recovery act funds are available to full - time ( 520 hours ) and part - time ( 300 hours ) employment , respectively .

employee 2 went from part - time ( 300 hours ) to full - time ( 520 hours ) .

employee 5 remained a part - time employee , but works an additional 100 hours in the reporting quarter .

taking the sum of actual hours worked in the reporting quarter ( 2460 ) and subtracting the hours worked in the hypothetical baseline quarter ( 1320 ) , we are left with 1140 created or retained hours .

for the first reporting quarter , lea officials divided the result by the number of fte hours in that quarter ( 520 ) .

the total ftes created or retained in quarter 1 is 2.19 .

results should be reported cumulatively , so in the second reporting quarter ( q2 ) , the total hours worked in q2 will be added to the hours worked in q1 and divided by the hours in a full - time schedule for two quarters ( 1040 hours ) .

for example , if in quarter 2 , all employees reported in quarter 1 are retained and the baseline remains unchanged , we would again have 1140 hours created or retained .

to get the final cumulative fte created or retained , officials would sum 1140 for quarter 1 with 1140 for quarter 2 to get 2280 total hours created or retained .

recipients should divide this by the sum of the hours in a full - time schedule for those two quarters ( 1040 ) .

the result is again 2.19 fte created or retained in quarter 2 .

j. christopher mihm or susan offutt at ( 202 ) 512-5500 .

the following staff contributed to this report: j. christopher mihm , nancy kingsbury , and katherine siggerud ( managing directors ) ; susan offutt ( chief economist ) ; susan irving , yvonne jones , thomas mccool , and mathew scire ( directors ) ; angela clowers ( acting director ) ; robert j. cramer ( associate general counsel ) ; thomas beall , james mctigue , max sawicky ( assistant directors ) ; judith c. kordahl ( analyst - in - charge ) ; and jaime allentuck , darreisha bates , don brown , stephen brown , tina cheng , andrew ching , steven cohen , michael derr , robert dinkelmeyer , shannon finnegan , timothy guinane , philip heleringer , don kiggins , courtney lafountain , john mcgrail , donna miller , elizabeth morrison , jason palmer , beverly ross , tim schindler , paul schmidt , jennifer schwartz , jonathan stehle , andrew j. stephens , james sweetman , and william trancucci .

the state teams for the bimonthly recovery act letter reports also contributed to this report .

