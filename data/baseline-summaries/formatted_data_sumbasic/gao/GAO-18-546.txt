the digital accountability and transparency act of 2014 ( data act ) was enacted , in part , to increase accountability and transparency of federal spending , which totaled almost $4 trillion for fiscal year 2017 .

among other things , the data act includes provisions requiring a series of office of inspector general ( oig ) and gao oversight reports evaluating the completeness , timeliness , quality , and accuracy of federal agencies' spending data and the implementation and use of data standards .

the act also requires the office of management and budget ( omb ) and department of the treasury ( treasury ) to establish data standards to generate uniform agency data that are consistent and comparable .

in accordance with the data act and omb and treasury guidance , federal agencies submitted their first round of spending data in may 2017 for the second quarter of fiscal year 2017 , and the oigs issued their first mandated data quality oversight reports beginning in october 2017 .

this report is part of our ongoing monitoring of data act implementation in response to provisions in the data act that call for us to review oig reports and issue reports assessing and comparing the quality of agency data submitted under the act and agencies' implementation and use of data standards .

the objectives of this report are to describe ( 1 ) the reported scope of work covered and type of audit standards oigs used in their reviews of agencies' data act spending data ; ( 2 ) any variations in the reported implementation and use of data standards and quality of agencies' data , and any common issues and recommendations reported by the oigs ; and ( 3 ) the actions , if any , that omb and treasury have reported taking or planning to take to use the results of oig data act reviews to help monitor agencies' implementation of the act .

to address these objectives , we obtained and reviewed 53 oig data act reports that were issued on or before january 31 , 2018 , from 24 chief financial officers act of 1990 ( cfo act ) agency oigs and 29 non - cfo act agency oigs .

we identified the oigs' reported ( 1 ) scope of work and type of audit standards oigs used in their reviews , ( 2 ) conclusions about agencies' implementation and use of the data standards and the quality of the agencies' data , ( 3 ) government - wide and agency - specific issues identified , and ( 4 ) recommendations to address identified deficiencies .

we also surveyed oigs to obtain additional information regarding error rates , sample sizes , control deficiencies identified , and other items associated with the reviews they conducted .

we received and reviewed responses from the 53 oigs that we obtained reports from and followed up with oigs for clarification and corroboration , as necessary .

finally , we interviewed omb staff and treasury officials to determine how they used or plan to use the results of the oig reviews in monitoring agencies' implementation of the data act .

appendix i provides additional details on our scope and methodology .

we conducted this performance audit from september 2017 to july 2018 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the data act was enacted may 9 , 2014 , for purposes that include expanding on previous federal transparency legislation by requiring the disclosure of federal agency expenditures and linking agency spending information to federal program activities , so that both policymakers and the public can more effectively track federal spending .

the act also calls for improving the quality of data submitted to usaspending.gov by holding federal agencies accountable for the completeness and accuracy of the data submitted .

the federal funding accountability and transparency act of 2006 ( ffata ) , as amended by the data act , identifies omb and treasury as the two agencies responsible for leading government - wide implementation .

for example , the data act requires omb and treasury to establish government - wide financial data standards that shall , to the extent reasonable and practicable , provide consistent , reliable , and searchable spending data for any federal funds made available to or expended by federal agencies .

these standards specify the data elements to be reported under the data act and define and describe what is to be included in each data element , with the aim of ensuring that information will be consistent and comparable .

the data act also requires omb and treasury to ensure that the standards are applied to the data made available on usaspending.gov .

usaspending.gov has many sources of data .

for example , agencies submit data from their financial management systems , and other data are extracted from government - wide federal financial award reporting systems populated by federal agencies and external award recipients .

a key component of the reporting framework is treasury's data act broker ( broker ) — a system that collects and validates agency - submitted data to create linkages between the financial and award data prior to their publication on the usaspending.gov website .

according to treasury guidance documents , agencies are expected to submit three data files with specific details and data elements to the broker from their financial management systems .

file a: appropriations account .

this includes summary information such as the fiscal year cumulative federal appropriations account balances and includes data elements such as the agency identifier , main account code , budget authority appropriated amount , gross outlay amount , and unobligated balance .

file b: object class and program activity .

this includes summary data such as the names of specific activities or projects as listed in the program and financing schedules of the annual budget of the u.s. government .

file c: award financial .

this includes award transaction data such as the obligation amounts for each federal financial award made or modified during the reporting quarter ( eg , january 1 , 2017 , through march 31 , 2017 ) .

the broker also extracts spending information from government - wide award reporting systems that supply award data ( eg , federal grants , loans , and contracts ) to usaspending.gov .

these systems — including the federal procurement data system - next generation ( fpds - ng ) , system for award management ( sam ) , financial assistance broker submission ( fabs ) , and the ffata subaward reporting system ( fsrs ) — compile information that agencies and external federal award recipients submit to report , among other things , procurement and financial assistance award information required under ffata .

the four files produced with information extracted by the broker from the four systems are as follows: file d1: procurement .

this includes award and awardee attribute information ( extracted from fpds - ng ) on procurement ( contract ) awards and contains elements such as the total dollars obligated , current total value of award , potential total value of award , period of performance start date , and other information to identify the procurement award .

file d2: financial assistance .

this includes award and awardee attribute information ( extracted from fabs ) on financial assistance awards and contains elements such as the federal award identification number , the total funding amount , the amount of principal to be repaid for the direct loan or loan guarantee , the funding agency name , and other information to identify the financial assistance award .

file e: additional awardee attributes .

this includes additional information ( extracted from sam ) on the award recipients and contains elements such as the awardee or recipient unique identifier ; the awardee or recipient legal entity name ; and information on the award recipient's five most highly compensated officers , managing partners , or other employees in management positions .

file f: subaward attributes .

this includes information ( extracted from fsrs ) on awards made to subrecipients under a prime award and contains elements such as the subaward number , the subcontract award amount , total funding amount , the award description , and other information to facilitate the tracking of subawards .

the key components of the broker and how the broker operated when the agencies submitted their data for the second quarter fiscal year 2017 are shown in figure 1 .

after agencies submit the three files to the data act broker , it runs a series of validations and produces warnings and error reports for agencies to review .

after passing validations for these three files , the agencies are to generate files d1 and d2 , the files containing details on procurement and assistance awards .

before the data are displayed on usaspending.gov , agency senior accountable officials are required to certify the data submissions in accordance with omb guidance .

certification is intended to assure alignment among files a , b , c , d1 , d2 , e , and f , and to provide assurance that the data are valid and reliable .

according to treasury officials , once the certification is submitted a sequence of computer program instructions or scripts are issued to transfer and map the data from broker data tables to tables set up in a database used as a source for the information on the website .

certified data are then displayed on usaspending.gov along with certain historical information from other sources , including monthly treasury statements .

the data act requires each oig to issue three reports on its assessment of the quality of the agency's data submission and compliance with the data act .

the first report was due november 8 , 2016 ; however , agencies were not required to submit spending data in compliance with the data act until may 2017 .

therefore , the council of the inspectors general on integrity and efficiency ( cigie ) developed an approach to address what it described as a reporting date anomaly ; encouraged interim oig readiness reviews and related reports on agencies' implementation efforts ; and delayed issuance of the mandated reports to november 2017 , with subsequent reports following a 2-year cycle and due november 2019 and 2021 .

cigie established the federal audit executive council ( faec ) to discuss and coordinate issues affecting the federal audit community , with special emphasis on audit policy and operations of common interest to faec members .

faec formed the faec data act working group to assist the oig community in understanding and meeting its data act oversight requirements by ( 1 ) serving as a working - level liaison with treasury , ( 2 ) consulting with gao , ( 3 ) developing a common approach and methodology for conducting the readiness reviews and mandated reviews , and ( 4 ) coordinating key communications with other stakeholders .

to assist the oig community , the faec data act working group developed a common methodology and published the inspectors general guide to compliance under the data act ( ig guide ) for use in conducting mandated reviews .

the ig guide includes procedures to test data in agencies' files a and b by reconciling these data to the information that agencies report in their quarterly sf 133 , report on budget execution and budgetary resources .

the ig guide also instructs oigs to select a statistically valid sample of spending data from the agencies' available award - level transactions in file c , and among other procedures , to confirm whether these data are also included in the agencies' files d1 and d2 .

the oigs are also to confirm whether the transactions in the sample were linked to the award and awardee attributes in files e and f. the data in files e and f are reported by award recipients in two external government - wide systems , and are outside the direct control of the federal agencies , except for the general services administration , which manages these external systems .

based on additional guidance from the faec data act working group , oigs are not required to assess the quality of the award recipient - entered data that the broker extracted from the two external government - wide systems used to create files e and f. according to the ig guide , the sampled spending data and testing results are to be evaluated using the following definitions for the requirements being assessed: completeness is measured in two ways: ( 1 ) all transactions that should have been recorded are recorded in the proper reporting period , and ( 2 ) as the percentage of transactions containing all applicable data elements required by the data act .

timeliness is measured as the percentage of transactions reported within 30 days of the end of the quarter .

accuracy is measured as the percentage of transactions that are complete and agree with the systems of record or other authoritative sources .

quality is defined in omb guidance as a combination of utility , objectivity , and integrity .

utility refers to the usefulness of the information to the intended users .

objectivity refers to whether the disseminated information is being presented in an accurate , clear , complete , and unbiased manner .

integrity refers to the protection of information from unauthorized access or revision .

the ig guide also states that oigs should assess agencies' implementation and use of the data standards , including evaluating each agency's process for reviewing the 57 required data elements and associated definitions that omb and treasury established and documenting any variances .

in november 2017 , we issued our first report on data quality as required by the data act , which identified issues with the completeness and accuracy of the data that agencies submitted for the second quarter of fiscal year 2017 , use of data elements , and presentation of the data on beta.usaspending.gov .

among other things , we recommended that treasury disclose known data quality issues and limitations on the new usaspending.gov website .

treasury agreed with that recommendation and stated that it would develop a plan to better disclose known data quality issues .

since the data act's enactment in 2014 , we have issued a series of interim reports on our ongoing monitoring of the implementation of the data act and made recommendations intended to help ensure effective government - wide implementation .

however , many of those recommendations still remain open .

these reports identified a number of challenges related to omb's and treasury's efforts to facilitate agency reporting of federal spending , as well as internal control weaknesses and challenges related to agency financial management systems that we and agency auditors reported that present risks to agencies' ability to submit quality data as required under the act .

for example , our prior work has identified issues with agency source systems that could affect the quality of spending data made available to the public .

in april 2017 , we reported a number of weaknesses and issues previously identified by agencies' auditors and oigs that affect agencies' financial reporting and may affect the quality of the information reported under the data act .

we also reported on findings and recommendations from prior reports with issues on the four key award systems — fpds - ng , sam , the award submission portal ( asp ) , and fsrs — which increase the risk that the data submitted to usaspending.gov may not be complete , accurate , and timely .

based on our review of the 53 oig reports , the scope of all of the oig reviews covered their agencies' submission of spending data for the second quarter of fiscal year 2017 ( i.e. , january through march 2017 ) .

however , the files that the oigs included in their scope to select and review sample transactions and the type of audit standards used — such as attestation examination engagement or performance audit — varied among the oigs .

according to the ig guide , the oigs were to select and review a statistically valid sample of transactions , preferably from the agencies' file c certified data submissions ; if file c was unavailable or did not contain data , they were to select their sample test items from files d1 and d2 .

based on their survey responses , we found that most oigs tested data from file c , file d1 , file d2 , or some combination of these agency file submissions .

we also found that some oigs tested a statistical sample of transactions in these files , while others tested all the transactions in the files because of the small population size .

further , we found that some oigs used different files when testing for completeness , timeliness , or accuracy .

for example , one oig used file c when testing for completeness , file d1 when testing for timeliness , and file d2 when testing for accuracy .

overall , as shown in figure 2 , the source files that 47 of the 53 oigs used for testing accuracy were as follows .

twenty - eight oigs selected items for testing accuracy from file c. twelve oigs selected items for testing accuracy from files d1 , d2 , or both .

seven oigs selected items for testing accuracy from a combination of files c , d1 , and d2 .

the ig guide also states that oigs should conduct either attestation examination engagements or performance audits in accordance with generally accepted government auditing standards ( gagas ) .

performance audits are audits that provide findings or conclusions based on an evaluation of sufficient , appropriate evidence against criteria .

attestation examination engagements involve obtaining sufficient , appropriate evidence with which to express an opinion stating whether the subject matter is in conformity with the identified criteria .

in contrast to these two types of engagements that provide conclusions or opinions , agreed - upon procedures attestation engagements do not result in opinions or conclusions , but instead involve auditors performing specific procedures on the subject matter and issuing a report of findings .

all 53 oigs reported that they performed their engagements in accordance with gagas ; 47 oigs reported that they conducted a performance audit , 5 reported that they performed an attestation examination engagement , and 1 reported that it performed an agreed - upon procedures attestation engagement .

twenty - one cfo act agency oigs and 26 non - cfo act agency oigs conducted performance audits , 3 cfo act agency oigs and 2 non - cfo act agency oigs conducted attestation examination engagements , and 1 non - cfo act agency oig conducted an agreed - upon procedures attestation engagement .

according to the oig reports , about half of the agencies met the omb and treasury requirements for implementation and use of data standards .

however , almost three - fourths of oigs determined that their respective agencies' submissions were not complete , timely , accurate , or of quality .

based on their reports and survey responses , certain oigs also found data errors related to problems with how treasury's data act broker extracted information from external award reporting systems .

the faec data act working group considered these data errors to be a government - wide issue .

other errors that the oigs identified may have been caused by agency - specific internal control deficiencies .

most of the oigs made recommendations to agencies to help address the concerns they identified in their reports .

based on our review of the 53 oig reports , we found that 27 oigs determined that their agencies met omb and treasury requirements for implementation and use of the data standards , whereas 23 oigs determined that their agencies did not meet these requirements .

in addition , 3 cfo act agency oigs did not include an assessment of their agencies' implementation and use of the data standards in their reports .

the oig reports described reasons why the 23 agencies did not meet the implementation and use of data standards requirements , including data submissions that did not include required data elements or included data elements that did not conform with the established data standards .

for example , one oig reported that 74 percent of transactions it tested did not contain program activity names or codes aligned with the president's budget , and as a result , 39 percent of total obligations and 57 percent of total expenditures from that agency's data submission could not be aligned with established programs .

another oig reported that because of inconsistent application of data standards and definitions across award systems , the agency's spending data were not complete , timely , or accurate .

in their survey responses , certain oigs identified additional concerns about their agencies' implementation and use of data standards and related data elements .

specifically , six oigs identified differences between their agencies' definitions of the data standards and omb guidance .

for example , two oigs noted differences between definitions in omb guidance and their agencies' definitions of “primary place of performance address.” one of these oigs noted that its agency submitted the wrong data , providing the address of the legal entity receiving the award instead of the address of the primary place where performance of the award will be accomplished or take place .

in our november 2017 report , we also noted that omb guidance for this data element was unclear and recommended that omb clarify and align existing guidance regarding the appropriate definitions agencies should use to collect and report on primary place of performance and establish monitoring mechanisms to foster consistent application and compliance .

in addition , based on their survey responses , 21 oigs reported error rates over 50 percent for 25 data elements .

this includes 10 data elements that were reported by multiple oigs and 15 data elements only reported by one oig , as shown in table 1 .

there were five other data elements with error rates over 50 percent that the faec data act working group determined to be government - wide broker - related data reporting issues , as discussed later in this report .

the oigs' survey responses did not indicate whether the data elements with errors were the result of issues related to the agencies' implementation or use of required data standards .

based on the oig reports , we found that 15 of the 53 oigs determined that their agencies' data were generally complete , timely , accurate , or of quality , comprising 6 cfo act agency oigs and 9 non - cfo act agency oigs ( see fig .

3 ) .

conversely , 38 of 53 oigs determined that their agencies' data were not complete , timely , accurate , or of quality , comprising 18 cfo act agency oigs and 20 non - cfo act agency oigs .

oig reports did not always include separate assessments for completeness , timeliness , and accuracy , but gave an overall assessment of the quality of the data .

as part of our oig survey , we requested the overall error rates , agency - specific error rates , and broker error rates for each requirement — completeness , timeliness , and accuracy — used to evaluate the quality of data tested to help provide more insights on the nature and extent of errors that the oigs identified .

for the purposes of our survey , based on guidance from the faec data act working group and in the ig guide , these error rates were defined as follows: overall error rate is the percentage of transactions tested that were not in accordance with policy , and includes errors due to the agency , broker , and external award reporting systems .

agency error rate is the percentage of transactions tested that were not in accordance with policy , and includes only errors that were within the agency's control .

broker error rate is the percentage of transactions tested that were not in accordance with policy , and includes only errors due to the broker and external award reporting systems .

with regard to overall error rates and the tests conducted , 40 oigs reported that they tested a statistical sample of transactions , 9 oigs reported that they tested all transactions in the populations of data , and 4 oigs reported that they did not test any transactions or were unable to complete their testing .

as shown in figure 4 , our survey results show that the 40 oigs that tested a statistical sample of transactions generally reported higher ( projected ) overall error rates for the accuracy and completeness of data than for the timeliness of data .

we found similar results based on our tests to assess the completeness , timeliness , and accuracy of government - wide spending data that we tested for the same time period , as described in our november 2017 report .

more than half of the 40 oigs reported projected overall error rates of 25 percent or greater for accuracy , including 8 oigs reporting projected accuracy error rates of over 75 percent .

in contrast , more than three - fourths of the oigs projected overall error rates of less than 25 percent for completeness and timeliness of their agencies' data .

see appendix ii for more details on the 53 oigs' individual agency testing results , including the actual overall error rates for those oigs that tested the full population of transactions included in their agencies' data submissions and the estimated range of projected overall error rates for oigs that conducted a statistical sample .

the oig survey responses that included agency - specific error rates showed that the agency - specific error rates were similar to the overall error rates , with accuracy of data having higher error rates than those for completeness and timeliness .

fourteen oigs provided agency - specific error rates for accuracy , 13 oigs provided agency - specific error rates for completeness , and 12 oigs provided agency - specific error rates for timeliness of the data sampled .

in addition , nine oigs reported error rates for broker - related errors that , similar to the overall and agency - specific error rates , had higher error rates for accuracy of data than for completeness and timeliness .

the faec data act working group determined that the broker - related errors had a government - wide impact , as discussed further below .

in october 2017 — 1 month before the mandated reports were to be issued — the working group provided guidance to the oigs suggesting that they determine and report these additional broker error rates separately because they were not within the agencies' control .

some oigs may not have reported separate agency - specific and broker error rates as their work was already substantially completed .

of the nine oigs that reported they tested all transactions in the populations of their agencies' data , five oigs reported actual overall error rates and found that overall error rates for accuracy were higher than the error rates for completeness or timeliness .

of the four oigs that reported agency - specific error rates , only one oig reported an error rate for accuracy , and it was greater than 75 percent .

one oig reported a broker error rate , and it was higher for accuracy than for completeness or timeliness .

in addition to using different testing methodologies ( eg , statistical sampling or testing the full population of transactions ) and source files , as previously discussed , the oigs also used different assumptions and sampling criteria to design and select sample items for testing .

as a result , the overall error rates are not comparable and a government - wide error rate cannot be projected .

based on discussions with oigs , the faec data act working group identified certain data errors caused by broker - related issues that it determined to be government - wide data reporting issues .

also , because the broker is maintained by treasury , these issues were beyond the control of the affected agencies .

according to the working group , these issues involve inconsistencies in data the broker extracted from government - wide federal financial award reporting systems , as described in table 2 .

to help provide consistency in reporting these issues , the working group developed standard report language used by oigs in their reports to describe the errors caused by the broker .

the standard reporting language stated that because agencies do not have responsibility for how the broker extracts data , the working group did not expect agency oigs to evaluate the reasonableness of treasury's planned corrective actions .

in april 2018 , a treasury official told us that the issues causing these problems have been resolved .

to address these issues , the treasury official stated that , among other things , treasury implemented the data act information model schema version 1.1 , loaded previously missing historical procurement data to usaspending.gov , updated how information from fpds - ng is mapped to file d1 , and replaced asp with fabs .

however , we plan to follow up on these efforts as a part of our ongoing monitoring efforts .

in their survey responses and oig reports , 43 oigs reported agency - specific control deficiencies that may have contributed to or increased the risk of data errors .

of these 43 oigs , 37 oigs identified deficiencies affecting accuracy , 32 oigs identified deficiencies affecting completeness , and 14 oigs identified deficiencies affecting timeliness .

a few oigs reported that they leveraged their financial statement audit results , which found deficiencies in certain financial reporting controls , in conducting their data act reviews .

we categorized the oigs' reported control deficiencies and found that the categories with the most frequently reported deficiencies related to their agencies' lack of effective procedures or controls , such as conducting reviews and reconciliations of data submissions to source systems , and information technology system deficiencies , as shown in figure 5 .

in their survey responses , oigs provided additional information about whether their agencies' controls over agency source systems and controls over the data act submission processes were properly designed , implemented , and operating effectively to achieve their objectives .

for both cfo act and non - cfo act agencies , oigs generally reported that agencies' internal controls over source systems and the data act submission process were designed effectively but were not implemented or operating effectively as designed .

some examples of agency - specific control deficiencies reported by the oigs are as follows .

lack of effective procedures or controls .

deficiencies where agency procedures for reviewing and reconciling data and files to different sources were not performed , or were performed ineffectively , or standard operating procedures for data submissions had not been designed and implemented .

for example , some of these deficiencies related to agencies' lack of review or reconciliation of data in files a and b to data in files d1 and d2 .

further , two oigs found that their agencies did not perform any sort of quality review of their data until after they were submitted to the broker .

another oig found that its agency did not ensure that its components developed objectives for accomplishing its data submissions , assessed the risks to achieving those objectives , or established corresponding controls to address them .

as a result , the agency's data act submissions included errors .

information technology system deficiencies .

deficiencies related to the lack of effective automated systems controls necessary to ensure proper system user access or automated quality control procedures and the accuracy and completeness of data , as well as systems that are not compliant with federal financial management system requirements .

for example , one oig noted that its agency experienced issues related to segregation of duties and access controls that affected the agency's ability to ensure completeness and accuracy of data in its financial , procurement , and grant processing systems .

another oig found that its agency did not complete necessary system updates to ensure that all data were certified prior to submission .

further , an oig reported that its agency's information system was unable to combine transactions with the same unique identifiers , resulting in over 12,000 transactions being removed because of broker warnings .

insufficient documentation .

deficiencies related to agencies' production and retention of documentary evidence supporting their data act submissions .

for example , three oigs found that their agencies were unable to provide supporting documentation for various portions of their data act submissions .

another oig reported that one of its agency's components did not take effective steps to ensure that procurement and grant personnel understood the specific documentation that should be maintained to support data entered in grant and contract files .

further , another oig found that its agency did not document the process for compiling the agency's data act submission files .

inappropriate application of data standards and data elements .

deficiencies related to the inappropriate use of data definition standards or the misapplication of data elements .

for example , one oig found that its agency did not identify the prior year funding activity names or codes for all transactions included in its spending data submission .

another oig found that its agency did not consistently apply standardized object class codes in compliance with omb guidance , as well as standardized u.s. standard general ledger account codes as outlined in treasury guidance .

similarly , an oig reported instances where agency users of certain award systems were not knowledgeable about how required data act elements were reported in their procurement system .

data entry errors or incomplete data .

deficiencies related to controls over data entry and errors or incomplete data in agency or government - wide external systems .

for example , an oig found that its agency did not include purchase card transactions greater than $3,500 , which represented about 1 percent of the agency's data submission .

another oig reported that its agency's service provider did not enter miscellaneous obligations in the data submission file because it expected the agency to enter such transactions in the federal procurement data system .

timing errors .

deficiencies related to delays in reporting information to external government - wide systems that result in errors in the data submitted .

for example , one oig reported that its agency did not take effective steps to ensure that contracting officers timely report required data act award attribute information in fpds - ng .

another oig reported that a bureau in its agency consistently submitted certain payment files 2 months late , resulting in incomplete files c and d2 in the agency's data submission .

inaccurate broker uploads .

deficiencies related to agencies uploading data to the broker .

for example , one oig found a lack of effective internal controls over data reporting from its agency's source systems to the data act broker for ensuring that the data reported are complete , timely , accurate , and of quality .

specifically , certain components were not able to consolidate data from multiple source systems and upload accurate data to the broker for file c. another oig reported that the broker could not identify and separate an individual component's award data from agency - wide award data .

specifically , the broker recognized only agency - wide award data and did not include award data from its agency's individual components .

as a result , the oig reported that the component did not comply with the data act requirements because its submission did not include all of the agency's required award data .

reliance on manual processes .

deficiencies that cause agencies to rely on manual processes and work - arounds .

for example , one oig found that in the absence of system patches to map data elements directly from feeder award systems to financial systems , its agency developed an interim solution that relied heavily on manual processes to collect data from multiple owners and systems and increased the risk for data quality to be compromised .

another oig reported that its agency's financial management systems are outdated and unable to meet data act requirements without extensive manual efforts , resulting in inefficiencies in preparing data submissions .

other .

other deficiencies including , among other things , instances where an agency's senior accountable official did not submit a statement of assurance certifying the reliability and validity of the agency account - level and award - level data submitted to the data act broker , an agency did not provide adequate training and cross - training of personnel on the various data act roles , and certain components of one agency were not included in the agency's data act executive governance structure .

to help address control deficiencies and other issues that resulted in data errors , 48 of the 53 oigs ( 23 cfo act agency oigs and 25 non - cfo act agency oigs ) included recommendations in their reports .

as shown in figure 6 , the most common recommendations oigs made to their agencies related to the need for agencies to develop controls over their data submissions , develop procedures to address errors , and finalize or implement procedures or guidance .

some examples of oig recommendations made to agencies to improve data quality and controls are as follows .

develop controls over submission process .

recommendations related to controls or processes to resolve issues in submitting agency financial system data to the broker .

for example , one oig recommended that its agency develop and implement a formal process to appropriately address significant items on broker warning reports , which could indicate systemic issues .

develop procedures to address errors .

recommendations related to procedures to address data errors in the agency's internal systems .

for example , one oig recommended that its agency correct queries to extract the correct information and ensure that all reportable procurements are included in its data act submissions .

finalize or implement procedures or guidance .

recommendations related to establishing and documenting an agency's data act - related standard operating procedures or agency guidance , including the roles and responsibilities of agency stakeholders .

for example , one oig recommended that its agency update its guidance on what address to use for primary place of performance to be consistent with omb and treasury guidance .

maintain documentation .

recommendations related to establishing or maintaining documentation of the agency's procedures , controls , and related roles and responsibilities for performing them .

for example , one oig recommended that its agency develop a central repository for grant award documentation and maintain documentation to support its data act submissions .

provide training .

recommendations related to developing , implementing , and documenting training for an agency's data act stakeholders .

for example , one oig recommended that its agency provide mandatory training to all contracting officers and grant program staff to ensure their understanding of data act requirements .

work with treasury , omb , and other external stakeholders .

recommendations for the agency to work with treasury , omb , or other stakeholders external to the agency to resolve government - wide issues .

for example , one oig recommended that its agency work closely with its federal shared service provider to address timing and coding errors that the service provider caused for future data act submissions .

implement systems controls or modify systems .

recommendations related to developing and implementing automated systems and controls .

for example , one oig recommended that its agency complete the implementation of system interfaces and new procedures that are designed to improve collection of certain data that were not reported timely to fpds - ng and improve linkages of certain financial transactions and procurement awards using a unique procurement instrument identifier .

increase resources .

recommendations related to increasing the staff , resources , or both necessary to fully implement data act requirements .

for example , one oig recommended that its agency allocate the resources to ensure that reconciliations are performed when consolidating source system data to the data act submission files .

management for 36 agencies stated that they concurred or generally concurred with the recommendations of their oigs ( see fig .

7 ) .

management at many of these agencies stated that they continued to improve their processes and controls for subsequent data submissions .

in addition , management for seven agencies stated that they partially concurred with the recommendations that their oigs made .

management for two agencies did not concur with their oigs' recommendations .

management for one agency that did not concur with the recommendations stated that they should not be held responsible for data discrepancies that other agencies caused , and management for the other agency stated that they followed authoritative guidance that omb and treasury issued related to warnings and error messages .

omb staff told us that they reviewed the oig reports — focusing on the 24 cfo act agencies — to better understand issues that the oigs identified and to determine whether additional guidance is needed to help agencies improve the completeness , timeliness , accuracy , and quality of their data act submissions .

omb staff explained to us how they have or are planning to address oig - identified issues .

omb staff told us that in april 2017 the cfo council's data act audit collaboration working group was formed , which includes officials from omb , treasury , and the chief financial officers ( cfo ) council to foster collaboration and understanding of the risks that were being identified as agencies prepared and submitted their data .

the working group also consults with cigie , which is not a member of the working group , but its representatives attend meetings to help the group members better understand issues involving the oig reviews and the ig guide .

according to omb staff , the working group is the focal point to identify government - wide issues and identify guidance that can be clarified .

they also told us that omb continues to meet with this working group to determine what new guidance is needed to meet the data act requirement to ensure that the standards are applied to the data available on the website .

in june 2018 , omb issued new guidance requiring agencies to develop data quality plans intended to achieve the objectives of the data act .

according to omb staff , omb is committed to ensuring integrity and providing technical assistance to ensure data quality .

treasury officials told us that they reviewed oig reports that were publicly available on oversight.gov and are collaborating with omb and the cfo council to identify and resolve government - wide issues , including issues related to the broker , so that agencies can focus on resolving their agency - specific issues .

in february 2018 , the working group documented certain topics identified for improving data quality and value .

omb staff and treasury officials also told us that omb and treasury have taken steps to address issues we previously reported related to their oversight of agencies' implementation of the data act .

for example , we recommended in april 2017 that omb and treasury take appropriate actions to establish mechanisms to assess the results of independent audits and reviews of agencies' compliance with the data act requirements .

the data act audit collaboration working group is one of the mechanisms omb and treasury use to assess and discuss the results of independent audits and to address identified issues .

in november 2017 , we also recommended , among other things , that treasury ( 1 ) reasonably assure that ongoing monitoring controls to help ensure the completeness and accuracy of agency submissions are designed , implemented , and operating as designed , and ( 2 ) disclose known data quality issues and limitations on the new usaspending.gov .

treasury has taken some steps and is continuing to take steps to address these recommendations .

for example , under the data quality section of the about page on usaspending.gov , treasury disclosed the requirement for each agency oig to report on its agency's compliance with the data act and noted the availability of the reports at oversight.gov .

we provided a draft of this report to omb , treasury , and cigie for comment .

we received written comments from cigie that are reproduced in appendix iii and summarized below .

in addition , omb , treasury , and cigie provided technical comments , which we incorporated as appropriate .

in its written comments , cigie noted that the report provides useful information on oig efforts to meet oversight and reporting responsibilities under the data act .

cigie further stated that it believes that the report will contribute to a greater understanding of the oversight work that the oig community performs and of agency efforts to report and track government - wide spending more effectively .

we are sending copies of this report to the director of the office of management and budget , the secretary of the treasury , the chairperson and vice chairperson of the council of the inspectors general on integrity and efficiency , as well as interested congressional committees and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staffs have any questions about this report , please contact me at ( 202 ) 512-9816 or rasconap@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iv .

the digital accountability and transparency act of 2014 ( data act ) includes provisions requiring us to review the offices of inspector generals' ( oig ) mandated reports and issue our own reports assessing and comparing the completeness , timeliness , accuracy , and quality of the data that federal agencies submit under the act and the federal agencies' implementation and use of data standards .

we issued our first report on data quality in november 2017 , as required .

this report includes our review of the oigs' mandated reports , which were also issued primarily in november 2017 .

our reporting objectives were to describe 1. the reported scope of work covered and type of audit standards oigs used in their reviews of agencies' data act spending data ; 2. any variations in the reported implementation and use of data standards and quality of agencies' data , and any common issues and recommendations reported by the oigs ; and 3. the actions , if any , that the office of management and budget ( omb ) and the department of the treasury ( treasury ) have reported taking or planning to take to use the results of oig reviews to help monitor agencies' implementation of the act .

to address our first and second objectives , we obtained and reviewed 53 oig reports that were issued on or before january 31 , 2018 , including reports related to 24 chief financial officers act of 1990 ( cfo act ) agencies and 29 non - cfo act agencies .

of 91 entities for which second quarter fiscal year 2017 spending data were submitted , we did not obtain and review oig data act reports for 38 entities with obligations totaling at least $1.2 billion ( as displayed on usaspending.gov on may 23 , 2018 ) because no reports for those entities were publicly available by our january 31 , 2018 , cutoff date .

table 3 lists the 53 agencies for which we obtained and reviewed the oig reports on the quality of data that agencies submitted in accordance with data act requirements .

we also developed and conducted a survey of oigs to provide further details on the design and results of their efforts to conduct statistical samples to select and test agencies' data submissions and reviews of internal controls .

in november 2017 , we sent the survey to those oigs whose agencies originally submitted data act data to treasury's data act broker .

we received and reviewed responses from the 53 oigs that we obtained reports from , with 9 oigs including the completed surveys in their published reports and the others providing us their completed survey responses separately .

we analyzed 53 oig reports and survey responses , following up with oigs for clarification when necessary .

we reviewed each of the 53 oig reports we obtained and identified the reported scope of work covered ( eg , the quarter of data reviewed ) and the type of audit standards oigs used to conduct their reviews ( eg , performance audit or attestation examination engagement ) .

we also developed and used a data collection instrument to compile and summarize the conclusions and opinions included in the oig reports on the completeness , timeliness , accuracy , and quality of agencies' data submissions and their implementation and use of data standards .

during this process , gao analysts worked in teams of three to reach a consensus on how these oig conclusions and opinions were categorized .

for oig reports that did not specifically state whether the agencies met the data act requirements , we considered the reported results in conjunction with the more detailed information provided in the oig responses to our survey and made conclusions about the oigs' assessments based on our professional judgment .

we also reviewed the oig reports and survey responses and used two data collection instruments to compile , analyze , and categorize common issues or agency - specific control deficiencies the oigs identified in their reviews and recommendations they made to address them .

during this process , gao analysts worked in teams of three to obtain a consensus in how these issues and deficiencies were categorized .

to address our third objective , we interviewed omb staff and treasury officials about how they used or planned to use the results of the oig data act reviews to assist them in their monitoring of agencies' implementation of the act .

we conducted this performance audit from september 2017 to july 2018 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in their survey responses , offices of inspector general ( oig ) for 45 agencies reported actual overall error rates or estimated error rates and estimated ranges of errors associated with the spending data transactions they tested for accuracy , completeness , or timeliness ( see table 4 ) .

these results include oigs that tested a statistical sample of transactions , tested the full population , and conducted an assessment of internal controls without additional substantive testing .

oigs that tested a sample responded that they used different sampling criteria , and the sources of files they used to select their statistical samples varied based on the files that were available .

regardless of whether the oig tested a sample or the full population , some of the oigs selected items for testing from file c , file d1 , file d2 , or some combination thereof .

as a result , the overall error rates the oigs reported are not from the same data submission files and are not fully comparable , but are intended to provide additional information on the individual results of the completeness , timeliness , and accuracy of the data each agency oig tested .

in addition to the contact named above , michael laforge ( assistant director ) , diane morris ( auditor in charge ) , umesh basnet , thomas hackney , and laura pacheco made major contributions to this report .

other key contributors include dave ballard , carl barden , maria belaval , jenny chanley , patrick frey , ricky harrison , jason kelly , jason kirwan , quang nguyen , samuel portnow , carl ramirez , anne rhodes - kline , and dacia stewart .

data act: omb , treasury , and agencies need to improve completeness and accuracy of spending data and disclose limitations .

gao - 18-138 .

washington , d.c.: november 8 , 2017 .

data act: as reporting deadline nears , challenges remain that will affect data quality .

gao - 17-496 .

washington , d.c.: april 28 , 2017 .

data act: office of inspector general reports help identify agencies' implementation challenges .

gao - 17-460 .

washington , d.c.: april 26 , 2017 .

data act: implementation progresses but challenges remain .

gao - 17- 282t .

washington , d.c.: december 8 , 2016 .

data act: omb and treasury have issued additional guidance and have improved pilot design but implementation challenges remain .

gao - 17-156 .

washington , d.c.: december 8 , 2016 .

data act: initial observations on technical implementation .

gao - 16- 824r .

washington , d.c.: august 3 , 2016 .

data act: improvements needed in reviewing agency implementation plans and monitoring progress .

gao - 16-698 .

washington , d.c.: july 29 , 2016 .

data act: progress made but significant challenges must be addressed to ensure full and effective implementation .

gao - 16-556t .

washington , d.c.: april 19 , 2016 .

data act: data standards established , but more complete and timely guidance is needed to ensure effective implementation .

gao - 16-261 .

washington , d.c.: january 29 , 2016 .

data act: progress made in initial implementation but challenges must be addressed as efforts proceed .

gao - 15-752t .

washington , d.c.: july 29 , 2015 .

