amid efforts to improve federal government performance and constrain federal spending , the obama administration has emphasized expanding the availability and use of rigorous program evaluation .

the aim is to establish an evaluation culture in which agencies regularly use evidence to inform program design and investment decisions , complementing existing efforts to strengthen performance measurement and management .

today , most federal agencies use performance measures to track progress toward goals .

few appear to conduct in - depth program evaluations regularly to assess their programs' impact or inform policymakers on how to improve results .

increasing demands for performance information prompted your expressed interest in how agencies , given their limited evaluation resources , can provide the information that is necessary for effective management and legislative oversight .

to help agencies plan their evaluation activities strategically , you asked us to study federal agencies that have mature program evaluation capacity , to learn 1 .

what criteria , policies , and procedures do they use to determine which programs to review ? .

2 .

what conditions influence an agency's choices ? .

to find answers that would apply broadly to other federal agencies about how to prioritize program evaluations , we selected four agencies in three departments with known evaluation capacity — the ability to systematically collect , analyze , and use data on program results — in a variety of substantive areas .

the four agencies were the department of education and the department of housing and urban development ( hud ) and two agencies in the department of health and human services ( hhs ) — the administration for children and families ( acf ) and the centers for disease control and prevention ( cdc ) .

to identify agencies fulfilling our criteria , we reviewed previous gao reports and agency documents for evidence of emphasis on conducting evaluations .

for example , we searched for examples of agencies' incorporating the results of program evaluations in their annual performance reports .

to obtain a diverse set of cases , we selected agencies that addressed a variety of content areas and program types — direct services or grants , regulation , and research — and that varied in the conditions likely to affect their decision process , such as whether they had a central office responsible for evaluation .

to identify evaluation planning procedures and criteria , we reviewed agency materials and interviewed agency evaluation officials about their evaluation planning process .

we identified conditions influencing their choices in interviews with agency officials and in comparisons of what we found across the agencies .

we conducted this performance audit from may 2010 through december 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

we requested comments on a draft of this report from the secretaries of education , health and human services , and housing and urban development .

hhs and hud provided comments that are reprinted in appendixes i and ii and described at the end of this letter .

education provided technical comments .

under the government performance and results act of 1993 ( gpra ) federal agencies are expected to focus on achieving results and to demonstrate , in annual performance reports and budget requests , how their activities help achieve agency or governmentwide goals .

in 2002 , to encourage greater use of program performance information in decision making , the office of management and budget ( omb ) created the program assessment rating tool ( part ) .

part was intended to provide a consistent approach for evaluating federal programs within the executive budget formulation process .

however , because part conclusions rely on available program performance and evaluation information , many of the initial recommendations focused on improving outcome and efficiency measures .

although gpra and part helped improve the availability of better performance measures , we and omb have noted that this did not result in their greater use by the congress or agencies .

in october 2009 , omb announced a plan to strengthen federal program evaluation , noting that rigorous independent program evaluations can help determine whether government programs are achieving their intended outcomes as well as possible and at the lowest possible cost .

program evaluations are systematic studies that assess how well a program is working , and they are individually tailored to address the client's research question .

process ( or implementation ) evaluations assess the extent to which a program is operating as intended .

outcome evaluations assess the extent to which a program is achieving its outcome - oriented objectives but may also examine program processes to understand how outcomes are produced .

when external factors such as economic or environmental conditions are known to influence a program's outcomes , impact evaluations may be used to measure a program's net effect by comparing outcomes with an estimate of what would have occurred had there been no program intervention .

thus , program evaluation can provide an important complement to agency performance data that simply track progress toward goals .

in announcing the evaluation initiative , the omb director expressed concern that many important programs had never been evaluated , evaluations had not sufficiently shaped budget priorities or management practices , and many agencies lack an evaluation office capable of supporting an ambitious strategic research agenda .

the initiative consisted of three efforts: posting information online on all agencies' planned and ongoing impact evaluations , establishing an interagency group to promote the sharing of evaluation expertise , and funding some new agency rigorous impact evaluations and capacity strengthening efforts .

as part of the fiscal year 2011 budget process , omb allocated approximately $100 million for the evaluation initiative to support 35 rigorous program evaluations and evaluation capacity - building proposals .

omb made a similar evaluation solicitation for the fiscal year 2012 budget in which nonsecurity agencies were asked to reduce their discretionary budgets by 5 percent .

the budget process evaluation initiative is focused on impact evaluations and is not intended to cover the full range of an agency's evaluation activities .

however , to be considered for additional evaluation funding , agencies must demonstrate that they are both using existing evaluation resources effectively and beginning to integrate evaluation into program planning and implementation .

with significant efforts under way to increase agencies' evaluation resources , it is especially timely now to learn how agencies with more evaluation experience prioritize their resources .

a recent gao review identified three elements that leading national research organizations consider essential to a sound federal research and evaluation program: research independence , transparency and accountability , and policy relevance .

these elements align well with omb's new evaluation initiative and expectations for a better integration of evaluation into program design and management .

in this report , we do not assess the quality of the agencies' research agendas or their achievement of these objectives .

however , we do describe practices these agencies took that were designed to achieve those elements .

the department of education establishes policy for , administers , and coordinates most federal assistance to elementary , secondary , and postsecondary education .

the department has supported educational research , evaluation , and dissemination not only since the congress created it in 1979 but also earlier , when it was the office of education .

for several years , two central offices in the department of education have been responsible for program and policy evaluation .

the policy and program studies service ( ppss ) , in the office of planning , evaluation , and policy development ( opepd ) , advises the secretary on policy development and review , strategic planning , performance measurement , and evaluation .

the institute of education sciences ( ies ) , established in 2002 ( replacing the office of educational research and improvement ) , is the research arm of the department .

ies is charged with producing rigorous evidence on which to ground education practice and policy , with program evaluation housed primarily in the national center for education evaluation and regional assistance ( ncee ) .

in 2009 , the department of education launched a review of its evaluation activities , comparing them to those of other government agencies , seeking to build analytic capacity , and intending to use available knowledge and evidence more effectively .

this review resulted in a comprehensive , departmentwide evaluation planning process and clarified the distinct evaluation responsibilities of the two offices .

starting in 2010 , opepd was to lead the planning process , in partnership with ies , to identify the department's key priorities for evaluation and related knowledge - building activities .

starting in fiscal year 2011 , ncee in ies will be responsible for longer - term ( 18 months or longer ) program implementation and impact studies , while ppss in opepd will focus on shorter - term evaluation activities ( fewer than 18 months ) , policy analysis , performance measurement , and knowledge management activities .

some program offices also conduct evaluation activities separate from studies conducted by either of the central offices , such as supporting grantee evaluations or analyzing grantee performance data for smaller programs where larger - scale evaluations are not practical .

the department of housing and urban development is the principal federal agency responsible for programs on housing needs , fair housing opportunities , and community improvement and development .

it insures home mortgages , subsidizes housing for low - and moderate - income families , promotes and enforces fair housing and equal opportunity housing , and provides grants to states and communities to aid community development .

at hud , program evaluation is primarily centralized in one office — the office of policy development and research ( pd&r ) — created in 1973 .

it conducts a mix of surveys , independent research , demonstrations , policy analyses , and short - and long - term evaluations that inform hud's decisions on policies , programs , and budget and legislative proposals .

pd&r provides hud's program offices with technical support , data , and materials relevant to their programs .

although the primary responsibility for evaluating programs falls to pd&r , some evaluation is found in program offices , such as the office of housing , which routinely conducts analyses to update its loan performance models for assessing credit risk and the value of its loan portfolio .

in 2006 , the congress , concerned about the quality of hud research , commissioned the national research council ( nrc ) to evaluate the pd&r office and provide recommendations regarding the course of future hud research .

a 2008 nrc report noted declining resources for data collection and research and insufficient external input to its research agenda .

on the heels of the report , the scope of the current economic and housing crisis led the incoming administration to acknowledge a need both to reform and transform hud and to sustain a commitment of flexible budget resources for these efforts .

in 2009 , hud proposed a departmentwide transformation initiative of organizational and program management improvements to position hud as a high - performing organization .

in fiscal year 2010 , much of pd&r's research and evaluation activities are funded through a set - aside created for the initiative , which also supports program measures , demonstrations , technical assistance , and information technology projects .

evaluation planning is decentralized at the department of health and human services .

we reviewed acf and cdc because they have significant evaluation experience .

hhs's centrally located office of the assistant secretary for planning and evaluation ( aspe ) coordinates agency evaluation activities , reports to the congress on the department's evaluations , and conducts studies on broad , cross - cutting issues while relying on agencies to evaluate their own programs .

in some cases , aspe conducts independent evaluations of programs housed within other hhs operating and staff divisions ( for example , acf and cdc ) .

acf oversees and helps finance programs to improve the economic and social well - being of families , individuals , and communities — the head start program is an example .

it also assists state programs for child support enforcement as well as temporary assistance to needy families ( tanf ) .

the office of planning , research , and evaluation ( opre ) is the principal office for managing evaluation at acf .

it also provides guidance , analysis , technical assistance , and oversight related to strategic planning , performance measurement , research , and evaluation methods .

it conducts statistical , policy , and program analyses and synthesizes and disseminates research and demonstration findings .

opre consults with outside groups on ideas that feed into program and evaluation planning .

in each policy area with substantial evaluation resources , opre consults with a group of researchers , program partners , and other content area experts who share their knowledge and ideas for research and evaluation .

cdc , as part of the public health service , is charged with protecting the public health by developing and providing to persons and communities information and tools for preventing and controlling disease , promoting health , and preparing for new health threats .

it supports some evaluation activities through the public health service ( phs ) evaluation set - aside ; in 2010 the secretary was authorized to use up to 2.5 percent of appropriations for evaluating programs funded under the phs act .

the set - aside is also used to fund databases of the national center for health statistics and programs that cut across cdc's divisions .

presently , the divisions within cdc control most evaluation funding focused on their respective programs , but evaluation planning across cdc is currently under review .

cdc recently created an office of the associate director for program which will have responsibility for supporting performance measurement and evaluation across cdc , among other duties .

we interviewed staff from evaluation offices in three cdc divisions: nutrition , physical activity , and obesity ( dnpao ) ; hiv / aids prevention ( dhap ) ; and adolescent and school health ( dash ) .

these three divisions oversee cooperative agreements with state and local agencies and plan a portfolio of evaluations .

cdc officials suggested that variation in evaluation planning in these three offices could provide insight into how cdc's centers generally prioritize evaluations to conduct .

dnpao is charged with leading strategic public health efforts to prevent and control obesity , chronic disease , and other health conditions through physical activity and healthy eating .

dnpao supports the first lady's lets move ! .

campaign to curb childhood obesity , which is considered an important public health issue but has a limited body of research on effective practices .

the nutrition , physical activity , and obesity program is a cooperative agreement between cdc and 25 state health departments to support a range of activities , including process and outcome evaluations .

a consulting group of state evaluators , outside experts , and divisional representation advises dnpao on proposing evaluation projects that would be useful to grantees and the divisions .

dhap , charged with leadership in helping control the hiv / aids epidemic , has a fairly large program evaluation branch that supports national performance monitoring and evaluation planning .

the evaluation branch is responsible for monitoring cdc - funded hiv prevention programs , including 65 health units and 150 community organizations .

within the branch , the evaluation studies team conducts specific evaluations of interest and in - depth process evaluations and outcome monitoring studies of selected hiv prevention interventions delivered by community - based organizations , state and local health departments , and health - care providers .

in addition to the division's strategic plan , the governmentwide national hiv / aids strategy for the united states , released in july 2010 , informs evaluation planning .

dhap's work is also shaped by an advisory committee and findings from an external peer review that provided input into programs and evaluations through the strategic plan .

dash is considered somewhat unique among cdc's divisions because it is not focused on disease or exposure but has a mission to promote the health and well - being of a particular population — children and adolescents .

dash funds tribal governments and state , territorial and local educational agencies to address child and adolescent health issues , including nutrition , risky sexual behavior , tobacco prevention , school infrastructure , and asthma management .

dash typically funds evaluations in one health risk area each year .

its framework , coordinated school health , involves community , family , teachers , and schools in addressing a diverse set of health issues .

it also partners with nongovernmental and community - based organizations to reach children who are not in school .

dash supports rapid evaluations to identify innovative programs and practices .

these evaluations typically last 12 to 24 months and data are collected within a school calendar year .

the evaluation team also has a small portfolio of evaluation research that includes large longitudinal randomized controlled trials that assess effectiveness over a 5-to - 6-year period .

the agencies we reviewed use a similar but informal evaluation planning process that involves collaboration between each agency's evaluation office and program offices , external groups , and senior officials .

typically , the evaluation office leads an iterative two - step process to develop ideas into full - scale proposals by obtaining feedback from senior officials and considering available resources .

the process varies across agencies in the breadth of the studies and programs considered , the use of ranked competitions , and the amount of oversight by senior officials .

figure 1 depicts the general process and the agencies' significant differences .

in most of the agencies we reviewed , evaluation planning generally starts and ends in the same fiscal year .

general procedures for submitting and clearing annual spending plans structure the evaluation planning process at several of these agencies because the approved evaluations may involve external expenditures .

the agencies must approve their evaluation plans by the start of the next fiscal year , or when appropriated funds become available , so that they can issue requests for proposals from the contractors that conduct the evaluations .

planning evaluations can include reviews by policy officials , such as deputy and assistant secretaries , and budget officials , such as an agency's chief financial officer .

for example , acf's evaluation staff develop evaluation proposals in the fall and early winter , before sending them to the agency's assistant secretary for approval in the late winter to allow the assistant secretary to make approval decisions in time to meet the deadlines for awarding contracts in that fiscal year .

although most of the agencies finish their planning by the start of the next fiscal year , the process can start as late as july at cdc's dnpao or in the fall of the current fiscal year at acf .

planning begins at each agency with internal coordination to define the goals and procedures for developing evaluation proposals .

at acf , this process begins informally , with evaluation and program staff meeting to discuss their priorities for the coming year .

the other agencies we reviewed ( including education beginning in 2010 ) issue memorandums describing the planning process to the staff members involved .

they may describe the staff members who will lead proposal - development teams , the substantive focus of the year's process , the evaluation plan's connection to spending plans , and the role of senior officials .

they may also give a schedule of key deadlines .

cdc's dash distributes a broader call for project nominations to agency staff members and researchers , state and local education agencies , and other program partners .

in recent years , the call has specified the type of interventions the division seeks to evaluate , stated deadlines for submitting nominations , and solicited information from nominators about particular interventions .

cdc's dnpao issues a call for proposals that addresses the process and broad criteria for project selections that can involve many people and proposals .

the calls at each agency are informal planning documents , however , as no agency we reviewed has an official policy that specifies the process for developing and selecting evaluations .

having developed informal processes over time , senior officials , and evaluation and program office staff have a common understanding of how they will develop , review , and select evaluations .

after the agencies identify their planning goals and steps , the evaluation and program staff begin to develop evaluation proposals .

at some agencies , the program staff may develop the initial proposals independently of the evaluation staff , in response to the same call for proposals .

the program staff may later consult with the evaluation staff to improve the proposals before they are reviewed further .

this process is common in the cdc divisions we reviewed , where the evaluation staff are located inside program offices dedicated to particular health issues , so both program and evaluation staff may individually or jointly submit proposals for consideration .

at other agencies , the evaluation staff meet with the program staff specifically to discuss ideas for evaluation and then develop initial proposals from the input they receive .

the evaluation staff at one of these agencies said they incorporate the priorities , questions , and concerns the program staff conveyed from their day - to - day experience into evaluation planning and that collaboration helps ensure later support for the approved evaluations .

alternatively , hud's evaluation unit includes program staff on the teams that develop proposals in specific policy areas , such as fair housing and homelessness .

the program offices also contribute to the initial proposals by providing comments to senior officials .

at all the agencies , the evaluation staff use their expertise in designing social research and assessing the reliability of data , among other skills , to ensure the quality and feasibility of proposals .

in addition to consulting internal program staff , most of the agencies we examined consult external groups to obtain ideas for evaluation proposals .

evaluation staff members cited a number of reasons for consulting external groups in developing proposals: the ability to identify unanswered research questions , coordinate evaluations across federal agencies , uncover promising programs or practices to evaluate , and inform strategic goals and priorities .

some evaluation staff reported consulting external groups as they develop program priorities and strategic plans , which they cited as criteria for planning evaluations .

over the past 2 years , pd&r has participated in a philanthropic foundation - funded partnership with research organizations that conducted several research projects to help inform the department's development of an evidence - based housing and urban policy agenda .

other staff said that they consult with state and local program partners , such as state welfare offices , to identify potentially useful projects .

external groups have formal roles in developing proposals at two agencies .

cdc's dash directly consults with external researchers , state and local education officials , and school health professionals for nominations of promising interventions to evaluate .

in planning for fiscal year 2011 , hud asked the public to submit ideas for evaluation on its “hud user” web site .

at most of the agencies , however , external groups do not explicitly develop evaluation proposals .

for example , acf staff said they informally consult with researchers about possible evaluation topics , partly in regular research conferences , but they do not ask their advisory panels or individual researchers to review specific evaluation proposals .

in recent years , pd&r also contacted the office of hud's inspector general for evaluation ideas that build on that office's work .

generally , the agencies review and approve evaluation proposals in two steps .

first , evaluation or program staff members develop ideas or brief concept papers for initial feedback from senior officials .

the feedback can involve a series of proposal development meetings , as at education and acf , where senior officials give staff members strategic direction to develop and refine their proposals .

alternatively , senior officials may review all draft proposals that the evaluation and program staff have developed independently , as at hud and cdc's dnpao and dhap .

initial feedback helps prevent staff from investing large amounts of time in proposals that would have a small chance of being approved .

the feedback expands proposal development beyond the evaluation and program offices and helps ensure that the proposals support the agency's broader strategic goals .

second , once the initial proposals are sufficiently well developed , senior officials review and select from a group of revised , full - scale proposals .

these may contain detailed information about cost and design for later use in the contracting process .

evaluation officials at acf and hud select from the pool of revised proposals those they wish to present to agency leaders , such as the secretary or assistant secretary , for final approval .

branch leaders at cdc's dnpao and dhap choose a group of proposals to compete for division resources against proposals from other branches within their divisions .

review panels rank - order all proposals ( discussed below ) , and then division leaders decide , based on the rankings and available resources , which proposals the division will fund .

in fiscal year 2011 , education staff plan to present to senior officials the entire proposed evaluation portfolio , identifying how evaluation studies address key questions and agency priorities .

some of the agencies we reviewed focus specifically on planning program evaluations , while others use the same annual process to plan a variety of analytic studies .

the central evaluation offices at acf , education , and hud perform a continuum of research and evaluation studies that may include collecting national survey data , conducting policy analyses , and describing local program activities , among other activities .

these agencies use the same process to make funding decisions across these various analysis proposals , which allows them to weigh the pros and cons of evaluation against other information needs when sufficient funds are available .

consequently , program evaluations may compete with other types of studies that require specific funding each year .

although the evaluation branch of cdc's dnpao provides a narrower range of services , the division uses a similar , unified process to decide how to develop proposals for all evaluations and research activities .

in contrast , dash plans their different types of studies separately .

it uses one annual process to develop evaluation proposals for promising practices and interventions often implemented by grantees .

it uses a different process to develop “evaluation research” proposals , which evaluation staff defined as national - level evaluations or long - term studies of program impact , often involving randomized controlled trials .

by considering these types of study separately , dash does not require longer - term evaluations to compete with shorter - term studies for the same funds .

programs compete against one another for evaluation resources at some but not most of the agencies we reviewed .

the scope of evaluation planning at one group of agencies is limited to the same programs or policy areas each year .

these agencies have designed their planning processes to select not programs to evaluate but evaluation questions to answer in a program area .

for example , the acf evaluation staff indicated that they identify important questions for each program with evaluation funding and then allocate funds to the most important questions for each program .

consequently , the agency typically conducts evaluations in programs with evaluation funds ( such as tanf ) every year but has not evaluated programs which do not have evaluation funding ( such as the community services and social services block grants ) .

hud and , to a certain extent , two cdc divisions seek to identify which programs are important to evaluate as well as what questions are important to answer about those programs .

agency staff have the flexibility to direct resources to the programs that they believe most need evaluation .

hud evaluation staff said that this broad scope allows them to build a portfolio of evaluations across policy areas and serve the agency's most pressing evaluation needs .

senior officials consider the value of proposals from all policy areas combined but make some effort to achieve balance across policy areas .

only cdc's divisions hold formal , ranked competitions to review and select proposals .

in each division , staff members or external panels rate and rank all evaluations the evaluation and program offices propose , once they have been fully developed .

senior leaders at cdc's dhap and dnpao select proposals by rank and available funds .

in addition , senior leaders at dnpao rank and select proposals within each of its three policy areas: nutrition , physical activity , and obesity .

senior leaders at dash consider information collected from site visits and interviews for a small group of semi - finalists that were selected based on the input of the external panel .

cdc staff reported that cdc often uses ranked competitions to award grants and contracts across the agency .

at the other agencies we reviewed , evaluation staff said that proposals are reviewed and selected in a series of discussions between the agency's policy officials , such as assistant or deputy secretaries , and the senior leaders of its evaluation and program offices .

none of these agencies reported formally ranking their proposed evaluations but , instead , qualitatively consider the relative strengths and weaknesses of each evaluation .

proposal review and selection in the cdc divisions involves less department - level input than at acf , education , and hud .

cdc's evaluation staff reported that division leadership makes the final decision on evaluation projects and does not need the approval of the office of the cdc director or hhs officials , although a key criterion in project ranking and selection is often alignment with larger cdc , hhs , and national priorities .

cdc is studying evaluation planning across the agency , however , and may increase central oversight in the future .

the assistant secretary at acf , not departmental officials , makes final approval decisions , but the evaluation staff reported consulting informally with staff of the office of the assistant secretary for planning and evaluation ( aspe ) when proposals are developed .

the processes at education and hud are more centralized than at cdc or acf .

at these agencies , senior department officials — such as the secretary , deputy secretary , or assistant secretary — make the final selection decisions , after the evaluation and program staff have developed and reviewed the initial proposals .

beginning in fiscal year 2010 , hud staff indicated that the agency funded many of its evaluations from a departmental transformation initiative fund , whose board must approve proposed evaluations and other projects .

board members include the assistant secretaries of pd&r and community planning and development , the chief information officer , as well as the director of strategic planning and management .

one agency does not strictly plan evaluations for the next fiscal year .

cdc's dash staff plan evaluations that are funded during the current fiscal year rather than evaluations that will be funded in the next fiscal year .

local education agencies typically partner with the agency to conduct evaluations during the school year , when parents , students , and teachers are available to researchers .

as a result , the agency cannot wait until funds are scheduled for appropriation in october or later , because their data collection plans and site selections must be final before the school year begins , typically in late august or early september .

education adjusted its evaluation planning guidance in 2010 to explicitly plan evaluations to be conducted in fiscal year 2011 as well as to inform its budget request for fiscal year 2012 .

the agency links its evaluation planning to the budget , partly to ensure that funding or authority will be available and that evaluations are aligned with program goals and objectives , congressional mandates , and the agency's strategic priorities .

in addition , education has proposed , for reauthorization of the elementary and secondary education act , to submit a biennial evaluation plan to the congress and establish an independent advisory panel to advise the department on these evaluations .

these plans align well with the american evaluation association's ( aea ) recommendation , made in a recent policy paper on federal government evaluation , that federal agencies prepare annual and multiyear evaluation plans to guide program decision - making and consult with the congress and nonfederal stakeholders in defining program and policy objectives , critical operations , and definitions of success .

we found these mature agencies remarkably similar in the four general criteria they used for selecting evaluations to conduct during the next fiscal year: strategic priorities , program concerns , critical unanswered questions , and the feasibility of conducting a valid evaluation study .

another important consideration , in situations in which several program offices draw on the same funding source , was establishing balance across program areas .

most agencies indicated no hierarchy among these criteria .

rather , they considered them simultaneously to create the best possible portfolio of studies .

the first criterion , strategic priorities , represents major program or policy areas identified as a focus of concern and reflected in a new initiative or increased level of effort .

strategic priorities might be expressed by a department or the white house as strategic goals or secretarial initiatives or by the congress in program authorizations or study mandates .

under gpra , agencies are expected to revise their strategic plans at least every 3 years , providing an opportunity to align their plans with current conditions , goals , and concerns .

the plans can chart expectations for both program and evaluation planning .

cdc's dhap officials described waiting for the white house release of the national aids strategy in july to finalize their strategic plan and objectives and to prioritize evaluation activities that would address them .

in addition to national priorities , division priorities are informed by their own research , surveillance , and program evaluation , identifying the subpopulations and geographic areas most affected by the disease .

hud's pd&r conducts the national housing discrimination study every 10 years , which provides a unique benchmark and input to the department's long - term planning .

strategic priorities may also arise from congressional mandates .

education officials noted that the congress generally mandates evaluations when it reauthorizes large formula grant programs , such as the national assessments of title i of the elementary and secondary education act , and that it has also mandated the evaluation of major new programs that might have great public interest or promise .

they said that they schedule evaluations so that they will produce useful information for reauthorization , usually every 6 to 8 years .

the second criterion , program - level concerns , represents more narrowly focused issues concerning an identified problem or opportunity .

evaluation staff reported that valuable ideas for evaluations often reflected the questions and concerns that arise in daily program operations .

acf noted that head start teachers' reports of disruptive children who prevented other children from learning led to a large - scale evaluation of several potentially effective practices to enhance children's socio - emotional development and teachers' classroom management practices .

accountability concerns that omb , gao , and inspector general reports raise may lead to follow - up studies to assess the effectiveness of corrective actions .

for example , pd&r staff stated that after a gao report criticized the section 202 demonstration grant program for not building housing projects in a timely fashion , the congress introduced a competition for grants to speed up development .

a follow - up evaluation will assess whether timeliness has improved .

other evaluation questions may address crosscutting issues that influence program success , such as a provider's ability to leverage resources or promote partnerships with other stakeholders .

cdc's dnpao places a priority on proposals that develop collaborations with external partners and among operational units within the division .

the third criterion , critical unanswered questions , reflects the state of knowledge and evidence supporting government policies and programs .

for example , agency staff talk with advisory groups , academics and other researchers in their field to identify useful research and evaluation questions that could advance understanding of the field and improve practice .

cdc staff indicated that filling knowledge gaps is a particularly important criterion for project selection , because some public health areas lack an extensive research base or evidence on effective practices .

an opre senior official described opre staff as looking for compelling , essential questions of enduring interest .

acf programs attempt to solve persistent social problems , such as testing diverse strategies to promote employment retention and advancement for low - wage workers and current or former tanf recipients .

because formal impact evaluations of these efforts may take 5 or 6 years to complete , opre staff look for questions that are persistent and studies that are likely to advance knowledge .

gathering information on emerging , promising practices was a consideration , particularly where evidence of effective practice has not yet been demonstrated .

this was particularly important to the cdc divisions , dnpao and dash , where the public health research base was limited and effectiveness evaluations of promising practices were needed to expand the pool of evidence - based interventions to offer grantees .

the fourth criterion , evaluation feasibility , encompassed a range of pragmatic issues such as whether data were available and at what cost , whether the proposed evaluation could answer questions persuasively , and whether grantees had the interest and capacity to participate in evaluation .

naturally , agencies weigh their evaluation priorities in the context of their fiscal and budget constraints .

evaluators described determining whether the most important questions could be answered and the resources that would be needed to answer them .

when “hard” data are lacking , some evaluators find that in - house exploratory work and investment in data gathering may be needed before scaling up to a contracted evaluation .

like the other evaluation units , pd&r compares the feasibility and cost of a study to alternative proposals .

the evaluation staff noted that cost cannot be the sole criterion , however , because studies of some programs , such as the large block grants , are more resource intensive than the approaches available for studying other programs , such as housing voucher programs .

when working with community - based organizations , agencies find grantee evaluation capacity can be very important .

to ensure that the selected grantee is implementing the program faithfully , is ready for evaluation , and can collect valid and reliable data , cdc's dash staff conduct site visits to assess candidate projects on such issues as appropriate logical links between program components and expected outcomes , staff turnover , political conflicts , fiscal sustainability , and staff interest in and capacity to conduct the evaluation .

acf evaluators were pleased to note that many state and local tanf officials participate in opre's annual welfare research conference , show interest in conducting evaluations , and have been willing to randomly assign recipients to new programs for research purposes .

although the agencies generally followed a similar process in developing their evaluation agendas , some agency characteristics or conditions appeared to influence their choices and may be important for other agencies to consider as they develop their own evaluation agendas .

the four conditions we identified as strongly influencing the evaluation planning process were 1. the location of evaluation funding , whether with the program or 2. the scope of the evaluation unit's responsibility within the agency ; 3. how much the evaluators rely on program partners ; and 4. the extent and form of congressional oversight over agency program evaluations .

where evaluation funds come from largely controls the selection of programs to evaluate .

in acf , cdc , and education , authority and funds for evaluation are primarily attached to the programs , not to the evaluation office .

this has implications for both how evaluation offices conduct their planning and for whether a program is likely to be evaluated .

where evaluation funds and authority are tied to the program , and funds are available , evaluation staff generally choose not which programs to evaluate but which research questions to answer .

thus , evaluators in acf and education work separately with each program office that has evaluation funds to develop proposals .

in contrast , at hud , when the evaluation office has uncommitted evaluation funds , selecting proposals can involve deciding between programs .

therefore , besides considering policy priorities and feasibility issues , hud senior managers try to balance available evaluation funding across programs or policy areas after proposals are developed within program areas .

this involves soliciting input from program office leaders on the preliminary agenda and discussing competing needs in the final selection process .

cdc's dnpao , with its three distinct program areas — nutrition , physical activity , and obesity — made similar efforts to obtain a balanced portfolio by forming teams to rank order proposals separately and having senior division leaders consider program balance in selecting proposals .

one consequence of tying evaluation funds and authority to programs is that programs that do not have their own evaluation authority may not get evaluated at all .

staff at acf and education told us that because their evaluation offices did not have significant discretionary funds for external contracts , they had not conducted any evaluations of several programs , even though they believed that some of those programs should be evaluated .

not discussing the pros and cons of evaluating a particular program can lead to inappropriately excluding some from evaluation .

hud officials noted that it was important to attempt to balance evaluation spending across program areas because , otherwise , some programs might be avoided as too difficult or expensive to evaluate .

education officials said they plan to address this issue by developing a departmental portfolio of strong evaluation proposals based on policy and management needs , without regard to existing evaluation authority , and then request funds for them .

then , in future legislative proposals , they plan to ask the congress for more flexibility in evaluation funds to better meet the field's needs .

the agency evaluation offices we examined were located at different organizational levels , affecting the scope of their program and analytic responsibilities as well as the range of issues they considered .

at cdc , the evaluation offices are generally within program offices , so they do not need a separate step for consulting with program staff to identify their priorities .

instead , the divisions solicit evaluation proposals from staff throughout the division .

in the other agencies we examined , evaluation offices are either parallel to program offices ( acf ) or at the departmental level ( education and hud ) , which leads them to consult more formally with the program offices during both development and selection .

location and scope of responsibilities also influenced evaluation approval .

cdc's divisions , with the narrowest scope among the units we examined , exerted considerable control over their evaluation funds and did not require approval of their evaluation agendas by either the director or the department .

dash did , however , report coordinating evaluation planning with other agencies and hhs offices on specific cross - cutting programs , and dhap reported delaying its selection of evaluation proposals this past spring to coordinate with the new national aids strategy .

in contrast , at education and hud , where evaluation offices have departmental scope , final approval decisions are made at the department level .

in the middle , opre selections are approved by the acf assistant secretary and do not require departmental approval .

being responsible for a wide range of analytic activities also influenced an evaluation office's choice of evaluations .

evaluators in the more centralized offices in acf and hud described having the flexibility to address the most interesting questions feasible .

for example , if it is too early to obtain hard data on an issue , pd&r staff said that they might turn to in - house exploratory research on that issue .

acf staff noted that they often conducted small descriptive studies of the operations of state tanf programs because of the decentralized nature of that program .

this flexibility can mean , however , that they must also consider the range of the program office's information needs when developing their portfolio of studies .

pd&r staff noted that they try to ensure that some studies are conducted in - house to meet program staff interest in obtaining quick turnaround on results .

dnpao aims to achieve a balanced portfolio of studies by ranking cross - cutting proposals within categories of purpose , such as monitoring or program evaluation .

education officials propose to create a comprehensive departmental evaluation plan that identifies the department's priorities for evaluation and other knowledge - building activities , is aligned with their strategic plan , and will support resource allocation .

several of the evaluation offices we examined also provide technical assistance in performance monitoring and evaluation .

while this may help strengthen relationships with program staff and understanding of program issues , the responsibility can also reduce the resources available for evaluation studies .

all three cdc divisions require evaluations or performance monitoring from their grantees ; therefore , providing grantees with technical assistance is a major activity for these evaluation offices .

in dhap and dnpao , staff workload , including providing technical assistance , was cited among the resource constraints in developing evaluation proposals .

acf staff noted that if program offices prioritize their available funds on technical assistance and monitoring , there may not be enough to conduct an evaluation .

in our cases , placing the evaluation office inside the program office ( as in the cdc divisions we examined ) was associated with conducting more formal proposal ranking .

we considered several possible explanations for this: ( 1 ) staff adopted the competitive approach they generally take in assessing proposals for project and research funds , ( 2 ) a large volume of proposals required a more systematic process for making comparisons , or ( 3 ) the visibility of the selections created pressure for a transparent rationale .

the first point may be true but does not explain why the other agencies are also deliberative in assessing and comparing evaluation proposals but do not rate them numerically .

the two other explanations appear to be more relevant and may be related to the fact that evaluations are being selected within the program office and thus cover a relatively narrow range of options .

cdc staff said that they did not need to formally rate and rank the three or four proposals they submitted for omb's evaluation initiative but might have done so had the number of proposals to consider been greater .

dash and dhap issue broad calls each year for nominations of promising practices to evaluate and , thus , gather a large number of proposals to assess .

staff in dash , which also solicits project nominations from the public , indicated that over time their process has become more formal , accountable , and transparent so that selections appear to the public to be more systematic and less idiosyncratic .

although information is limited , we believe that systematically rating and ranking proposals may be a useful procedure to consider case by case .

the influence of nonfederal program partners on developing and selecting evaluation proposals was observed in most of the agencies we examined , although it did not vary much among them .

the importance of program stakeholders to planning should be expected because these particular agencies generally rely on external partners — state and local agencies and community – based organizations — to implement their programs .

however , the extent of coordination with external parties on evaluation planning seen here may not be necessary in agencies that are not so reliant on third parties .

acf's evaluation staff pointed out that they cannot evaluate practices that a state or local agency is not willing to use .

efforts to engage the academic and policy communities in discussing ideas for future research at acf and education also reflect these agencies' decades - long history of sponsoring research and evaluation .

cdc's dhap and dnpao also employ advisory groups , including cdc staff and external experts , to advise them on strategic planning and topics that will help meet the needs of their grantees , but only dash involved external experts directly in assessing evaluation proposals .

dash evaluators assemble panels to assess nominations of sites implementing a promising practice ; depending on the topic and stage of the process , these panels might include external experts and experts from across cdc or other agencies serving children and families .

program partners' evaluation capacity is especially important to evaluation planning in the cdc divisions we examined because their evaluations tend to focus on the effectiveness of innovative programs or practices .

each year , dash publicly solicits nominations of promising projects of a designated type of intervention and uses review panels and site visits to rank dozens of sites on an intervention's strength and promise , as well as the feasibility of conducting an evaluation .

staff said that it was important to ensure that the grantee organization was stable and able to cooperate fully with an evaluation and noted that evaluation is sometimes difficult for grantees .

congress influences agencies' evaluation choices in a variety of ways .

congress provides agencies with the authority and the funds with which to conduct evaluations and may mandate specific studies .

the evaluation offices in acf , education , and hud all noted their responsibility to conduct congressionally mandated evaluation studies in describing the criteria they used in evaluation planning .

the cdc offices indicated that they did not have specific study mandates but , rather , authority to conduct studies with wide discretion over the particular evaluation topics or questions .

of course , in addition to legislatively mandating studies , the congress expresses interest in evaluation topics through other avenues , such as oversight and appropriations hearings .

dhap evaluators noted that they receive a lot of public scrutiny and input from the congress and the public health community that works its way into project selection through the division's setting of priorities .

agency evaluators described a continuum of evaluation mandates , from a general request for a study or report to a list of specific questions to address .

education officials noted that the congress generally mandates evaluations of the largest programs when they are reauthorized or new programs or initiatives for which public interest or promise might be great .

some evaluators noted that sometimes the congress and agency leaders want answers to policy questions that research cannot provide .

they indicated that , where legislative language was vague or confusing , they did their best to interpret it and create a feasible evaluation .

in a previous study of agency studies' not meeting congressional information needs , we suggested that expanding communication between congressional staff and agency program and evaluation staff would help ensure that information needs are understood and that requests and reports are suitably framed and adapted as needs evolve .

evaluators told us that whether and how much funding was attached to an evaluation mandate also influenced how the mandate was implemented .

they said that when appropriate funding was available , they always conducted congressionally mandated evaluations .

however , sometimes the amounts available do not reflect the size of the evaluation needs in a program .

this was particularly a problem for small programs where a fixed set - aside of program funds for evaluation might yield funds inadequate for rigorous evaluation .

evaluators described a related challenge when evaluation authorities are attached to single programs which preclude pooling funds across programs .

such limitations on using evaluation funds could lead to missed opportunities to address cross - cutting issues .

in cases where no additional funding was provided for legislatively mandated studies , agencies had to decide how and whether to fund them .

some agency evaluators told us that they generally conducted what they saw as “unfunded mandates” but would interpret the question and select an approach to match the funds they had available .

this might mean that without funds to collect new data , a required report might be limited to simply analyzing or reporting existing data .

hud receives considerable congressional oversight of its research and evaluation agenda , reflecting congressional concern about its past research priorities and greater decision - making flexibility under the new transformation initiative .

in 2008 , a congressionally requested national research council review of hud's research and evaluation lauded most of pd&r's work as “high quality , relevant , timely , and useful” but noted that its resources had declined over the previous decade , its capacity to perform effectively was deteriorating , and its research agenda was developed with limited input from outside the department .

nrc recommended that , among other things , hud actively engage external stakeholders in framing its research agenda .

in response , pd&r solicited public suggestions online for research topics for fiscal year 2011 and beyond .

in addition , hud proposed a transformation initiative of organizational and program management improvement projects in 2009 and asked that up to 1 percent of its program budget be set aside in a proposed transformation initiative fund to support research and evaluation , information technology , and other projects .

the house and senate appropriations committees approved the fund ( at somewhat less than the requested amount ) with a proviso that hud submit a plan for appropriations committee approval , detailing the projects and activities the funds would be used for .

an effective evaluation agenda aims to provide credible , timely answers to important policy and program management questions .

in setting such agendas , agencies may want to simultaneously consider the four general criteria we identified: strategic priorities , program concerns , critical unanswered questions , and the feasibility of conducting a valid study .

in the short run , because agency evaluation resources are limited , ensuring balance in evaluations across programs may not be as important as addressing strategic priorities .

however , developing a multiyear evaluation plan could help ensure that all an agency's programs are examined over time .

to produce an effective evaluation agenda , agencies may want to follow the general model we identified at the agencies we reviewed: professional evaluators lead an iterative process of identifying important policy and program management questions , vetting initial ideas with the evaluations' intended users , and scrutinizing the proposed portfolio of studies for relevance and feasibility within available resources .

since professional evaluators have the knowledge and experience to identify researchable questions and the strengths and limitations of available data sources , they are well suited to leading a consultative process to ensure that decision makers' information needs can be met .

however , agencies may need to adapt the general model's steps to match their own organizational and financial circumstances .

for example , they may not need to formally rank proposals unless they have many more high - quality proposals than they can fund .

they may find advantages to placing evaluation offices within program offices ( for focusing on program needs , for example ) and at higher levels ( for addressing broader policy questions ) .

where analytic demands are significant and resources permit , they may find a combined approach best - suited to their needs .

to ensure that their evaluations provide the information necessary for effective management and legislative oversight , evaluation offices are likely to need to seek out in advance the interests and concerns of key program and congressional stakeholders , especially program partners , and discuss preliminary proposals with the intended users .

the departments of health and human services and housing and urban development provided comments on a draft of this report , which are reprinted in appendixes i and ii .

hhs appreciated the attention that this report gives to the importance of strong prioritization processes for selecting evaluation studies and allocating resources to complete them , and was pleased that the practices of acf and cdc in this area are models for emulation by others .

it also noted that , given the diversity of purposes for evaluations , the optimal location and organization of evaluation activities will vary with the circumstances .

this is consistent with our concluding observation that agencies may need to adapt the general model — including where to locate evaluation offices — to match their own organizational and financial circumstances .

hud agreed with our description of how it plans evaluations but was concerned that the report did not place enough emphasis on the appropriations process as a major influence on what projects it funds and when it can begin the contracting process .

we have added text to note that the congress influences the agencies' evaluation processes through providing them with both the authority and funds with which to conduct evaluations , as well as mandating specific studies .

education , hhs , and hud also provided technical comments that were incorporated where appropriate throughout the text .

we are sending copies of this report to the secretaries of education , health and human services , and housing and urban development ; the director of the office of management and budget ; and appropriate congressional committees .

the report is also available at no charge on gao's web site at www.gao.gov .

if you have questions about this report , please contact me at ( 202 ) 512- 2700 or kingsburyn@gao.gov .

contacts for our office of congressional relations and office of public affairs are on the last page .

key contributors are listed in appendix iii .

nancy kingsbury managing director applied research and me , ph.d .

in addition to the person named above , stephanie shipman , assistant director ; valerie caracelli ; and jeff tessin made significant contributions to this report .

american evaluation association .

an evaluation roadmap for a more effective government .

september 2010. www.eval.org / eptf.asp leviton , laura c. , laura kettel khan , and nicola dawkins , eds .

“the systematic screening and assessment method: finding innovations worth evaluating.” new directions for evaluation no .

125 , 2010 .

national research council , committee to evaluate the research plan of the department of housing and urban development , center for economic , governance , and international studies , division of behavioral and social sciences and education .

rebuilding the research capacity at hud .

washington , d.c.: national academies press , 2008 .

office of management and budget .

analytical perspectives — budget of the united states government , fiscal year 2011 .

washington , d.c.: executive office of the president , feb. 1 , 2010 .

office of management and budget .

evaluating programs for efficacy and cost - efficiency .

m - 10-32 memorandum for the heads of executive departments and agencies .

washington , d.c.: executive office of the president , july 29 , 2010. www.whitehouse.gov / sites / default / files / omb / memoranda / 2010 / m10-32.pdf office of management and budget .

increased emphasis on program evaluations .

m - 10-01 memorandum for the heads of executive departments and agencies .

washington , d.c.: executive office of the president , oct. 7 , 2009. www.whitehouse.gov / sites / default / files / omb / assets / memoranda_2010 / m10- 01.pdf u.s. department of education , office of planning , evaluation , and policy development .

a blueprint for reform: the reauthorization of the elementary and secondary education act .

washington , d.c.: march 2010 .

u.s. department of health and human services .

evaluation: performance improvement 2009 .

washington , d.c.: 2010 .

employment and training administration: increased authority and accountability could improve research program .

gao - 10-243 .

washington , d.c.: january 29 , 2010 .

program evaluation: a variety of rigorous methods can help identify effective interventions .

gao - 10-30 .

washington , d.c.: november 23 , 2009 .

continuing resolutions: uncertainty limited management options and increased workload in selected agencies .

gao - 09-879 .

washington , d.c.: september 24 , 2009 .

results - oriented management: strengthening key practices at fema and interior could promote greater use of performance information .

gao - 09-676 .

washington , d.c.: august 17 , 2009 .

performance budgeting: part focuses attention on program performance , but more can be done to engage congress .

gao - 06-28 .

washington , d.c.: october 28 , 2005 .

program evaluation: omb's part reviews increased agencies' attention to improving evidence of program results .

gao - 06-67 .

washington , d.c.: october 28 , 2005 .

managing for results: enhancing agency use of performance information for management decision making .

gao - 05-927 .

washington , d.c.: september 9 , 2005 .

performance measurement and evaluation: definitions and relationships .

gao - 05-739sp .

washington , d.c.: may 2005 .

program evaluation: an evaluation culture and collaborative partnerships help build agency capacity .

gao - 03-454 .

washington , d.c.: may 2 , 2003 .

program evaluation: improving the flow of information to the congress .

gao / pemd - 95-1 .

washington , d.c.: january 30 , 1995 .

