with a life cycle cost of $12.3 billion , the 2010 census was the most expensive population count in u.s. history , costing over 30 percent more than the $9.4 billion 2000 census ( in constant 2020 dollars ) .

for the 2020 census , the u.s. census bureau ( bureau ) aims to reduce its per - household cost .

controlling costs is a priority because the cost of enumerating each housing unit has increased from $16 per housing unit in 1970 to $92 per housing unit in 2010 , and the estimated number of housing units has increased markedly from 2010 to 2020 .

to achieve savings , the bureau is extensively changing how it plans to conduct the 2020 census .

specifically , the bureau is changing how it builds its address list , seeking to improve self - response by encouraging the use of the internet and telephone , using administrative records to reduce field work , and reengineering field operations using technology to reduce manual effort and improve productivity , among other things .

in october 2015 , the bureau estimated that with these innovations it could conduct the 2020 census for a life - cycle cost of $12.5 billion .

this is in contrast to its estimate of $17.8 billion to repeat the design and methods of the 2010 census ( both in constant 2020 dollars ) .

in building the address list for 2020 , the bureau aims to use a mix of street and satellite imagery , as well as address and other administrative records from state , local , and tribal governments and commercial partners , to canvass most addresses virtually .

by doing so , the bureau plans to reduce the need for more costly door - to - door or “in - field” address canvassing .

you asked us to evaluate the bureau's reengineered approach to 2020 address canvassing .

this report ( 1 ) describes the bureau's design for its reengineered 2020 census address canvassing , ( 2 ) evaluates the extent to which the bureau assessed the cost and quality implications of its reengineered address canvassing approach , and ( 3 ) assesses the status of the bureau's efforts to reduce the in - field address canvassing workload .

for all three objectives , we reviewed documentation related to 2020 address canvassing , such as operational planning documents and test plans , and interviewed cognizant bureau officials .

for the first objective we reviewed the bureau's december 2015 address canvassing operational plan to highlight design features and innovations affecting cost and quality .

for the second objective , we assessed how the bureau was implementing its stated plans related to testing and evaluating the reengineered address canvasing approach , in particular how the bureau was assessing cost and quality implications .

for the third objective , we analyzed production and payroll data in december 2016 and march 2017 to report on the productivity of the bureau's reengineered approach to address canvassing in light of the work required to complete the operation and reduce fieldwork .

we verified the reliability of these data sources by interviewing officials responsible for the data , reviewing data dictionaries , and carrying out various integrity checks on the data received .

we found the data to be sufficiently reliable for our purposes .

more information on our scope and methodology can be found in appendix i .

we conducted this performance audit from august 2016 to july 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the bureau's address canvassing operation updates its address list and maps , which are the foundation of the decennial census .

an accurate address list both identifies all households that are to receive a notice by mail requesting participation in the census ( by internet , phone , or mailed - in questionnaire ) and serves as the control mechanism for following up with households that fail to respond to the initial request .

precise maps are critical for counting the population in the proper locations — the basis of congressional reapportionment and redistricting .

if the bureau's address list and maps are inaccurate , people can be missed , counted more than once , or counted at the wrong location .

however , our prior work has shown that developing an accurate address list has been both labor - intensive and costly — in part because people can reside in unconventional dwellings , such as converted garages , basements , and other forms of hidden housing .

for example , as shown in figure 1 , what appears to be a single - family house could contain an apartment , as suggested by its two doorbells .

in order to account for the complexity of the nation's housing stock , the bureau uses different procedures for canvassing and enumerating different types of living arrangements .

these different types of living arrangements include housing units ( eg , multi - unit structures and single - family units ) and group quarters ( such as nursing facilities and college residence halls ) .

the bureau also uses different procedures for canvassing and enumerating different areas of the country , depending on the predominant way that people receive their mail in those areas .

the bureau generally uses methods that rely on individuals self - responding to either notices or questionnaires in areas where mail is generally delivered to the same individual locations where people live , and it calls these areas “self - response” areas .

in other areas where mail is typically delivered at post office boxes or along rural routes , the bureau relies on door - to - door visits to canvass and deliver notices and questionnaires or to conduct interviews .

the bureau refers to these areas as update / enumerate areas , where the bureau canvasses the addresses in the areas while enumerating the residents .

the bureau also uses similar procedures in sparsely settled and isolated parts of alaska , which often require special travel or other arrangements .

during address canvassing , the bureau verifies that its master address list and maps are accurate so the tabulation for all housing units and group quarters is correct .

for the 2010 census , the address canvassing operation mobilized almost 150,000 field workers to canvass almost every street in the united states and puerto rico to update the bureau's address list and map data — at a cost of nearly $450 million .

the cost of in - field address canvassing in 2010 , along with the emerging availability of imagery data , led the bureau to explore an approach for 2020 address canvassing that would not involve walking every street in the country .

to achieve cost savings , the bureau decided in september 2014 that while it would still verify all the addresses in the country , not all of this canvassing would take place by going door - to - door ( or “in - field” ) , as it had in prior decennials .

rather , some areas might only need a review of their address and map information using computers and what the bureau refers to as “in - office” procedures .

the bureau cited multiple factors in support of this decision: advancements in technology allow the bureau to continually update address and spatial data throughout the decade using its partnerships with state , local , and tribal governments .

the availability of up - to - date , high quality , high - resolution aerial and street - level imagery now provides a viable and effective tool to help reduce field work for many parts of the united states .

more efficient and effective uses of administrative records , such as state , local , and tribal address lists as well as address information from other federal agencies like the u.s .

postal service , can provide a substitute for field work , especially in areas that have been relatively stable residentially .

the bureau estimated in october 2015 that these changes to its address canvassing operation could save $900 million for the 2020 census .

these savings are part of an overall redesign that the bureau estimated could reduce costs by $5.2 billion compared to conducting another census using methods from the 2010 census .

the bureau's 2015 estimates were the latest available when we concluded our audit work .

according to the bureau , the potential for savings from the reengineered approach comes from the fact that a vast majority of housing units are eligible for a substantial reduction in in - field canvassing in 2020 .

of the over 144 million projected housing units for 2020 , nearly 132 million ( 91 percent ) are in self - response areas .

because housing units in these areas are physically located where their mail is delivered , the bureau can eliminate the need for fieldwork if it is able to canvass addresses in these areas in - office .

the remaining housing units lie in update / enumerate and related rural areas .

because people in these areas would not receive a mailed request to participate in the census at their homes , the bureau will still require fieldwork to complete interviews for the census .

the potential for savings from reduced housing unit workload for in - field canvassing is moderated by the fact that update / enumerate areas account for a disproportionate share of land area throughout the country .

figure 2 illustrates what the 2010 distribution ( the most recent publicly available ) of types of enumeration areas across the country would be using 2020 categories .

as shown , bureau employees would still have to traverse large sections of the country door - to - door , even if in - office address canvassing removed all of the self - response areas from fieldwork .

the bureau began its in - office address canvassing operation in september 2015 .

in december 2016 , the bureau estimated that its new approach would reduce the in - field canvassing workload by 75 percent — that is , about 25 percent of housing units would still require in - field canvassing .

table 1 shows several elements that go into the overall reengineered approach .

in addition to the two forms of address canvassing ( in - office and in - field ) , the bureau included multiple ways for state , local , and tribal governments to contribute to the address list .

the bureau also planned for an annual address list coverage measurement study – in part to validate the ongoing work of in - office address canvassing .

as described in the bureau's december 2015 address canvassing operational plan , in - office canvassing had two phases to identify the geographic areas that will not need to be canvassed in - field: during the first phase , known as “interactive review,” bureau employees are to use current aerial imagery to determine if areas have housing changes , such as new residential developments or repurposed structures , or if the areas match what is in the bureau's master address file .

the bureau would assess the extent to which the number of housing units in the master address file is consistent with the number of units visible in the current imagery .

if the housing shown in the imagery matches what is listed in the master address file , those areas would be resolved and set aside as not needing in - field address canvassing .

during the second phase , known as “active block resolution,” employees would try to resolve coverage concerns identified during the first phase and verify every housing unit by virtually canvassing the entire area .

as part of this virtual canvass , the bureau would compare what is found in imagery to the master address file data and external data sources to attempt to resolve any discrepancies .

if bureau employees could not reconcile housing unit count and geographic feature ( such as street location or highway median ) with what is in the address list , they would refer these blocks to in - field address canvassing .

figure 3 shows the intended relationships between these phases of address canvassing , and the master address file .

in both phases of in - office address canvassing , areas could be temporarily set aside if there is insufficient imagery or other data to fully review the area .

the bureau plans to further review these areas again as the necessary imagery and data become available .

any geographic areas not resolved after completing the two phases of in - office address canvassing will be canvassed in - field .

the bureau has not completed evaluations of results from tests and activities it carried out in 2016 to assess the effectiveness of in - office address canvassing .

information from these tests would help the bureau: assess the implications on cost and quality of the reengineered design , and make more informed decisions about census operations .

specifically , the bureau has not completed evaluations for its 2016 address canvassing test or its 2016 master address file coverage study .

the bureau's 2016 address canvassing test measured the cost and quality of address canvassing in two sites from october to december 2016 — in buncombe county , north carolina , and st. louis , missouri .

meanwhile , the 2016 master address file coverage study ran from october 2015 to april 2016 gathering information to measure the quality of in - office canvassing .

the bureau's november 2014 address canvassing recommendation report , which summarized the proposed components of the reengineered address canvassing approach and recommended implementing it , noted that future research would be needed to refine the process and determine impacts on cost and quality .

yet , citing budget uncertainty , in january 2017 the bureau announced it was cancelling 2017 master address file coverage study activities , removing another data point for evaluating in - office address canvassing .

bureau officials described this activity as a lower priority than other activities that are more necessary to conduct the census .

in particular , bureau officials pointed to the need to prioritize preparations for the 2018 end - to - end test , where all major decennial operations are to undergo a dress rehearsal prior to 2020 .

appendix ii depicts the bureau's reengineered address canvassing key activities , from its 2014 decision to proceed through its recently delayed and canceled evaluations .

according to its december 2015 address canvassing operational plan , the bureau was going to decide whether to refine several design assumptions based , in part , on the results of these two evaluation efforts – including whether the bureau would be able to meet its in - field canvassing goal of 25 percent without sacrificing address list quality .

according to the address canvassing operational plan , these decisions related to anticipated workload and the quality of the data collected were to have occurred by january 2017 .

because the 2016 master address file coverage study and 2016 address canvassing test evaluations have not been completed , they have not supplied information to the bureau to help make this decision .

access to large multi - unit structures ; reconciliation of conflicts between household responses and visible clues about unreported or hidden housing units ; and allocation of assignments in an efficient manner ( i.e. , clustering housing units geographically ) .

additionally , in october 2016 we visited the test sites of the bureau's address canvassing test .

given the bureau's timeline for making planning and additional testing decisions , we reported to the bureau in near - real - time any observed implementation challenges experienced during the test with respect to in - field canvassing ( see sidebar ) .

these observations underscore the complexity of address canvassing and the need to test design innovations .

the bureau was aware of these challenges ; however , structured evaluations of cost and quality are still not complete .

bureau officials have stated that these planned evaluations will be delayed until august 2017 .

the bureau agreed with these observations and indicated that it would examine them as part of its evaluation of the 2016 test .

delaying evaluations and canceling the 2017 master address file coverage study and the 2017 census test has meant missed opportunities to assess and potentially improve the effectiveness of reengineered address canvassing .

it has also left the bureau without clear justification for making decisions related to its reengineered address canvassing approach — such as suspending the second phase of in - office address canvassing .

should the delayed evaluations of the 2016 address canvassing test and 2016 master address file coverage study show that in - office address canvassing yields accurate and cost effective results , it will be important to prioritize the bureau's investments in this approach to achieve planned savings .

in addition to the cancellation of the 2017 master address file coverage study , the cancellation of the field work components of the 2017 census test , which the bureau similarly attributed to budgetary constraints it faced , represents a lost opportunity to collect data that could have been used to evaluate the quality of in - office address canvassing .

specifically , this fieldwork would have provided the bureau with more in - field collected address information that could have been used as a check on in - office collected data collected in the same areas .

table 2 provides the status of key address canvassing tests and their evaluations .

after we raised the issue of missed testing opportunities with the bureau , bureau officials indicated in march 2017 that they would include within its 2018 end - to - end test a sample of in - field canvassing at one of the three test sites which would let it assess the quality of the in - office canvassing in those areas .

citing budget uncertainty , the bureau altered its design for reengineered address canvassing in march 2017 by suspending the second phase of in - office address canvassing .

without the second phase of in - office address canvassing , blocks that are not resolved by phase one will have a greater chance of requiring in - field canvassing .

bureau officials told us that they anticipate that canceling the second phase of in - office address canvassing altogether would increase their estimated in - field canvassing workload by 5 percentage points , from 25 percent to 30 percent of housing units — increasing costs .

we could not independently weigh the costs of continuing this phase , because current bureau estimates do not provide enough details on the specific costs of each phase .

however , based on cost assumptions underlying its october 2015 life - cycle cost estimate we found that the potential addition of 5 percentage points to the field workload alone could reduce the bureau's cost savings by $26.6 million .

the results of the second phase of in - office address canvassing suggested that the operation potentially could reduce fieldwork by a significant amount .

we found that in phase two , prior to its suspension ( september 2016 to march 2017 ) , the bureau reviewed 38,170 blocks that phase one had indicated needed additional review .

we found that the bureau had resolved 25,777 of these geographic areas ( 68 percent ) , meaning those geographic areas would not need to be worked in - field , saving the costs associated with that additional workload .

the bureau did not develop cost and quality information on address canvassing projects , and detailed information on cost tradeoffs was not readily available when we requested it .

what information the bureau did have did not break out the estimated cost of the different phases of in - office address canvassing through 2020 .

however , the totaled estimated cost for both phases one and two was approximately $22 million .

thus , this suspension might save a portion of the $22 million , but it will potentially increase the cost of the address canvassing operation downstream .

for example , the bureau cannot say what $1 saved on suspending the second phase of in - office address canvassing today costs in increased in - field workload down the road , or what the marginal cost of canvassing x percent more in - field is .

additionally , the lack of details in the cost estimate means it is unclear how the potential 5 percentage point increase in workload would impact the potential downstream savings that were factored into the bureau's estimated $900 million savings .

the bureau is continuing to weigh whether and when to resume the second phase of in - office address canvassing .

without cost and quality information , the bureau is limited in its ability to determine what tradeoffs to make as part of this and related future operational decisions .

by canceling the 2017 master address file coverage study , the bureau has limited itself to two evaluations — both of which are delayed — to assess the workload , quality and cost of in - office address canvassing ( i.e. , the 2016 address canvassing test and 2016 master address file coverage study ) .

as a result , the bureau has had limited information to demonstrate the cost and quality of in - office address canvassing along the way and has made major design decisions , such as the suspension of the second phase of in - office address canvassing , without the benefit of information from testing .

we have previously reported on the need for the bureau to rethink its approach to , among other things , testing and evaluating the census .

the bureau agreed , and its early plans for the 2020 census called for the use of incremental , small - scale testing throughout the decade .

such an approach would have been less susceptible to budgetary constraints and better allowed the bureau to demonstrate the effectiveness and accuracy of the full in - office address canvassing operation to congress and stakeholders .

scarce time remains in this decennial cycle to alter testing strategy .

however , as the bureau begins planning for future census operations in the coming years , it will be important to commit to an approach of small - scale , more frequent testing that provides real - time , actionable data to inform decisions .

the bureau includes work that has been put on hold in its measure of completed workload .

in measuring the day - to - day process of the in - office address canvassing operation , the bureau regularly tracks the number of blocks for which it has conducted an initial round of review .

the bureau uses this high - level data to describe what it calls a first - pass of canvassing of all 11 million blocks of the nation .

however , the workload the bureau identifies as completed as part of this first pass includes blocks that have been put on hold .

a block may be put on hold because the bureau did not have sufficient information to identify all the housing units within that block at the time of the review .

for example , this may occur if imagery was blocked by cloud cover .

in these cases , the bureau will need to review the blocks again later to try to resolve them without fieldwork .

as of march 2017 , the bureau reported completing a first pass of 89 percent of all blocks .

however , after accounting for blocks initially placed on hold , the bureau had only completed 79 percent of the workload , and 64 percent of all blocks had been marked as not needing fieldwork .

the bureau has planned additional rounds of review in order to complete the blocks it has placed on hold .

however , the bureau is counting these blocks as completed , according to the bureau's external progress reporting and other documents used to determine staff needs .

by calculating completed workload as including the blocks that will require further review to resolve in - office , the bureau does not have a true indication of how much progress it is making on the operation .

a measure of completed workload that focuses on the amount of in - office workload that has been completed and resolved without need for fieldwork would help the bureau make more informed future staffing decisions .

specifically , the bureau may ultimately have greater fieldwork demands that it will need to staff than its measure is currently showing .

the bureau has additional work to update previously - reviewed blocks .

the bureau also plans additional rounds of in - office review to try to resolve blocks initially flagged for in - field canvassing , which it anticipates will help it to minimize its fieldwork , which it initially estimated would be only 25 percent of housing units .

yet the bureau's estimates do not include projections for how much of the workload will undergo these additional rounds of review .

according to the bureau , these additional rounds of review are essential to reducing fieldwork and ensuring that previously - reviewed blocks are current with housing changes during the decade .

these types of blocks by definition require additional review to be resolved in - office , so by not including estimates for this workload , the bureau may be underestimating the time and effort needed to use in - office canvassing to maximize fieldwork reductions and keep the master address file up to date .

the bureau's average review time measure does not capture the total time it is taking to complete the workload .

the bureau calculates the average time it takes to review blocks so it can estimate daily and monthly production , as well as how it is performing against annual production goals .

the bureau measures the average time each employee spends reviewing each block onscreen at a given point ( 1.02 minutes per block ) .

the bureau uses this measure to track the productivity of individual employees and the day - to - day efficiency of the overall operation .

however , not every one of these reviews results in a block being completed in - office or removed from the later in - field workload , and some of these reviews are duplicate reviews by others as part of the quality control process .

so the actual average review time required to remove blocks in this process from the operation's workload — to complete them — is longer , with significant implications for estimates of how long it will take to complete the work .

when we adjusted the bureau's calculations for the additional reviews as of march 2017 , we found that the average time it takes to complete a block through in - office address canvassing — including resolving blocks put on - hold and conducting any additional quality control reviews — was about 20 percent longer ( 1.22 minutes per block ) .

as discussed below , this difference in average review times may not seem like much , but multiplied over the millions of block reviews that the bureau has remaining , the implications of this difference become significant .

moreover , using a single measure of average review time obscures significant variation across in - office address canvassing outcomes that can affect estimates of future resources needed .

it is likely that the remaining workload disproportionately contains more blocks that will need to be referred to fieldwork , because the bureau prioritized its earlier work based in part on the availability of quality imagery and data , which facilitates resolving blocks in - office .

among blocks completed in - office , we found that the average time per review for blocks referred to the field ( 2.8 minutes per review ) was well over twice as long as the average time spent on blocks removed from fieldwork ( 1.0 minutes per block ) .

if the bureau has disproportionately more blocks in the former category yet to review , the remainder of the operation will likely take longer than the bureau's base measure would suggest , possibly requiring more resources than planned .

in december 2016 , the bureau estimated that employees would spend 6.5 hours per day on in - office canvassing .

the bureau's payroll data offer the ability to include the effect that non - canvassing activities , such as training , systems downtime , and supervisory tasks , have on the productive workday .

when taking these activities into account for production through march 2017 , for example , we found that the average productive workday was less than 6.2 hours per day .

at this rate , the average employee would conduct 19 fewer block reviews per day than what the bureau had assumed .

the bureau had also anticipated in december 2016 that 5 percent of blocks would undergo quality assurance procedures , whereby blocks can receive up to two additional reviews each .

in our march 2017 review of bureau data , however , we found that nearly three times , or 14 percent , of the reviews that the bureau had conducted were part of the quality assurance procedures , meaning that there was more canvassing activity required to complete individual blocks .

if carried forward , the result of these discrepancies is that the bureau would have more remaining in - office canvassing , and would get less done per day , than its estimates show .

at first glance , the above issues can each seem like small differences in measurement , but as table 3 shows , even small differences can add up to large cumulative effects on calculations of workload and estimates of the time to complete it .

in a scenario that updates the bureau's assumptions and accounts for the total number of reviews needed to complete a first pass of the operation , the bureau would have nearly 3 million more reviews and — at estimated production rates and staffing levels — nearly 220 days of production .

moreover , as discussed , the bureau has not indicated how many blocks it estimates will undergo additional rounds of review to capture housing changes during the remainder of the operation .

the bureau has indicated that these additional rounds of review are key to resolving as much workload as possible in - office while ensuring that the master address file stays current with housing changes .

going forward , the bureau will also need to account for the fact that , as discussed above , remaining blocks may be more difficult and time - consuming to complete given that the bureau has previously prioritized the assignment of blocks with more readily available imagery .

best practices for project planning state that organizations should establish estimates that contain information consistent with project requirements to determine effort , cost , and schedule .

we have also noted that when scheduling major initiatives , agencies should include all activities necessary to accomplish a program's objectives .

doing so is critical to verifying that an agency is properly allocating resources and accounting for scheduling risks .

with address canvassing , the risk is not that the bureau will not be able to produce a master address file in time for the 2020 census , but rather that the bureau may have to devote more staff and fieldwork — at greater cost — to complete the operation within needed timeframes .

the bureau had initially planned on resolving 75 percent of housing units in - office , but as of march 2017 the blocks that the bureau had successfully removed from fieldwork account for 51 percent of all housing units .

furthermore , it is uncertain how much more workload the bureau will be able to resolve in - office during the remainder of the operation , particularly since officials have indicated that the bureau is not likely to resume the second - phase of in - office address canvassing .

if the bureau does not adequately gauge the amount of remaining in - office address canvassing workload within its time frames , it risks having to complete more address canvassing in - field .

given the uncertainty about the scope of the remainder of the operation , then , it is important for the bureau to incorporate measures of productivity that reflect the progress and effectiveness of the operation to date .

given the escalating costs of recent censuses , it is critical for the bureau to apply innovative approaches that cost - effectively steward resources and deliver quality results throughout the decennial cycle .

with its design for reengineered address canvassing , the bureau made important strides in the types of data and stakeholders it intended to leverage in order to produce an accurate , cost - effective address list .

however , recent management and testing decisions raise questions about whether the bureau can fully deliver on the bureau's initially estimated gains of the reengineered approach .

the bureau suspended the second phase of in - office address canvassing without detailed data on the cost , value , or accuracy of the operation itself , citing budget uncertainty .

given that the bureau will need to make consequential decisions about the scope and nature of its fieldwork for 2020 , it will be important for the bureau to make use of information on the effectiveness of its reengineered operation , in terms of both the cost and quality of constructing the address list , so that it can justify these decisions and achieve desired outcomes .

our work has also found that the bureau needs more and earlier testing before implementing major design changes , yet the fact that the bureau has completed much of its work on in - office address canvassing without finalizing the evaluations for any related testing leaves questions unanswered about the cost and quality of the operation .

a testing program that as laid out early in the decennial cycle , includes smaller and more flexible test opportunities , which could be less susceptible individually to budget uncertainty , can better position the bureau to support key design decisions and modifications .

in order to achieve savings , the bureau also needs to make more effective use of productivity information from in - office address canvassing and use estimates that reflect the actual time it takes to complete work .

a number of gaps in the bureau's production measures , while individually small , appear to have a large cumulative effect on time and workload calculations that could affect resource allocation decisions .

with the start of 2020 in - field address canvassing scheduled to begin in a little over 2 years , the bureau has limited time to finalize its decisions about the scope and nature of address canvassing .

using measures that reflect the progress and effectiveness of the operation — in addition to the process - oriented measures that the bureau already tracks — can help the bureau make more informed decisions .

we recommend that the secretary of commerce direct the under secretary of the economics and statistics administration and the director of the u.s. census bureau to take the following actions: 1 .

use the bureau's evaluations before 2020 to determine the implications of in - office address canvassing on the cost and quality of address canvassing , and use this information to justify decisions related to its re - engineered address canvassing approach .

2 .

early in the next decennial cycle , plan and execute more flexible , and perhaps smaller , address canvassing test and evaluation activity needed to support key design decisions having significant effect on the cost and quality of the census .

3 .

use productivity measures that track the progress in completing in - office address canvassing workload and the effectiveness of in - office address canvassing in reducing fieldwork in order to make informed decisions on allocating resources to current and future address canvassing workload so that the operation is completed in a timely and cost - effective manner .

we provided a draft of this report to the department of commerce .

in its written comments , reproduced in appendix iii , the department of commerce had no disagreements with gao's findings and recommendations .

in addition , the secretary noted that the department was conducting its own review .

we are sending copies of this report to the secretary of commerce , the under secretary of economic affairs , the acting director of the u.s. census bureau , and interested congressional committees .

the report also will be available at no charge on gao's website at http: / / www.gao.gov .

if you have any questions about this report please contact me at ( 202 ) 512-2757 or goldenkoffr@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

the gao staff that made major contributions to this report are listed in appendix iv .

you asked us to evaluate the u.s. census bureau's ( bureau ) reengineered approach to 2020 address canvassing .

we ( 1 ) described the bureau's design for its reengineered 2020 census address canvassing , ( 2 ) evaluated the extent to which the bureau assessed the cost and quality implications of its reengineered address canvassing approach , and ( 3 ) assessed the status of the bureau's efforts to reduce the in - field address canvassing workload .

for the first objective , we interviewed bureau staff responsible for the bureau's 2020 re - engineered address canvassing approach , and reviewed relevant documentation , such as the bureau's december 2015 operational plan for address canvassing , to highlight design features and innovations affecting cost and quality .

we also observed the 2016 address canvassing test in - field activities to verify our understanding of how in - office canvassing interfaces with in - field canvassing .

given the bureau's timeline for making planning and additional testing decisions , we reported to the bureau in near - real - time any observed implementation challenges experienced during the test with respect to in - field or in - office canvassing .

for the second objective we interviewed officials in the bureau's geography and decennial census management divisions and reviewed relevant documentation to determine how the bureau was managing address canvassing testing , assessing cost and quality implications , making decisions regarding the design of the reengineered address canvassing approach , and managing related risks .

in particular , we followed up with officials regarding updates we received to the operational design as well as project - level and program - level risk registers .

we assessed how the bureau was implementing its stated plans for these efforts .

we also interviewed officials and reviewed documentation , such as available breakdowns of cost estimates , specific to the bureau's estimated cost savings from the reengineered address canvassing approach .

for the third objective we interviewed officials in the bureau's geography division to determine how they are measuring productivity of the in - office address canvassing operation .

we also visited and interviewed officials managing and conducting the bureau's in - office address canvassing production to better understand how bureau employees receive and complete workload assignments and how managers assure quality and track time management data of the operation .

we collected and analyzed bureau production data and payroll data for in - office address canvassing from two as - of dates during the operation — december 2016 and march 2017 — to cover in - office address canvassing since the start of production .

we tested the data quality with both sets of data by interviewing officials responsible for the data ; reviewing manuals , data dictionaries , and related explanatory documentation ; and running calculations to verify the record counts and check the integrity of the data .

we found the bureau's production data to be sufficiently reliable to calculate estimates of time and workload , while we found the bureau's payroll data to be sufficiently reliable to comment on aggregate time spent on the in - office address canvassing operation as a whole ( as opposed to the specific phases of the operation ) .

using the bureau's block - level production data and staffing estimate documentation , we identified different ways to articulate the productivity of the operation , and explored the sensitivity of estimated completion dates and production rates to variation in key productivity assumptions and measures .

we also synthesized the bureau's payroll data for in - office address canvassing to test the bureau's assumptions about staff time allocation .

combined , these activities allowed us to illustrate the effects on time and resource estimates of having accurate productivity measures that focus on the progress and effectiveness of the in - office address canvassing operation in reducing fieldwork .

we did this evaluation in light of best practices for scheduling major projects and outlining resource needs .

we conducted this performance audit from august 2016 to july 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , ty mitchell , assistant director ; devin braun ; robert gebhart ; richard hung ; cynthia saunders ; kayla robinson ; robert robinson , and timothy wexler made key contributions to this report .

