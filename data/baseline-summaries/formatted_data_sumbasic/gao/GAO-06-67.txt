in the 1990s , congress and the executive branch laid out a statutory and management framework for strengthening government performance and accountability .

the government performance and results act of 1993 ( gpra ) was its centerpiece .

the act was designed to provide congressional and executive decision makers with objective information on the relative effectiveness and efficiency of federal programs and spending .

the current administration has made integrating performance information into budget deliberations one of five governmentwide management priorities under its president's management agenda .

a central element of this initiative is the program assessment rating tool ( part ) , designed by the office of management and budget ( omb ) to provide a consistent approach to assessing federal programs in the executive budget formulation process .

part is a standard series of questions meant to serve as a diagnostic tool , drawing on available program performance and evaluation information to form conclusions about program benefits and recommend adjustments that may improve results .

however , part's ability to do this relies on omb's access to credible information on program performance and on policy makers' confidence in the credibility of their analysis .

in our january 2004 review of part , we found that limited availability of credible evidence on program results constrained the ability of omb staff to use part to rate programs' effectiveness .

when omb first applied part , for the fiscal year 2004 budget , it judged fully half the programs it reviewed as not having adequate information on results .

moreover , although omb's assessments recommended improvements in program design , management , and assessment , half the recommendations were to improve program assessment — to identify outcome measures and obtain improved performance data or program evaluations .

to examine progress in improving the evidence base for the part assessments , you asked us to examine 1. progress agencies have made in responding to omb's part recommendations that they obtain program evaluations , 2. factors that facilitated or impeded agencies' progress in obtaining 3. whether the evaluations appear to have been designed to yield the information on program results that omb anticipated .

to answer these questions , we examined progress on 20 of the 40 evaluation recommendations in the president's fiscal year 2004 budget proposal .

these 20 recommendations reflect a diverse array of programs concentrated in the department of energy ( doe ) , the department of health and human services ( hhs ) , the department of labor ( dol ) , and the small business administration ( sba ) .

we reviewed omb and agency documents and interviewed officials in the four agencies to learn the status of the evaluations and the factors that influenced how they were conducted .

we also reviewed the available evaluation plans and reports to assess whether they were likely to yield the desired information on results .

we conducted our review from december 2004 through august 2005 in accordance with generally accepted government auditing standards .

a list of the programs reviewed and their evaluation recommendations appears in appendix i. omb provided written comments on a draft of this report that are reprinted in appendix iii .

part's standard series of questions is designed to determine the strengths and weaknesses of federal programs by drawing on available program performance and evaluation information .

omb applies part's 25 questions to all programs under four broad topics: ( 1 ) program purpose and design , ( 2 ) strategic planning , ( 3 ) program management , and ( 4 ) program results ( that is , whether a program is meeting its long - term and annual goals ) .

during the fiscal year 2004 , 2005 , and 2006 budget cycles , omb applied part to approximately 20 percent of programs each year and gave each program one of four overall ratings: “effective,” “moderately effective,” “adequate,” or “ineffective,” depending on the program's scores on those questions .

omb gave a fifth rating of “results not demonstrated” when it decided that a program's performance information , performance measures , or both were insufficient or inadequate .

the summary assessments published with the president's annual budget proposal include recommended improvements in program design , management , and assessment .

for example , a summary of the review's findings might be followed by the clause “the administration will conduct an independent , comprehensive evaluation of the program,” or “the budget includes to conduct independent and quality evaluations,” both of which we interpreted as an omb recommendation to the agency to conduct such an evaluation .

in our previous analysis of the fiscal year 2004 part reviews , we analyzed over 600 recommendations made for the 234 programs assessed and found that half of those recommended improvements in program assessment .

part not only relies on previous program evaluation studies to answer many of the questions but also explicitly asks , in the strategic planning section , “are independent evaluations of sufficient scope and quality conducted on a regular basis or as needed to support program improvements and evaluate effectiveness and relevance to the problem , interest , or need ? ” program evaluations are systematic studies that assess how well a program is working , and they are individually tailored to address the client's research question .

process ( or implementation ) evaluations assess the extent to which a program is operating as intended .

outcome evaluations assess the extent to which a program is achieving its outcome - oriented objectives ; they focus on program outputs and outcomes but may also examine program processes to understand how outcomes are produced .

omb first applied part to the fiscal year 2004 budget during 2002 , and the assessments were published with the president's budget in february 2003 .

in january 2004 , we reported on omb and agency experiences with part in the fiscal year 2004 budget formulation process .

we noted that part had helped structure omb's use of performance information in its budget review and had stimulated agency interest in budget and performance integration .

however , its effectiveness as a credible , objective assessment tool was challenged by inconsistency in omb staff application of the guidance and limited availability of credible information on program results .

moreover , part's influence on agency and congressional decision making was hindered by failing to recognize differences in focus and issues of interest among the various parties involved in programmatic , policy , and budget decisions .

we noted that part's potential value lay in recommended changes in program management and design but would require sustained attention if the anticipated benefits were to be achieved .

to strengthen part and its use , in our january 2004 report we recommended that omb ( 1 ) centrally monitor and report on agency progress in implementing the part recommendations ; ( 2 ) improve part guidance on determining the unit of analysis , and defining program outcomes and “independent , quality evaluation” ; ( 3 ) clarify expectations regarding agency allocation of scarce evaluation resources among programs ; ( 4 ) target future reviews based on the relative priorities , costs , and risks associated with clusters of programs ; ( 5 ) coordinate assessments to facilitate comparisons and trade - offs between related program ; ( 6 ) consult with congressional committees on performance issues and program areas for review ; and ( 7 ) articulate an integrated , complementary relationship between gpra and part .

requesting that we follow up on the findings in our january 2004 report , you asked that we examine ( 1 ) omb and agency perspectives on the effects of part recommendations on agency operations and results , ( 2 ) omb's efforts at ensuring an integrated relationship between part and gpra , and ( 3 ) steps omb has taken to involve congress in the part process .

a companion report addresses all three objectives — including omb's outreach to congress — with regard to all part reviews .

because of the fundamental role that the availability of program evaluations plays in conducting part assessments , we conducted an in - depth analysis of agencies' responses to omb recommendations that they conduct program evaluations .

these recommendations were identified through the analysis of recommendations for our january 2004 review .

this report focuses on agencies' progress on those evaluations and the issues involved in obtaining them .

for both analyses , we examined the same four agencies' experiences with part .

the four agencies were selected to represent a range of program types ( such as research and regulatory programs ) , large and small agencies , and , for the purposes of this report , a large proportion of the omb evaluation recommendations .

all but two of the programs we reviewed had responded to some extent to omb's recommendations to conduct an evaluation ; agencies did not plan evaluations of the other programs because they were canceled or restructured .

however , after 2 years , only about half the programs had completed evaluations , partly because of lengthy study periods and partly because of some lengthy planning phases .

the evaluations used a variety of study designs , reflecting differences in the programs and in the questions posed about program performance .

about half of the programs we reviewed ( 11 of the 20 ) had completed an evaluation by june 2005 — 2 years after the fiscal year 2004 part reviews and recommendations were published .

four evaluations were in progress , while 3 were still in the planning stage .

agencies did not plan an evaluation of 2 programs because those programs had been canceled or restructured .

 ( see table 1. ) .

most of omb's evaluation recommendations asked for evaluation of the specific program reviewed , while some part reviews at doe and dol asked the agencies to develop a plan for conducting multiple evaluations .

at dol , where two entire regulatory agencies had been assessed , these agencies had completed multiple studies .

omb gave doe seven evaluation recommendations in its fiscal year 2004 part reviews .

six were for research programs in basic science and nuclear energy and one was for its formula grant program to weatherize the homes of low - income families .

since one research program in the office of science had previously been evaluated by a panel of external experts called a committee of visitors , omb explicitly recommended that the other research programs in that office also institute such a process by september 2003 .

in response , doe completed evaluations of five of the six research programs , but it did not plan to evaluate the sixth , the nuclear energy research initiative , because it considered this not a stand - alone program but , rather , a source of funding for follow - up projects to other nuclear energy research programs .

doe revised this program's objective and now authorizes funds for its projects through the other nuclear energy research programs ; thus it is no longer considered a separately funded program to be evaluated .

finally , doe officials indicated that they had only recently gained funding for planning the evaluation of the weatherization assistance program .

 ( a bibliography of related agency evaluation reports appears in app .

ii. ) .

omb gave dol five evaluation recommendations for fiscal year 2004 .

two were for evaluations of specific dol programs: grants to state and local agencies to provide employment - related training to low - income youths and administration of the federal employees compensation act regarding work - related injuries and illnesses .

the three others were regulatory enforcement offices or agencies of dol that were reviewed in their entirety: the office of federal contract compliance programs , regarding equal employment opportunity ; the employee benefits security administration ; and the occupational safety and health administration ( osha ) .

omb recommended that the last , which is a large regulatory agency , develop plans to evaluate the results of its regulatory and nonregulatory programs .

the two dol regulatory administrations each completed several evaluations of their enforcement activities by spring 2005 , as did two of the three other dol programs we reviewed .

dol is waiting to conduct an evaluation of the fifth — the youth employment program — until after its reauthorization because that is expected to result in an increased focus on out - of - school youths and a significant change in program activities .

in addition , osha completed two regulatory “lookback” reviews — assessing the cumulative effects of a regulation over time — one in 2004 and another in 2005 .

program officials indicated that they had developed a plan for conducting lookback reviews of employee benefit regulations beginning in fiscal year 2006 .

omb recommended evaluations for four diverse hhs programs: ( 1 ) grants and technical assistance to states to increase childhood disease immunization , ( 2 ) grants to states to help recently arrived refugees find employment , ( 3 ) education loan repayment and scholarships for nurses in return for serving in facilities facing a nursing shortage , and ( 4 ) direct assistance in constructing sanitation facilities for homes for american indians and alaskan natives .

evaluations of the two state grant programs were still in progress during our review , although an interim report on the immunization program was available .

reports from the two other program evaluations had recently been completed and were under departmental review .

omb recommended evaluations for four sba programs: ( 1 ) support for existing business information centers that provide information and access to technology for small businesses ; ( 2 ) use of volunteer , experienced business executives to provide basic business counseling and training to current and prospective entrepreneurs ; ( 3 ) small business development centers that provide business and management technical assistance to current and prospective entrepreneurs ; and ( 4 ) the small business loan program that provides financing for fixed assets .

omb also asked all three counseling programs to develop outcome - oriented annual and long - term goals and measures .

sba is conducting customer surveys and had recently initiated a comprehensive evaluation of one its counseling programs , and is planning one for the other in fiscal year 2006 .

another evaluation has begun to compare the costs , benefits , and potential duplication of its business loan programs .

sba planned no evaluation of the business information centers program because the program was canceled , partly as a result of the part review and an internal cost allocation study .

in reassessing the need for the program , sba decided that because of the increase in commercially available office supplies and services and the accessibility of personal computers over the years , such a program no longer needed federal government support .

because evaluations are designed around programs and what they aim to achieve , the form of the evaluations reflected differences in program structure and anticipated outcomes .

the evaluations were typically multipurpose , including questions about results as well as the agency processes that managers control in order to achieve those results , and designed to respond to omb and yield actionable steps that programs could take to improve results .

the nursing education loan repayment and scholarship programs aim to increase the recruitment and retention of professional nurses by providing financial incentives in exchange for service in health care facilities that are experiencing a critical shortage of nurses .

the ongoing evaluation of the two programs combined was shaped by the reporting requirements of the nurse reinvestment act of 2002 .

the act requires hhs to submit an annual report to congress on the administration and effect of the programs .

each yearly report is to include information such as the number of enrollees , scholarships , loan repayments and grant recipients , graduates , and recipient demographics to provide a clear description of program beneficiaries .

program beneficiaries are compared with the student , nurse applicant and general populations to assess success in outreach .

information pertaining to beneficiaries' service in health care facilities is important for determining whether program conditions and program goals have been met .

the number of defaulters , default rate , amount of outstanding default funds , and reasons for default are reported for each year .

these data as well as follow - up data on whether beneficiaries remain in targeted facilities after their term of commitment will be important in assessing the overall cost - benefit of the program .

subsequent data collection will establish trends and allow for a cost - benefit analysis in the future .

the indian health service sanitation facilities construction delivers construction and related program services to provide drinking water and waste disposal facilities for american indian and alaska native homes , in close partnership with tribes .

among other issues , the evaluation examined key areas of service delivery , while the health benefits of clean water were assumed .

specifically , project needs identification and project portfolio management were evaluated to see how well construction efforts are prioritized and targeted to areas of greatest need , and whether facilities construction projects are competently designed , timely , and cost - effective .

the completed evaluation recommended that the agency consider integrating its separate data systems into a single portfolio management system to represent all projects or , at least , to adopt standardized project management and financial tracking systems .

the primary responsibility of dol's office of federal contract compliance programs is to implement and enforce rules banning discrimination and establishing affirmative action requirements for federal contractors and subcontractors .

because of the time and expense involved in conducting compliance reviews and complaint investigations , the office is attempting to target establishments for review based in part on an analytic prediction that they will be found to discriminate .

the focus of its effectiveness evaluation , therefore , was on identifying a targeting approach and measuring change in the rate of discrimination among federal contractors during the period of oversight .

the logic for this choice of outcome measure was based on the expectation that overall rates of discrimination would decrease if the oversight programs were effective .

using data on the characteristics of establishments that had already been reviewed , evaluators used statistical procedures to estimate a model of the probability of discrimination .

the coefficients from that model were then used to predict rates of discrimination among contractors who had not been reviewed and among noncontractors .

the analysis showed that the office effectively targeted selected establishments for review , but there was no measurable effect on reducing employment discrimination in the federal contractor workforce overall .

to improve the office's effectiveness , the evaluators recommended that the office focus on establishments with the highest predicted rates of discrimination rather than employ its previous approach , targeting larger establishments that are likely to affect a greater number of workers .

the doe office of science used a peer review approach to evaluating its basic research programs , adapting the committee of visitors model that the national science foundation had developed .

because it is difficult to predict the findings of individual basic research projects , science programs have adapted the peer review model they use for merit selection of projects to evaluate their portfolios of completed ( and ongoing ) research .

the office of science convenes panels of independent experts as external advisers to assess the agency's processes for selecting and managing projects , the balance in the portfolio of projects awarded , and progress in advancing knowledge in the research area and in contributing to agency goals .

panel reviews generally found these programs to be valuable and reasonably well - managed and recommended various management improvements such as standardizing and automating documentation of the proposal review process , adopting program - level strategic planning , and increasing staffing or travel funds to increase grantee oversight .

osha , pursuant to section 610 of the regulatory flexibility act and section 5 of executive order 12866 , must conduct lookback studies on osha standards , considering public comments about rules , the continued need for them , their economic impacts , complexity , and whether there is overlap , duplicity , or conflict with other regulations .

osha recently concluded a lookback review on its ethylene oxide standard and issued a final report on another lookback review that examined the presence sensing device initiation standard for mechanical power presses.a press equipped with a sensing device initiates a press cycle if it senses that the danger zone is empty , and if something should enter the zone , the device stops the press .

accidents with mechanical presses result in serious injuries and amputations to workers every year .

in the sensing device lookback review , osha examined the continued need for the rule , its complexity , complaints levied against the rule , overlap or duplication with other rules , and the degree to which technology , economic conditions , or other factors have changed in the area affected by the rule .

typically , once a standard is selected for a lookback review , the agency gathers information on experience with the standard from persons affected by the rule and from the general public through an announcement in the federal register .

in addition , available health , safety , economic , statistical , and feasibility data are reviewed , and a determination is made about any contextual changes that warrant consideration .

in conducting such reviews , osha determines whether the standards should be maintained without change , rescinded , or modified .

osha found that there was a continued need for the rule but that to achieve the expected benefits of improved worker safety and employer productivity , the rule needed to be changed .

although the technology for sensing device systems had not changed since their adoption in 1988 , the technology for controlling mechanical presses had changed considerably , with press operation now often controlled by computers , introducing hazards that were not addressed initially by the standard .

agency officials described two basic barriers to completing the evaluations that omb recommended: obtaining valid measures of program outcomes to assess effectiveness and obtaining the financial resources to conduct independent evaluations .

although most of the program officials claimed that they had wanted to conduct such evaluations anyway , they noted that the visibility of an omb recommendation brought evaluation to the attention of their senior management , and sometimes evaluation funds , so that the evaluations got done .

indeed , in response to the part reviews and recommendations , two of the agencies initiated strong , centrally led efforts to build their evaluation capacity and prioritize evaluation spending .

to evaluate program effectiveness , agencies needed to identify appropriate measures of the outcomes they intended to achieve and credible data sources for those measures .

however , as noted in our previous report , many programs lacked these and needed to develop new outcome - oriented performance measures in order to conduct evaluations .

agency officials identified a variety of conceptual and technical barriers to measuring program outcomes similar to those previously reported as difficulties in implementing performance reporting under gpra .

sba officials acknowledged that before the part reviews , they generally defined their programs' performance in terms of outputs , such as number of clients counseled , rather than in outcomes , such as gains in small business revenue or employment .

sba revised its strategic plan in fall 2003 and worked with its program partners to develop common definitions across its counseling programs , such as who is the client or what constitutes a counseling session or training .

since sba had also had limited experience with program evaluation , it contracted for assistance in designing evaluations of the economic impact of its programs .

dol had difficulty conceptualizing the outcomes of regulations in monetary terms to produce the cost - benefit analyses that part ( and the regulatory flexibility act ) asks of regulatory programs .

for instance , osha has historically considered the likely controversy of quantifying the value of a human life in calculating cost - benefit ratios for developing worker health and safety regulations .

osha officials explained that the assistant secretary had helped to mitigate such a controversy by issuing a july 2003 memorandum that directed osha staff to identify costs , benefits , net benefits , and the impact of economically significant regulations and their significant alternatives , as well as discuss significant nonmonetized costs and benefits .

dol officials noted that designing a cumulative assessment of the net benefits of employer reporting requirements for pension and health benefit plans was complicated .

for example , a primary benefit of reporting is to aid the agency's ability to enforce other benefit plan rules and thereby protect or regain employees' benefits .

they also pointed out that although health and safety regulations are mandatory , employers are not required to offer benefit plans , so a potential cost of regulators' overreaching in their enforcement actions could be discouraging employers from offering these pension and health benefits altogether .

doe officials acknowledged that they could not continue to use state evaluations to update the national estimates of energy savings from a comprehensive evaluation of weatherization assistance conducted a decade ago .

they recognized that assumptions from the original national evaluation could no longer be supported and that a new , comprehensive national evaluation design was needed .

they noted new hurdles to measuring reductions in home heating costs since the previous evaluation: ( 1 ) monthly electric bills typically do not isolate how much is spent on heating compared with other needs , such as lighting , and ( 2 ) the increased privatization of the utility industry is expected to reduce government access to the utilities' data on individual household energy use .

other barriers were more operational , such as the features of a program's data system that precluded drawing the desired evaluative conclusions .

for one , regulations need to be in place for a period of years to provide data adequate for seeing effects .

hhs officials noted that their databases did not include the patient outcome measures omb asked for and that they would need to purchase a longitudinal study to capture those data .

they also noted that variation in the form of states' refugee assistance programs and data systems , as well as regional variation in refugees' needs , made it difficult to conduct a national evaluation .

their evaluation especially relied on the cooperation of state program coordinators .

dol officials pointed out that the federal employees' compensation program's data system was developed for employee and management needs and did not lend itself to making comparisons with the very different state employee compensation programs .

evaluation generally competes for resources with other program and department activities .

contracts for external program evaluations that collect and analyze new data can be expensive .

in a time of tight resources , program managers may be unwilling to reallocate resources to evaluation .

agencies responded to such limitations by delaying evaluations or cutting back on an evaluation's scope .

some agency officials thought that evaluations should not be conducted for all programs but should be targeted instead to areas of uncertainty .

hhs's office of refugee resettlement — which was allotted funds especially for its evaluation — is spending $2 million to evaluate its refugee assistance program over 2 years .

costs are driven primarily by the collection of data through surveys , interviews , and focus groups and the need for interpreters for many different languages .

given the size and scope of the program , even with $2 million , program officials would have liked to have more time and money to increase the coverage of their national program beyond the three sites they had selected .

dol program officials explained that although they had had a large program evaluation organization two decades ago , the agency downsized in 1991 , the office was eliminated , and now they must search for program evaluation dollars .

the program spent $400,000 for an 18-month evaluation of the federal employees compensation act program , which relied heavily on program administrative data , but they also spent a large amount of staff time educating and monitoring the contractor .

program officials were disappointed with the lack of depth in the evaluation .

they believed that their evaluation contractor did not have enough time to plan and conduct a systematic survey , and consequently , their selective interview data were less useful than they would have liked .

doe program officials indicated that they have been discussing an evaluation of weatherization assistance since spring 2003 , but not having identified funds for an evaluation , they have not been able to develop a formal evaluation plan .

they had no budget line item for evaluation , so they requested one in their fiscal year 2005 appropriations .

although there was congressional interest in an evaluation , additional funds were not provided in fiscal year 2005 .

doe instructed program officials to draw money for evaluation from the 10 percent of the program's funds that are set aside for training and technical assistance , increase the federal share from 1.5 percent to 2 percent , and reduce the states' share to 8 percent .

program officials indicated that the amount from the technical assistance account would cover only planning and initial implementation activities , not the bulk of the evaluation itself .

and they were concerned about displacing existing training , so they were still looking for an evaluation funding commitment .

agency officials also questioned part's assumption that all programs should have evaluations .

sba officials indicated that some agency appropriations generally precluded sba's spending program funds on any but specifically identified program activities .

thus , evaluations had to be funded from agency administrative funds .

they thought that it was unreasonable to ask a small agency to finance several program evaluations , as might be expected of a larger agency .

sba dealt with this by conducting evaluations sequentially as funds became available .

dol program officials also thought that spending several hundred thousand dollars for a comprehensive evaluation study was a reasonable investment for a $2.5 billion program but not for small programs .

they did not believe that all programs need to be evaluated — especially in a time of budget deficits .

they recommended that omb and agencies should “pick their shots” and should be more focused in choosing evaluations to conduct .

they suggested a risk - based approach , giving higher priority to evaluating programs for which costs are substantial and effectiveness uncertain .

most of the agency officials we interviewed declared that they valued evaluation .

for example , hhs and doe officials described evaluation as part of their culture .

many said they had already been planning to do something similar to the evaluation that omb had recommended .

in a couple of cases , omb's recommendation appeared to have been shaped by planned or ongoing activities .

however , officials in all four agencies indicated that the visibility of a part recommendation and associated omb pressure brought management attention , and sometimes funds , to getting the evaluations done .

hhs departmental officials said that the agency was a federal leader in terms of evaluation capacity , and that they spend approximately $2.6 billion a year on agency - initiated research , demonstrations , and evaluation .

they stated that it is part of their culture to conduct evaluations — because their program portfolio is based in the physical and social sciences .

doe officials said that they embraced the part process because , as an agency with a significant investment in advancing science and technology , doe had already been using similar processes , such as peer review , to evaluate its programs .

doe officials noted that doe had developed a basic evaluation mechanism — independent peer review — that all its research programs undertake .

officials in the office of energy efficiency and renewable energy developed a corporate peer review guide summarizing best practices in this field and considered their peer review process as “state of the art,” as it is used as a model nationally and globally .

in other cases , agency or congressional interest in evaluation seemed to set the stage for omb evaluation recommendations .

for example , while omb was reviewing the nursing education loan repayment program , the nursing reinvestment act of 2002 was enacted , expanding the program and instituting a requirement for annual reports after the first 18 months .

the reports were to include data on the numbers of loan applicants and enrollees , the types of facilities they served in , and the default rates on their loans and service commitments and an evaluation of the program's overall costs and benefits .

omb then recommended that the agency evaluate the program's impact , develop outcome measures , and begin to track performance against newly adopted benchmarks .

to respond to omb's request for a long - term outcome measure , the agency agreed to also collect information on how long beyond their service commitment nurses stay in service in critical shortage facilities .

in another example previously discussed , the doe office of science had already initiated committee of visitors reviews for its basic energy sciences program , which omb then recommended for other research programs in that office .

the part and president's management agenda pressed agencies to report progress on the recommendations .

omb published the cumulative set of completed part review summaries , including the recommendations , in the president's budget proposals for fiscal years 2004 through 2006 .

in the fiscal year 2006 budget , omb reported on the status of its previous recommendations in the part summaries , whether action had been taken or completed .

omb also asked agencies to report on their progress in implementing part recommendations to provide input into its quarterly scorecards on agencies' progress in implementing the president's management agenda initiatives .

in addition , omb precluded agencies from being scored “green” on budget and performance integration if more than 10 percent of their programs were rated “results not demonstrated” 2 years in a row .

doe and dol program officials reported being asked to update the status of the recommendations every 2 to 3 months .

hhs officials noted that since fall 2004 , they have been reporting on part recommendations to omb twice a year , tracking approximately 100 part recommendations ( with about 200 separate milestones ) for the 62 programs reviewed for fiscal years 2004 through 2006 .

most of the officials we interviewed believed that because of part and the president's management agenda , their agencies were paying greater attention to program results and evaluation .

officials at dol noted that the department spends much time and effort making sure it scores green on the next president's management agenda assessment ; for example , the department's management review board , chaired by labor's assistant secretary for management and administration , discusses these issues monthly .

in addition , dol's center for program planning and results reviews programs' progress on omb's recommendations , scores programs internally on the budget and performance integration scorecard , and provides agencies with training and preparation before their part reviews .

the sba administrator initiated a series of steps after august 2003 to increase the agency's focus on achieving results .

sba rewrote its strategic plan to focus on a limited number of strategic goals and integrated its strategic plan , annual performance plan , and performance report .

the agency formed a central office of analysis , planning , and accountability to help each program office develop results - oriented performance measures and conduct program assessments .

although hhs officials said that the department had invested in evaluation long before the part reviews , indian health service program officials indicated that they had not planned an evaluation of their sanitation facilities program before the part review .

however , they thought it was a good idea and said that the recommendation brought their lack of a recent evaluation to hhs's attention , making it easier to justify efforts to quantify their program's benefits .

sba and dol responded to demands for more performance information by centrally coordinating their assessment activities , helping to address evaluation's measurement and funding challenges .

centralization helped the agencies to leverage their evaluation expertise throughout the agency and helped them prioritize spending on the evaluations they considered most important .

sba program offices had little experience with outcome measurement and evaluation before the 2002 part reviews .

the central planning office was formed to help the program offices develop outcome measures linked to the agency's strategic goals and collect and validate their performance data .

the office also conducts an annual staff activity survey to support cost allocation across programs , a key step toward performance budgeting .

this office took advantage of the similarity in outcome goals across sba's programs and the evaluation methodology developed for the counseling programs to contract for the development of a standard methodology for assessing other sba programs' economic impacts on small businesses .

the central office is also funding the subsequent evaluations .

for a small agency , this type of coordination can result in important savings in contract resources as well as staff time .

dol , much larger than sba , has measurement and evaluation experience , but capacity had declined over time .

dol established the center for program planning and results in 2001 to provide leadership , policy advice , and technical assistance to gpra - related strategic and performance planning .

the center was expanded in fiscal year 2003 to respond to the president's management agenda and manage the part process .

with a budget of $5 million a year , the center solicits and selects evaluation proposals focusing on program effectiveness submitted by dol's component agencies , funds the studies , and helps oversee the external contractors .

the center's officials claimed that the secretary's and assistant secretary's support for evaluation , combined with pressure from omb , has led to increased interest by the component agencies in evaluation , resulting in $6 million to $7 million in proposals competing for $5 million in evaluation funds .

some dol agencies retained their evaluation expertise and design , fund , and oversee their own evaluations .

in addition to helping program offices develop research questions and evaluation designs , the center helps develop agency evaluation capacity by holding “vendor days,” when evaluation contractors are invited to exhibit for agency staff the specialized design , data collection , and analysis skills that could inform future studies .

because the omb evaluation recommendations were fairly general , agencies had flexibility in interpreting the information omb expected and the evaluations to fund .

some program managers disagreed with omb on the scope and purpose of their evaluations , their quality , and the usefulness of evaluations conducted by independent third parties .

program managers concerned about an increased focus on process said that they were more interested in learning how to improve program performance than in meeting an omb checklist .

since a few programs did not discuss their evaluation plans with omb , it is not certain whether omb will accept their ongoing evaluations .

agencies had a fair amount of flexibility to design their evaluations .

except for the recommendations to the doe office of science to conduct committee of visitors reviews , omb's evaluation recommendations were fairly general , typically telling agencies to conduct an independent evaluation of a program's effectiveness .

agencies reported little guidance from omb on how to conduct these evaluations , beyond the part written guidance and the rationale the examiner provided for not accepting their previous evaluations or measures of program outcomes .

they said that follow - up on previous part recommendations was generally limited to providing responses to the omb reporting template , unless omb conducted a second formal part review .

agencies also had flexibility to determine the timing of their evaluations .

agency officials reported that omb did not prioritize its recommendations within or among programs .

moreover , because evaluation resources were limited , dol and sba officials reported that they had to choose which evaluations to conduct first .

the recommendations for the two dol regulatory agencies explicitly acknowledged their need to balance responsibility for several programs .

omb asked these agencies to develop plans to evaluate their programs or expand existing efforts for more comprehensive and regular evaluation .

in the reviews of recommendation status for the fiscal year 2006 budget , omb credited both agencies with having conducted one or more program reviews and planning others .

agencies were free to choose which programs to evaluate but were likely to be influenced by the potential effect of part reassessments on their president's management agenda scores and , thus , to attempt to reduce the number of programs rated “results not demonstrated.” research and development programs were held to a somewhat higher standard than other programs were , since their agencies could not be scored “green” on the separate r&d investment criteria initiative if less than 75 percent of their programs received a score of “moderately effective” or better .

doe officials noted that their office of energy efficiency and renewable energy now requires programs to outline their plans for evaluations in their multiyear plans .

omb and the agencies significantly differed in defining evaluation scope and purpose .

program officials were frustrated by omb's not accepting their prior evaluations of program effectiveness in the part review .

some of the difficulties seemed to derive from omb expecting to find , in the agencies' external evaluation studies , comprehensive judgments about program design , management , and effectiveness , like the judgments made in the omb part assessments .

part's criteria for judging the adequacy of agency evaluations are complex and may have created some tension as to the importance of one dimension over another .

for example , question 2.6 read: “are independent evaluations of sufficient scope and quality conducted on a regular basis or as needed to support program improvements and evaluate effectiveness and relevance to the problem , interest , or need ? ” omb changed the wording of the question to help clarify its meaning and added the reference to “relevance.” however , while omb's revised guidance for this question defines quality , scope , and independence , it does not address the assessment of program “relevance.” specifically , sufficient scope is defined as whether the evaluation focuses on achievement of performance targets and the cause and effect relationship between the program and target — i.e. , program effectiveness .

this is different from assessing the relevance — i.e. , appropriateness — of the program design to the problem or need .

instead , questions in section 1 ask whether the design is free of major flaws and effectively targeted to its purpose .

another potential contribution to differences between omb and agency expectations for program evaluations is that evaluations designed for internal audiences often have a different focus than evaluations designed for external audiences .

evaluations that agencies initiate typically aim to identify how to improve the allocation of program resources or the effectiveness of program activities .

studies requested by program authorizing or oversight bodies are more likely to address external accountability — to judge whether the program is properly designed or is solving an important problem .

hhs officials reported differences with omb over the acceptability of hhs evaluations .

hhs officials were particularly concerned that omb sometimes disregarded their studies and focused exclusively on omb's own assessments .

one program official complained that omb staff did not adequately explain why the program's survey of refugees' economic adjustment did not qualify as an “independent , quality evaluation,” although an experienced , independent contractor conducted the interviews and analysis .

in the published part review , omb acknowledged that the program surveyed refugees to measure outcomes and monitored grantees on - site to identify strategies for improving performance .

in our subsequent interview , omb staff explained that the outcome data did not show the mechanism by which the program achieved these outcomes , and grantee monitoring did not substitute for obtaining an external evaluation , or judgment , of the program's effectiveness .

other hhs officials said that omb had been consistent in applying the standards for independent evaluation , but these standards were set extremely high .

in reviewing the vaccination program , omb did not accept the several research and evaluation studies offered , since they did not meet all key dimensions of “scope.” omb acknowledged that the program had conducted several management evaluations to see whether the program could be improved but found their coverage narrow and concluded “there have previously been no comprehensive evaluations looking at how well the program is structured / managed to achieve its overall goals.” omb also did not accept an external institute of medicine evaluation of how the government could improve its ability to increase immunization rates because the evaluation report had not looked at the effectiveness of the individual federal vaccine programs or how this program complemented the other related programs .

however , in reviewing recommendation status , omb credited the program with having contracted for a comprehensive evaluation that was focused on the operations , management , and structure of this specific vaccine program .

doe office of science officials described much discussion with omb examiners about what was or was not a good committee of visitors review in following up on the status of the evaluation recommendations .

although omb had revised and extended its guidance on what constituted quality in evaluation , program officials still found this guidance difficult to apply to research programs .

they also acknowledged that their first committee of visitors reviews might have been more useful to the program than to omb .

omb and agencies differed in identifying which evaluation methods were sufficiently rigorous to provide high - quality information on program effectiveness .

omb guidance encouraged the use of randomized controlled trials , or experiments , to obtain the most rigorous evidence of program impact but also acknowledged that these studies are not suitable or feasible for every program .

however , as described above , without guidance on which — and when — alternative methods were appropriate , omb and agency staff disagreed on whether specific evaluations were of acceptable quality .

to help develop shared understandings and expectations , federal evaluation officials and omb staff held several discussions on how to assess evaluation quality according to the type of program being evaluated .

when external factors such as economic or environmental conditions are known to influence a program's outcomes , an impact evaluation attempts to measure the program's net effect by comparing outcomes with an estimate of what would have occurred in the absence of the program intervention .

a number of methodologies are available to estimate program impact , including experimental and quasi - experimental designs .

experimental designs compare the outcomes for groups that were randomly assigned to either the program or to a nonparticipating control group prior to the intervention .

the difference in these groups' outcomes is believed to represent the program's impact , assuming that random assignment has controlled for any other systematic difference between the groups that could account for any observed difference in outcomes .

quasi - experimental designs compare outcomes for program participants with those of a comparison group not formed through random assignment , or with participants' experience prior to the program .

systematic selection of matching cases or statistical analysis is used to eliminate any key differences in characteristics or experiences between the groups that might plausibly account for a difference in outcomes .

randomized experiments are best suited to studying programs that are clearly defined interventions that can be standardized and controlled , and limited in availability , and where random assignment of participants and nonparticipants is deemed feasible and ethical .

quasi - experimental designs are also best suited to clearly defined , standardized interventions with limited availability , and where one can measure , and thus control for , key plausible alternative explanations for observed outcomes .

in mature full - coverage programs where comparison groups cannot be obtained , program effects may be estimated through systematic observation of targeted measures under specially selected conditions designed to eliminate plausible alternative explanations for observed outcomes .

following our january 2004 report recommendation that omb better define an “independent , quality evaluation,” omb revised and expanded its guidance on evaluation quality for the fiscal year 2006 part reviews .

the guidance encouraged the use of randomized controlled trials as particularly well suited to measuring program impacts but acknowledged that such studies are not suitable or feasible for every program , so it recommended that a variety of methods be considered .

omb also formed an interagency program evaluation working group in the summer of 2004 to provide assistance on evaluation methods and resources to agencies undergoing a part review that discussed this guidance extensively .

evaluation officials from several federal agencies expressed concern that the omb guidance materials defined the range of rigorous evaluation designs too narrowly .

in the spring of 2005 , representatives from several federal agencies participated in presentations about program evaluation purposes and methods with omb examiners .

they outlined the types of evaluation approaches they considered best suited for various program types and questions ( see table 2 ) .

however , omb did not substantively revise its guidance on evaluation quality for the fiscal year 2007 reviews beyond recommending that “agencies and omb should consult evaluation experts , in - house and / or external , as appropriate , when choosing or vetting rigorous evaluations. .

a related source of tension between omb and agency evaluation interests was the importance of an evaluation's independence .

part guidance stressed that for evaluations to be independent , nonbiased parties with no conflict of interest , for example , gao or an inspector general , should conduct them .

omb subsequently revised the guidance to allow evaluations to be considered independent if the program contracted them out to a third party or they were carried out by an agency's program evaluation office .

however , disagreements continued on the value and importance of this criterion .

hhs officials reported variation among examiners in whether their evaluations were considered independent .

two programs objected to omb examiners' claims that an evaluation was not independent if the agency paid for it .

omb changed the fiscal year 2005 part guidance to recognize evaluations contracted out to third parties and agency program evaluation offices as possibly being sufficiently independent , subject to examination case by case .

but hhs officials claimed that they were still having issues with the independence standard in the fiscal year 2006 reviews and that omb's guidance was not consistently followed from one examiner to the next .

dol program officials stated that using an external evaluator who was not familiar with the program resulted in an evaluation that was not very useful to them .

in part , this was because program staff were burdened with educating the evaluator .

but more important , they claimed that the contractor designed the scope of the work to the broad questions of part ( such as questions on program mission ) rather than focusing on the results questions the program officials wanted information on .

in combination , this led to a relatively superficial program review , in their view , that provided the external , independent review omb wanted but not the insights the program managers wanted .

in reviewing the status of its part recommendations , omb did not accept advisory committee reviews for two research programs that doe offered in response because omb did not perceive the reviews as sufficiently independent .

these two program reviews involved standing advisory committees of approximately 50 people who review the programs every 3 years .

the omb examiner believed that the committee was not truly independent of the agency .

doe program officials objected , noting the committee's strong criticisms of the program , but have reluctantly agreed to plan for an external review by the national academies .

program officials expressed concern that because evaluators from the national academies may not be sufficiently familiar with their program and its context , such reviews may not address questions of interest to them about program performance .

hhs program officials were also concerned about the usefulness of an evaluation of the sanitation facilities program if it was conducted by a university - based team inexperienced with the program .

the agency deliberately guarded against this potential weakness by including two ex - agency officials ( one an engineer ) on the evaluation team , and by taking considerable effort with the team to define the evaluation questions .

agencies' freedom to design their evaluations , combined with differences in expectations between agencies and omb , raises the strong possibility that the evaluations that agencies conduct may not provide omb with the information it wants .

most of the agency officials we interviewed said that they had discussed their evaluation plans with their omb examiners , often as part of their data collection review process .

sba and dol , in particular , appeared to have had extensive discussions with their omb examiners .

however , a few programs have not discussed their plans with omb , presumably on the assumption that they will meet omb's requirements by following its written guidance .

officials in sba's and dol's central planning offices described extensive discussions of their evaluation plans with their omb examiners .

sba vetted the evaluation design for sba's counseling programs with omb in advance , as well as the questionnaire used to assess client needs .

dol planning and evaluation officials noted that they had worked with omb examiners to moderate their expectations for agencies' evaluations .

they said that omb understands their “real world” financial constraints and is allowing them to “chip away” at their outcome measurement issues and not conduct net impact evaluations in program areas where they do not have adequate funds to do this type of evaluation .

hhs program officials were concerned about whether omb will accept their ongoing evaluation of the immunization program when they receive their next part review .

the evaluation recommendation was general , so they based their design on the fiscal year 2004 criteria and to provide information useful to the program .

however , the officials had heard that the fiscal year 2007 evaluation quality criteria were more rigid than those previously used , so they were concerned about whether the program will meet omb's evaluation criteria when it is reviewed again .

they said they would have liked omb to consider its evaluation progress and findings so far and to have given them input as to whether the evaluation will meet the current criteria .

omb officials denied that the part criteria for evaluation quality had changed much in the past two years .

they also expected , from their review of the design , that this new evaluation would meet current part criteria , assuming it was carried out as planned .

several program officials expressed the view that in designing their evaluations , they were more concerned with learning how to improve their programs than in meeting an omb checklist .

program officials complained that omb's follow - up on whether evaluations were being planned sent the message that omb was more interested in checking off boxes than in having a serious discussion about achieving results .

when one program official was asked for the program's new evaluation plan , he answered “who needs a plan ? .

i've got an evaluation.” doe program officials indicated that they believe a comprehensive evaluation of weatherization assistance should include all the questions that state , regional , and local officials would like to ask and not just establish a new national energy savings estimate .

those questions — also of interest to doe — include: which weatherization treatments correlate with energy savings ? .

should they use their own crews or hire contractors ? .

what are the nonenergy benefits , such as improved air quality or employment impacts ? .

program officials indicated that they had conducted a great deal of planning and discussion with their stakeholders over the past 5 to 6 months and expect to conduct five or six studies to meet those needs .

the part review process has stimulated agencies to increase their evaluation capacity and available information on program results .

the systematic examination of the array of evidence available on program performance has helped illuminate gaps and has helped focus evaluation questions .

the public visibility of the results of the part reviews has brought management attention to the development of agency evaluation capacity .

evaluations are useful to specific decision makers to the degree that the evaluations are credible and address their information needs .

agencies are likely to design evaluations to meet their own needs — that is , in - depth analyses that inform program improvement .

if omb wants evaluations with a broader scope , such as information that helps determine a program's relevance or value , it will need to take steps to shape both evaluation design and execution .

because agency evaluation resources tend to be limited , they are most usefully focused on illuminating important areas of uncertainty .

while regular performance reporting is key to good program management and oversight , requiring all federal programs to conduct frequent evaluation studies is likely to result in many superficial reviews that will have little utility and that will overwhelm agency evaluation capacity .

in light of our findings and conclusions in this report , we are making the following recommendations to omb reiterating and expanding on recommendations in our previous report: omb should encourage agencies to discuss their plans for program evaluations — especially those in response to an omb recommendation — with omb and with congressional and other program stakeholders to ensure that their findings will be timely , relevant , and credible and that they will be used to inform policy and management decisions .

omb should engage in dialogue with agencies and congressional stakeholders on a risk - based allocation of scarce evaluation resources among programs , based on size , importance , or uncertain effectiveness , and on the timing of such evaluations .

omb should continue to improve its part guidance and training of examiners on evaluation to acknowledge a wide range of appropriate methods .

we provided a draft of this report to omb and the agencies for review and comment .

omb agreed that evaluation methodology should be appropriate to the size and nature of the program and that randomized controlled trials may not be valuable in all settings .

it noted its intent to provide additional guidance in this area .

omb disagreed with the reference to the part as a checklist .

this view was not ours but the view of agency officials who expressed concern about the focus of the assessment process .

omb also provided a number of technical comments , which we incorporated as appropriate throughout the report .

omb's comments appear in appendix iii .

we also received technical comments from doe , dol , and hhs that we incorporated where appropriate throughout the report .

sba had no comments .

we are sending copies of this report to the director of the office of management and budget ; the secretaries of energy , labor , and health and human services ; the administrator of the small business administration ; appropriate congressional committees ; and other interested members of congress .

we will also make copies available to others on request .

in addition , the report will be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have questions about this report , please contact me at ( 202 ) 512-2700 or kingsburyn@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iv .

advanced fuel cycle initiative: nuclear energy research advisory committee ( nerac ) evaluation subcommittee .

evaluation of doe nuclear energy programs .

washington , d.c.: sept. 10 , 2004 .

advanced scientific computing research program: advanced scientific computing research .

committee of visitors report .

washington , d.c.: april 2004 .

generation iv nuclear energy systems initiative: nuclear energy research advisory committee ( nerac ) evaluation subcommittee .

evaluation of doe nuclear energy programs .

washington , d.c.: sept. 10 , 2004 .

high energy physics program: committee of visitors to the office of high energy physics .

report to the high energy physics advisory panel .

washington , d.c.: apr .

7 , 2004 .

nuclear physics program: committee of visitors .

report to the nuclear science advisory committee .

washington , d.c.: department of energy , office of science , feb. 27 , 2004 .

317 immunization program: rti international .

section 317 grant immunization program evaluation: findings from phase i .

draft progress report .

atlanta , ga.: centers for disease control and prevention , january 2005 .

indian health service sanitation facilities program: department of health and human services , u.s. public health service , federal occupational health service .

independent evaluation report summary .

prepared for indian health service sanitation facilities construction program , rockville , maryland .

seattle , wash.: mar .

8 , 2005 .

nursing education loan repayment and scholarship program: department of health and human services , health resources and services administration , bureau of health professions .

hrsa responds to the nursing shortage: results from the 2003 nursing scholarship program and the nursing education loan repayment program: 2002 – 2003 .

first report to the united states congress .

rockville , md .

: n.d .

employee benefits security administration reports: mathematica policy research , inc. case opening and results analysis ( cora ) fiscal year 2002: final report .

washington , d.c.: mar .

31 , 2004 .

royal , dawn .

u.s. department of labor , employee benefits security administration: evaluation of ebsa customer service programs participant assistance program customer evaluation .

washington , d.c.: the gallup organization , february 2004 .

royal , dawn .

u.s. department of labor , employee benefits security administration: evaluation of ebsa customer service programs participant assistance mystery shopper evaluation .

washington , d.c.: the gallup organization , january 2004 .

royal , dawn .

u.s. department of labor , employee benefits security administration: evaluation of ebsa customer service programs participant assistance outreach programs evaluation .

washington , d.c.: the gallup organization , january 2004 .

royal , dawn .

u.s. department of labor , employee benefits security administration: evaluation of ebsa customer service programs participant assistance web site evaluation .

washington , d.c.: the gallup organization , january 2004 .

federal employees compensation act program: icf consulting .

federal employees compensation act ( feca ) : program effectiveness study .

fairfax , va.: u.s. department of labor , office of workers' compensation programs , mar .

31 , 2004 .

office of federal contract compliance programs: westat .

evaluation of office of federal contract compliance programs: final report .

rockville , md .

: december 2003 .

occupational safety and health administration reports: erg .

evaluation of osha's impact on workplace injuries and illnesses in manufacturing using establishment – specific targeting of interventions .

final report .

lexington , mass .

: july 23 , 2004 .

marker , david and others .

evaluating osha's national and local emphasis programs .

draft final report for quantitative analysis of emphasis programs .

rockville , md .

: westat , dec. 24 , 2003 .

osha , directorate of evaluation and analysis .

regulatory review of osha's presence sensing device initiation ( psdi ) standard [29 cfr 1910.217 ( h ) ] .

washington , d.c.: may 2004. www.osha.gov / dcsp / compliance_assistance / lookback / psdi_final2004.ht ml ( oct. 21 , 2005 ) .

in addition to the contact named above , stephanie shipman , assistant director , and valerie caracelli made significant contributions to this report .

denise fantone and jacqueline nowicki also made key contributions .

performance budgeting: part focuses attention on program performance , but more can be done to engage congress .

gao - 06-28 .

washington , d.c.: oct. 28 , 2005 .

managing for results: enhancing agency use of performance information for managerial decision making .

gao - 05-927 .

washington , d.c.: sept. 9 , 2005 .

21st century challenges: performance budgeting could help promote necessary reexamination .

gao - 05-709t .

washington , d.c.: june 14 , 2005 .

performance measurement and evaluation: definitions and relationships .

gao - 05-739sp .

washington , d.c.: may 2005 .

results - oriented government: gpra has established a solid foundation for achieving greater results .

gao - 04-38 .

washington , d.c.: mar .

10 , 2004 performance budgeting: observations on the use of omb's program assessment rating tool for the fiscal year 2004 budget .

gao - 04-174 .

washington , d.c.: jan. 30 , 2004 .

program evaluation: an evaluation culture and collaborative partnerships help build agency capacity .

gao - 03-454 .

washington , d.c.: may 2 , 2003 .

program evaluation: strategies for assessing how information dissemination contributes to agency goals .

gao - 02-923 .

washington , d.c.: sept. 30 , 2002 .

program evaluation: studies helped agencies measure or explain program performance .

gao / ggd - 00-204 .

washington , d.c.: sept. 29 , 2000 .

performance plans: selected approaches for verification and validation of agency performance information .

gao / ggd - 99-139 .

washington , d.c.: july 30 , 1999 .

managing for results: measuring program results that are under limited federal control .

gao / ggd - 99-16 .

washington , d.c.: dec. 11 , 1998 .

