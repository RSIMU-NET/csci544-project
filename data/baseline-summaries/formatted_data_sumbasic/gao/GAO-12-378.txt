in 2005 , hurricane katrina devastated the gulf coast , damaging critical infrastructure , such as oil refineries ; electric power lines ; water mains ; and cellular phone towers .

more recently , in 2011 , a major earthquake and related tsunami devastated eastern japan , damaging critical infrastructure , such as dams , buildings , and power plants .

the damage and resulting chaos disrupted government and business functions alike , producing cascading effects far beyond the physical location of these events .

threats against critical infrastructure are not limited to natural disasters , as demonstrated by the terrorist attacks of september 11 , 2001 , and the 2005 suicide bombings in london where terrorists disrupted the city's transportation and mobile telecommunication infrastructure .

in march 2007 , we reported that our nation's critical infrastructures and key resources ( cikr ) — assets and systems , whether physical or virtual , so vital to the united states that their incapacity or destruction would have a debilitating impact on national security , national economic security , national public health or safety , or any combination of those matters — continue to be vulnerable to a wide variety of threats .

because the private sector owns the vast majority of the nation's cikr — banking and financial institutions , commercial facilities , telecommunications networks , and energy production and transmission facilities , among others — it is vital that the public and private sectors work together to protect these assets and systems .

in 2006 , the department of homeland security ( dhs ) issued the national infrastructure protection plan ( nipp ) , which provides the overarching approach for integrating the nation's cikr protection and resilience initiatives into a single national effort .

the nipp sets forth a risk management framework and details the roles and responsibilities for dhs and other federal , state , regional , local , tribal , territorial , and private sector partners implementing the nipp .

the nipp also outlines the roles and responsibilities of sector - specific agencies ( ssa ) , the various federal departments and agencies that are responsible for cikr protection and resilience activities in 18 specific cikr sectors , such as the chemical , commercial facilities , energy , and transportation sectors .

the nipp emphasizes the importance of collaboration and partnering with and among the various partners and voluntary information sharing between the private sector and dhs .

consistent with the nipp , dhs issues the national critical infrastructure and key resources protection annual report ( nar ) that summarizes risk mitigation and resiliency activities across dhs and the 18 cikr sectors .

further , as part of its risk management strategy , dhs has established the national critical infrastructure prioritization program ( ncipp ) , which uses a tiered approach to identify nationally significant cikr each year .

this high - priority cikr are categorized as either level 1 or level 2 based on the consequence to the nation in terms of loss of life or economic impact .

the levels are used to enhance decision making related to cikr protection and can include a range of businesses or assets in a local geographic area , such as refineries , water treatment plants , or commercial facilities , as well as the information and data systems that ensure their continued operation .

according to dhs , the overwhelming majority of the assets and systems identified as high priority are classified as level 2 .

only a small subset of assets meet the level 1 consequence threshold — those whose loss or damage could result in major national or regional impacts similar to the impacts of hurricane katrina or the september 11 , 2001 , attacks .

within dhs , the office of infrastructure protection ( ip ) in the national protection and programs directorate ( nppd ) is responsible for cikr protection and resilience .

dhs coordinates through ssas and other sector partners for each of the cikr sectors to identify security overlaps and gaps as they implement the nipp framework .

while other entities may possess and exercise regulatory authority over cikr to address security , such as for the chemical , transportation , and nuclear sectors , ip generally relies on voluntary efforts to secure cikr because of its limited authority to directly regulate most cikr .

the voluntary efforts include the enhanced critical infrastructure protection ( ecip ) security surveys and site assistance visit ( sav ) vulnerability assessments dhs conducts at assets across the 18 sectors .

ecip security surveys are voluntary half to full - day surveys dhs conducts to assess overall asset security and increase security awareness , the results of which are presented to cikr owners and operators in a way that allows them to see how their assets' security measures compare to those of similar assets in the same sector .

ecip security survey results do not provide assets with recommendations or options to enhance protective measures .

participation in sav vulnerability assessments is also voluntary and these assessments can take up to 3 days to complete .

these assessments identify security gaps at assets and are used to provide options to enhance protective measures and resilience to cikr owners and operators .

dhs field representatives , called protective security advisors ( psa ) , are responsible for working with cikr owners and operators to conduct these surveys and assessments .

as of july 2011 , there were 88 psa positions in various locations throughout the country .

dhs shares information on the results of these efforts with various stakeholders , generally on a need - to - know basis .

given the voluntary nature of dhs's cikr program , the importance of collaboration and partnering with and among the various partners , and the need to identify and mitigate security and resilience gaps , you asked that we examine dhs efforts to manage and measure the impact of its voluntary security survey and vulnerability assessment programs .

specifically , we assessed the extent to which dhs has taken action to conduct security surveys and vulnerability assessments among high - priority cikr to improve their security postures ; shared the results of security surveys and vulnerability assessments with asset owners and operators and ssas ; and assessed the effectiveness of the security survey and vulnerability assessment programs and identified actions , if any , to improve management of these programs .

to meet our objectives , we reviewed applicable laws , regulations , and directives as well as ip policies and procedures for conducting security surveys and vulnerability assessments , providing their results , and assessing the effectiveness of these programs .

we also interviewed ip officials in washington , d.c. , responsible for administering these programs and obtained and assessed ip data on the conduct and management of its security surveys and vulnerability assessments .

in so doing , we ( 1 ) compared ip data on security surveys and vulnerability assessments conducted for fiscal years 2009 through 2011 with ip lists of high - priority assets over the same period , ( 2 ) reviewed ip data on the conduct of security surveys and vulnerability assessments and the delivery of those surveys and assessments to asset owners and operators , and ( 3 ) analyzed ip data on its efforts to follow up with asset owners and operators to measure whether they had made changes to enhance the security of their assets as a result of dhs security surveys and vulnerability assessments conducted at their assets for fiscal years 2009 through 2011 .

we then compared the results of our analyses with various criteria , including dhs policies and procedures outlined in the nipp ; ip guidelines on the conduct and delivery of and follow - up to security surveys and vulnerability assessments ; our standards for internal control in the federal government ; and our reports on performance measurement , including those on ways to use program data to measure results .

we discussed the sources of the data and ip's quality assurance procedures with agency officials and determined that the data were sufficiently reliable to provide a general overview of the program ; limitations are discussed later in this report .

also , we interviewed ssa officials in washington , d.c. , representing four selected sectors — the water , commercial facilities , energy , and dams sectors — to determine whether these ssas received the results of security surveys and vulnerability assessments and , if so , how the information was used .

we selected these sectors because assets in these sectors underwent numerous vulnerability assessments and security surveys over the period and had a mix of ssa partners .

specifically , dhs was the ssa for two of the sectors — the commercial facilities and dams sectors .

the department of energy and the environmental protection agency were the ssas for the other two sectors — the energy sector and the water sector , respectively .

in addition , we met with owners and operators of 10 high - priority assets in new jersey , virginia , and california to obtain their views on dhs efforts to conduct security surveys and vulnerability assessments and provide results .

we selected these locations because they contained assets on the high - priority list and had a number of security surveys and vulnerability assessments performed .

during our visits to the 10 assets in these locations , we also met with responsible psas from our sample states to discuss actions that they take to conduct these surveys and assessments , promote participation among asset owners and operators , provide them the results of security surveys and assessments at their assets , and measure results .

the information from our interviews with ssa officials , as well as asset owners and operators and psas in the three states , are not generalizable to the universe of cikr sectors and assets and psas throughout the country .

however , they provide valuable insights into ip efforts to conduct and manage its security survey and vulnerability assessment programs .

with regard to psas , we also conducted a survey of 83 of the 88 psas nationwide — those with 1 or more years of experience — to obtain their views on various aspects of the security survey and vulnerability assessment programs , including the value of the programs in enhancing cikr protection and resilience and the challenges psas face in promoting and executing the surveys and assessments .

we administered our survey from october to november 2011 and received a 96 percent ( 80 of 83 ) response rate .

we conducted this performance audit from june 2011 through may 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

appendix ii discusses our objectives , scope , and methodology and our survey of psas in greater detail .

the homeland security act of 2002 created dhs and gave the department wide - ranging responsibilities for , among other things , leading and coordinating the overall national critical infrastructure protection effort .

homeland security presidential directive ( hspd ) 7 further defined critical infrastructure protection responsibilities for dhs and ssas .

hspd - 7 directed dhs to establish uniform policies , approaches , guidelines , and methodologies for integrating federal infrastructure protection and risk management activities within and across cikr sectors .

various other statutes and directives provide specific legal authorities for both cross sector and sector - specific protection and resiliency programs .

for example , the public health security and bioterrorism preparedness and response act of 2002 was enacted to improve the ability of the united states to prevent , prepare for , and respond to acts of bioterrorism and other public health emergencies , and the pandemic and all - hazards preparedness act of 2006 addresses , among other things , public health security and all - hazards preparedness and response .

also , the cyber security research and development act , enacted in january 2002 , authorized funding through fiscal year 2007for the national institute of standards and technology and the national science foundation to facilitate increased research and development for computer and network security and to support related research fellowships and training .

cikr protection issues are also covered under various presidential directives , including hspd - 5 and hspd - 8 .

hspd - 5 calls for coordination among all levels of government as well as between the government and the private sector for managing domestic incidents , and hspd - 8 establishes policies to strengthen national preparedness to prevent , detect , respond to , and recover from threatened or actual domestic terrorist attacks , major disasters , and other emergencies.directives are tied together as part of the national approach for cikr protection through the unifying framework established in hspd - 7 .

according to the nipp , these separate authorities and nppd's ip is responsible for working with public and private sector cikr partners in the 18 sectors and leads the coordinated national effort to mitigate risk to the nation's cikr through the development and implementation of cikr protection and resilience programs .

using a sector partnership model , ip's partnership and outreach division works with sector representatives , including asset owners and operators , to develop , facilitate , and sustain strategic relationships and information sharing .

ip's protective security coordination division ( pscd ) provides programs and initiatives to enhance cikr protection and resilience and reduce risk associated with all - hazards incidents .

in so doing , pscd works with cikr owners and operators and state and local responders to ( 1 ) assess vulnerabilities , interdependencies , capabilities , and incident consequences ; ( 2 ) develop , implement , and provide national coordination for protective programs ; and ( 3 ) facilitate cikr response to and recovery from incidents .

related to these efforts , pscd has deployed the aforementioned psas in 50 states and puerto rico , with deployment locations based on population density and major concentrations of cikr .

in these locations , psas are to act as the link between state , local , tribal , and territorial organizations and dhs infrastructure mission partners and are to assist with ongoing state and local cikr security efforts by establishing and maintaining relationships with state , local , tribal , territorial , and private sector organizations ; support the development of the national risk picture by conducting vulnerability and security assessments to identify security gaps and potential vulnerabilities in the nation's most critical infrastructures ; and share vulnerability information and protective measure suggestions with local partners and asset owners and operators .

as part of their ongoing activities , psas are responsible for promoting the ecip initiative .

launched in september 2007 , the ecip initiative is a voluntary program focused on forming or maintaining partnerships between dhs and cikr owners and operators of high - priority level 1 and level 2 assets and systems , as well as other assets of significant value .

according to dhs guidance , psas are to schedule ecip visits with owners and operators in their districts using lists of high - priority and other significant assets provided by pscd each year , with visits to level 1 assets being the first priority , and visits to level 2 assets being the second priority .

visits to other significant assets are to receive subsequent priority based on various factors , including whether they are of significant value based on the direction of ip ; have been identified by the state homeland security office ; or represent a critical dependency associated with higher - priority assets already identified .

if an asset owner or operator agrees to participate in an ecip visit , psas are to meet with the owner or operator to assess overall site security , identify gaps , provide education on security , and promote communication and information sharing among asset owners and operators , dhs , and state governments .

one of the components of the ecip initiative is the security survey , formally called the infrastructure survey tool , which a psa can use to gather information on the asset's current security posture and overall security awareness .

if the asset owner or operator agrees to participate in the security survey , the psa works with the owner or operator to apply the survey , which assesses more than 1,500 variables covering six major components — information sharing , security management , security force , protective measures , physical security , or dependencies — as well as 42 more specific subcomponents within those categories .

for example , within the category “physical security” possible subcomponents include fences , gates , parking , lighting , and access control , among others .

once the survey is complete , the psa submits the data to argonne national laboratory , which analyzes the data to produce protective measures index scores ranging from 0 ( low protection ) to 100 ( high protection ) for the entire asset and for each component of the survey .

argonne national laboratory also uses the data to produce a “dashboard” — an interactive graphic tool that is provided to the asset owner or operator by the psa .

the dashboard displays the asset's overall protective measures score , the score for each of the six major components , the mean protective measures score and major component scores for all like assets in the sector or subsector that have undergone a security survey , and high and low scores recorded for each component for all sector or subsector assets that have undergone a security survey .

the asset score and the scores for other like assets show the asset owner or operator how the asset compares to similar assets in the sector .

the asset owner can also use the dashboard to see the effect of making security upgrades to its asset .

for example , if the dashboard shows a low score for physical security relative to those of other like assets , the owner or operator can add data on perimeter fencing to see how adding or improving a fence would increase the asset's score , thereby bringing it more in line with those of other like assets .

figure 1 provides an example of the dashboard produced as a result of the security survey .

related to these security surveys , dhs also produced , from calendar years 2009 through 2011 , summaries of the results of the security surveys related to sector or subsector security postures , known as sector summaries .

these sector summaries were provided directly to ssas in 2009 and 2010 , and according to program officials were made available to ssas in 2011 for sectors upon request .

unlike the summaries in past years , the 2011 summaries also included an “options for consideration” section that identified specific protective measures that had been adopted by the top 20 percent of assets in the sector or subsector as measured by the overall protective measures score .

dhs also uses vulnerability assessments to identify security gaps and provide options for consideration to mitigate these identified gaps .

these assessments are generally on - site , asset - specific assessments conducted at the request of asset owners and operators .

as of september 30 , 2011 , dhs had conducted more than 1,500 vulnerability assessments .

generally , vulnerability assessments are conducted at individual assets by ip assessment teams in coordination with psas , ssas , state and local government organizations ( including law enforcement and emergency management officials ) , asset owners and operators , and the national guard , which is engaged as part of a joint initiative between dhs and the national guard bureau ( ngb ) .

these assessment teams are staffed via an interagency agreement between dhs and ngb and include two national guardsmen — a physical security planner and a systems analyst , one of whom serves as the team lead .

they may also be supplemented by contractor support or other federal personnel , such as psas or subject matter experts , when requested .

argonne national laboratory staff then finalize the vulnerability assessment report — which includes options for consideration to increase an asset's ability to detect and prevent terrorist attacks and mitigation options that address the identified vulnerabilities of the asset — and provide it to the psa for delivery .

the asset owners and operators that volunteer for the vulnerability assessments are the primary recipients of the analysis .

the vulnerability assessment is developed using a questionnaire that focuses on various aspects of the security of an asset , such as vulnerabilities associated with access to asset air handling systems , physical security , and the ability to deter or withstand a blast or explosion .

the vulnerability assessment report also contains a section called “options for consideration” where dhs makes suggestions to improve asset security or reduce identified vulnerabilities .

for example , one vulnerability assessment report made suggestions to the asset owners or operators to explore the option of installing additional cameras to improve video surveillance in certain locations , install additional barriers to prevent vehicles from entering the facility at high speeds , and increase the training of its security staff .

dhs revised the vulnerability assessment methodology in 2010 to enhance the analytical capabilities of ip .

according to dhs officials , vulnerability assessments developed prior to 2010 did not have a consistent approach for gathering data on assets and did not produce results that were comparable from asset to asset .

they also did not incorporate an approach for assessing asset resilience .

dhs reported that the revised vulnerability assessment is intended to incorporate about 75 percent of the questions currently asked during an ecip security survey , including questions on resilience , to bring the tool more in line with the security survey .

as a result , vulnerability assessments completed beginning in 2011 have the capability to produce a dashboard similar to that produced from security surveys .

by revising the assessment methodology , dhs intends to ensure that the data collected during the vulnerability assessment can be compared within and across sectors and subsectors while still providing each asset an assessment specific to that asset , including options for consideration to reduce vulnerability .

while not the focus of this review , dhs has developed the regional resiliency assessment program ( rrap ) to assess vulnerability and risk associated with resiliency .

the rrap is an analysis of infrastructure “clusters,” regions , and systems in major metropolitan areas that uses security surveys and vulnerability assessments , along with other tools , in its analysis .

the rrap evaluates cikr on a regional level to examine vulnerabilities , threats , and potential consequences from an all - hazards perspective to identify dependencies , interdependencies , cascading effects , resiliency characteristics , and gaps .

the rrap assessments are conducted by dhs officials , including psas in collaboration with ssas ; other federal officials ; state , local , territorial , and tribal officials ; and the private sector depending upon the sectors and assets selected as well as a resiliency subject matter expert ( s ) .

the results of the rrap are to be used to enhance the overall security posture of the assets , surrounding communities , and the geographic region covered by the project and is shared with the state .

according to dhs officials , the results of specific asset - level assessments conducted as part of the rrap are made available to asset owners and operators and other partners ( as appropriate ) , but the final analysis and report are delivered to the state where the rrap occurred .

further , according to dhs , while it continues to perform surveys and assessments at individual assets , prioritizing efforts to focus on regional assessments allows dhs to continue to meet evolving threats and challenges .

dhs conducted about 2,800 security surveys and vulnerability assessments during fiscal years 2009 through 2011 .

in so doing , dhs directed psas to contact owners and operators of high - priority assets to offer to conduct voluntary security surveys and vulnerability assessments at their assets and psas used these as part of their outreach efforts among these assets .

however , dhs faces challenges tracking whether security surveys and vulnerability assessments have been performed at high - priority assets .

furthermore , dhs has not developed institutional performance goals that can be used to measure the extent to which owners and operators of high - priority assets participate in security surveys and vulnerability assessments .

in addition , dhs is not positioned to assess why some high - priority asset owners and operators decline to participate in these voluntary surveys and assessments so that it can develop strategies for increasing participation .

dhs is not positioned to track the extent to which it is conducting security surveys and vulnerability assessments on high - priority assets because of inconsistencies between the databases used to identify high - priority assets and to identify surveys and assessments completed .

consistent with the nipp , dhs prioritizes the participation of high - priority assets in its voluntary security survey and vulnerability assessment programs and uses the ncipp list of high - priority assets to guide its efforts .

in february 2011 , dhs issued guidance to psas that called for them to form partnerships with owners and operators of high - priority assets in their areas .

under the guidelines , psas are to use ncipp lists of high - priority assets to identify and contact owners and operators of the these assets in their areas that could benefit from participation in the security surveys , for the purpose of reducing potential security vulnerabilities and identifying protective measures in place .

psas are to conduct outreach directly by meeting with the asset owners and operators to provide information about dhs efforts to improve protection and resiliency , sharing information about how an asset owner or operator can request a vulnerability assessment , and offering to conduct a security survey .

if the owner or operator agrees to a visit from the psa , the psa is to record the date of the visit , and if the owner or operator agrees to participate in a security survey or vulnerability assessment , the psa is likewise to record the day the security survey or vulnerability assessment was conducted .

dhs analysts are then required to record the data provided by the psas in dhs's link encrypted network system ( lens ) database — dhs's primary database for tracking efforts to promote and complete security surveys and annual assessments .

according to dhs guidelines , these data are subject to weekly reviews to ensure that data recorded in lens are accurate , consistent , and complete .

thus , data on each individual asset should be recorded so that asset sector , name , and physical address reflect a single asset in a specified location throughout the database .

for example , according to the guidelines , asset names recorded in lens should not be recorded with stray asterisks , other special characters , and notes , and to the extent possible , address fields , such as “st” should be captured as “street.” to determine how many of these activities have been conducted on high - priority assets , we used an automated statistical software program to compare data on security surveys and vulnerability assessments completed in dhs's lens database with data on high - priority assets on the ncipp lists for fiscal years 2009 through 2011 — the lists psas are to use to contact officials representing high - priority assets in their areas .

out of 2,195 security surveys and 655 vulnerability assessments conducted during fiscal years 2009 through 2011 , we identified a total of 135 surveys and 44 vulnerability assessments that matched assets on the ncipp lists of high - priority assets .

we also identified an additional 106 security surveys and 23 vulnerability assessments that were potential matches with assets on the ncipp lists of priority assets , but we could not be certain that the assets were the same because of inconsistencies in the way the data were recorded in the two different databases .

for example , we found instances where assets that appeared to be the same company or organization were listed in different sectors .

we also encountered instances where names of companies at the same address did not match exactly or where companies with the same names had slightly different addresses in the two databases .

for example , an asset at 12345 main street in anytown , usa , might appear as abc company on one list and abc on another .

conversely , we also found instances where company names appeared to be the same or similar on both lists , but they were listed at different street addresses or on different streets .

in this case , for example , abc company might appear as being located on main street on one list , and e. main st. on another .

we contacted dhs officials responsible for maintaining the lens database and the ncipp list and told them that we had encountered difficulty matching company names and addresses in the two lists .

we explained that our results depended on an asset being described in a similar manner — same name , same address , same sector — in both the ncipp and lens databases .

these officials acknowledged that the two databases do not match and explained that they have had to match the data manually because of the inconsistencies .

specifically , dhs reported that it reviewed over 10,000 records — including records of security surveys , vulnerability assessments , and the ncipp lists for fiscal years 2009 through 2011 — and manually matched assets that had participated in surveys or assessments with the ncipp lists of high - priority assets using dhs officials' knowledge of the assets .

based on its efforts , dhs analysts provided a table that showed that dhs conducted 2,128 security surveys and 652 vulnerability assessments , of which it identified 674 surveys and 173 assessments that were conducted on high - priority assets .

thus , by manually matching assets across the two lists , dhs was able to show that the percentage of high - priority assets surveyed and assessed increased significantly .

table 1 illustrates the results of our efforts to match the data using an automated software program and the results of dhs's efforts to manually match the data .

dhs officials noted that beginning with the fiscal year 2012 ncipp lists , they have begun to apply unique numerical identifiers to each asset listed in lens and the ncipp lists .

according to these officials , once a match is made , the application of unique identifiers to the same assets in both databases is intended to remove uncertainty about which asset is which , regardless of variations in the name or address of the asset .

related to this , dhs officials also said that they have initiated a quality assurance process whereby they use descriptive data — such as geographic coordinates ( longitude and latitude ) — to verify street addresses and names , thereby giving ip the ability to more readily make matches in those instances where it may have previously experienced difficulty doing so .

nonetheless , they said that the ncipp list continues to present matching challenges because there have been “significant” changes in the ncipp list from year to year , but they anticipate fewer changes in the future .

most recently , the format and the organization of the list has changed to focus on clusters — groups of related assets that can be disrupted through a single natural or man - made hazard , excluding the use of weapons of mass destruction — rather than on individual assets .

thus , some assets previously considered high priority as stand - alone assets are now listed as part of a system or as clusters that in and of themselves are no longer considered high priority .

according to dhs officials , the introduction of clusters has resulted in other data matching challenges , including the duplicate entry of an ncipp asset that spans two states ; multiple entries for a single asset that is listed both individually and in relation to a cluster or a system , and multiple entries for a single asset within several clusters or systems .

dhs officials added that with the assignment of the unique identifier , they expect to be better positioned to cross - reference their program activities with the ncipp list .

dhs officials have stated that the discrepancies between our analyses and the analysis performed by ip , as well as the confusion created by factors such as changing data sets , made it clear that improvements should be made in the collection and organization of the data .

accordingly , dhs officials said that they are continuing to work with various partners within dhs and its contractors to streamline and better organize the list of high - priority assets and data associated with assessments , surveys , and other ip field activities .

however , dhs did not provide milestones and time frames for completing these efforts .

dhs appears to be heading in the right direction in taking actions to resolve many of the issues we identified with regard to matching data and data inconsistencies .

however , moving forward , dhs would be better positioned if it were to develop milestones and time frames for its plans to accomplish these tasks .

standard practices for project management state that managing a project involves , among other things , developing a timeline with milestone dates to identify points throughout the project to reassess efforts under way to determine whether project changes are necessary .

by developing time frames and milestones for streamlining and organizing the lists of high - priority assets and data associated with surveys , assessments , and field activities , dhs would be better positioned to provide a more complete picture of its approach for developing and completing these tasks .

it also would provide dhs managers and other decision makers with insights into ( 1 ) ip's overall progress in completing these tasks and ( 2 ) a basis for determining what , if any , additional actions need to be taken .

as dhs moves forward to improve its efforts to track the hundreds of security surveys and vulnerability assessments it performs each year , dhs could also better position itself to measure its progress in conducting these surveys and assessments at high - priority assets .

we have previously reported that to efficiently and effectively operate , manage , and oversee programs and activities , agencies need reliable information during their planning efforts to set realistic goals and later , as programs are being implemented , to gauge their progress toward achieving those goals .

in july 2011 , the pscd deputy director told us that pscd had a goal that 50 percent of the security surveys and vulnerability assessments conducted each year be on high - priority assets .

however , this goal was not documented ; pscd did not have written goals and the results to date indicate that this goal was not realistic .

specifically , according to dhs's 2010 nar , less than 40 percent ( 299 of 763 ) of security surveys were conducted on high - priority assets from may 1 , 2009 , through april 30 , 2010 .

for the same time period , dhs's nar reported that about 33 percent ( 69 of 212 ) of vulnerability assessments were conducted on high - priority assets .

setting institutional realistic goals for the number of security surveys and vulnerability assessments conducted at high - priority assets — consistent with dhs's efforts to improve its data on these assets — would enable dhs to better measure its performance and assess the state of security and resiliency at high - priority facilities , across the 18 sectors , over time .

for example , if there is a high - priority list consisting of 2,000 facilities , a dhs goal of 500 security surveys and vulnerability assessments conducted on high - priority facilities annually would allow for the potential assessment of all high - priority facilities over a defined period of time .

therefore , dhs could be in a better position to identify security strengths and weaknesses at high - priority facilities and within and across sectors and target areas for improvement .

consistent with hspd - 7 , dhs pursues a voluntary approach to critical infrastructure protection and coordination .

dhs officials told us that many of these assets do not receive voluntary surveys and assessments conducted by pscd .

rather , as we previously reported , pscd staff told us that they work with the responsible federal entity , such as the u.s. coast guard and the nuclear regulatory commission , to identify and address vulnerabilities .

finally , according to the pscd deputy director , shifting priorities based on terrorist threat information , budget constraints , and other department wide priorities , affect the prioritization and distribution of assets participating in these voluntary programs .

for example , dhs officials stated that given dhs is placing increased emphasis on regional activities , such as rraps , voluntary surveys and assessments are not necessarily focused on individual high - priority assets .

they said that expanded focus on regional activities enables ip to meet evolving threats and challenges , but in a budget constrained environment , forces them to prioritize activities so that they can leverage existing resources .

see gao / aimd - 00-21.3.1 .

standards for internal control in the federal government also calls for accurate and timely recording of information and periodic record reviews to help reduce the risk of errors .

dhs officials told us that they conduct data quality checks and dhs guidelines direct such actions .

however , the extent to which data were inconsistent indicates that information was not always accurately captured .

process to enable dhs to objectively and quantitatively assess improvement in cikr protection and resiliency .

specifically , the nipp states that performance metrics allow nipp partners to track progress against these priorities and provide a basis for dhs to establish accountability , document actual performance , promote effective management , and provide a feedback mechanism to decision makers .

consistent with the nipp risk management framework , our past work has shown that leading organizations strive to align their activities to achieve mission - related goals .

by using lens and ncipp data to establish performance goals , dhs could also be better positioned to identify gaps between expected and actual participation , track progress in achieving higher levels of participation , and ultimately gauge the extent to which protection and resiliency are enhanced for the nation's most critical assets .

relying on institutional goals rather than informal goals would also provide assurance that dhs has a common framework for measuring performance in the face of organizational or personnel changes over time .

dhs guidelines issued in february 2011 call for psas to document the names and addresses of cikr asset owners or operators that decline to participate in security survey outreach activities as well as the reasons they declined .

dhs officials told us that currently they track aggregate data on declinations but they do not document the reasons why asset owners and operators decline to participate in the security survey and vulnerability assessment programs .

in november 2011 , dhs provided a list of 69 asset owners or operators that psas recorded as having declined to participate in the security surveys from march 2009 through 2011 , but these records did not identify reasons for the declinations .

program officials told us that the tool with which they collect declination information is not designed to capture such information .

the deputy director for pscd said that in 2012 , dhs is developing a survey tool that psas can use to record why asset owners or operators decline to participate .

nonetheless , dhs could not provide specifics as to what would be included in the tool , which office would be responsible for implementing it , or time frames for its implementation .

rather , officials told us that they intend to use the results of our review to inform improvements to the process .

regarding vulnerability assessments , the assessment guidance is silent on whether psas are to discuss declinations with asset owners and operators and why they declined .

however , pscd issued guidance in january 2012 that states that the vulnerability assessment guidance is designed to complement the ecip guidance issued in february 2011 .

in our survey of psas , psa respondents provided some anecdotal reasons as to why asset owners and operators may decline to participate .

for example , when asked how often they had heard various responses from asset owners and operators that declined to participate in security surveys or vulnerability assessments , psas responded that reasons for declinations can include ( 1 ) the asset was already subject to federal or state regulation or inspections , ( 2 ) the identification of security gaps could render the owner of the asset liable for damages should an incident occur , or ( 3 ) assets owner or operator had concerns that the information it provides will not be properly safeguarded by dhs .

figure 2 shows the frequencies of psa responses of either “often” or “sometimes” to our survey question about the various reasons for declinations that they have heard .

appendix iii shows the results of our survey in greater detail .

while these psa perceptions may reflect some reasons asset owners and operators decline to participate , it is important that dhs systematically identify reasons why high - priority asset owners and operators may decline to participate , especially if reasons differ from psa region to psa region or by sector or subsector .

by doing so , dhs may be able to assess which declinations are within dhs's ability to control or influence and strategize how the security survey and vulnerability assessment program and dhs's approach toward promoting it can be modified to overcome any barriers identified .

for example , 39 percent ( 31 of 80 ) of the psas who responded to our survey suggested that senior - level partners , including senior leaders within dhs , could better support the promotion of the security survey program when those leaders interact with cikr partners at high - level meetings .

according to dhs , nppd and ip officials meet often with nonfederal security partners , including sector coordinating councils ( scc ) , industry trade associations , state and local agencies , and private companies , to discuss the security survey and vulnerability assessment and other programs to assist in educating mission partners about the suite of available ip tools and resources .

meeting with security partners to discuss ip's surveys , assessments , and other programs is consistent with the nipp partnership model whereby dhs officials in headquarters are to promote vulnerability assessments at high - level meetings where corporate owners are present — such as at scc or federal senior leadership council meetings — and through the ssas responsible for sector security .

the nipp also calls for dhs to rely on senior - level partners , such as the sccs and state representatives , to create a coordinated national framework for cikr protection and resilience within and across sectors and with industry representatives that includes the promotion of risk management activities , such as vulnerability assessments .

given the barriers to participation identified in our psa survey , we contacted officials with 12 industry trade associations representing the water , commercial facilities , dams , and energy sectors to get their views on their awareness of dhs security surveys and vulnerability assessments .

officials representing 10 of the 12 trade associations said that they were aware of dhs's voluntary survey and vulnerability assessment programs , but only 6 of 12 knew if some of their members' had participated in these programs .

as noted earlier , at the time of our review dhs was not systematically collecting data on reasons why some owners and operators of high - priority assets decline to participate in security surveys or vulnerability assessments .

officials stated that they realize that some of the data necessary to best manage these programs are not currently being collected and said that one example is that psas are not consistently reporting assessment and survey declinations from assets .

dhs officials added that in an effort to increase efficiency and accuracy , they are developing additional data protocols to ensure that all the applicable data are being collected and considered to provide a more holistic understanding of the programs .

given that dhs efforts are just beginning , however , it is too early to assess the extent to which they will address these data collection challenges .

nevertheless , by developing a mechanism to systematically collect data on the reasons for declinations , consistent with dhs guidelines , dhs could be better positioned to identify common trends for such declinations , determine what programmatic and strategic actions are needed to manage participation among high - priority assets , and develop action plans with time frames and milestones to serve as a road map for addressing any problems .

this could enhance the overall protection and resilience of those high - priority cikr assets crucial to national security , public health and safety , and the economy .

given that dhs officials recognize the need to collect these data to obtain a more holistic understanding of these programs , dhs could be better positioned if it had a plan , with time frames and milestones , for developing and implementing these protocols .

standard practices for project management state that managing a project involves , among other things , developing a plan with time frames and milestones to identify points throughout the process to reassess efforts under way to determine whether project changes are necessary .

by having plan with time frames and milestones for developing additional data protocols , ip could be better positioned to provide a more complete picture of its effort to develop and complete this task .

this could also provide dhs managers and other decision makers with ( 1 ) insights into ip's overall progress and ( 2 ) a basis for determining what , if any , additional actions need to be taken .

dhs shares security survey and vulnerability assessment information with asset owners and operators that participate in these programs and shares aggregated sector information with ssas .

however , dhs faces challenges ensuring that this information is shared with asset owners and operators in a timely manner and in providing ssas security survey - derived products that can help ssas in their sector security roles .

according to dhs officials , they are working to overcome these challenges , but it is unclear whether dhs actions will address ssa concerns about the use of aggregate security survey data .

dhs security surveys and vulnerability assessments can provide valuable insights into the strengths and weaknesses of assets and can help asset owners and operators make decisions about investments to enhance security and resilience .

for example , our survey of psas showed that most psas believe that the survey dashboard and the vulnerability assessment were moderately to very useful tools for reducing risk at cikr assets .

specifically , 89 percent of psas ( 71 of 80 ) and 83 percent of psas ( 66 of 80 ) responded that the security surveys and vulnerability assessments , respectively , were moderately to very useful products for reducing risk .

one psa commented that “the dashboard is the first tool of its kind that allows the owner / operator a clear and measurable quantitative picture of existing security profile” while another commented that “ provide specific , actionable items for the owner / operator to take action on to decrease vulnerabilities.” our discussions with various cikr stakeholders — specifically asset owners and operators and ssa representatives — also showed that these tools can be useful to the asset owners and operators that participate in these programs .

as will be discussed later in greater detail , 6 of the 10 asset owners and operators we contacted used the results of these survey and assessment tools to support proposals for security changes at the assets that had been assessed .

as one owner and operator said , these voluntary programs provide a fresh look at facility security from a holistic perspective .

another asset operator told us that it is nice to be able to see how its security practices compare to those of others within its sector .

the representatives of the four ssas we spoke with also believe the security survey and vulnerability assessments were beneficial to the asset owners and operators that received them .

the usefulness of security survey and vulnerability assessment results could be enhanced by the timely delivery of these products to the owners and operators that participated in them .

for example , facility owners may not see the importance of an identified security weakness if they do not receive this information soon after a security survey or vulnerability assessment is completed .

furthermore , the inability to deliver results within the expected time frame could undermine the relationship dhs is attempting to develop with asset owners and operators .

as mentioned earlier , psas rely on argonne national laboratory to provide them with the results of the vulnerability assessments , which psas , in turn , deliver directly to asset owners and operators .

while psas find the voluntary programs useful , 14 percent of psas we surveyed ( 11 of 80 ) described late delivery of the reports as a factor that undermines the usefulness of vulnerability assessments .

one psa commented that “the program is broken in regard to timely completion of reports and deliverables ( protective measures and resiliency dashboards ) for the asset owners / operators .

i have yet to receive anything from ( a vulnerability assessment conducted several months ago ) .

i have not even received the draft report for review nor the dashboard .

this creates a big credibility problem for me with my stakeholders who are looking for the results.” the nipp states that in order to have an effective environment for information sharing , cikr partners need to be provided with timely and relevant information that they can use to make decisions .

consistent with the nipp , dhs guidelines state that psas are to provide the results of security surveys in the form of a survey dashboard within 30 days of when the security survey was completed .

in addition , according to pscd officials , although there is no written guidance , pscd expects that vulnerability assessment results are to be provided to assets within 60 days of completion of the vulnerability assessment .

we analyzed dhs lens data to determine the extent to which survey dashboards were delivered to asset owners and operators on a timely basis , using dhs's 30-day criteria for timeliness .

our analysis showed that for fiscal year 2011 , more than half of all dashboards and vulnerability assessment reports were delivered to owners and operators late .

specifically , of the 570 dashboard reports that were supposed to be delivered during fiscal year 2011 , about 24 percent ( 139 of 570 ) were delivered on time and approximately 60 percent ( 344 of 570 ) were late , with almost half of those delivered 30 days beyond the 30-day deadline established by dhs guidelines .

data were missing for about 15 percent ( 85 of 570 ) of the remaining dashboards.of dashboard deliveries for all security surveys conducted in fiscal year 2011 .

dhs has taken actions to determine whether asset owners or operators have made security improvements based on the results of security surveys .

however , dhs has not developed an overall approach to determine ( 1 ) the extent to which changes have enhanced asset protection and resilience over time or ( 2 ) why asset owners and operators do not make enhancements that would help mitigate vulnerabilities identified during security surveys and vulnerability assessments .

as a result , dhs may be overlooking an opportunity to make improvements in the management of its voluntary risk mitigation programs that could also help dhs work with asset owners and operators to improve security and resilience .

according to dhs , moving forward , it may consider changes to the types of information gathered as part of its effort to measure improvements , but it has not considered what additional information , if any , should be gathered from asset owners or operators that participate in security surveys and vulnerability assessments .

according to the nipp , the use of performance measures is a critical step in the risk management process to enable dhs to objectively and quantitatively assess improvement in cikr protection and resiliency at the sector and national levels .

the nipp states that the use of performance metrics provides a basis for dhs to establish accountability , document actual performance , promote effective management , and provide a feedback mechanism to decision makers .

consistent with the nipp , dhs has taken action to follow up with security survey participants to gather feedback from asset owners and operators that participated in the program regarding the effect these programs have had on asset security using a standardized data collection tool , hereafter referred to as the follow - up tool or tool .

dhs first began to do follow - ups with asset owners and operators in may 2010 but suspended its follow - up activities shortly thereafter to make enhancements to the tool it used .

in january 2011 , ip introduced its revised follow - up tool , which was to be used by psas to ask asset representatives whose assets had undergone a security survey and received a dashboard about enhancements made in six general categories — information sharing , security management , security force , protective measures , physical security , and dependencies .

whereas the original follow - up tool focused on changes asset owners and operators made to enhance security and resilience , the revised tool focused on changes that were made directly as a result of dhs security surveys .

according to dhs guidance , the tool was to be used 180 days after the completion of a security survey at an asset .

the tool , which directs psas to ask a series of questions about improvements made as a result of the survey , instructs psas to request information on specific enhancements within those categories that were discussed in the dashboard provided to the asset owners and operators .

for example , within the physical security category , the tool instructs the psas to ask about any enhancements to things like fences , gates , parking , lighting , and access control , among others , and to ask asset owners or operators whether an identified change was made as a result of the security survey the asset had received .

in february 2011 , shortly after the revised tool was introduced , ip issued guidelines that instructed psas to implement the follow - up tool .

according to ip officials , psas used the tool to follow up with owners and operators of 610 assets from january 2011 through september 2011 .

data provided by ip showed that about 21 percent ( 126 of the 610 ) of the respondents to the psa follow - ups reported that they had completed improvements , and 81 percent of these ( 102 of 126 ) reported that those improvements were implemented as the result of the security survey the asset received .

according to ip's data , the most common types of improvements identified by assets that had completed improvements since receiving the security survey were changes to information sharing , which could include activities such as participating in working groups , and physical security .

dhs guidance states that psas are to conduct a follow - up with the asset owners and operators 180 days after an asset receives a security survey .

we compared dhs data on 522 security surveys conducted from july 1 , 2010 , through march 31 , 2011 , with dhs data on the follow - ups performed from january 1 , 2011 , through september 30 , 2011 — 180 days after dhs completed the security surveys .

we found that dhs did not contact some asset owners or operators that should have received a 180- day follow - up and contacted some owners and operators that had participated in a security survey more than 180 days prior to the introduction of the tool .

for example , of the 522 security survey participants that participated in a security survey from july 1 , 2010 , through march 31 , 2011 , 208 ( 40 percent ) received the 180-day follow - up and 314 ( 60 percent ) did not .

furthermore , dhs recorded an additional 402 follow - ups on assets that had received their security survey more than 180 days prior to the introduction of the tool .

thus , the data dhs reported included improvements assets made beyond the 180-day scope of the follow - up tool , making it difficult to measure the effectiveness of the security survey in prompting enhancements within 180 days of the survey .

according to pscd officials , there are two key reasons why dhs used the follow - up tool to capture data on changes made beyond 180 days .

first , program officials said that completion of the 180-day follow - up depends upon the asset representative's willingness to participate and availability to answer these questions .

if the asset representative does not agree to participate , or neither the representative nor the psa is available , the 180-day follow - up cannot be completed on schedule .

however , when dhs provided the follow - up data in november 2011 , officials said that they were not aware of any asset owners or operators that had refused to participate in the 180-day follow - up at that time .

second , program officials noted that the inclusion of assets that had received a security survey more than 180 days prior to the introduction of the revised follow - up tool occurred because they believed that it was necessary to capture data on as many assets as possible .

they said that ip intends that follow - ups be completed as close to the 180-day mark as possible , but they believed it was important to initially document whether the security survey resulted in changes to security , regardless of when the change was made .

ip officials further explained that they had developed a similar follow - up tool to capture data on enhancements resulting from vulnerability assessments .

however , at the time of our review , results were not available from the vulnerability assessment follow - up tool , which was also implemented in january 2011 and was designed to capture data on enhancements made 365 days following the delivery of the vulnerability assessment report .

consistent with the security survey , dhs officials explained that the 365-day follow - up for vulnerability assessments was determined as a means to begin the process of collecting and assessing data on improvements being made as a result of the assessments .

they added that as more data are collected , ip will review the information to determine if the follow - up visits for security surveys and vulnerability assessments should remain at 180 and 365 days , respectively , or be moved as a result of information collected from asset owners and operators .

nonetheless , dhs officials did not provide a road map with time frames and milestones showing when they planned to revisit the 180-day follow - up time frame or the intervals between follow - ups .

consistent with the standards for project management , by having a road map with time frames and milestones for revisiting these time frames , ip could be better positioned to provide a more complete picture of its overall progress making these decisions and a basis for determining what , if any , additional actions need to be taken or data inputs need to be made .

gao / aimd - 00-21.3.1 .

especially true if asset owners and operators are implementing more complicated enhancements over a longer term because of the need to develop and fund plans for particular types of improvements .

for example , gathering these data could help dhs measure not only what improvements asset operators are implementing , but also how long it takes to complete the planning phase of a security enhancement project and how this time frame might vary by the type of improvement .

furthermore , while it is important to capture information about improvements made as a result of these activities over time , it is also important that dhs either capture the information within the prescribed times outlined in dhs guidance , adjust the time frames based on an analysis of data captured over time , or perform follow - ups at additional intervals beyond those initially performed .

this would also be consistent with standards for internal control in the federal government , which calls for the establishment and review of performance measures and indicators to monitor activities and top - level reviews by management to track major agency achievements and compare these with plans , goals , and objectives .

by doing so , ip could be better positioned to document actual performance , promote effective management , provide a feedback mechanism to decision makers , and enhance overall accountability .

according to dhs officials , moving forward , dhs may consider additional changes to its follow - up tool depending on the results they gather over time .

the nipp states that performance measures that focus on outputs , called output measures , such as whether an asset completes a security improvement , should track the progression of a task .

the nipp further states that outcome measures are to track progress toward an intended goal by beneficial results rather than level of activity .

our review of dhs's approach for following up with assets that had undergone a security survey showed that psas were instructed to focus on security enhancements completed as result of the security survey , not enhancements that were planned or in process .

nonetheless , our review of dhs's follow - up results for the period from january through september 2011 showed that dhs reported the following: 41 percent ( 250 of 610 ) of the owners and operators surveyed reported that security enhancements were either in process or planned and the results did not indicate whether these planned or in - process enhancements were attributable to dhs's security survey at these assets .

after we discussed our observation with dhs officials , they informed us that they believe completed improvements are the best initial measurement of the impact of security surveys and vulnerability assessments .

they added that other metrics can be added as the process matures and is refined .

however , as of march 2012 , dhs did not document whether planned or in - process improvements are the result of security surveys .

given that the nipp calls for cikr partners to measure performance in the context of the progression of the task , dhs could be missing an opportunity to measure performance associated with planned and in - process enhancements , especially if they are attributable to dhs efforts via security surveys and vulnerability assessments .

dhs could also use this opportunity to consider how it can capture key information that could be used to understand why certain improvements were or were not made by assets owners and operators that have received surveys and assessments .

for example , the follow - up tool could ask asset representatives what factors — such as cost , vulnerability , or perception of threat — influenced the decision to implement changes , either immediately or over time if they chose to make improvements ; what factors — such as perception of risk , cost , or budget constraints — influenced an asset owner or operator to choose to not make any improvements ; why were the improvements made chosen over other possible improvements , if improvements were made ; and did the improvements , if any , involve the adoption of new or more cost - effective techniques that might be useful as an option for other owners and operators to consider as they explore the feasibility making improvements ? .

understanding why an asset owner or operator chooses to make , or not make , improvements to its security is valuable information for understanding the obstacles asset owners or operators face when making security investments .

for example , the cost of security upgrades can be a barrier to making enhancements .

as one psa who responded to our survey commented , “there is no requirement for the owner / operator to take action .

they are left with making a “risk - reward” decision .

some see great value in making security upgrades , while others are less inclined to make improvements due to costs.” likewise , one asset representative told us that security is one of the most important things to management until budget time .

in this regard , a better understanding of the complexity of the security improvement decision at the asset could also help dhs better understand the constraints asset owners or operators face in making these decisions — information that could possibly help dhs determine how , if at all , to refine its security survey program to assist asset owners or operators in making these decisions .

for example , the nipp states that effective cikr programs and strategies seek to use resources efficiently by focusing on actions that offer the greatest mitigation of risk for any given expenditure .

additional information on the cost of improvements made and the reasons why improvements were or were not made could also assist dhs in understanding the trade - offs asset owners and operators face when making decisions to address vulnerabilities identified as a result of dhs security surveys and enhancements .

ip officials told us they are wary of attempting to gather too much information from asset representatives with the follow - up tool because of a concern that being too intrusive may damage the relationships that the psas have established with asset representatives .

they said that gathering additional information is not as important as maintaining strong relationships with the asset representatives .

we recognize that dhs operates its security survey program in a voluntary environment and that dhs can only succeed at improving asset and sector security if asset owners and operators are willing to participate , consistent with dhs's interest in maintaining good relationships with asset representatives .

however , by gathering more information from assets that participate in these programs — particularly high - priority assets — dhs could be better positioned to measure the impact of its programs on critical infrastructure security at the sector and national levels .

moreover , by collecting and analyzing this type of information , dhs could be better informed in making decisions about whether adjustments to its voluntary programs are needed to make them more beneficial to cikr assets — a factor which could help dhs further promote participation by asset owners and operators that may previously have been reluctant to participate in dhs security surveys and assessments .

having this type of information could also be important in light of dhs's efforts to better understand interdependencies between assets via the rraps .

for instance , by knowing what factors influence decisions to make an improvement , or not , at one asset or a group of assets , dhs could be better positioned to understand how that decision influences the security of other assets that are also part of the rrap .

as a result , dhs and psas could then be better positioned to work with owners and operators to mitigate any vulnerabilities arising out of these decisions .

it could also help dhs develop and target strategies for addressing why certain enhancements were not made and ultimately put dhs in a better position to measure outcomes , rather than outputs , associated with its efforts to promote protection and resilience via its voluntary risk mitigation programs .

dhs has taken important actions to conduct voluntary cikr security surveys and vulnerability assessments , provide information to cikr stakeholders , and assess the effectiveness of security surveys and vulnerability assessments .

however , further actions could enhance each of these endeavors and provide dhs managers the information they need to ensure that ip is taking appropriate steps toward completing them or making adjustments where needed .

dhs has not institutionalized realistic goals that could help dhs measure the effects of its efforts to promote and conduct security surveys and vulnerability assessments among high - priority assets .

by developing realistic institutional goals , dhs could , for example , better measure the effects of its efforts to promote and conduct security surveys and vulnerability assessments among high - priority assets .

further , developing a road map with milestones and time frames for ( 1 ) taking and completing actions needed to resolve issues associated with data inconsistencies and matching data on the list of high - priority assets with data used to track the conduct of security surveys and vulnerability assessments , ( 2 ) completing protocols to systematically collect data on the reasons why some owners and operators declined to participate in the voluntary surveys and assessments , and ( 3 ) improving the timely delivery of the results of security surveys and vulnerability assessments could better position dhs to target high - priority assets and provide them with the information they need to make decisions related to security and resiliency .

moreover , by revising its plans to include when and how ssas will be engaged in designing , testing , and implementing the web - based tool , consistent with its recent efforts to coordinate with cikr partners , dhs could be positioned to better understand and address their information needs .

consistent with the nipp , dhs is also continuing to take actions to follow up with asset owners and operators that have participated in security surveys and vulnerability assessments to gauge the extent to which these surveys and assessments have prompted owners and operators to improve security and resilience at their assets .

dhs officials said that they intend to review the information it gathers from asset owners and operators to determine if the follow - up visits should remain at 180 days after dhs completed the security surveys .

by establishing a road map with milestones and time frames for conducting this review , dhs would be better positioned to provide a picture of its overall progress in making these decisions and a basis for determining what , if any , additional actions need to be taken or data inputs need to be made and whether additional follow - ups are appropriate at intervals beyond the follow - ups initially performed .

in addition , collecting detailed data on actions started and planned and , for example , why actions were not taken , could provide dhs valuable information on the decision - making process associated with making security enhancements and enable dhs to better understand what barriers owners and operators face in making improvements to the security of their assets .

to better ensure that dhs's efforts to promote security surveys and vulnerability assessments among high - priority cikr are aligned with institutional goals , that the information gathered through these surveys and assessments meet the needs of stakeholders , and that dhs is positioned to know how these surveys and assessments could be improved , we recommend that the assistant secretary for infrastructure protection , department of homeland security , take the following seven actions: develop plans with milestones and time frames to resolve issues associated with data inconsistencies and matching data on the list of high - priority assets with data used to track the conduct of security surveys and vulnerability assessments ; institutionalize realistic performance goals for appropriate levels of participation in security surveys and vulnerability assessments by high - priority assets to measure how well dhs is achieving its goals ; design and implement a mechanism for systematically assessing why owners and operators of high - priority assets decline to participate and a develop a road map , with time frames and milestones , for completing this effort ; develop time frames and specific milestones for managing dhs's efforts to ensure the timely delivery of the results of security surveys and vulnerability assessments to asset owners and operators ; revise its plans to include when and how ssas will be engaged in designing , testing , and implementing dhs's web - based tool to address and mitigate any ssa concerns that may arise before the tool is finalized ; develop a road map with time frames and specific milestones for reviewing the information it gathers from asset owners and operators to determine if follow - up visits should remain at 180 days for security surveys and whether additional follow - ups are appropriate at intervals beyond the follow - ups initially performed ; and consider the feasibility of expanding the follow - up program to gather and act upon data , as appropriate , on ( 1 ) security enhancements that are ongoing and planned that are attributable to dhs security surveys and vulnerability assessments and ( 2 ) factors , such as cost and perceptions of threat , that influence asset owner and operator decisions to make , or not make , enhancements based on the results of dhs security surveys and vulnerability assessments .

we provided a draft of this report to the secretary of homeland security for review and comment .

in its written comments reprinted in appendix iv , dhs agreed with all seven of the recommendations ; however , its implementation plans do not fully address two of these seven recommendations and it is unclear to what extent its plans will address two other recommendations .

with regard to the first recommendation that dhs develop plans to resolve issues associated with data inconsistencies between its databases , dhs stated its efforts to assign unique identifiers to assets on the high - priority list that have received security surveys and vulnerability assessments will make matching easier and that other quality assurance processes have been implemented to better verify individual asset data .

we agree these are positive steps ; however , to fully address the recommendation , we believe dhs should develop a plan with time frames and milestones that specify how the steps it says it is taking address the data inconsistencies we cited , and demonstrate the results — how many high - priority assets received security surveys , vulnerability assessments , or both in a given year — of that effort .

by doing so , dhs would be better positioned to provide a more complete picture of its approach for developing and completing these tasks .

it would also provide dhs managers and other decision makers with insights into ( 1 ) ip's overall progress in completing these tasks and ( 2 ) a basis for determining what , if any , additional actions need to be taken .

with regard to the second recommendation that dhs institutionalize realistic performance goals for levels of participation in security surveys and vulnerability assessments by high - priority assets , dhs stated that the participation of high - priority assets continues to be a concern but reiterated its view that the voluntary nature of its programs and competing priorities makes setting goals for high - priority participation difficult .

dhs stated that its fiscal year 2012 project management plans for protective security advisor and vulnerability assessment projects established realistic goals concerning the total number of assessments to be conducted .

however , they said these plans do not include goals for assessments performed at high - priority assets .

furthermore , dhs stated the shift in emphasis to regional resilience suggested metrics and goals intended to measure the participation of high - priority assets in vulnerability assessments and surveys may not be a strong or accurate indicator of the degree to which dhs is achieving its infrastructure protection and resilience goals .

we agree that the voluntary nature of these programs and changing priorities make the process of setting goals difficult .

however , the nipp and dhs guidance emphasize the importance of high - priority participation in these programs , and dhs can take factors like the voluntary nature of the program and dhs's shift toward regional resilience into account when setting realistic goals for the number of security surveys and vulnerability assessments it conducts at high - priority facilities .

by establishing realistic performance goals for levels of participation by high priority assets , dhs would be better positioned to compare actual performance against expected results and develop strategies for overcoming differences or adjust its goals to more realistically reflect the challenges it faces .

with regard to the third recommendation that dhs design and implement a mechanism for systematically assessing why owners and operators of high priority assets decline to participate and develop a road map , with time frames and milestones , for completing this effort , dhs stated it recognizes that additional clarification and guidance are needed to ensure effective implementation of existing guidance .

specifically , dhs stated it will review and revise the guidance to ( 1 ) determine if revisions to the existing process are required and ( 2 ) develop supplementary guidance to aid psas in executing the process .

dhs stated it will initiate this review in the fourth quarter of fiscal year 2012 , after which time it will develop additional milestones for mechanism improvement .

we believe that dhs's proposed actions appear to be a step in the right direction , but it is too early to tell whether dhs's actions will result in an improved mechanism for systematically assessing why owners and operators decline to participate .

regarding the fourth recommendation to develop time frames and specific milestones for managing its efforts to improve the timely delivery of the results of security surveys and vulnerability assessments to asset owners and operators , dhs stated it is working with contractors and program staff to advance the processes and protocols governing the delivery of assessment and survey products to facilities .

dhs also stated that it had begun a review of assessments lacking delivery dates in lens and is working with psas to populate the missing information .

in addition , dhs noted that its plan to transition to a web - based dashboard system will help mitigate the issue of timely report delivery by eliminating the need for in - person delivery of the dashboard product .

however , dhs did not discuss time frames and milestones for completing these efforts .

thus , it is unclear to what extent dhs's actions will fully address this recommendation .

as noted in our report , developing time frames and milestones for completing improvements that govern the delivery of the results of surveys and assessments would provide insights into ip's overall progress .

with regard to the fifth recommendation to revise its plans to include when and how ssas will be engaged in designing , testing , and implementing dhs's web - based tool , dhs stated that it is currently taking actions to develop and test a web - based dashboard tool for individual owners and operators , which is expected to be widely available in january 2013 .

dhs stated that it anticipates the development of a state and local “view,” or dashboard , following the successful deployment of the web - based owner and operator dashboards .

regarding ssas , dhs stated that a concept for a sector - level view of assessment data has been proposed and that the requirements and feasibility of such a dashboard will be explored more fully following the completion of the state - level web - based dashboard .

dhs noted that that ip will engage the ssas to determine any associated requirements .

dhs's proposed actions appear to be a step in the right direction .

however , given that the sector level view of assessment data is in the proposal stage and further action will be explored more fully after completion of the state level web - based dashboard , it is too early to tell when and how ssa's will be engaged in designing , testing and implementing the web - based tool .

in response to the sixth recommendation to develop a road map with time frames and specific milestones to determine if follow - up visits should remain at 180 days for security surveys , and whether additional follow - ups are appropriate at intervals beyond the follow - ups initially performed , dhs stated it will analyze and compare security survey follow - up results in early calendar year 2013 to determine whether modifications are required .

dhs also stated that given that the 365-day follow - up process went into effect in january 2011 , the first follow - up evaluations of vulnerability assessments have only recently begun and ip will collect , at a minimum , 1 year of vulnerability assessment data .

dhs said that ip intends to review the results for both the security survey 180-day follow - up and the 365-day follow - up in early calendar year 2013 to determine whether modifications to the follow - up intervals are required .

dhs's proposed actions are consistent with the intent of this recommendation .

in response to the seventh recommendation to consider the feasibility of gathering and acting upon additional data , where appropriate , on ( 1 ) ongoing or planned enhancements attributable to security surveys and assessments and ( 2 ) factors that influence asset owner and operator decisions to make or not make security enhancements , dhs stated that it collects information on ongoing or planned enhancements .

however , as noted in the report , dhs does not collect information that would show whether these enhancements are attributable to security surveys and assessments .

dhs also stated that ip will continue to work with argonne national laboratory and field personnel to determine the best method for collecting information related to those factors influencing an asset's decision to implement or not implement a new protective measure or security enhancement .

however , it is not clear to what extent dhs's actions will fully address this recommendation because it did not discuss whether it will consider the feasibility of gathering data on whether ongoing or planned enhancements are attributable to security surveys and assessments or how it will act upon the data it currently gathers or plans to gather to , among other things , measure performance in the context of the progression of the task , consistent with the nipp .

by gathering and analyzing data on why an asset owner or operator chooses to make , or not make , improvement to security , dhs would be better positioned to understand the obstacles asset owners face when making investments .

dhs also provided technical comments , which we incorporated as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies to the secretary of homeland security , the under secretary for the national protection programs directorate , and other interested parties .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have questions about this report , please contact me at ( 202 ) 512-8777 or caldwells@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix v .

this appendix provides information on the 18 critical infrastructure sectors and the federal agencies responsible for sector security .

the national infrastructure protection plan ( nipp ) outlines the roles and responsibilities of the department of homeland security ( dhs ) and its partners — including other federal agencies .

within the nipp framework , dhs is responsible for leading and coordinating the overall national effort to enhance protection via 18 critical infrastructure and key resources ( cikr ) sectors .

homeland security presidential directive ( hspd ) 7 and the nipp assign responsibility for cikr sectors to sector - specific agencies ( ssa ) .

as an ssa , dhs has direct responsibility for leading , integrating , and coordinating efforts of sector partners to protect 11 of the 18 cikr sectors .

the remaining sectors are coordinated by eight other federal agencies .

table 2 lists the ssas and their sectors .

to meet our first objective — determine the extent to which dhs has taken action to conduct security surveys and vulnerability assessments among high - priority cikr — we reviewed dhs guidelines on the promotion and implementation of the security surveys and vulnerability assessments , records of outreach to cikr stakeholders regarding these tools , and various dhs documents , including dhs's national critical infrastructure and key resources protection annual report , on efforts to complete security surveys and vulnerability assessments .

we also interviewed officials in the protective security coordination division , which is part of the office of infrastructure protection ( ip ) in dhs's national protection and program directorate , who are responsible for managing and administering dhs's security surveys and vulnerability assessments to learn about the actions they took to conduct these programs .

we obtained and analyzed dhs data on the conduct of voluntary programs for fiscal years 2009 through 2011 — which are maintained in dhs's link encrypted network system ( lens ) database and compared those records with the national critical infrastructure prioritization program ( ncipp ) list of the high - priority cikr assets — to determine the extent to which dhs performed security surveys and vulnerability assessments at high - priority assets .

to assess the reliability of the data , we reviewed existing documentation about the data systems and spoke with knowledgeable agency officials responsible for matching the two databases to discuss the results of our comparison and to learn about their efforts to match lens data with the ncipp lists .

while the information in each database was sufficiently reliable for the purposes of providing a general overview of the program , issues with the comparability of information in each database exist , which are discussed in this report .

to do our comparison , we used a statistical analysis system ( sas ) program to match the different data sets and summarize the results .

because we found that assets in the lens database and ncipp lists did not share common formats or identifiers that allowed us to easily match them , we had to match the data based on asset names and addresses .

however , names and addresses were generally not entered in a standardized way , so we had to develop a process to standardize the available information and identify potential matches based on similar names or addresses .

in our attempt to match the data sets , we did the following: standardized the date formats for fields that tracked when assessments were conducted ( dates across lists might have formats such as 01 / 01 / 10 or 1 / 1 / 2010 and needed to be standardized to ensure appropriate matching within certain time frames ) .

standardized the labels for sectors ( across data sets , a sector might be listed as chemical & hazardous materials industry , chemical and hazardous materials indus , or ‘chemical' ) .

standardized state fields ( across data sets , a state might be listed as alabama or al ) .

identified exact matches between the data sets on the asset name and the state name .

identified potential matches between the data sets based on asset name , asset address , and state .

specifically , we used a sas function ( spedis ) that measures asymmetric spelling distance between words , to determine the likelihood that names and addresses from two data sets did match and to generate possible pairs of matching assets .

the possible matches for an asset were written to a spreadsheet , which we reviewed to determine a potential match .

as noted in the report , the inconsistencies between the data sets prevented us from determining definitively the extent to which assets on one list were also present in the other list .

for example , in some cases assets seemed to be potential matches but there were differences in the sector listed or inconsistent company names and addresses .

thus we report separately on assets that were exact matches based on asset name and those that were potential matches .

we also examined the inconsistencies we found with respect to dhs's guidance on gathering data on participation in the security survey and vulnerability assessments and compared the findings to the criteria in standards for internal control in the federal government .

we also compared the results of our analyses with gao reports on performance measurement , including ways to use program data to measure results .

in addition , to address the first objective , we also interviewed representatives — asset owners and operators — at 10 selected assets , also known as facilities , in 4 of the 18 sectors — the water , dams , commercial facilities , and energy sectors — to discuss their views on dhs efforts to work with asset owners and operators and conduct dhs's voluntary security surveys and vulnerability assessments .

we also contacted industry association representatives from the 4 sectors to discuss their views on dhs efforts to promote and conduct these activities .

we selected these asset and industry representatives to take into account ( 1 ) sectors with a mix of regulations related to security ; ( 2 ) sectors where dhs's ip and non - dhs agencies are the ssas — dhs for the commercial facilities sector and dams sector , the department of energy for the energy sector , and the environmental protection agency for the water sector ; ( 3 ) sectors where security surveys and vulnerability assessments had been conducted ; and ( 4 ) geographic dispersion .

we selected three states — california , new jersey , and virginia — where , based on our preliminary review of dhs's lens database and the ncipp lists , security surveys and vulnerabilities assessments may have been performed at high - priority assets .

at these assets , we , among other things , focused on the role of protective security advisors ( psa ) who serve as liaisons between dhs and security stakeholders , including asset owners and operators , in local communities .

we also reviewed psa program guidance and interviewed 4 of 88 psas — psas from california , new jersey and from the national capital region ( encompassing washington , d.c. , suburban virginia , and suburban maryland ) — to discuss the roles and responsibilities in partnering with asset owners and operators and in promoting security surveys and vulnerability assessments .

while the results of our interviews cannot be generalized to reflect the views of all asset owners and operators and psas nationwide , the information obtained provided insights into dhs efforts to promote participation in its security survey and vulnerability assessment programs .

we also conducted a survey of 83 of 88 psas , those who , based on lists provided by dhs officials , had been in their positions for at least 1 year .

we conducted the survey to gather information on psas' efforts to promote and implement security surveys and vulnerability assessments , and identify challenges psas face when conducting these .

gao staff familiar with the critical infrastructure protection subject matter designed draft questionnaires in close collaboration with a social science survey specialist .

we conducted pretests with three psas to help further refine our questions , develop new questions , clarify any ambiguous portions of the survey , and identify any potentially biased questions .

we launched our web - based survey on october 3 , 2011 , and received all responses by november 18 , 2011 .

log - in information for the web - based survey was e - mailed to all participants .

we sent one follow - up e - mail message to all nonrespondents 2 weeks later and received responses from 80 out of 83 psas surveyed ( 96 percent ) .

because the survey was conducted with all eligible psas , there are no sampling errors .

however , the practical difficulties of conducting any survey may introduce nonsampling errors .

for example , differences in how a particular question is interpreted , the sources of information available to respondents , or the types of people who do not respond can introduce unwanted variability into the survey results .

we included steps in both the data collection and data analysis stages to minimize such nonsampling errors .

we collaborated with a gao social science survey specialist to design draft questionnaires , and versions of the questionnaire were pretested with 3 psas .

in addition , we provided a draft of the questionnaire to dhs's ip for review and comment .

from these pretests and reviews , we made revisions as necessary .

we examined the survey results and performed computer analyses to identify inconsistencies and other indications of error .

a second independent analyst checked the accuracy of all computer analyses .

regarding our second objective — to determine the extent to which dhs shared the results of security surveys and vulnerability assessments with asset owners and operators and ssas — we reviewed available dhs guidelines and reports on efforts to share security survey and vulnerability assessment results with stakeholders and compared dhs's sharing of information with standards in the nipp .

we accessed , downloaded , and analyzed lens data for information regarding the asset owners and operators that participated in dhs security surveys and vulnerability assessments during fiscal years 2009 through 2011 .

to assess the reliability of the data , we spoke with knowledgeable agency officials about their quality assurance process .

during the course of our review dhs began taking action to clean up the data and address some of the data inconsistencies we discuss in this report .

we found the data to be sufficiently reliable for providing a general overview of the program , but issues with the missing information in the lens database exist and are discussed in this report .

we compared the results of our analysis with dhs criteria regarding the timeliness of security surveys and vulnerability assessments , criteria in standards for internal control in the federal government , and the nipp .

we also used the lens database , the ncipp lists , and dhs documentation showing all assets that had received a security survey or a vulnerability assessment to select a nonrandom sample of high - priority assets from 4 sectors — the commercial facilities , dams , energy , and water sectors — and spoke with representatives from these selected assets to garner their opinions on the value of these voluntary programs and how they used the information dhs shared with them .

in addition , we reviewed the 2009 and 2010 sector annual reports and the 2010 sector - specific plans for all cikr sectors to assess if and how results of the security surveys and vulnerability assessments were included .

we also interviewed ssa officials from our 4 selected sectors to learn what information dhs shared with them and how that information was used , and to discuss their overall relationship with dhs with respect to receiving and using data from dhs security surveys and vulnerability assessments .

while the results of these interviews cannot be generalized to all ssas , the results provided us with valuable insight into the dissemination and usefulness of information dhs provided from security surveys and vulnerability assessments .

furthermore , we interviewed dhs officials regarding their efforts to enhance the information they provide to ssas from security surveys and vulnerability assessments .

with regard to our third objective — determine the extent to which dhs assessed the effectiveness of the security survey and vulnerability assessment programs , including any action needed to improve dhs's management of the programs — we reviewed dhs documents and our past reports , and dhs office of inspector general ( oig ) reports on dhs efforts to assess the effectiveness of its programs .

we interviewed dhs officials and reviewed dhs guidelines on procedures for following up with asset owners and operators that have participated in these programs and to discuss the results of dhs efforts to conduct these follow - ups .

we also ( 1 ) examined dhs documents that discussed the results of dhs efforts to conduct follow - ups and ( 2 ) analyzed the instrument used to contact owners and operators , as well as the questions asked to assess its effectiveness .

in addition , we analyzed available data on dhs efforts to perform follow - ups for the period from january 2011 through september 30 , 2011 , and compared dhs data with dhs guidelines that discussed the number of days dhs officials were to begin follow - ups after providing the results of security surveys and vulnerabilities to asset owners and operators .

we also compared the results or our work with criteria in standards for internal control in the federal government and the nipp , particularly those related to performance measurement .

finally , we spoke to cikr officials in our sample sectors to learn how dhs personnel in the field had followed up on security surveys and vulnerability assessments and whether asset owners and operators were making changes based on the results , and if not why .

we conducted this performance audit from june 2011 through may 2012 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

this appendix provides information on our survey of protective security advisors , which we used to gather information on efforts to promote and implement the voluntary programs offered by dhs and the challenges faced when conducting security surveys and vulnerability assessments .

we conducted a web - based survey of all 83 protective security advisors who had been in their positions for at least one year .

we received responses from 80 , for a response rate of 96 percent .

our survey was composed of closed - and open - ended questions .

in this appendix , we include all the survey questions and aggregate results of responses to the closed - ended questions ; we do not provide information on responses provided to the open - ended questions .

percentages may not total to 100 due to rounding .

for a more detailed discussion of our survey methodology , see appendix ii .

1 .

please provide the following information about the protective security advisor responsible for completing this questionnaire .

number of years as a psa ( round up to nearest year ) 2 .

did you receive the enhanced critical infrastructure protection ( ecip ) initiative standard operating procedures ( sop ) guidance dated february 2011 ? .

80 3 .

 ( if yes to q2 ) how useful did you find the ecip sop guidance for promoting ecips ? .

if you answered "slightly useful" or "not at all useful" , please explain why: 4 .

 ( if yes to q2 ) how useful did you find the ecip sop guidance for conducting ecips ? .

if you answered "slightly useful" or "not at all useful" , please explain why: 5 .

did you receive training on the enhanced critical infrastructure protection ( ecip ) initiative program ? .

6 .

 ( if yes to q5 ) how useful did you find the ecip training ? .

if you answered "slightly useful" or "not at all useful" , please explain why: 7 .

in your opinion , how useful is the ecip initiative program for reducing risk at ci facilities ? .

please explain your opinion about the usefulness of the ecip initiative program: 8 .

in your opinion , how useful is the ecip infrastructure survey tool ( ist ) for reducing risk at ci facilities ? .

please explain your opinion about the usefulness of the ecip ist: 9 .

in your opinion , how useful is the ecip facility dashboard for reducing risk at ci facilities ? .

10 .

how often have you heard each of the following reasons from facilities who declined to participate in an ecip site visit ? .

 ( select one answer in each row. ) .

a .

the facility does not want to participate in additional facility assessments because it is already subject to federal or state regulation / inspection .

b .

the facility does not have time or resources to participate .

80 c. facility owners and operators are not willing to sign protected critical infrastructure information express statements due to legal concerns over the protection and dissemination of the data collected .

d. the entity that owns / oversees the facility declines to participate as a matter of policy .

e. facility owners and operators have a diminished perception of threat against the facility .

f. the facility already received a risk assessment through a private company and participation in the voluntary assessment would be redundant or duplicative .

g. identification of security gaps may render the owner of the facility liable for damages should an incident occur .

what other reasons , if any , have you heard for facilities declining ecip site visits ? .

11 .

have you found that higher priority facilities ( level 1 or 2 ) are more or less likely to participate in ecip site visits than lower priority facilities ? .

80 12 .

if you answered somewhat less likely or much less likely , what do you see as the reasons for the lower participation by the higher priority facilities ? .

13 .

what factors do you believe are important to facilities considering participating in an ecip site visit ? .

14 .

how often have you heard each of the following reasons from facilities who declined to participate in an ecip ist ? .

 ( select one answer in each row. ) .

a .

the facility does not want to participate in additional facility assessments because it is already subject to federal or state regulation / inspection .

b .

the facility does not have time or resources to participate .

c. facility owners and operators are not willing to sign protected critical infrastructure information express statements due to legal concerns over the protection and dissemination of the data collected .

d. the entity that owns / oversees the facility declines to participate as a matter of policy .

e. facility owners and operators have a diminished perception of threat against the facility .

79 f. the facility already received a risk assessment through a private company and participation in the voluntary assessment would be redundant or duplicative .

g. identification of security gaps may render the owner of the facility liable for damages should an incident occur .

h. facility's security program is not yet mature enough to benefit from participation .

what other reasons , if any , have you heard for facilities declining to participate in an ecip ist ? .

15 .

have you found that higher priority facilities ( level 1 or 2 ) are more or less likely to participate in ecip ists than lower priority facilities ? .

16 .

if you answered somewhat less likely or much less likely , what do you see as the reasons for the lower participation by the higher priority facilities ? .

17 .

how much of an incentive do you believe each of the following are for encouraging participation in an ecip ist ? .

 ( select one answer in each row. ) .

no incentive don't know d. appeal to public service ( patriotic duty ) if you responded not applicable to any of the sectors above , please explain .

20 .

are you aware of any factors that drive differing levels of participation in the voluntary ecip initiative program by sector ? .

please explain .

21 .

in your opinion , how useful are savs as a tool for reducing risk at ci facilities ? .

please explain your opinion about the usefulness of savs: 22 .

how often have you heard each of the following reasons from facilities who declined to participate in a sav ? .

 ( select one answer in each row. ) .

a .

the facility does not want to participate in additional facility assessments because it is already subject to federal or state regulation / inspection .

b .

the facility does not have time or resources to participate .

c. facility owners and operators are not willing to sign protected critical infrastructure information express statements due to legal concerns over the protection and dissemination of the data collected .

d. the entity that owns / oversees the facility declines to participate as a matter of policy .

80 e. facility owners and operators have a diminished perception of threat against the facility .

f. the facility already received a risk assessment through a private company and participation in the voluntary assessment would be redundant or duplicative .

g. identification of security gaps may render the owner of the facility liable for damages should an incident occur .

h. facility's security program is not yet mature enough to benefit from participation .

if you responded not applicable to any of the sectors above , please explain .

28 .

are you aware of any factors that drive differing levels of participation in the voluntary sav program by sector ? .

please explain .

29 .

what challenges , if any , do you face when implementing voluntary ci protection programs associated with ecips and savs ? .

30 .

are you ready to submit your final completed survey to gao ? .

 ( this is equivalent to mailing a completed paper survey to us .

it tells us that your answers are official and final. ) .

no , my survey is not yet complete - to submit your final responses , please click on "exit" below" save your responses for later , please click on "exit" below" you may view and print your completed survey by clicking on the summary link in the menu to the left .

in addition to the contact named above , john f. mortin , assistant director , and anthony defrank , analyst - in - charge , managed this assignment .

andrew m. curry , katherine m. davis , michele c. fejfar , lisa l. fisher , mitchell b. karpman , thomas f. lombardi , and mona e. nichols - blake made significant contributions to the work .

critical infrastructure protection: dhs has taken action designed to identify and address overlaps and gaps in critical infrastructure security activities .

gao - 11-537r .

washington , d.c.: may 19 , 2011 .

critical infrastructure protection: dhs efforts to assess and promote resiliency are evolving but program management could be strengthened .

gao - 10-772 .

washington , d.c.: september 23 , 2010 .

critical infrastructure protection: update to national infrastructure protection plan includes increased emphasis on risk management and resilience .

gao - 10-296 .

washington , d.c.: march 5 , 2010 .

the department of homeland security's ( dhs ) critical infrastructure protection cost - benefit report .

gao - 09-654r .

washington , d.c.: june 26 , 2009 .

information technology: federal laws , regulations , and mandatory standards to securing private sector information technology systems and data in critical infrastructure sectors .

gao - 08-1075r .

washington , d.c.: september 16 , 2008 .

risk management: strengthening the use of risk management principles in homeland security .

gao - 08-904t .

washington , d.c.: june 25 , 2008 .

critical infrastructure: sector plans complete and sector councils evolving .

gao - 07-1075t .

washington , d.c.: july 12 , 2007 .

critical infrastructure protection: sector plans and sector councils continue to evolve .

gao - 07-706r .

washington , d.c.: july 10 , 2007 .

critical infrastructure: challenges remain in protecting key sectors .

gao - 07-626t .

washington , d.c.: march 20 , 2007 .

homeland security: progress has been made to address the vulnerabilities exposed by 9 / 11 , but continued federal action is needed to further mitigate security risks .

gao - 07-375 .

washington , d.c.: january 24 , 2007 .

critical infrastructure protection: progress coordinating government and private sector efforts varies by sectors' characteristics .

gao - 07-39 .

washington , d.c.: october 16 , 2006 .

information sharing: dhs should take steps to encourage more widespread use of its program to protect and share critical infrastructure information .

gao - 06-383 .

washington , d.c.: april 17 , 2006 .

risk management: further refinements needed to assess risks and prioritize protective measures at ports and other critical infrastructure .

gao - 06-91 .

washington , d.c.: december 15 , 2005 .

protection of chemical and water infrastructure: federal requirements , actions of selected facilities , and remaining challenges .

gao - 05-327 .

washington , d.c.: march 28 , 2005 .

homeland security: agency plans , implementation , and challenges regarding the national strategy for homeland security .

gao - 05-33 .

washington , d.c.: january 14 , 2005 .

