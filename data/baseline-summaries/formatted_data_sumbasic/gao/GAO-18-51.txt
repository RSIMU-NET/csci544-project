the federal government plans to spend almost $96 billion on information technology ( it ) in fiscal year 2018 .

however , we have previously found that the effective and efficient acquisition of it investments has been a longstanding challenge in the federal government and that it expenditures have often resulted in significant cost overruns , schedule delays , and questionable mission - related achievements .

due to these and other issues , in february 2015 , we added improving the management of it acquisitions and operations to our list of high - risk areas of concern for federal government agencies .

in addition , to achieve increased oversight of federal it programs , in december 2014 , congress stated in its explanatory statement accompanying the consolidated and further continuing appropriations act , 2015 , that the office of management and budget ( omb ) is to identify the 10 highest priority it investment programs ( hereafter referred to as the top 10 high priority programs ) that are under development across federal agencies and report on their status each quarter .

further , in an explanatory statement accompanying the consolidated appropriations act , 2016 , congress stated that the u.s. digital service ( usds ) , a component of omb , is to provide a quarterly status report on current usds projects , including the top 10 high priority programs .

you asked us to review omb's efforts to provide oversight of the top 10 high priority it programs .

our specific objectives were to evaluate ( 1 ) omb's process for identifying , overseeing , and reporting on the high priority it investment programs and ( 2 ) usds's process for identifying and prioritizing its projects , including its consideration of the high priority programs , and its reporting on the projects .

to address the first objective , we obtained and reviewed reports that omb sent to congress in june 2015 and june 2016 on the high priority it programs to discern any information about the process that was used for identifying and overseeing these programs .

in the absence of documented procedures specific to the high priority programs , we also reviewed omb memorandums addressing the identification and oversight of high impact programs , which are programs that omb has determined merit additional support and oversight by omb and / or agency leadership due to factors such as their size and cost .

according to omb staff , these high impact programs are a broader set of programs from which omb selects the top 10 high priority programs .

further , we reviewed an omb work plan charting the process that omb's office of e - government and information technology ( e - gov ) used to identify and oversee high impact programs .

we also reviewed screenshots of a tool that omb uses to analyze agency it investment data as a part of their process to identify and oversee high impact programs .

the tool draws information from it portfolio summaries that agencies submit to omb as part of the budget process .

in addition , we interviewed relevant e - gov office staff responsible for preparing the top 10 high priority reports , including the unit chief for the agency oversight and implementation team , regarding the processes they used for identifying the programs and the oversight they provide to such programs .

we corroborated what we learned from omb about its oversight of high priority and high impact programs by obtaining information from eight agencies identified as having one of the top 10 high priority it programs in the june 2016 report to congress .

we asked relevant officials within these agencies , among other things , how the oversight that omb provided to their high priority it programs had differed from that provided to other major investments .

in addition , we asked the officials whether this oversight included the programs being discussed with omb during quarterly portfolio review meetings known as portfoliostat .

finally , we analyzed omb's june 2015 and june 2016 reports to congress identifying the top 10 high priority it , which were issued in response to congress's explanatory statement for fiscal year 2015 .

to address the second objective , we reviewed usds's documentation , including its january 2017 master project list , and interviewed staff , including the former deputy administrator .

in addition , we discussed with usds staff the findings from a previous gao engagement that assessed the organization's project selection process to determine any changes to their process since august 2016 .

upon determining that the process had not changed , we relied on the prior engagement's evaluation of the process to determine if it addressed the best practice identified in gao's information technology investment management framework .

this framework calls for organizations to use their defined selection process , including predefined selection criteria , to select their projects .

we also analyzed omb's december 2016 and july 2017 reports to congress that were issued in response to the explanatory statement for fiscal year 2016 , which stated that omb was to report on the status of usds's projects , including the top 10 high priority it programs .

we conducted this performance audit from july 2016 to november 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the federal government plans to invest about $96 billion in fiscal year 2018 for it that is critical to the health , economy , and security of the nation .

however , prior it expenditures have often resulted in significant cost overruns , schedule delays , and questionable mission - related achievements .

for example , the department of health and human services' website , healthcare.gov , and its supporting systems , which were to facilitate the establishment of a federal health insurance marketplace by january 2014 , encountered significant cost increases , schedule slips , and delayed functionality .

in a series of reports , we identified numerous planning , oversight , security , and system development challenges faced by this program .

for almost two decades , the department of veterans affairs ( va ) has undertaken numerous initiatives with the department of defense ( dod ) that were intended to advance the ability of the two departments to share electronic health records .

in our report of the departments' efforts in 2015 , we reported that the departments had not identified outcome - oriented goals and metrics to clearly define what they aimed to achieve from their interoperability efforts , resulting in numerous failures .

during most of the last 20 years , va has also been planning to modernize its system separately from dod .

recently , the secretary of va announced that the department plans to use the same electronic health record system that dod is in the process of acquiring .

however , the significant challenges that have confronted va in its efforts contributed to our designation of va health care as a high risk area .

the department of homeland security's u.s .

citizenship and immigration services' transformation program , which was initiated to address processing inefficiencies and transform the agency's current paper - based system into an electronic account - based system , has faced continual management and development challenges , limiting its progress and ability to achieve its goals of enhanced national security and system integrity , better customer service , and operational efficiency .

the u.s .

citizenship and immigration services estimates that the program's cost increased by approximately $1 billion and its schedule was delayed by over 4 years from its initial approved baseline .

the office of personnel management's retirement systems modernization program , which was intended to improve the efficiency and effectiveness of its retirement claims processing , was canceled in february 2011 after the agency had spent approximately $231 million on a third attempt to automate the processing of federal employee retirement claims .

as previously stated , due to the challenges associated with acquiring it across the federal government , in 2015 , we added improving the management of it acquisitions and operations to our list of high - risk areas .

we recently issued an update to our high - risk report and determined that , while progress has been made in addressing the high - risk area of it acquisitions and operations , significant work remains to be completed .

for example , as of may 2017 , omb and federal agencies had implemented 380 ( or about 47 percent ) of the 803 recommendations that we had made from fiscal years 2010 through 2015 related to it acquisitions and operations .

by law , omb is to oversee federal agencies' management of information and information technology .

within omb , primary responsibility for oversight of federal it has been given to the administrator of the office of electronic government and information technology , who is also called the federal chief information officer ( federal cio ) .

according to omb , this oversight responsibility covers about 800 major and nearly 5,700 non - major it investments across the federal government .

as a part of the oversight , the e - gov office develops policy and reviews federal agencies' it strategic plans .

in addition , omb establishes processes to analyze , track , and evaluate the risks and results of it investments made by executive agencies , and issues guidance on processes for selecting and overseeing agency privacy and security protections for information and information systems .

omb has also implemented a series of initiatives to improve the oversight of underperforming investments and more effectively manage it .

these initiatives include the following: federal it dashboard .

in june 2009 , to further improve the transparency into and oversight of federal agencies' it investments , omb deployed the federal it dashboard , a public website with information on the performance of these investments .

omb provided guidance to the agencies on the information they should maintain and update on the dashboard .

currently , the dashboard displays information on the cost , schedule , and performance of close to 800 major it investments at 26 federal agencies .

in addition , agencies are to submit ratings from their cios to the dashboard , which , according to omb's instructions , should reflect the level of risk facing an investment relative to that investment's ability to accomplish its goals .

the public display of these data is intended to allow omb , other oversight bodies , and the general public to hold agencies accountable for mission - related outcomes .

over the past 7 years , we have issued a series of reports that have noted both significant steps omb has taken to enhance the oversight , transparency , and accountability of federal it investments by creating the dashboard , as well as issues with the accuracy and reliability of the data it contains .

techstat reviews .

in january 2010 , the federal cio began leading techstat reviews — face - to - face meetings to discuss whether to terminate or turn around it investments that are in danger of failing or are not producing results .

these meetings involved omb and agency leadership and were intended to increase accountability and improve performance .

omb reported that federal agencies achieved over $3 billion in cost savings or avoidances as a result of these reviews in 2010 .

subsequently , it empowered agency cios to begin holding their own techstat reviews by june 2012 .

omb's 2015 guidance specified that the techstat reviews were to be held with agency leadership , not led by omb as in the past , and that agencies were only required to notify omb of the meetings' occurrence and report the results .

in november 2015 , we testified that omb had conducted only one techstat review between march 2013 and october 2015 , and had not listed any related savings in its quarterly reporting to congress since june 2012 .

in april 2017 , omb reported that it had not led a techstat review since 2015 .

a report issued by the federal cio council in january 2017 , entitled the state of federal information technology , noted that shifting techstat reviews from omb to agencies had diminished the executive scrutiny and impact of the initiative .

portfoliostat sessions .

to better manage existing it systems , in 2012 , omb launched the portfoliostat initiative , which required agencies to conduct an annual , agency - wide portfolio review to , among other things , reduce commodity it spending and demonstrate how their investments aligned with the agency's mission and business functions .

these reviews were to be held between the federal cio and agency leadership .

in 2014 and 2015 , omb's portfoliostat guidance also called for it and agencies to identify high impact it programs that merited additional support and oversight by omb and / or agency leadership , and for these programs to be discussed during a portfoliostat session .

the 2015 guidance also changed the frequency of the portfoliostat sessions from annually to quarterly , and the level of participation to no longer require attendance by the federal cio or the agency's deputy secretary .

the federal information technology acquisition reform provisions ( commonly referred to as fitara ) enacted as a part of the carl levin and howard p. ‘buck' mckeon national defense authorization act for fiscal year 2015 aimed to improve federal it acquisition and operations and recognized the importance of these omb initiatives by incorporating certain requirements into the law .

for example , among other things , the act requires omb to publicly display investment performance information and review federal agencies' it investment portfolios .

further , as previously mentioned , the december 2014 explanatory statement for the consolidated appropriations act , 2015 , stated that omb was to identify the top 10 high priority it programs under development in the federal government and report on their status quarterly .

additionally , in december 2015 , in the explanatory statement for the consolidated appropriations act , 2016 , congress stated that usds , an omb component , was to provide a quarterly status report on its current projects , including the top 10 high priority programs .

the current mission of usds is to deliver better government services to the american people through technology and design .

usds is focused on , among other things , improving the nation's most important digital services used by the public , and modernizing procurement processes and practices for the digital era .

to execute its mission , usds recruits private sector experts , such as it engineers and designers and leading civil servants , and deploys small teams to federal agencies .

it selects which projects it will apply resources to and generally initiates the effort with the federal agency that owns the it projects .

in august 2016 , we reported that usds had developed procedures and criteria for selecting and prioritizing projects to work on .

the current administration has initiated additional efforts aimed at improving federal it , including digital services .

specifically , in march 2017 , the administration established the office of american innovation , which has a mission to , among other things , make recommendations to the president on policies and plans aimed at improving federal government operations and services and modernizing federal it .

in doing so , the office is to consult with both omb and the office of science and technology policy .

further , in may 2017 , the administration established the american technology council , which has a goal of helping to transform and modernize federal agency it and how the federal government uses and delivers digital services .

the federal cio and the usds administrator are members of this council .

omb reported that it undertook a structured approach to identifying the top 10 high priority it programs it reported to congress in june 2015 and june 2016 .

specifically , staff in the e - gov office , including the unit chief for the agency oversight and implementation team , stated that they chose the top 10 programs from a list of high impact programs that omb separately maintains .

they added that their approach was not guided by any documented procedures or scoring techniques to distinguish the programs .

analysts in the e - gov office told us that , to identify the high impact programs from which the top 10 high priority programs were selected , they used program information from it portfolio summaries , monthly it dashboard updates , and quarterly integrated data collection submissions that omb receives from agencies .

they also considered several additional factors , such as risk exposure , public impact , public use , criticality to agency mission , size , and cost .

in addition , they considered input from usds leadership and omb budget examiners , as well as our reports , inspectors general reports , and cio risk ratings .

further , the analysts stated that omb sought input from officials of the 24 chief financial officers act agencies to gain a better understanding of the importance of each program .

in the end , based on all of the information considered , e - gov staff made a judgment call regarding which of the agencies' programs to identify as being high impact .

according to these staff , on average , two high impact programs are identified for each of the 24 agencies , with at least one program being identified for each agency , and as many as four programs being identified for some larger agencies .

further , according to the staff , the federal cio approved the process for selecting the high impact programs .

in addition , in determining the top 10 high priority programs , the e - gov staff stated that they identified on the high impact list those programs that they believed were representative of the most important it programs across federal agencies .

they also considered other factors , such as whether a program had generated legislative interest or had performance issues .

they added that the federal cio then approved the list of top 10 programs that they had selected .

omb subsequently issued two reports to congress — in june 2015 and june 2016 — that identified the top 10 high priority programs .

along with the status of each program , the reports identified the it investments that were a part of each program as well as total it spending , average cio risk rating , major milestones , and the level of involvement usds has with the program .

table 1 lists the programs that omb identified in the two reports .

as shown in the table , omb made two changes to the top 10 list in 2016 .

specifically , it replaced the social security administration's service modernization program with the agency's disability case processing system due to a change in focus within the agency .

in addition , it replaced va's medical appointment scheduling system with va's medical 21st century development core program , which is a larger effort that encompasses the scheduling system .

e - gov staff reported that the high priority programs were already receiving greater oversight than what was provided to the other major programs due to their high impact designation .

specifically , the staff stated that omb provides additional oversight to high impact it programs by regularly communicating about them through quarterly , monthly , weekly , or daily meetings with agency cios and program staff , depending on the risk and profile of the program .

in contrast , communication is much less frequent for other major it systems .

the staff added they also discuss high impact programs with agencies during quarterly portfoliostat meetings .

further , they stated that they can request that agencies perform a techstat review , if needed , for a troubled high impact program .

most agencies that owned the high priority programs identified in the june 2016 report confirmed that omb had already provided increased oversight for the high priority programs when they were originally designated as high impact programs .

specifically , officials from six of the eight agencies stated that the programs were discussed during quarterly portfoliostat sessions , and officials from five agencies stated that omb had provided action items for them to address with regard to their programs .

further , officials from six agencies stated that omb's oversight included periodic meetings ( eg , daily , weekly , or bi - weekly ) .

its oversight also included participation in a techstat review of a high impact program performed by the social security administration .

nevertheless , while additional omb oversight of the high impact programs ( and , accordingly , the identified high priority programs ) is a positive step , the federal cio was not directly involved in this oversight .

according to the e - gov staff , the federal cio does not typically get involved with overseeing individual it programs due to the large number of programs .

however , the results of past cio - led techstat reviews suggest that the federal cio's involvement in overseeing such programs does have significantly positive results .

specifically , as previously mentioned , cio - led techstat reviews of it investments performed in 2010 resulted in $3 billion in savings and cost avoidance .

further , during a september 2016 comptroller general forum to explore challenges and opportunities for improving federal it acquisitions and operations , current and former cios and other participants pointed to the importance of omb's oversight and guidance , and specifically , to the role of the federal cio in helping to ensure effective it governance .

the participants cited specific omb initiatives undertaken by the federal cio , including the techstat reviews that had resulted in greater accountability and positive results .

thus , without the involvement of the federal cio more directly and regularly in the oversight of high impact and high priority programs , including leading techstat reviews for the programs , omb is likely to miss significant opportunities to improve accountability for and achieve positive results from the federal government's it investments .

omb issued two reports on the high priority programs — one in 2015 and another in 2016 .

while these reports provided the requested information they were not issued quarterly .

according to the e - gov staff , omb was not able to report quarterly on the programs because of other competing reporting requirements , limited resources available to draft the report , and the amount of time it takes to get its reports fully reviewed .

moreover , the staff stated that they stopped issuing the high priority reports in 2016 because they believe the explanatory statement to the consolidated appropriations act , 2016 , no longer directed them to continue reporting on the programs .

specifically , the explanatory statement directed usds to provide a quarterly report to the committees on appropriations of the house and senate describing the status of current usds teams and projects , including the top 10 high priority programs , a list of usds accomplishments , and agency project proposals .

both e - gov staff and usds staff said they determined this to mean that omb should report on the usds projects considered to be high priority given usds's responsibilities .

in addition , usds staff stated that they did not receive any feedback from congressional stakeholders indicating otherwise .

however , continued identification and reporting on the top ten high priority programs , and not just usds projects , would further enhance congressional oversight by providing congressional stakeholders with information on high priority programs that is not readily available .

such information could also be useful for current administration it governance entities such as the office of american innovation and the american technology council , to assist them with prioritizing their efforts to modernize federal agency it .

usds has developed a process for identifying and prioritizing the it projects to which it provides support .

moreover , as we have previously reported , its project selection process is consistent with best practices , which state that organizations should establish and implement procedures for prioritizing projects that include identifying selection criteria to help consistently select projects based on their contributions to the strategic goals of the organization .

in explaining the process used to identify projects , usds staff stated that they obtained input from various sources , including the federal it dashboard , leadership of the relevant federal agency , e - gov analysts , and gao reports .

in addition , the staff said they considered the high impact programs identified by the e - gov office ; they also coordinated with e - gov analysts through monthly meetings to incorporate issues of significance to e - gov into their selection process .

to further facilitate the selection process , the usds staff established three questions as criteria for prioritizing agencies' it projects , in the following order of importance: ( 1 ) what will do the greatest good for the greatest number of people in the greatest need ? .

 ( 2 ) how effective and cost efficient will the usds investment be ? .

 ( 3 ) what potential exists to scale or reuse a technological solution across the government ? .

the staff said they used the criteria to create a list of all potential projects , including their descriptions and information on resource needs ; they updated the list when they identified additional projects that met the criteria .

this list was subsequently used by usds leadership to make decisions about which projects to pursue .

according to the staff , an important consideration when selecting a project was whether there was executive sponsorship from the agency .

they added that executive sponsorship ensures that usds has the help it needs to make changes to the projects , and it affects the efficiency and cost - effectiveness of usds's investment .

projects that were not selected went into a backlog that is to be used to select a project when a team becomes available to work on a new engagement .

usds has issued two reports to congress on the status of its projects — one in december 2016 and the other in july 2017 .

the report issued in december 2016 summarized the status of 11 projects that usds is engaged in at federal agencies , and 3 broader initiatives that are intended to improve the performance and cost - effectiveness of government digital services .

according to usds staff , the 11 projects had broad impact and had made the most significant progress .

for example , these included the va's vets.gov project , which is intended to assist the department in developing a new digital application for healthcare , and dod's defense travel system , a system that facilitates travel for all dod employees .

further , usds reported on the status of their efforts for these three initiatives: modernizing procurement processes , development of federal shared services , and hiring top technical talent .

the july 2017 report included summaries for 10 projects , including 5 new projects .

specifically , the new projects were the general service administration's login.gov project , the small business administration's effort to modernize small business certification for government contractors , dod's advisor network system , dod's defense personal property system , and transforming federal it procurement through digital acquisition training .

in addition , 6 projects that were discussed in the december 2016 report were not included in the july 2017 report .

table 2 provides a complete list of the projects identified in the two reports .

our analysis determined that four of the projects identified in usds's reports were among the high priority programs that omb's e - gov office had identified in its june 2015 and june 2016 reports to congress .

these projects were healthcare.gov , disability claim processing , modernizing the immigration system at the department of homeland security , and improving the visa program at the department of state .

however , usds's report does not specify this or provide an update on the status of the other high priority programs .

usds staff stated that they did not address all of the top 10 high priority programs because , as stated earlier in this report , they interpreted congress's 2016 request as being focused on usds's priority projects and not on the programs previously identified by e - gov .

as mentioned earlier , however , continuing to identify and report on the top 10 high priority it programs while also reporting on usds's projects would further enhance congressional oversight and current administration it governance entities' efforts by providing stakeholders with information on high priority programs and usds projects that is not readily available .

further , although usds was directed to report quarterly , it did not do so .

instead , it issued a report in december 2016 , nearly a year after congress's direction , and in july 2017 , nearly 7 months after its first report .

in discussing this matter , usds staff said they were not able to report quarterly due to the time and effort needed to prepare and review a report .

as a result , usds's reporting did not provide congressional stakeholders with the timely information needed to support their oversight responsibilities .

while omb's 2015 and 2016 reports to congress on the top 10 high priority programs included the status of the programs , their total it spending , and other information to assist congress in monitoring the progress of critical programs , omb did not issue the reports quarterly as directed in the explanatory statement .

in addition , omb does not plan on continuing to issue the top 10 high priority reports because it believes that , in 2016 , congress directed the agency to instead focus on providing a status of usds's most important projects .

further , omb's december 2016 and july 2017 reports on usds's projects did not address the top 10 high priority programs across the government .

however , continued reporting on the top 10 high priority programs would further enhance congressional oversight by providing congressional stakeholders with information that is not readily available on those programs in the greatest need of attention .

reporting on the top 10 high priority programs could also be useful for it governance entities such as the office of american innovation and the american technology council , to assist them with prioritizing their efforts to modernize federal agency it .

moreover , omb did not issue the high priority programs and usds reports on a quarterly basis as requested .

without omb's quarterly reporting on the progress of both the top 10 high priority programs and the status of the usds projects , congressional stakeholders and others may lack the timely information they need to support their oversight and other responsibilities .

finally , while additional omb oversight of the high impact programs ( and , accordingly , the identified high priority programs ) is a positive step , the federal cio was not directly involved in the oversight of these programs .

based on the positive impact of direct federal cio involvement in leading investment reviews in the past , such involvement could significantly improve program outcomes .

we are making the following three recommendations to omb: the director of omb should continue to identify and report to congress on the status of the top 10 high priority it programs and the extent to which usds is involved in the programs , as was done in june 2015 and june 2016 .

in doing so , the director should ensure that these reports are issued quarterly .

 ( recommendation 1 ) the director of omb should ensure that the federal cio is directly involved in the oversight of high priority programs .

 ( recommendation 2 ) the director of omb should continue to report on the status of usds projects .

in doing so , the director should ensure that the reports are issued quarterly .

 ( recommendation 3 ) .

we received comments on our draft report via e - mail from the omb liaison to gao .

in the comments , omb did not specifically state whether it agreed or disagreed with our recommendations .

rather , omb stated that it has concerns with gao's alternative interpretations of law and that gao's findings and conclusions are rooted in an incorrect legal interpretation of omb's annual appropriation .

specifically , it stated that gao considers reporting requirements specific to an annual appropriation to apply for all future annual appropriations .

however , omb's characterization is incorrect , as we did not assert this legal conclusion .

as stated in our report , in the explanatory statement accompanying the consolidated and further continuing appropriations act , 2015 , congress directed omb to identify the 10 highest priority it investment programs ( referred to in our report as the top 10 high priority programs ) that are under development across federal agencies and report on their status each quarter .

subsequently , the explanatory statement accompanying the consolidated appropriations act , 2016 , directed usds to provide a quarterly status report on , among other things , current usds projects , including the top 10 high priority programs .

our report does not conclude that the language of either explanatory statement establishes a legally binding requirement , whether applicable only to the subject fiscal year or beyond .

gao's conclusions are based on the view that continued identification and reporting on the top 10 high priority programs , and not just usds projects , would help to enhance congressional oversight .

identifying and reporting on the top 10 high priority programs is important because such information is not readily available .

we have revised relevant statements in the report to clarify our message in this regard .

further , while gao did not assert a legal conclusion , we have , nonetheless , removed all references to our “interpretation” of the explanatory statement so as to avoid the inference that we are making legal conclusions .

omb also said our conclusion that it has stopped reporting altogether is incorrect .

however , our report does not state that omb has stopped reporting altogether .

rather , our report states that omb stopped issuing reports on the top 10 high priority it programs due to its interpretation of the 2016 explanatory statement and , instead , switched to reporting on the status of usds's projects .

in addition , omb stated that , while it agreed that the reports have not been submitted on a quarterly basis , it provided congress with the requested information for the four quarters of the relevant fiscal years .

in addition , omb stated that usds is currently discussing with congressional stakeholders whether providing quarterly briefings , instead of reports , would address the quarterly reporting requirement .

as noted in our report , timely ( i.e. , quarterly ) information would enhance congressional and other stakeholders' oversight responsibilities .

therefore , we maintain that our recommendation for reporting quarterly is appropriate .

finally , omb stated that it disagreed with our conclusion that the lack of personal federal cio involvement in high priority it programs had resulted in inadequate oversight .

while we state that the federal cio was not directly involved in overseeing the high priority programs , we did not conclude that this resulted in inadequate oversight .

rather , we stated that the results of past cio - led techstat reviews suggest that more direct and regular involvement of the federal cio would improve accountability and achieve positive results for the federal government's investments .

thus , we continue to believe that our recommendation to ensure that the federal cio is directly involved in overseeing high priority programs is appropriate .

omb also provided technical comments , which we have incorporated into the report as appropriate .

we are sending copies of this report to interested congressional committees , the director of the office of management and budget , and other interested parties .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix i .

in addition to the contact named above , sabine paul ( assistant director ) , scott borre ( analyst in charge ) , nancy glover , lori martinez , bradley roach , and marshall williams , jr. made key contributions to the report .

