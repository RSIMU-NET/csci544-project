the federal government faces a number of significant fiscal , financial management , and performance management challenges in responding to the diverse and increasingly complex issues it seeks to address .

the reporting requirements of the government performance and results act of 1993 ( gpra ) were intended to provide both congressional and executive decision makers with more objective information on the relative effectiveness and efficiency of federal programs and spending .

although gpra helped improve the availability of agency performance information , federal managers reported limited use of performance data for decision making .

the gpra modernization act of 2010 ( gprama , or the act ) aims to ensure that agencies use performance information in decision making and holds them accountable for achieving results and improving government performance .

the office of management and budget ( omb ) , too , has encouraged agencies to strengthen their program evaluations — systematic studies of program performance — and expand their use of evidence and evaluation in budget , management , and policy decisions to improve government effectiveness .

however , in our 2013 survey of federal managers , we found that their use of performance information had stagnated since our 2007 survey and that , among other things , their use was hindered by inadequate staff expertise in performance measurement and analysis as well as a widespread lack of program evaluations .

this report is one of a series responding to gprama's mandate that we examine implementation of the act .

in this report , we assess federal agencies' current evaluation capacity and identify how some gprama provisions and other activities have contributed to its improvement .

specifically , our objectives were to learn: 1 .

what are the key elements and extent of agency evaluation capacity — that is , the ability to obtain and use evaluations in decision making ? .

2 .

what progress , if any , has been made since 2010 across the government in improving evaluation capacity ? .

3 .

what activities , if any , especially those related to gprama provisions , have agencies found useful in building their evaluation capacity ? .

to answer our first objective , we reviewed published domestic and international research and commentary on key components of organizational capacity for program evaluation , including gao reports and recommendations of national and international evaluation organizations .

we identified the key organizational characteristics , expertise , and policies believed either to be required for or to indicate the ability to obtain credible evaluations of agency programs and policies and to use the results in management and policy decisions .

we also identified strategies used or proposed for building an organization's evaluation capacity .

to answer all three objectives , we surveyed the performance improvement officers ( pio ) of the 24 executive branch agencies covered by the chief financial officers ( cfo ) act of 1990 , as amended .

the survey questionnaire was designed to obtain information on their agencies' elements of evaluation capacity as described above , and their observations and perceptions of the usefulness of various resources and activities for building their agencies' capacity to produce evaluations and use the results in decision making .

we administered the web - based survey from may through june 2014 , receiving responses from all 24 agencies .

throughout this report except where specifically noted , when we refer to agencies , we are referring to both cabinet departments and independent agencies .

 ( more information on the survey is in appendix i .

the survey questions and summarized results are in appendix ii. ) .

in addition , we reviewed examples of agency evaluation plans and policies that the survey respondents provided .

we also interviewed omb and office of personnel management ( opm ) staff about their capacity - building efforts , reviewed agency guidance and memorandums , and attended related interagency information - sharing forums .

we conducted this performance audit from september 2013 to november 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

program evaluations are systematic studies that use research methods to address specific questions about program performance .

evaluation is closely related to performance measurement and reporting .

whereas performance measurement entails the ongoing monitoring and reporting of program progress toward preestablished goals ; program evaluation typically assesses the achievement of a program's objectives and other aspects of performance in the context in which the program operates .

in particular , evaluations can be designed to isolate the causal impacts of programs from other external economic or environmental conditions in order to assess a program's effectiveness .

thus , an evaluation study can provide a valuable supplement to ongoing performance reporting by measuring results that are too difficult or expensive to assess annually , explaining the reasons why performance goals were not met , or assessing whether one approach is more effective than another .

evaluation can play a key role in program planning , management , and oversight by providing feedback on both program design and execution to program managers , legislative and executive branch policy officials , and the public .

in our 2013 survey of federal managers , we found that while only about a third had recent evaluations of their programs or projects , the majority of those who had evaluations reported that they contributed to understanding program performance , sharing what works with others , and making changes to improve program management or performance .

gprama made changes to agency performance management roles , planning and review processes , and reporting intended to ensure that agencies used performance information in decision making and were held accountable for achieving results and improving government performance .

the act required the 24 cfo act agencies and omb to establish agency and governmentwide cross - agency priority goals , review progress on those goals quarterly , and report publicly on their progress and strategies to improve performance , as needed , on a governmentwide performance website .

it also encouraged a more detailed and comprehensive understanding of those strategies by requiring agencies to identify and coordinate the program activities , organizations , regulations , policies , and other activities — both internal and external — that contribute to each agency priority goal .

gprama , along with related omb guidance , established and defined performance management responsibilities for agency officials in key management roles: the chief operating officer ( coo ) , the pio , and a goal leader responsible for coordinating efforts to achieve the cross - agency and agency priority goals .

the pio role was created in 2007 by executive order .

gprama established the role in law and specified that it be given to a “senior executive” at each agency who reports directly to the agency's coo or to its deputy agency head .

the pio is to advise the head of the agency and the coo on goal setting , measurement , and reviewing progress on the agency priority goals .

omb guidance gave pios a central role in promoting agency use of evaluation and other evidence to improve program performance , describing their roles as “ .

 .

 .

driving performance improvement efforts across the organization , by using goal - setting , measurement , analysis , evaluation and other research , data - driven performance reviews on progress , cross - agency collaboration , and personnel performance appraisals aligned with organizational priorities.” “help components , program office leaders and goal leaders to identify and promote adoption of effective practices to improve outcomes , responsiveness and efficiency , by supporting them in .

 .

 .

securing evaluations and other research as needed .

 .

 .

and creating a network for learning and knowledge sharing about successful outcome - focused , data - driven performance improvement methods across all levels of the organization and with delivery partners.” the act also charged the performance improvement council ( pic ) , the office of personnel management ( opm ) , and omb with responsibilities to improve agency performance management capacity .

the pic is an interagency council that was created by executive order , but gprama established it in law and specified that it would be chaired by the omb deputy director for management and that membership would include the pios from all 24 cfo act agencies , as well as any others .

the pic's duties include facilitating agencies' exchange of successful practices and the development of tips and tools to strengthen agency performance management , and assisting omb in implementing certain gprama requirements .

the pic holds “principals only” and broader meetings open to other agency staff , has formed several working groups that focus on issues relating to implementing gprama and related guidance , and provides a networking forum for staff from different agencies who are working on similar issues .

in 2012 through 2014 , omb and the pic supported several interagency forums on evaluation and evidence that were open to all federal agency staff .

the act charged opm with ( 1 ) identifying key skills and competencies needed by federal employees for developing goals , evaluating programs , and analyzing and using performance information for improving governmental efficiency and effectiveness ; ( 2 ) incorporating those skills and competencies into relevant position classifications ; and ( 3 ) working with agencies to incorporate these skills and competencies into agency training .

opm identified core competencies for performance management staff , pios , and goal leaders and published them in a january 2012 memorandum .

opm identified relevant existing position classifications that are related to the competencies and worked with the pic capacity building working group to develop related guidance and tools for agencies .

in december 2012 , the pic released a draft performance analyst position design , recruitment , and selection toolkit .

opm worked with the chief learning officers council and the pic capacity building working group to develop a website — the training and development policy wiki — that lists some resources for personnel performance management and implementing gprama .

opm is currently conducting pilot studies through 2015 , in collaboration with the chief human capital officers council , of how to build staff capacity in several competencies identified as mission critical across government , including data analysis .

opm officials also noted that they make databases , such as the federal employee viewpoint survey , available to agencies for their staff to use in program evaluations .

omb has taken several steps to help agencies develop evaluation capacity by issuing guidance , promoting the exchange of evaluation expertise through the pic , and working selectively with certain agencies .

since 2009 , omb has issued several memorandums urging efforts to strengthen the use of rigorous impact evaluation , and demonstrate the use of evidence and evaluation in budget submissions , strategic plans , and performance plans .

in may 2012 , omb encouraged agencies to designate a high - level official responsible for evaluation who could develop and manage the agency's research agenda and provide independent input to agency policymakers on resource allocation and to program leaders on program management .

in july 2013 , the directors of omb , the domestic policy council , the office of science and technology policy , and the chairman of the council of economic advisers , jointly issued a memorandum encouraging agencies to adopt an “evidence and innovation agenda”: applying existing evidence on what works , generating new knowledge , and using experimentation and innovation to test new approaches to program delivery .

in particular , the memorandum encouraged agencies to exploit existing administrative data to conduct low - cost experiments , and implement outcome - focused grant designs and research clearinghouses to catalyze innovation and learning .

omb staff established an interagency group to promote sharing of evaluation expertise , and organized a series of workshops and interagency collaborations .

the workshops addressed issues such as potential procedural barriers to evaluation ( eg , the paperwork reduction act information collection reviews ) and promising practices for collecting evidence ( eg , developing a common evidence framework ) .

omb staff facilitated the collaboration of staff from the department of education and the national science foundation in developing common standards of evidence for reviewing research proposals , and another group of agencies in developing a common framework of standards for reviewing completed evaluations .

studies of organization or government evaluation capacity have found that it requires analytic expertise and access to credible data as well as organizational support both within and outside the organization to ensure that credible , relevant evaluations are produced and used .

our survey found levels of evaluation expertise , support , and use uneven across the government .

for example , 7 of the 24 agencies have central leaders responsible for evaluation ; in contrast , 7 agencies reported having no recent evaluations for any of their performance goals .

to address our first objective and guide our assessment of agency evaluation capacity , we reviewed the research and policy literature on evaluation capacity , including assessments of agencies in canada and the united kingdom , and guidance from the american evaluation association ( aea ) and the united nations evaluation group .

while the details vary , these frameworks commonly emphasize three general categories of elements of organizational , especially national , evaluation capacity: an enabling environment supporting the use of evidence in management and policymaking: credible information and statistical systems , legislation or policies to institutionalize monitoring and evaluation , public interest in evidence of government performance , and senior leadership commitment to transparency , accountability , and managing for results .

organizational resources to support the supply and use of credible evaluations: a senior evaluation leader ; an evaluation office with clearly defined roles and responsibilities , a stable source of funding , and independence ; an evaluation agenda , policies and tools to ensure study credibility and utility ; staff expertise and access to experts ; and collaboration with program managers and stakeholders .

evaluation results and use: evaluation quality and credibility ; coverage of the agency's key programs or goals ; transparent reporting and public dissemination of reports ; recommendation follow - up ; and the use of evaluation results in program management , policy making , and budgeting .

to learn about federal agencies' evaluation capacity , we surveyed the pios or their deputies at the 24 cfo act agencies because of the central role gprama and omb assigned them to promote agency performance assessment and improvement efforts .

our 2012 survey of pios found that they held senior leadership positions and that most of them were involved in the central aspects of agency performance management to a large extent .

although the pio position was created in 2007 , only one of the initial pios continued to hold this position at the time of our 2014 survey .

half had started serving in this position within the past 2 years .

many of our survey respondents held key senior leadership positions in their agencies: 8 pios served as the agency's chief financial officer , another 4 as assistant secretary or deputy for administration or management .

seventeen reported to their agency's coo , 2 to the agency's administrator or commissioner , and 3 to the agency's cfo .

in order to report on the policies and practices of offices throughout these agencies , we encouraged the pios to consult with others when completing the survey and several indicated that they did so .

gpra represents a central component of the enabling environment for u.s. government evaluation capacity by providing , for over 20 years , a statutory framework for performance management and accountability across the government .

accordingly , most pios reported that their senior leadership demonstrated a commitment to using evidence in management and policy making through agency guidance ( 17 ) , internal agency memorandums ( 12 ) , congressional hearings ( 9 ) , and speeches ( 8 ) .

other avenues offered in comments included budget justifications ( 10 ) and town hall meetings or videos for agency managers and staff ( 2 ) .

moreover , as we have noted previously , gpra has produced a solid foundation of generating and reporting performance information .

three - quarters of the agencies ( 18 ) said that reliable performance data are available on outcomes for all their priority goals , 3 more said data are available for more than half their priority goals .

 ( one of the independent agencies was exempt from developing priority goals. ) .

however , our survey respondents indicated that congressional interest in and requests for program evaluation are not widespread .

although the federal government has long invested in evaluation , about half the agencies ( 13 ) reported having explicit agency - wide authority to use appropriated funds for evaluation .

some pointed to specific legislative authorities , while one pio commented , “evaluation is considered inherent to responsible management and programs use appropriated fund for this purpose.” less than half the agencies ( 10 ) indicated that they had congressional mandates to evaluate specific programs .

however , one - third ( 7 ) indicated that they had neither explicit agency - wide authority nor a program - specific requirement to conduct evaluations .

the importance of this is that in a prior study agency evaluators told us that not having explicit evaluation authority represented a barrier to the use of program funds for evaluation .

our survey asked the pios about the agency resources and policies committed to obtaining credible , relevant evaluations .

their responses indicated uneven levels of development across the agencies .

about half the agencies ( 11 ) reported committing resources to obtain evaluations by establishing a central office responsible for evaluating agency programs , operations , or projects .

however , less than a third of agencies have an evaluation plan or agency - wide policies or guidance for ensuring study credibility .

about one - third of the agencies ( 7 ) reported having assigned responsibility to a single high - level official to oversee their evaluation studies .

although agencies do not need a central evaluation leader in order to conduct credible evaluations , establishing such a position with clear responsibilities sends a message about the importance of evaluation to agency managers .

almost all these individuals ( 6 ) were responsible for setting these agencies' evaluation agendas but only half ( 3 ) were responsible for following - up evaluation recommendations .

similar numbers of departments and independent agencies reported having such officials with titles such as chief evaluation officer , chief strategic officer , and assistant secretary .

according to aea guidance , a central evaluation office can promote an agency's evaluation capacity and provide a stable organizational framework for planning , conducting , or procuring evaluation studies .

all the agencies with a single official responsible for overseeing evaluations also reported having a central office responsible for evaluating agency programs , operations , or projects , but only about half the agencies in total ( 11 ) had a central office .

the central offices could have other responsibilities as well , such as strategic planning .

most of these offices were said to be independent of program offices in making decisions about evaluation design , conduct , and reporting and to have access to analytic expertise through external experts or contractors , but about half were reported to have a stable source of funding ( 6 ) .

funding generally came through regular appropriations , although two agencies reported having evaluation set - asides — that is , the ability to tap a percentage of operating divisions' appropriations for evaluation .

a larger proportion of independent agencies ( 5 of 9 ) than departments ( 6 of 15 ) reported having central offices .

as discussed earlier , having analytic expertise is a critical element of evaluation capacity .

most agencies with a central office responsible for evaluations ( 7 — 8 of 11 ) reported that the evaluation staff had training and experience to a great or very great extent in each of the following areas: research design and methods , data management and statistical analysis , performance measurement and monitoring , and translating evaluation results into actionable recommendations .

slightly fewer reported that central evaluation office staff had great or very great subject matter expertise ( 5 ) .

three survey respondents also volunteered that their staff had additional expertise , including economic analysis , geographical information systems and lean cost reduction analysis .

organizations , whether government agencies or professional societies , develop written policies or standards in order to provide benchmarks for ensuring the quality of their processes and products .

aea has published guides for the individual evaluator's practice and for developing and implementing u.s. government evaluation programs .

about one - quarter of agencies reported having agency - wide written policies or guidance for key issues addressed in those guides: ensuring internal or external evaluator independence and objectivity ; ensuring completeness and transparency of evaluation reports ; selecting and prioritizing evaluation topics ; consulting program staff and subject matter experts ; selecting evaluation approaches and methods ; timely , public dissemination of evaluation findings and recommendations ; or tracking implementation of evaluation findings .

a few more agencies , but less than half , reported having policies on ensuring quality of data collection and analysis , which could apply to research as well as program evaluation .

central evaluation leadership was not required to adopt evaluation policies ; as only about half of the agencies with agency - wide evaluation policies had a central evaluation office .

agencies provided us with examples of guidance on information quality or scientific integrity as well as program evaluations specifically .

we , along with omb and aea , have all noted that developing an evaluation agenda is important for ensuring that an agency's often scarce research and evaluation resources are targeted to the most important issues and can shape budget and policy priorities and management practices .

less than a third of the agencies ( 7 ) reported having an agency - wide evaluation plan .

most such plans were reported to cover multiple years and programs across all major agency components .

senior agency officials and program managers were said to have been consulted in developing all these plans , but few agencies reported consulting congressional stakeholders or researchers .

all but 1 of the 7 agencies that had a plan also had a central evaluation office .

because we found in a previous report that stakeholder involvement facilitates the use of evaluation studies , we asked whether stakeholders were consulted in designing and conducting evaluation studies , either formally or informally .

almost all the pios reported consulting senior agency officials ( 20 ) and program managers ( 21 ) and three - quarters consulted researchers , but few ( 5 ) reported consulting congressional staff , less than local program providers or regulated entities .

agency evaluation offices are located at different organizational levels , which we have previously found affects the scope of their program and analytic responsibilities as well as the range of issues they consider .

in a previous study , we found that evaluators in central research and evaluation offices described having a broader and more flexible choice of topics than did evaluators in program offices .

in our 2014 survey , half the federal agencies ( 12 ) reported that some agency components ( such as an administration or bureau ) had a central office responsible for evaluation and that the number of such components ranged from 1 to 12 within a department or independent agency .

these offices generally existed in addition to , rather than instead of an agency - wide office responsible for evaluation ; as a result , 10 agencies had neither type of office .

as might be expected , component offices were less likely than central offices to be considered independent of program offices ( 6 of 12 agencies reported that all or many of their offices had independence in decision making ) , but 10 of 12 reported that all or many of these offices had access to external experts , and , like the central offices , few reported having a stable source of funds .

about half the agencies with component central offices for evaluation reported that the evaluation staff had training and experience to a great or very great extent in research design and methods , data management and statistical analysis , performance measurement and monitoring , and translating evaluation results into actionable recommendations .

these were slightly lower than the ratings for the central office staff's training .

as one might expect , staff were characterized as having great to very great subject matter expertise more often in component offices ( 9 of 12 ) than in central evaluation offices ( 5 of 11 ) .

only a few pios ( 2 to 4 ) reported that many or all component central offices for evaluation had written evaluation policies or guidance for any of the issues we listed .

more often pios ( 2 to 6 ) reported not knowing if they had those specific policies .

to assess the results or outcomes of agency evaluation activity , our survey asked the pios about the characteristics of the evaluations they produced and their use in decision making .

in line with the level of resources they committed to evaluation , the availability and use of program evaluations were uneven across the 24 federal agencies .

even though agencies may not have many evaluations , more than a third report using them from a moderate to a very great extent to support several aspects of program management and policy making .

because agencies use the term “program” in different ways , we chose to assess agencies' evaluation coverage of key programs and missions by the proportion of performance goals for which evaluations had been completed in the past 5 years or were in progress .

the number of performance goals may vary across agencies but , per omb guidance , they are supposed to be specific , near - term , realistic targets that an agency seeks to influence to advance its mission , and publicly reports .

only four agencies reported full evaluation coverage of their performance goals .

two - thirds of the agencies reported evaluation coverage of less than half their performance goals ; including 7 that reported having evaluations for none of their performance goals .

evaluation coverage was greater in agencies that established centralized authority for evaluation .

three of the 4 agencies with full coverage of their performance goals had both a central evaluation leader and central evaluation office , while all 7 agencies with no coverage had neither .

interestingly , 2 of the 7 agencies that reported having no evaluations of their performance goals did report having component evaluation offices , so they might have had some evaluations that simply did not address topics considered key to advancing their mission .

gao guidance notes that strong evaluations rely on sufficient and appropriate evidence ; document their assumptions , procedures , and modes of analysis ; and rule out competing explanations .

thus , transparent reporting of data sources and analyses are critical for ensuring that evaluations are considered credible and trustworthy .

about half the pios ( 10 ) reported that their evaluation reports are transparent to a great or very great extent in describing the data sources used and the analyses performed forming the basis of conclusions ; another 7 indicated that they did not know or did not respond to the question .

according to the evaluation capacity literature , timely , public dissemination of evaluation findings is important to support government accountability for results to the legislature and the public and to ensure that findings are available to inform decision making .

half the agencies ( 11 ) reported publicly disseminating their evaluation results by posting reports to a searchable database on their websites ; fewer reported presenting findings at professional conferences ( 9 ) , sending a notice and link to the report through electronic mailing lists ( 7 ) , or conducting webinars on findings for the policy community ( 6 ) .

a couple of the pios commented that they post some , but not all , reports on the agency website .

of the 11 agencies posting evaluation reports to a website , half reported that they did so within 3 months of completion , although 1 indicated it can take from 6 months to a year .

in addition , a few agencies sponsor research clearinghouses that review evaluations of social interventions and provide the results in searchable databases on their websites to help managers and policy makers identify and adopt effective practices .

if program evaluations or any form of performance information are to lead to performance improvement , they must be acted on .

seven agencies reported that they had procedures for obtaining management's response to evaluation recommendations , 8 for obtaining follow - up action on those recommendations .

in their comments , a few pios noted that they had policies for responding to reports or recommendations from gao or the inspector general .

another pio reported that a number of internal briefings are held to ensure management awareness of evaluation findings as well as a cross - agency research utilization committee composed of staff from program , public affairs , and congressional and intergovernmental relations offices that decides on the appropriate level of publicity effort for the report .

over a third of the agencies ( 9 to 10 ) reported that evaluations were used to a moderate or greater extent to support policy changes , budget changes , or internal proposals for change in resource allocation or management , or to award competitive grants ( figure 1 ) .

five agencies reported using evaluation to support all these activities to a moderate or greater extent on average .

in comments , pios described a variety of ways in which evaluation evidence could be used in awarding competitive grants: reviewing the merit of research proposals , evaluating grantee prior performance and outcomes , assessing credit worthiness , and allocating tiered evidence - based funding , which varies the level of funding based on the extent and quality of the evaluation evidence supporting a program's effectiveness .

agencies with centralized evaluation authority , independence , and expertise reported greater evaluation use in management and policy making , demonstrating its importance .

more than half of the 7 agencies that reported great use of evaluation had a senior evaluation leader or a central evaluation office .

moreover , the agencies whose central offices were independent of the program office , those with access to external experts or contractors , and those whose staff were rated as having great or better expertise in research methods and subject matter reported greater use of evaluation in decision making .

gprama was enacted in january 2011 , revising existing gpra provisions and adding new reporting requirements .

around the same time , omb increased its outreach to agencies to encourage them to conduct program evaluations .

we assessed change in agency evaluation capacity in this period through survey questions about when an office started conducting evaluations and whether the frequency of certain activities had changed .

while organizational changes in evaluation capacity were few during this period , half the agencies reported a greater use of evaluation in decision making since 2010 .

organizational evaluation capacity has grown some since 2010 .

one - third of the agencies have a high - level official responsible for oversight of the agency's evaluation studies , and 2 of those 7 positions were created after 2010 , both in 2013 .

in fact , in its may 2012 memorandum , omb encouraged agencies to designate a high - level official responsible for evaluation who can conduct or oversee rigorous and objective studies ; provide independent input to agency policymakers on resource “develop and manage the agency's research agenda ; allocation and to program leaders on program management ; attract and retain talented staff and researchers , including through flexible hiring authorities such as the intergovernmental personnel act ; and refine program performance measures , in collaboration with program managers and the performance improvement officer.” in addition , 4 of 11 agencies with a central office responsible for evaluation reported that this office started conducting evaluations after 2010 .

one agency added both a central leader and a central office in 2013 ; 3 others just added a central office .

of the 12 agencies that reported having evaluation offices in their major components , most existed before gprama was enacted , but 5 agencies have established new component evaluation offices since then .

presumably in response to greater administration attention to program evaluation , half the agencies reported that efforts to improve their capacity to conduct credible evaluations had increased at least somewhat since gprama was enacted in january 2011 .

about half the pios reported increases in staff participation in evaluation conferences and knowledge sharing forums , hiring staff with research and analysis expertise , training staff in research and evaluation skills , and consultation with external research and evaluation specialists .

nine agencies reported increases in all these activities .

most of the remaining agencies reported no change in training or consultation with specialists ( 4 to 5 ) , or decreases in hiring or participating in conferences ( 4 to 5 ) in this period .

these decreases may reflect federal budget constraints and the general decline in federal hiring in recent years .

in line with the increases reported in capacity building activities and organizational resources , about half the agencies reported that their use of evaluation as supportive evidence had increased at least somewhat since 2010 ( only a few reported great increases ) .

about half the pios reported that the use of evaluation had increased for implementing changes in program management or performance , designing or supporting program reforms , sharing what works or other lessons learned with others , allocating resources within a program , or supporting program budget requests .

the rest reported that their use of evaluation evidence remained about the same in this period , with none reporting a decline in use of evaluation as evidence .

eight agencies reported increased use in all these activities , and an equal number reported that their use remained the same on all .

since , in a separate question , 5 agencies either provided no opinion or reported little or no current use of evaluation evidence to support budget , policy , or program management , we conclude that this group has continued to make little or no use of evaluations since 2010 .

our survey asked the pios how useful various activities or resources were for improving their agency's capacity to conduct credible evaluations .

several pios did not answer these questions , in part because they were not familiar with such activities .

many of those who did respond found that hiring , professional networking , consulting with experts , and training as well as some of the gprama accountability provisions were very useful for improving capacity to conduct evaluations .

our survey also asked about the usefulness of various activities or resources for improving an agency's capacity to use evaluations in decision making .

again , several agencies did not respond , but most of those that did reported that engaging program staff , conducting quarterly progress reviews , and holding goal leaders accountable for progress on agency priority goals were very useful in improving agency capacity to make use of evaluation information .

some other gprama - related activities were not found as useful for enhancing evaluation use .

in addition , agencies had not taken full advantage of available technology to disseminate evaluation results , thus potentially limiting their influence on decision making .

our survey asked the pios about the usefulness of 14 different actions or resources for improving their capacity to conduct evaluations , drawn from the literature and some gprama provisions related to building agency capacity .

about a third of the respondents indicated either that they had no opinion or did not respond to these questions , similar to the number not responding or reporting no change in the use of capacity - building activities since 2010 .

about two - thirds of agencies ( 15 ) reported hiring staff with research and analysis expertise , and 11 — nearly half of the pios — thought it was very useful for improving agency capacity to conduct credible evaluations .

almost half the agencies used special hiring authorities , such as the presidential management fellows , intergovernmental personnel act , or american association for the advancement of science ( aaas ) fellows program , and generally found them useful for improving agency evaluation capacity .

other agency - specific means of obtaining staff were mentioned in comments — for example , an evaluation fellowship program at the centers for disease control and prevention .

figure 4 summarizes agencies' reports on the usefulness of the full range of activities and resources posed for building capacity to conduct evaluations .

the pio survey respondents also gave high marks to professional networking for building staff capacity .

two - thirds of the pios reported that staff participation in professional conferences or evaluation interest groups for knowledge sharing was useful , with 9 pios citing these activities as very useful in improving agency capacity to conduct credible evaluations .

examples mentioned included the association for public policy analysis and management research conference and an evaluation day conference sponsored by the u.s. department of health and human services ( hhs ) office of the assistant secretary for planning and evaluation .

the exchange of evaluation tips and leading practices through the pic or other network was considered moderately useful for capacity building by a third of the pios .

pios provided examples of information - sharing networks besides the pic , such as omb's evaluation working group , which holds governmentwide meetings on government performance topics ; federal evaluators , an informal association of evaluation officials across government ; washington evaluators , a local affiliate of the american evaluation association ; and the national academy of public administration .

some agencies have established informal networks to share information internally , such as hhs and the u.s. department of labor .

also mentioned were communities of practice that engage both public and private sectors but are focused on a specific domain — for example , the organisation for economic co - operation and development's evalnet , which focuses on international development , and the environmental evaluators network .

consultation with external experts for conceptual or technical support was rated as very useful for improving the capacity to conduct evaluations by most using it ( 9 of 15 ) .

however , this did not apply to other forms of external consultation .

seven agencies reported having an annual or multi - year evaluation agenda , and 3 of them reported consulting with congressional or other external stakeholders on their plan .

these 3 found consultation useful to varying degrees for building their agency's evaluation capacity to conduct evaluation .

training in specific skills and knowledge — for example , types of evidence , assessing evidence quality , report writing , and communication — is frequently cited in the evaluation literature as a way to build organizational or individual evaluation capacity .

besides asking about participating in professional conferences and networks , our survey asked about the usefulness of training in evaluation skills — for example , describing program logic models , choosing appropriate evaluation designs , and collecting and analyzing data .

half the agencies reported engaging in internal or external training — whether delivered in a classroom , online , or in webinars .

half the agencies using internal training reported that it was very useful for improving capacity to conduct credible evaluations .

pios who reported on agency experience with external evaluation training were less enthusiastic , but still considered the training useful for developing evaluation skills overall .

omb , in addition to encouraging agencies to conduct evaluations through guidance , sponsored a number of governmentwide open forums on performance issues .

about half the pios reported a range of opinions on the usefulness of the omb forums on the paperwork reduction act , procurement , data sharing , and related rules and procedures to help improve agency capacity for conducting credible evaluations .

nevertheless , 7 or more of the agencies identified training or guidance in several skills as still needed to a great or very great extent to improve their agencies' capacity to conduct credible evaluations .

these skills included: translating evaluation results into actionable recommendations — a requirement for getting evaluation results used — data management and statistical analysis , and performance measurement and monitoring .

few reported that more training in research design and methods or subject matter expertise was greatly needed .

our survey asked what other types of training or guidance might be needed to improve agency capacity .

a few pios commented that training is needed in preparing statements of work for evaluation contracts , data analytics and visualization of information , and learning how to effectively use evidence and evaluation information .

our survey asked about several activities and resources related to gprama provisions linked to creating an enabling environment for agency evaluation capacity .

majorities of pios stated that conducting quarterly progress reviews on their priority goals , and holding goal leaders accountable for progress on those goals , were moderately to very useful in improving their agency's ability to conduct credible evaluations .

in response to gprama provisions to improve agency performance management capacity , the pic and opm developed a performance analyst position design , recruitment , and selection toolkit to assist agencies' hiring .

seven pios reported that their agencies used the toolkit , and 3 did not find it useful for building agency evaluation capacity .

about a third of the pios reported that their agencies made an effort to incorporate the core competencies that opm identified for performance management staff into internal agency training .

however , 2 of the 7 agencies did not find the effort useful for improving staff evaluation capacity .

the competencies primarily address general management skills and define planning and evaluating fairly simply — as setting and monitoring progress on performance goals — so they do not address some of the specific analytic skills pios reported were still needed for conducting evaluations .

gao previously recommended that opm , in coordination with the pic and the chief learning officer council , identify performance management competency areas needing improvement and work with agencies to share information about available agency training in those areas .

opm agreed with those recommendations and has embarked on a 2-year pilot program to test how to build capacity in several mission critical competencies identified across government , such as strategic thinking , problem solving , and data analysis , to ensure that both program staff and management can use evaluation and analysis of program performance .

omb senior officials also engaged with agency officials on the performance improvement council to collaborate on improving program performance .

eight of the 14 agencies that responded considered the exchange of evaluation tips and leading practices through the pic or other networks as at least moderately useful for improving their evaluation capacity .

for example , the pic developed a guide to best practices for setting milestones and a guide and evaluation tool to help agencies set their agency priority goals .

previously , we found that experienced evaluators emphasized three basic strategies to facilitate evaluation's influence on program management and policy: demonstrate leadership support of evaluation for accountability and program improvement , build a strong body of evidence , and engage stakeholders throughout the evaluation process .

accordingly , our survey asked the pios how useful various activities or resources were for improving their agency's capacity to use evaluations in decision making .

several did not answer these questions because they did not use the particular activity or resource or had no opinion .

the pios who responded mainly cited engaging program staff , conducting quarterly progress reviews , and holding goal leaders accountable for progress on agency priority goals as very useful for improving agency capacity to make use of evaluation information in decision - making .

over two - thirds of the pios responded that involving program staff in planning and conducting evaluation studies was useful for improving agency use of evaluation ; 11 saw it as very useful .

engaging staff throughout the process can gain their buy - in on the relevance and credibility of evaluation findings ; providing program staff with interim results or lessons learned from early program implementation can help ensure timely data for program decisions .

majorities of pios affirmed that other forms of program staff engagement were also very useful: providing program staff and grantees with technical assistance on evaluation and its use and agency peer - to - peer presentations of evaluation studies to discuss methods and findings as mentioned earlier , majorities of pios viewed the new gprama activities of conducting quarterly reviews and holding goal leaders accountable as moderately to very useful for improving agency capacity to conduct credible evaluations .

majorities of the responding pios also viewed those same activities as moderately to very useful for improving agency capacity to use evaluations in decision making .

however , another gprama provision — coordinating with omb and other agencies to review progress on cross - agency priority ( cap ) goals — met with a range of opinions .

equal numbers reported that it was moderately to very useful , somewhat useful , or not useful at all for improving an agency's use of evaluation .

because the 14 cap goals for this period cover 5 general management improvement and 9 cross - cutting but specific policy areas , some of the 24 pios may have been more involved than others in those reviews .

other activities potentially useful for improving the capacity to use information from evaluations rely on leveraging resources .

a third of the pios reported that exchanging leading practices , tips , and tools for using evidence to improve program or agency performance through the pic or other network was moderately or very useful in improving agency capacity to use evaluation results in decision making .

many of the same networks named as helping to improve their capacity to conduct credible evaluations were also named with regard to improving capacity to use evaluations in decision making .

these included the environmental evaluators network , federal evaluators , the national academy of public administration , and the omb evaluation working group .

seven agencies reported having an agency - wide annual or multi - year evaluation plan or agenda of planned studies , and 6 pios reported consulting with congressional and other external stakeholders on that plan .

however , these consultations were not viewed as useful for improving their agency's capacity to use evaluations in decision making .

the absence of consultation may miss an opportunity to ensure that evaluations will address the questions of greatest interest to congressional decision makers and will be perceived as credible support for proposed policy or budget changes .

in previous work , we found that dialog between congressional committees and executive branch agencies was necessary to achieve a mutual understanding that would allow agencies to provide useful information for oversight .

previously we found that a key strategy for promoting the use of evaluation findings was to make them digestible and usable and to proactively disseminate them .

our survey posed various options that agencies could take to publicly disseminate their evaluation findings .

half the respondents reported posting evaluation reports in a searchable database on their websites , and half of them viewed this practice as moderately to very useful for improving their agency's capacity to use evaluations in decision making .

however , 3 did not find the practice useful .

electronic mailing lists are more proactive than posting a report to a website and permit tailoring the message to different audiences .

a third of all respondents disseminated evaluation reports by electronic mailing lists , which most saw as somewhat to very useful for facilitating the use of evaluations in decision making .

tailoring messages for particular audiences — for example , federal policy makers , state and local agencies , and local program affiliates — may , however , increase the applicability and use of evaluation findings by these other audiences .

gprama requires omb to provide quarterly updates on agency and cross - agency priority goals on a central , government - wide website , performance.gov , to make federal program and performance information more accessible to the congress and the public .

in our survey , pio reviews were mixed about the utility of this website to improve agency capacity to use evaluations in decision making .

almost half the agencies found the practice somewhat to moderately useful for improving the agencies' use of evaluation findings in decision making , but one - fourth of the agencies did not .

in 2013 , gao reviewed performance.gov and recommended that omb work with the general services administration and the pic to clarify specific ways that intended audiences could use the website and specify changes to support these uses .

omb staff agreed with our recommendations , and performance.gov continues to evolve .

currently each agency has a home page that provides links to the agency's strategic plan , annual performance plans and reports , and other progress reviews .

data.gov is a federal government website that provides descriptions of datasets generated or held by the federal government in order to increase the ability of the public to locate , download , and use those datasets .

a third of the pios reported that sharing databases in public repositories such as data.gov for researchers and the public to use helped in improving agency capacity to use evaluations in decision making , but 1 thought it was not useful .

however , a third of pios stated that the agency did not use this vehicle .

vehicles such as data.gov and performance.gov are primarily intended to improve government transparency and expand information's use by the congress and the public , but they can also help support agency requests for budget and policy changes to improve government performance .

although omb and several agencies have taken steps since 2010 to expand federal evaluation efforts , most agencies demonstrate rather modest evaluation capacity .

those with centralized evaluation authority reported greater evaluation coverage and use in decision making , but additional effort will be required to expand agencies' evaluation capacity beyond those that already possess evaluation expertise .

in addition to hiring and training staff and consulting experts , promoting information sharing through informal and formal evaluation professionals' networks offers promise for building agencies' capacity to conduct evaluation in a constrained budget environment .

engaging program staff , regularly reviewing progress on agency priority goals , and holding goal leaders accountable can help build agency use of evaluation in decision making , as our survey results show .

while timely , public dissemination of performance and evaluation results may not directly influence agency decision making , it is important to support government transparency and accountability for results to the congress and the public .

directly engaging intended users ( for example , involving program staff in planning and conducting evaluations and holding regular progress reviews ) was strongly associated with increasing evaluation use in internal agency decision making .

in contrast , few agencies reported consulting congressional and other external stakeholders in conducting their evaluation studies or developing their evaluation agendas .

however , some program reforms require program partners and legislators to take action .

engaging congressional and other stakeholders in evaluation planning might increase their interest in evaluation as well as their adoption of evaluation findings and recommendations .

in the absence of explicit authority or congressional request , agencies may be reluctant to spend increasingly scarce funds on evaluation studies that are perceived as resource intensive .

a stable source of evaluation funding could help maintain a viable evaluation program that produced a steady stream of information to guide program management and policy making .

even so , only a quarter of the agencies in our survey reported that their evaluation offices had a stable source of funding .

congressional appropriators could direct the use of program or agency funds for evaluating federal programs and policies .

as we have noted before , congressional committees can also communicate their interest in evaluation in a variety of ways to encourage agencies to produce credible , relevant studies that inform decision making: consult with agencies on proposed revisions to their strategic plans and priority goals , as gprama requires them to do every 2 years , to ensure that agency missions are focused , goals are specific and results - oriented , and strategies and funding expectations are appropriate and reasonable ; request agency evaluations to address specific questions about the implementation and results of major program or policy reforms , in time to consider their results in program reauthorization ; and review agencies' annual evaluation plans or agendas to ensure that they address issues that will inform budgeting , reauthorization , and ongoing program management .

we requested comments on a draft of this report from the director of the office of management and budget , whose staff provided technical comments that we incorporated as appropriate , and from the director of the office of personnel management , who provided none .

we are sending copies of this report to other interested congressional committees , and the director of the office of management and budget and the director of the office of personnel management .

in addition , the report will be available on our web site at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-2700 or by e - mail at kingsburyn@gao.gov .

contacts for our office of congressional relations and office of public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iii .

we administered a web - based questionnaire from may 2 , 2014 , to june 19 , 2014 , on federal agency evaluation capacity resources and activities to the performance improvement officers ( pio ) or their deputies at the 24 agencies covered by the chief financial officers act of 1990 ( cfo act ) .

we received responses from all 24 agencies ( listed at the end of this appendix. ) .

the survey gave us information about agencies' evaluation resources , policies , and activities , and the activities and resources they have found useful in building their evaluation capacity .

 ( the survey questions and summarized results are in appendix ii. ) .

we sent respondents an e - mail invitation to complete the survey on a secure gao web server .

each e - mail contained a unique username and password .

during the data collection period , we sent follow - up e - mails and , if necessary , called nonresponding agencies on the telephone .

because this was not a sample survey , it has no sampling errors .

in practice , however , any survey may introduce nonsampling errors that stem from differences in how a particular question is interpreted , the availability of sources of information , or how the survey data are analyzed .

all can introduce unwanted variability into the survey results .

we took a number of steps to minimize these nonsampling errors .

a social science survey specialist designed the questionnaire , in collaboration with our staff who had subject matter expertise .

in addition , we pretested the questionnaire in person with pios at three federal agencies to make sure that the questions were relevant , clearly stated , easy to comprehend , and unbiased .

we also affirmed that data and information the pios would need to answer the survey were readily obtainable and that answering the questionnaire did not place an undue burden on them .

additionally , a senior methodologist within our agency independently reviewed a draft of the questionnaire before we administered it .

we made appropriate revisions to its contents and format after the pretests and independent review .

when we analyzed data from the completed survey , an independent analyst reviewed all computer programs used in our analysis .

since this was a web - based survey , respondents entered their answers directly into the electronic questionnaire ; thus , we did not key the data into a database , avoiding data entry errors .

additionally , in reviewing the agencies' answers , we confirmed that the pios had correctly bypassed inapplicable questions ( such as questions we expected them to skip ) .

we concluded from our review that the survey data were sufficiently reliable for the purposes of this report .

the 24 agencies subject to the cfo act include agency for international development department of agriculture department of commerce department of defense department of education department of energy department of health and human services department of homeland security department of housing and urban development department of the interior department of justice department of labor department of state department of transportation department of the treasury department of veterans affairs environmental protection agency general services administration national aeronautics and space administration national science foundation nuclear regulatory commission office of personnel management small business administration social security administration .

in addition to the contact named above , stephanie shipman ( assistant director ) , thomas beall , valerie caracelli , timothy carr , joanna chan , stuart kaufman , and penny pickett made key contributions to this report .

administration for children and families .

evaluation policy .

washington , d.c.: department of health and human services , november 2012 .

accessed september 24 , 2014. http: / / www.acf.hhs.gov / programs / opre / resource / acf - evaluation - policy .

america achieves .

“investing in what works index: better results for young people , their families , and communities.” results for america , washington , d.c. , may 2014 .

accessed september 11 , 2014. http: / / www.investinwhatworks.org / policy - hub .

american evaluation association .

an evaluation roadmap for a more effective government .

n.p .

: revised october 2013 .

accessed september 22 , 2014. http: / / www.eval.org / d / do / 472 .

auditor general of canada .

2013 spring report of the auditor general of canada .

ch .

1 .

“status report on evaluating the effectiveness of programs.” ottawa: 2013 .

accessed september 15 , 2014. http: / / www.oag - bvg.gc.ca / internet / english / parl_oag_201304_01_e_38186.html .

bourgeois , isabelle , and j. bradley cousins .

“understanding dimensions of organizational evaluation capacity,” american journal of evaluation , 34:3 ( 2013 ) : 299 — 319 .

chapel , thomas .

“building and sustaining evaluation capacity in a diverse federal agency.” paper presented at federal evaluators conference , washington , d.c. , november 1 , 2012 .

accessed september 11 , 2014. http: / / www.fedeval.net / presen.htm .

clapp - wincek , cindy .

“the complexity of building capacity at usaid.” paper presented at federal evaluators conference , washington , d.c. , november 1 , 2012 .

accessed september 11 , 2014. http: / / www.fedeval.net / presen.htm .

cousins , j. bradley , swee c. goh , catherine j. elliott , and isabelle bourgeois .

“framing the capacity to do and use evaluation,” new directions for evaluation , 133 ( spring 2014 ) : 7 — 24 .

dawes , katherine .

“program evaluation at epa.” paper presented at federal evaluators conference , washington , d.c. , november 1 , 2012 .

accessed september 11 , 2014. http: / / www.fedeval.net / presen.htm .

goldman , ian .

“developing a national evaluation system in south africa,” evaluation matters: a quarterly knowledge publication of the african development bank , 2 ( 3 ) ( september 2013 ) : 42 — 49 .

labin , susan n. , jennifer l. duffy , duncan c. meyers , abraham wandersman , and catherine a. lesesne .

“a research synthesis of the evaluation capacity building literature,” american journal of evaluation , 33:307 ( 2012 ) .

national audit office .

cross - government: evaluation in government .

report by the national audit office .

london , eng .

december 2013 .

accessed september 24 , 2014. www.nao.org.uk .

partnership for public service and grant thornton .

a critical role at a critical time: a survey of performance improvement officers .

washington , d.c.: april 2011 .

accessed september 16 , 2014. http: / / ourpublicservice.org / ops / publications / viewcontentdetails.php ? id=1 60 .

partnership for public service and grant thornton .

taking measure: moving from process to practice in performance management .

washington , d.c.: september 2013 .

accessed september 16 , 2014. http: / / ourpublicservice.org / ops / publications / viewcontentdetails.php ? id=2 32 .

partnership for public service and ibm center for the business of government .

from data to decisions iii: lessons from early analytics programs .

washington , d.c.: november 2013 .

accessed september 16 , 2014. http: / / ourpublicservice.org / ops / publications / viewcontentdetails.php ? id=2 33 .

pew charitable trusts and macarthur foundation .

states' use of cost - benefit analysis: improving results for taxpayers .

philadelphia: pew - macarthur results first initiative , july 29 , 2013 .

accessed october 31 , 2014. http: / / www.pewtrusts.org / en / research - and - analysis / reports / 2013 / 07 / 29 / states - use - of - costbenefit - analysis .

rist , ray c. , marie - helene boily , and frederic martin .

influencing change: building evaluation capacity to strengthen governance .

washington , d.c.: the world bank , 2011 .

accessed september 24 , 2014. https: / / openknowledge.worldbank.org / segone , marco , caroline heider , riitta oksanen , soma de silva , and belen sanz .

“towards a shared framework for national evaluation capacity development,” evaluation matters: a quarterly knowledge publication of the african development bank , 2 ( 3 ) ( september 2013 ) : 7 — 25 .

segone , marco , and jim rugh ( eds. ) .

evaluation and civil society: stakeholders' perspectives on national evaluation capacity development .

new york: unicef , evalpartners , ioce , 2013 .

accessed september 24 , 2014. http: / / www.mymande.org / evaluation_and_civil_society .

treasury board of canada .

2011 annual report on the health of the evaluation function .

ottawa: 2012 .

accessed september 24 , 2014. http: / / www.tbs - sct.gc.ca / report / orp / 2012 / arhef - raefetb - eng.asp .

united nations evaluation group .

national evaluation capacity development: practical tips on how to strengthen national evaluation systems .

a report for the united nations evaluation group task force on national evaluation capacity development .

new york: 2012 .

accessed september 24 , 2014. www.uneval.org / document / detail / 1205 .

u.s. agency for international development .

evaluation: learning from experience .

usaid evaluation policy .

washington , d.c.: january 2011 .

accessed september 25 , 2014. http: / / www.usaid.gov / evaluation .

u.s. department of health and human services , centers for disease control and prevention .

improving the use of program evaluation for maximum health impact: guidelines and recommendations .

atlanta: november 2012 .

accessed september 24 , 2014. http: / / www.cdc.gov / eval .

u.s. department of labor .

u.s. department of labor evaluation policy .

washington , d.c.: november 2013 .

accessed september 25 , 2014. http: / / www.dol.gov / asp / evaluation / evaluationpolicy.htm .

u.s. department of state .

department of state program evaluation policy .

washington , d.c.: february 23 , 2012 .

accessed september 24 , 2014. http: / / www.state.gov / s / d / rm / rls / evaluation / .

managing for results: agencies' trends in the use of performance information to make decisions .

gao - 14-747 .

washington , d.c.: september 26 , 2014 .

managing for results: enhanced goal leader accountability and collaboration could further improve agency performance .

gao - 14-639 .

washington , d.c.: july 22 , 2014 .

managing for results: omb should strengthen reviews of cross - agency goals .

gao - 14-526 .

washington , d.c.: june 10 , 2014 .

education research: further improvements needed to ensure relevance and assess dissemination efforts .

gao - 14-8 .

washington , d.c.: december 5 , 2013 .

managing for results: executive branch should more fully implement the gpra modernization act to address pressing governance challenges .

gao - 13-518 .

washington , d.c.: june 26 , 2013 .

program evaluation: strategies to facilitate agencies' use of evaluation in program management and policy making .

gao - 13-570 .

washington , d.c.: june 26 , 2013 .

managing for results: leading practices should guide the continued development of performance.gov .

gao - 13-517 .

washington , d.c.: june 6 , 2013 .

managing for results: agencies have elevated performance management leadership roles , but additional training is needed .

gao - 13-356 .

washington , d.c.: april 16 , 2013 .

managing for results: data - driven performance reviews show promise but agencies should explore how to involve other relevant agencies .

gao - 13-228 .

washington , d.c.: february 27 , 2013 .

managing for results: a guide for using the gpra modernization act to help inform congressional decision making .

gao - 12-621sp .

washington , d.c.: june 15 , 2012 .

president's emergency plan for aids relief: agencies can enhance evaluation quality , planning and dissemination .

gao - 12-673 .

washington , d.c.: may 31 , 2012 .

designing evaluations: 2012 revision .

gao - 12-208g .

washington , d.c.: january 2012 .

employment and training administration: more actions needed to improve transparency and accountability of its research program .

gao - 11-285 .

washington , d.c.: march 15 , 2011 .

program evaluation: experienced agencies follow a similar model for prioritizing research .

gao - 11-176 .

washington , d.c.: january 14 , 2011 .

employment and training administration: increased authority and accountability could improve research program .

gao - 10-243 .

washington , d.c.: january 29 , 2010 .

program evaluation: a variety of rigorous methods can help identify effective interventions .

gao - 10-30 .

washington , d.c.: november 23 , 2009 .

program evaluation: an evaluation culture and collaborative partnerships help build agency capacity .

gao - 03-454 .

washington , d.c.: may 2 , 2003 .

program evaluation: improving the flow of information to congress .

gao / pemd - 95-1 .

washington , d.c.: january 30 , 1995 .

