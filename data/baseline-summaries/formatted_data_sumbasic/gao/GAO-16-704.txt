the screening of airport passengers and their checked baggage is a critical component in securing our nation's commercial aviation system .

since the terrorist attacks of september 11 , 2001 , the transportation security administration ( tsa ) has been tasked with screening airline passengers and their accessible and checked baggage for prohibited and other potentially dangerous items that could pose a threat to the aircraft and passengers .

in fiscal year 2014 alone , approximately 660 million passengers and 2 billion bags were screened at nearly 440 tsa - regulated airports across the nation .

each year , tsa conducts certification testing for its airport security screeners , and in an effort to measure the performance of aviation security screening , both tsa and the department of homeland security office of inspector general ( dhs - oig ) conduct regular covert testing of tsa screening operations .

in response to the failure rates stemming from recent covert testing conducted by the dhs - oig , the secretary of the department of homeland security ( dhs ) directed tsa in june 2015 to take a number of actions to address the vulnerabilities identified in the testing .

specifically , the secretary directed tsa to revise its standard operating procedures ( sop ) for screening , brief all federal security directors ( fsd ) across the country on the inspector general's findings , and to conduct further training for all transportation security officers ( tso ) and supervisors , among other things .

in october 2015 , the tsa administrator testified before congress on the steps tsa was taking to respond to the secretary's directive , including delivering further training to every tso and supervisor across the country .

in 2005 , we reviewed actions tsa had taken to enhance training for tsos , how tsa ensured all required tso training was completed , and what actions tsa had taken to measure and enhance tso performance .

we found that tsa lacked adequate internal controls to ensure screeners received legally mandated remedial training and to monitor its recurrent training program .

specifically , we found that tsa policy did not specify the responsible party for ensuring screeners completed training and that tsa could not document that screeners received training .

we made recommendations to tsa to close these gaps , which tsa concurred with , and has since taken steps to address .

more recently , you asked us to review issues related to tsa's training and testing of airport security screeners .

this report addresses the following objectives: ( 1 ) how does tsa train tsos and to what extent does tsa evaluate the training ; ( 2 ) how does tsa measure the performance of tsos and what do the performance data show ; and ( 3 ) to what extent does tsa use tso performance data to enhance tso performance .

this report is a public version of a prior sensitive report that we provided to you .

tsa deemed some of the information in the prior report sensitive security information , which must be protected from public disclosure .

therefore , this report omits sensitive information regarding specific details and results of tsa training and testing programs for its tsos in addition to the names of airports visited during our review .

the information provided in this report is more limited in scope , in that it excludes such sensitive information , but it addresses the same questions as the sensitive report and the methodology used for both reports is the same .

to address the first objective on how tsa trains tsos and to what extent tsa evaluates the training , we reviewed relevant tsa policies and procedures for training , including management directives and the national training plan ( ntp ) , which prescribes the annual training curriculum for tsos .

we also reviewed documentation on training requirements , including those contained in the aviation and transportation security act ( atsa ) , as well as documents on tsa's training development and completion .

we interviewed tsa headquarters officials from the office of training and development ( otd ) , the office of human capital ( ohc ) , and the office of security operations ( oso ) , who are responsible for developing and monitoring tso training , and staff from a total of 10 airports — including federal security directors ( fsd ) , transportation security managers , instructors , training managers , tsos , and others to determine how training is carried out in the field .

specifically , we conducted site visits to 6 airports of different sizes , including 3 airports in category x , and one airport each in categories i , ii , and iii .

further , we conducted phone interviews with officials at 1 airport each in categories i , ii , iii , and iv to obtain additional perspectives on how airport officials carry out training requirements locally — particularly at airports with smaller numbers of flights and passenger boardings .

we selected the airports to visit in person based on factors such as airport risk category , geographic proximity to one another , and our analysis of the airports' tso performance on annual screening certification tests from 2009 through 2014 .

we selected at least one airport from the high , low , and middle of the performance distribution .

our visits to airports provide insights about tsa training , but observations from these airports are not generalizable to all airports across the country .

to assess the extent to which tsa evaluates tso training , we reviewed tsa documents used for evaluating training courses , including end - of - course surveys administered to participants .

further , we reviewed draft documents on tsa's training evaluation plan , which tsa is currently developing , including a draft management directive and draft sops for evaluating training courses .

we compared the training evaluation documentation to the kirkpatrick model for training evaluation , which tsa uses as guidance for its evaluations of tso training .

we also interviewed tsa headquarters officials responsible for evaluating tso training and for developing and implementing the tsa training evaluation plan .

further , we interviewed management officials at each of the 10 airports to further understand how , if at all , training at individual airports is evaluated locally .

to address the second objective on how tsa measures the performance of tsos and what the performance data show , we analyzed data from the following performance evaluation programs: annual proficiency review ( apr ) , which is an annual certification test tsos must pass to remain employed as a screener .

we analyzed apr pass rates from calendar year 2009 ( the first year for which data were available ) through 2015 ( the last year for which data were available at the time of our review ) .

threat image projection ( tip ) system data from fiscal year 2009 through 2014 , the last year the data were available at the time of our data request .

the tip system helps tsa determine whether operators correctly identify threat items that are electronically superimposed on the x - ray monitor during the screening of passenger property at the checkpoint .

presence , advisement , communication , and execution ( pace ) testing program , which tsa uses to measure whether tsos are adhering to standard operating procedures while screening at the passenger checkpoint .

we analyzed pace data from calendar 2011 , the year the program was started , through 2014 , the last year for which the data were available .

aviation screening assessment program ( asap ) , a covert testing program designed to assess the operational effectiveness of screeners by evaluating screeners' ability to properly follow tsa's standard operating procedures for screening and keep prohibited items from being taken through the checkpoint .

we analyzed asap data from calendar year 2013 through 2015 because tsa made adjustments to the asap testing program in 2013 , and therefore the pre - 2013 testing data are not comparable to the 2013 through 2015 data .

results of asap testing are classified at the secret level and are not included in this report .

we assessed the reliability of the apr , tip , pace , and asap data by ( 1 ) interviewing agency officials regarding data collection practices and ( 2 ) testing the data for missing data and duplicates , among other things .

we found the apr and pace data sufficiently reliable to present average pass rates for tsos during calendar years 2009 through 2015 .

we found the tip data to be incomplete , and thus unreliable for describing national trends , because tsa could not provide tip scores for every airport for the period in which we conducted our analysis .

therefore , we do not present tip data in this report .

see appendix i for additional details .

to analyze what the tso performance data show , we examined trends in apr , pace , and asap results across time , and analyzed the results by airport categories ( x , i , ii , iii , iv ) to identify any trends among airports in different risk categories .

see appendix i for further details on our data analysis .

to address the third objective on the extent to which tsa uses tso performance data to enhance screening performance , we reviewed tsa's processes and actions for using screener testing results to inform its operations and training , and assessed these processes against standards in standards for internal control in the federal government .

further , we interviewed program officials at tsa headquarters and at each of the airports we visited about how they analyze performance data such as apr , tip , asap , and pace data , and how , if at all , they use the results to adjust training or take any other actions .

we conducted this performance audit from february 2015 to september 2016 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

after the terrorist attacks of september 11 , 2001 , the president signed the aviation and transportation security act ( atsa ) into law on november 19 , 2001 , with the primary goal of strengthening the security of the nation's civil aviation system .

atsa created tsa as the agency with responsibility for securing all modes of transportation , including civil aviation .

as part of this responsibility , tsa performs or oversees the performance of security operations at the nation's nearly 440 commercial ( i.e. , tsa - regulated ) airports , including passenger and checked baggage screening operations .

fsds are tsa officials responsible for overseeing tsa security activities , including passenger and checked baggage screening , at one or more commercial airports .

tsa classifies commercial airports in the united states into one of five security risk categories ( x , i , ii , iii , and iv ) based on various factors , such as the total number of takeoffs and landings annually , and other special security considerations .

in general , category x airports have the largest number of passenger boardings and category iv airports have the smallest .

tsa periodically reviews airports in each category and , if appropriate , updates airport categorizations to reflect current operations .

figure 1 shows the number of commercial airports by airport security category as of july 2015 .

tsa uses a multilayered security strategy aimed to enhance aviation security .

within those layers of security , tsa's airport passenger checkpoint screening system includes , among other things , ( 1 ) screening personnel ( i.e. , tsos ) ; ( 2 ) sops that guide screening processes conducted by tsos ; and ( 3 ) technology , such as advanced imaging technology systems ( often referred to as body scanners ) or walk - through metal detectors , used to conduct screening of passengers .

to carry out passenger and checked baggage screening operations , tsa employs tsos at the vast majority of the nation's commercial airports .

there are several levels of screening officers deployed at the passenger checkpoint: transportation security officer ( tso ) : performs the majority of security functions to screen people and property to mitigate threats .

screening may include pat downs , search of property , and operating technology including walk - through metal detectors , x - ray machines , and explosives detection equipment , among other things .

lead transportation security officer ( ltso ) : leads a staff of tsos , including distributing and adjusting workload and tasks among employees and oversees the security screening team on a daily basis .

implements security procedures and provides coaching and guidance to tsos in performing screening duties , among other things .

ltsos also perform screening functions along with added responsibilities , such as resolving alarms and supervising screening locations when a supervisor is not available .

supervisory transportation security officer ( stso ) : oversees screening checkpoints and / or baggage screening , supervises ltsos and tsos in performance of security screening and ensures all required screening is performed in accordance with sops .

reviews and evaluates work and performance of ltsos and tsos , approves leave , and recommends corrective or disciplinary actions , among other things .

stsos also perform screening functions and resolve passenger alarms .

transportation security manager ( tsm ) : coordinates and facilitates tsa security activities and manage one or more programs as assigned by the federal security director .

a tsm assigned to oversee screening checkpoints manages security activities , including recognizing and correcting improper use or application of equipment or screening procedures , monitors screening operations , and implements changes to enhance security and efficiency at screening locations .

tsos inspect individuals and property as part of the passenger screening process to deter and prevent the carriage of any unauthorized explosive , incendiary , weapon , or other prohibited items on board an aircraft or into the airport sterile area — in general , an area of an airport that provides passengers access to boarding aircraft and to which access is controlled through the screening of persons and property .

ordinarily , screening of accessible property at the screening checkpoint begins when an individual places accessible property on the x - ray conveyor belt or hands accessible property to tsa personnel .

as shown in figure 2 , tsos then review images of the property running through the x - ray machine and look for signs of prohibited items .

the passengers themselves are typically screened via a walk - through metal detector or an advanced imaging technology machine , and passengers generally have the option to request screening by a pat down if they do not wish to be screened by these technologies .

passengers will also be subject to a pat down if they are screened by the walk - through metal detector or advanced imaging technology system and the equipment alarms ( in order to resolve the alarm ) .

tsos also inspect checked baggage to deter , detect , and prevent the carriage of any unauthorized explosive , incendiary , or weapon onboard an aircraft .

figure 3 shows the general process used to screen checked bags .

checked baggage screening is accomplished through the use of explosives detection systems or explosives trace detection systems , and through the use of alternative means , such as manual searches and canine teams when the explosives detection systems are unavailable .

in accordance with atsa , screeners must complete a minimum of 40 hours of classroom instruction , 60 hours of on - the - job training , and successfully complete an on - the - job training examination before they are certified as security screeners .

screeners can be certified to conduct passenger screening or checked baggage screening , or they may be certified as dual function and can then conduct passenger and checked baggage screening .

atsa also requires that tsa provide operational testing of screening personnel , and any individual who fails an operational test must successfully complete remedial training on that specified security function before returning to duty .

in addition , screeners must also undergo an annual proficiency review to ensure they continue to meet all qualifications and standards required to perform a screening function .

tsa also requires remedial training for tsos who fail an annual proficiency review .

covert tests recently conducted by the dhs - oig highlighted areas of concern for tsa regarding the effectiveness of the passenger screening process .

specifically , the dhs - oig conducted covert testing to determine the effectiveness of tsa's advanced imaging technology screening equipment , its related automated target recognition software , and checkpoint screener performance in identifying and resolving potential security threats at airport checkpoints .

tsa has responded to the dhs secretary's direction regarding the results of the dhs - oig covert testing , in part , by updating its screening sops and retraining tsos to address the inspector general's findings .

also in response to the dhs - oig findings , tsa has developed new measures of effectiveness that it expects will better emphasize the agency's goals for improving security effectiveness by focusing the measures on both the screening system and workforce in the areas of readiness and performance .

for example , improved workforce measures , now being reported monthly , include those to track tsos' progress against training requirements , absences due to injuries or other reasons , and whether they are meeting performance thresholds on various tests of performance and job proficiency .

tso training is comprised of a compendium of courses that includes basic training for initial hires , recurrent training , remedial training , and return - to - duty training .

for example , all new hires receive a combination of classroom , hands - on , and web - based training .

after tsos finish their initial new hire training , they receive recurrent and specialized training courses throughout the year that are provided either via classroom instruction or through the tsa online learning center .

recurrent training typically focuses on core screening skills and policies such as x - ray image interpretation , detection techniques , and screening sops .

tsos receive remedial training when they have failed an operational or certification test , or if a supervisor identifies a need for further training , among other things .

further , according to tsa , tsos who are absent from their screening duties for a period of time must undergo some level of “return - to - duty” training based on the amount of time they were absent .

for example , tsos certified in a screening function but who have not performed that function for a period of 15 consecutive days or more are required to complete a return - to - duty training program before being allowed to perform that function independently .

table 1 describes the various types of training tsos receive .

the office of training and development ( otd ) , within tsa headquarters , oversees the development , delivery , and evaluation of training programs for tsa employees .

the national training plan ( ntp ) , developed jointly by otd and the office of security operations , contains the core curriculum for tsos to meet their annual training , including the classes and hours required for tsos to complete for the year .

tsa headquarters officials implement the ntp to provide ongoing training throughout the year aimed at continually improving screeners' knowledge , skills , and abilities .

however , the responsibility for managing the individual training of tsos is largely decentralized and it primarily falls on security training instructors at individual airports to train tsos on parts of the ntp by certain dates throughout the year .

managers in the field track the percentage of the ntp curriculum that tsos have completed on a monthly basis using the online learning center database .

in addition , tsa officials at all 10 airports we contacted stated that they monitor various testing results for their tsos and observe screening operations at their airports' checkpoints , to determine any local , specialized training needs their screening force may need — over and above that included in the ntp issued by tsa headquarters .

tsa headquarters can also add training requirements throughout the year as needed , such as the recently completed “mission essentials — threat mitigation” training discussed later .

tsa officials we spoke with at airports noted challenges associated with completing not only the required training under the ntp , but also training associated with frequent changes to the screening sops for how particular screening practices are to be conducted at the checkpoint .

for example , tsa personnel at 8 of the 10 airports stated that it was sometimes difficult to meet the training requirements in the ntp because they did not have the tso personnel to both staff the checkpoints and get all the required training accomplished .

specifically , tsa officials at larger airports with more passenger throughput , such as category x and category i airports , reported having ongoing challenges balancing training with the operational needs at the checkpoint .

in contrast , tsa officials at smaller airports , such as category iii and iv airports did not report having this challenge frequently .

tsa officials at all three of the category x airports stated they addressed the challenge of meeting training requirements by scheduling large amounts of training during slower travel seasons for their airport so they would not have to spend time training tsos during peak travel periods .

starting in fiscal year 2014 , tsa headquarters began sending the majority of the ntp training requirements for the entire fiscal year out to the field at the beginning of the year — rather than at quarterly intervals throughout the year — allowing airports the flexibility to train tsos at different rates depending on the operational needs of the airport .

tsa training officials at 6 of the 10 airports stated that it is challenging for them to keep the tsos trained on the frequent changes to screening sops .

for example , tsa officials from two airports stated that during the delivery of a recent nhtp class , screening sops were updated to require an officer to use a handheld metal detector to resolve an alarm arising from a passenger going through an advanced imaging technology scanner .

due to the change happening while the new officers were in the middle of their introductory training , the steps for using the handheld metal detector were not integrated into the nhtp curriculum .

as a consequence , after the nhtp course was completed , tsa instructors separately trained the new hires on how to conduct this type of alarm resolution .

in addition , tsa officials at 9 airports we spoke with stated that the tsos used “read and sign” binders to train on some sop changes , where the officers sign a document stating they read the change to the screening sops .

however , officers reported that this type of training did not ensure they understood how to implement the change at the screening checkpoint .

according to tsa headquarters officials , they plan to conduct more hands - on training to teach screening sop changes moving forward .

further , tsa personnel at 7 of the 10 airports added that many of the screening sops can have room for interpretation , which also prompted officials at 2 of these airports to create new airport - level training to address whether to let particular items through the checkpoint such as bowling balls and other heavy , blunt objects .

tsa implemented a tso re - training program in fiscal year 2015 to retrain its screening workforce in response to findings of the dhs - oig , which conducted its own covert testing of tsa's checkpoint operations and technology in the spring of 2015 .

specifically , in response to the dhs - oig findings , tsa provided additional training nationwide to all tsos — referred to as “mission essentials — threat mitigation” training .

according to tsa documentation , the purpose of this 8-hour classroom training was to provide the opportunity for the workforce to become familiar with the intelligence and threat information that underlies tsa's use of checkpoint technologies , operational procedures , and the tso workforce to mitigate threats .

tsa officials described the training as covering the “why” behind the equipment and procedures tsa uses to screen passengers and baggage .

for example , the training included: instruction on how social engineering techniques may be used in an attempt to defeat tsa risk mitigation procedures , updates on sop changes for screening certain types of passengers , demonstrations on improvised explosive devices ( ied ) and how pat downs are used to mitigate the threat , and an overview of checkpoint equipment capabilities and limitations and the role of using screening sops and best practices to mitigate gaps caused by equipment limitations .

in addition to the 8-hour course provided for screening officers , supervisors were provided additional training on their responsibilities for ensuring the correct implementation of the checkpoint sops and how to provide on - the - spot corrections and constructive feedback to officers .

tsa officials added that , in order to ensure enhanced mission focus , the agency will begin sending all new - hire tsos to the tsa academy at the federal law enforcement training center in glynco , georgia rather than conducting the classroom portion of the nhtp at individual airports .

the officials stated this would help standardize the new hire training and provide a sense to the new hires that they are part of something larger than just their local airport .

tsa officials stated the first new - hire classes started at the tsa academy in january 2016 .

to evaluate its training of tsos , tsa generally follows the kirkpatrick model , which is a commonly accepted training evaluation model endorsed by the office of personnel management ( opm ) and used throughout the federal government .

currently , using the model , tsa implements training evaluation surveys and conducts analysis of the responses for a select number of training courses .

tsa's goal for conducting kirkpatrick - style training evaluation is to answer questions such as how well a training course met a learner's needs ; what knowledge and skill a course imparted to learners ; what impact the training had on learner performance ; and what the benefits of the training were .

the kirkpatrick model consists of a four - level approach for soliciting feedback from training course participants and evaluating the impact the training had on individual development , among other things .

table 2 provides a description of what each level within the kirkpatrick model is to accomplish and tsa's progress in implementing the levels .

according to tsa officials , the agency is developing a training evaluation program that will allow it to standardize and expand training evaluation efforts .

in 2013 , tsa assessed its training evaluation practices and found that existing training evaluation efforts did not meet tsa's needs because they lacked a formal , comprehensive approach to training evaluation .

as a result , tsa identified the need to establish a formal training evaluation program , based on the kirkpatrick model , to standardize its policy , processes , and procedures for evaluating training and has been working to establish the program since december of 2013 .

tsa's standards and integration office , within the office of training and development , has developed a plan for implementing the new evaluation program , which is intended to support agency leadership in making decisions on how to use training resources .

in addition , tsa expects to approve a management directive and standard operating procedures for the training evaluation program by may 2016 to define the roles and responsibilities for tsa offices running the training evaluation program as well as lay out the steps for analyzing and reporting data collected from the training evaluations .

tsa officials stated the training evaluation plan will be subject to annual revision , and otd will continue to update and review the plan .

standards and integration office officials are responsible for developing the training evaluations and collecting the evaluation data while tsa's training operations division will administer the training evaluations .

tsa's training evaluation plan describes the types of stakeholders involved in training evaluation , the communications strategy for sharing information on training across the agency , and the reporting requirements for training evaluation .

for example , the plan identifies , in broad terms , which kirkpatrick level evaluation reports the standards and integration office evaluations team will generate , who will receive the reports , and how they will be used .

in one example , the reporting plan shows that levels 1 , 2 , and 3 reports should be sent to program managers to help them allocate screening resources and modify training .

this program strategy , if followed , should position tsa to make data - based strategic decisions on the effectiveness of training courses once the training evaluation plan is fully implemented .

for example , tsa plans to use training evaluation data to conduct curriculum reviews to improve training courses and programs .

tsa is planning to implement its new training evaluation program in four phases .

during the first phase , tsa plans to implement level 1 and level 3 training evaluations for their tso basic training program and for core operational courses , and to collect and analyze the data from these evaluations .

in phase two tsa plans to expand level 1 and 3 training evaluations to key courses in their national training plan .

phase two is scheduled to begin in late 2016 .

once tsa has implemented these training evaluations for tso basic training and for courses in the ntp , tsa plans to add selected online learning center courses to their training evaluation program which constitutes phase three .

finally , in phase four , tsa plans to evaluate whether they need new training courses , and if so , all newly approved training courses would be required to develop an evaluation plan .

tsa uses a variety of methods to measure the performance of its tsos , including the annual proficiency review ( apr ) — an annual certification program to evaluate tsos' skill in performing the various screening functions .

portions of the apr are computer - based x - ray image tests done in a non - operational setting away from the active checkpoints while the remaining tests are skills demonstrations performed in a realistic , but inactive , screening environment such as an unused screening lane .

which components of the apr an individual tso must take are dependent on whether that tso is certified to perform passenger screening , baggage screening , or has dual certification to perform both functions .

tsa has other testing programs that take place during active operations at the checkpoints to assess tsos' level of adherence to screening sops and associated management directives .

these include the threat image projection ( tip ) image testing ; the aviation screening assessment program ( asap ) ; and presence , advisement , communication , and execution ( pace ) covert testing .

table 3 provides a summary of tso performance measurement tests .

in addition to the asap covert testing and other tests detailed in table 3 for assessing the effectiveness of tsos in carrying out screening functions , the tsa office of inspection ( ooi ) special operations division ( sod ) regularly conducts independent covert “red team” testing to measure the effectiveness of tsa security systems and identify vulnerabilities in transportation security as a whole .

tsa develops and deploys red team tests based upon current intelligence of threats against transportation systems .

in addition to assessing tsos' ability to detect threat items similar to asap testing , ooi's red team covert testing also assesses the effectiveness of other aspects of the screening operation — including screening procedures followed by the tsos and the technology they use at the checkpoints .

tsa policy requires fsds to provide remedial training to tsos who either fail components of the apr ( before being allowed to retake those portions ) or do not maintain a minimum score on tip image tests .

similarly , tsos who fail asap or red team covert tests — that is , operational tests — must take , in accordance with atsa , remedial training before returning to their screening duties .

tsa policy has not specifically required remedial training for any tsos who failed pace tests .

instead , each airport's fsd was expected to make their own determination regarding any necessary retraining based on the pace testing results .

tsa data on the results of apr and pace testing show that tsos' pass rates on both of these tests varied by airport risk category over the time periods we reviewed .

specifically , from calendar year 2009 through 2015 , the percentage of tsos that passed their apr certification tests on the first attempt remained relatively constant , with a dip occurring in calendar year 2010 followed by an increase by a similar percentage in 2015 .

according to tsa officials , this performance dip occurred because tsa ended the practice of using an outside contractor to evaluate tsos during the apr tests .

tsa officials explained that the tsa personnel who took over the evaluation function displayed less flexibility than was previously allowed in scoring of the various apr component tests in that first year after the transition ( 2010 ) .

according to tsa officials , the aforementioned changes to the apr testing program for 2015 ( including practice runs prior to grading the practical skills evaluation portions of the test and dividing the testing by quarters ) have led to improvement in the overall apr pass rates for 2015 compared to prior years .

tsa officials explained that they decided to re - examine how they conducted apr testing and implemented the resulting changes in response to feedback from tsos that certain aspects of the testing created unnecessary anxiety which affected their performance .

as described earlier , apr consists of several component tests that evaluate specific tso functions .

as shown in table 4 , these component tests include x - ray image testing and passenger pat downs , which cover actions taken by tsos in routine screening operations at the passenger and baggage screening checkpoints .

in addition to the overall apr pass rates varying by airport security category , the results of these individual component tests also varied by the type of test administered during the 2009 to 2015 timeframe .

scores for specific apr components tests are sensitive security information and not included in this report .

in addition , due to issues with both the reliability and sensitivity of tip and asap testing , we are not discussing results of those testing programs in this report .

the specific data reliability concerns related to these two testing programs are discussed later in this report .

tsa also conducted pace tests at category x , i , and ii airports to determine tsos' adherence to tsa management directives and sops in areas such as overall appearance and demeanor , properly communicating and providing instruction to passengers , and following proper procedures .

tsos' scores on pace tests generally remained above 80 percent from fiscal years 2009 through 2014 .

also , based on our review of pace test results from fiscal years 2012 through 2014 , we determined that tsos scored higher at smaller airports than larger airports during this period with the difference being most pronounced between category x airports and category ii airports .

as noted previously , tsa normally uses apr testing results primarily to assess individual tsos' skills for performing screening functions in order to annually re - certify them to continue participating in screening operations .

according to tsa officials responsible for developing the annual ntp , in fiscal year 2014 , tsa's office of training and workforce engagement ( otwe ) also examined results of specific component apr tests to inform their development of related courses for the ntp .

specifically , the officials stated that they reviewed the results of selected 2013 apr component tests — screening of individuals with disabilities ( iwd ) , bag searches , and standard pat downs .

in response , the tsa training officials said they added training to the fiscal year 2015 ntp to specifically address the deficiencies they identified in their review of the 2013 apr component tests .

tsa policy requires airport personnel to manually download tip testing results from their individual x - ray machines and upload the monthly data into tsa's national database repository for tsa results .

according to tsa headquarters personnel responsible for overseeing tip , they use these uploaded results to determine if any adjustments are needed to the quality or usefulness of the library of images maintained in the tip system nationwide .

for example , an image for which tsos have a high degree of accuracy in identifying might be removed and replaced with an image that presents more of a challenge .

conversely , an image that is frequently missed might be reassessed to determine if the image is unrealistically difficult and an adjustment needs to be made .

however , tsa's database of tip results is missing data for some airports for some years .

additionally , tsa does not analyze the tip data it collects on a nationwide basis to identify potential trends in tip test scores or opportunities for improving screener performance .

while tsa uses data submitted by the airports to update its tip image library , it is doing so with incomplete data .

as shown in figure 4 , some airports in all five airport risk categories did not report any tip results nationally over the course of a year from fiscal year 2010 through fiscal year 2013 .

during the fiscal year 2009 through 2014 time frame , fiscal year 2013 had the highest percentage of airports failing to report any tip data at nearly 14 percent .

for category x and i airports , these results had generally improved by fiscal year 2014 with all of these airports reporting tip data that year .

however , the percentage of category iii and iv airports that did not report tip data generally increased during fiscal years 2013 and 2014 compared to prior years .

tsa attributed this incomplete data to a transition to new x - ray screening equipment at certain airports from fiscal year 2009 through fiscal year 2012 .

officials stated that , due to software compatibility issues with the new machines , tip image capability was turned off for an extended period of time , meaning that tip testing was not occurring on these machines and , therefore , tip data were neither collected nor reported for these airports .

tsa officials also told us that their older x - ray machines do not have the capability to automatically upload tip data results to headquarters .

as a result , some airports relying on these older x - ray machines were not able to submit tip data automatically by electronic means and did not submit it manually .

tsa officials reported that they do not have a process for determining whether tip data have been submitted by all airports , on a regular basis , as required .

tsa officials told us they are making efforts to install automatic uploading capabilities to all new machines that they expect will help ensure that tip data reporting is complete and timely .

however , tsa has placed these efforts on hold pending security concerns that must first be addressed stemming from the recent cybersecurity breaches at the office of personnel management that have led to tsa reviewing its own cybersecurity efforts before moving forward with installation of automatic uploading capabilities on its x - ray machines .

tsa officials also acknowledged that , in addition to the airports discussed above that did not report any tip data for a year or more at a time , other airports may have reported only partial tip results data during this same time frame .

tsa officials stated that , in the nationwide results data provided to gao , it would be difficult to ascertain how much data might be missing from individual airports ( during the time period covered by our data ) since the number and type of machines in use at those airports at any particular point in time could vary .

tsa policy requires tsa officials at airports to report all of their tip results data , on a monthly basis , to a national database .

further , fsds must monitor tip results monthly and require tsos to attend remedial training if their threat identification rate falls below a target percentage .

standards for internal control in the federal government states that the information requirements needed to achieve the agency's objectives should be identified and communicated to management .

specifically , management should obtain relevant data from reliable internal and external sources in a timely manner based on the identified information requirements that allows them to carry out their internal control and other responsibilities .

we acknowledge that because the full universe of x - ray machines , and their uploading capabilities , is difficult to determine on a daily basis , it is unlikely that tsa can fully confirm whether all of the tip data across the nation are being submitted .

however , our review of tip data from fiscal year 2009 through 2014 found that up to 14 percent of airports did not submit any tip data in one of the years reviewed ( 2013 ) .

unless tsa takes steps to ensure that all airports submit complete , nationwide tip data , tsa lacks assurance that the decisions it makes on the content of the tip image library are fully informed , and also lacks assurances that tsos are receiving remedial training from the tip program which has been developed to aid their ability to identify prohibited items .

for example , while tsa is working to install automatic uploading capabilities on all x - ray machines , enforcing the requirement for airport officials to manually submit their tip data would help ensure more complete data by which to assess and address tip results .

in addition , by not ensuring the collection of available tip data , as required , the effectiveness of any potential further use of tip testing results to inform tso training or testing ( as described below ) programs is limited .

with regard to any potential further use of the tip results , tsa headquarters officials told us that , to date , they have not systematically used the tip results data to analyze national trends for purposes of informing future training programs or changes to screening processes or procedures .

tsa officials said that they have not used national tip data in this manner due to the agency's expectation that tip is a tool primarily for the benefit of local fsds to use in monitoring the training needs , and determining areas of focus , for their individual tsos locally .

tsa officials at all 10 airports we contacted stated that their fsds monitored tip results and used tip data to inform their decisions on remedial or other training needs of their tsos .

according to the tsa headquarters official responsible for overseeing the tip program , tsa formed an integrated project team in fiscal year 2015 specifically tasked with studying , developing , and implementing an effective nationwide strategy and process for using tip testing to enhance tsos' threat detection skills .

in developing the planned strategy , this team is examining six focus areas — including the improvement of tip capabilities for enhancing tso effectiveness through improved remedial training and updating the tip image library to be responsive to emerging threats .

since the team is newly formed , it has yet to complete its work .

due to the fact that the bulk of the team's work is yet to be done , it is unclear how or whether these six focus areas include plans to monitor , on a national basis , trends in the results of tip testing that could help highlight areas for improvement to future image - based screening tests ( such as the image mastery assessment component of apr testing ) or tso training .

standards for internal control in the federal government states that an agency's management should perform ongoing monitoring of its internal control system and associated operations , evaluate the results of those monitoring activities , and take corrective actions when warranted to achieve objectives and address risks .

by not including analyses of tip results data in nationwide efforts to inform either tso training or other image - based testing outside of tip , tsa is missing an opportunity to utilize this extensive , nationwide tso performance data for enhancing screening operations in addition to lacking assurance that remedial training is occurring , as required , at all airports .

in an effort to assess the quality of asap testing conducted by tsa field officials at commercial airports , tsa headquarters officials brought in a contractor in fiscal year 2015 to independently perform asap covert testing at 40 airports and thereby verify the validity of the testing results at the airports .

the contractor personnel performed the same type of asap testing that had previously been performed by local tsa personnel at the airports .

the contractor's initial round of covert testing was completed in october 2015 , and tsa has analyzed the results of the contractor's tests and compared them to asap tests performed previously at the 40 airports .

in doing this analysis , tsa found differences in the test results for most of the 40 airports when comparing the contractor's results versus the local tsa testers' results for the same airports .

according to tsa officials , tsos at these 40 airports performed more poorly in the asap tests conducted by the contractor personnel as compared to the prior asap testing done by the local tsa personnel — indicating that these prior - year pass rates were likely showing a higher level of performance than was actually the case .

also , according to the officials , these differences in test results have led them to question the extent to which the asap tests accurately measure tso performance .

tsa is in the process of determining root causes for these variances of testing results between the contractor and tsa personnel at the airports .

according to tsa officials , initial results from the contractor's work seem to confirm their prior concerns ( before the contractor testing was conducted ) that problems exist with successfully maintaining the covert nature of tests at airports .

tsa officials explained that these prior concerns were based on the high detection rates at some airports when compared to other airports on the same tests .

with respect to the difficulty in maintaining the covert nature of the tests , tsa officials at 7 of 10 of the airports we contacted indicated challenges with obtaining anonymous role players to ensure that the asap tests remain covert .

for example , tsa officials at one airport we visited reported having to rely on the availability of state and local government employees and u.s. customs and border protection personnel to perform as role players .

another smaller airport we visited reported challenges finding role players among local tsa personnel that the tsos working the screening lanes would not recognize .

as a result , they tend to use new hires , national guard , federal aviation administration , and federal bureau of investigation personnel .

tsa officials stated that proposed changes to the asap sop will provide fsds greater authority to use role players that they have vetted and accepted responsibility for , beyond state , local , and federal government officials .

in an effort to address concerns stemming from their initial analysis of the contractor's test results , tsa briefed its fsds on these results and stated that it expects the fsds will use this information as input in overseeing their local asap testing programs .

in addition , tsa has extended the work of the contractor by 6 months in order to do further testing that it can compare to local asap test results going forward .

tsa stated it will continue to analyze the contractor's results and compare them against the ongoing results from local asap testing overseen by the fsds to determine if the previously - identified variances in results are continuing .

tsa officials stated that the findings of the contractor during the 6-month extension period indicated that the variances previously identified in results for the contractor testing versus the local asap testing at the airports have been reduced .

tsa headquarters officials attributed the reduction in variance to more frequent and improved communication with the fsds and those responsible for conducting the local asap tests — specifically with regard to the contractor's test findings and potential corrective actions they should undertake to improve the local asap testing programs .

tsa headquarters officials added that it is through these measures that they are improving the accountability of the local fsds and their staff for ensuring the quality and reliability of the local asap testing going forward .

tsa officials added that , after the start of the contractor's work , they had initiated an effort to improve aspects of the asap testing program that will include better identification of root causes for asap testing failures , which they expect will improve the development of associated corrective actions moving forward .

this effort is still ongoing and also includes merging aspects of pace testing into the asap program to help identify instances where a lack of standardization in the application of specific screening sops ( which pace testing is designed to measure ) may negatively impact the screening process .

regarding tsa's efforts to better identify root causes of asap failures to improve the program , tsa has developed a data collection tool that tsa officials said would support these efforts by gathering critical data from test failures that they will analyze to determine root causes of the failure .

according to tsa officials , the tool has recently been developed and field tested and is pending initial roll out .

in addition to the asap / pace merger , program enhancements related to the identification of root causes , and ongoing contractor asap testing , tsa officials are adding asap headquarters testing to supplement the asap testing that will continue to be performed by tsa personnel in the field ( referred to as the field evaluation team or fet ) .

tsa stated this new headquarters - based testing effort will be referred to as the headquarters evaluation team ( het ) and would be formed from the former pace evaluation teams .

according to tsa , these headquarters - based covert testing teams will perform quality assurance and validation activities for asap that are currently being performed by the contract test teams .

in addition , tsa expects that the contractor and new headquarters asap testing program will provide assurance that the asap testing still being conducted by tsa personnel at the airports is accurate .

however , field asap testing will still account for the majority of tsa's asap covert tests .

tsa officials stated they expect once the het program is initiated , the contractor testing will be discontinued .

also , according to tsa , a newly - developed data collection tool will be used by all of the asap testing groups moving forward ( i.e. , fet , het , and the contract test teams ) to determine the root causes of test failures that will better inform tsa's corrective actions .

tsa conducts asap testing in 6-month increments and produces a summary report of results across all airports , complete with recommendations , at the end of each 6-month cycle .

in these reports , tsa details the analysis it has performed on the nationwide results of the asap testing that shows how tsos have performed in their duties at the various decision points on the passenger and checked baggage screening lanes .

this analysis includes failure rates at these various points , reasons for the failures , and related recommendations where appropriate to improve tso performance .

these recommendations may include , among other things , additional training for certain points in the screening process and further testing in certain areas .

according to tsa officials , they have recently moved to more frequent weekly and monthly reporting of asap results to the field as part of the aforementioned effort to improve communication with fsds and staff with regard to findings and trends coming from the asap testing results — including those results from the asap contractor .

tsa headquarters does not require fsds to implement recommendations from the six - month cycle reports nor does it track whether the recommendations have been implemented , or conversely , reasons for not implementing them .

tsa officials stated that the various recommendations cited in the cycle reports are strictly for the consideration of fsds in the field and implementation is not mandatory .

tsa officials also stated that the asap cycle reports are intended to analyze nationwide trends in tso performance and identify causes of potential deficiencies .

tsa invests time and resources to produce these reports — which include test results and corrective actions — on a routine basis and disseminates the information to airport fsds .

given this investment , tracking implementation of the recommendations detailed in those reports , in addition to any recommendations that may be present in the more frequently - implemented weekly or monthly reporting , would help tsa ensure that corrective actions are being taken at airports nationwide to improve tso performance , which the agency has identified as an area of concern based on the nationwide trend analysis .

moreover , tracking the implementation of its recommendations , including the extent to which identified corrective actions are improving future tso performance and test results , will help tsa better determine the extent to which its implemented recommendations are leading to improvements in screening operations and appropriately addressing identified root causes for previous test failures .

standards for internal control in the federal government requires that internal controls be designed to ensure that ongoing monitoring occurs during the course of normal operations .

specifically , internal controls direct managers to ( 1 ) promptly evaluate and resolve findings from audits and other reviews , including those showing deficiencies and recommendations reported by auditors and others who evaluate agencies' operations ; ( 2 ) determine proper actions in response to findings and recommendations from audits and reviews ; and ( 3 ) complete , within established time frames , all actions that correct or otherwise resolve the matters brought to management's attention .

we recognize the efforts tsa has recently initiated to improve the accuracy and reliability of asap testing .

however , without the assurance that recommendations for corrective actions based on the root causes identified in asap testing will be fully implemented — where appropriate — nationwide , tsa will be limited in its ability to take full advantage of any findings from the program .

training tsos and obtaining an accurate understanding of their effectiveness in detecting prohibited items on passengers , and in their baggage , can have a critical impact on the security of millions of air travelers each year .

tsa has put an extensive program in place to train its tsos to perform these critical screening functions and responded to recent covert test findings of the dhs oig by implementing a retraining program for all its screening officers to address issues identified in the testing .

tsa has also begun implementing a plan to expand evaluations of its tso training efforts in order to better inform future management decisions .

in addition to its training and evaluation efforts , tsa conducts wide - ranging covert testing and annual certification testing of its tsos .

while we commend tsa's recent efforts to re - examine its testing programs , such as steps to improve the accuracy and reliability of asap testing , the agency could further enhance its testing programs to more accurately gauge the true level of tso performance and ensure continuing improvement in screening operations .

for example , enforcing its requirement that all airports submit tip results data would help tsa continually improve the test .

further , the agency could use these data on a nationwide level to inform and potentially improve training of tsos in screening passenger carry - on baggage for prohibited items .

in addition , given that tsa uses asap covert testing results to assess whether tsos follow proper screening procedures and successfully detect prohibited items , ensuring that any recommendations stemming from the asap testing failures are tracked and implemented , where appropriate , would further support the program's objective to improve the performance and quality of security screening .

to improve tsa's ability to take full advantage of testing results to inform and potentially improve screening operations , we recommend that the secretary of the department of homeland security direct the administrator of tsa to take the following three actions: ensure that tsa officials at individual airports submit complete tip results to the tsa national database as required , including manually submitting data when automated uploading is not available .

conduct analysis of national tip data for trends that could inform training needs and improve future training and tso performance assessments .

track implementation by airports of asap recommendations to ensure that corrective actions identified through asap testing are being applied .

we provided a draft of the sensitive version of this report to dhs for their review and comment .

dhs provided written comments , which are noted below and reproduced in full in appendix ii , and technical comments , which we incorporated as appropriate .

dhs concurred with all three recommendations in the report and described actions underway or planned to address them .

with regard to the first recommendation that tsa ensure tip data is submitted to the tsa national database as required , dhs concurred and stated that tsa is working to establish a tracking system that will automatically identify and highlight specific airports that may be missing from the database .

the automated system will allow tsa to establish an internal webpage that will automatically generate a list of airports that have not submitted tip data as required , and which managers will be able to use to follow - up with federal security directors to ensure tip data is submitted .

the agency stated that the automated process is dependent on the development of an information technology ( it ) tool which they anticipate will be piloted by may 31 , 2017 .

in the interim , while this it tool is being developed , tsa officials will monitor compliance with tip reporting requirements and follow up with those airports missing tip data , including identifying reasons for the airport's non - compliance .

tsa is also drafting a revised tip operations directive that is intended to provide further guidance and direction to the field on tip requirements .

tsa estimates they will complete these actions to address the first recommendation by september 30 , 2016 .

with regard to the second recommendation to conduct analysis of national tip data for trends that could inform training needs and improve future tso performance , dhs concurred and detailed the following actions to address this recommendation: tsa's office of training and development ( otd ) has begun to update tip remediation requirements and work with airports that have achieved the highest tip scores to identify any best practices that could be shared with other airports .

otd plans to work with airports that struggle with tip to identify information about their oversight and remediation program with the goal of using the highest and lowest scoring airports to assess the effect of oversight and remediation on performance .

tsa plans to analyze data across the network to determine what remediation training best supports improvements in tip scores .

tsa is developing a process to analyze specific data connected to threat categories of tip images which will allow officials to identify the specific types of threats that are presenting challenges to the workforce .

otd will then be able to identify what additional training should be developed to improve performance for that particular threat category .

tsa plans to assess tip training and assessments over the next 12 months to determine if performance improvement has been realized , and if so , what contributed to the improvement .

otd is working with a contractor to design a report that is intended to capture officer performance results connected to specific types of tip images to better drive training content and improve performance .

tsa's office of security capabilities is working with both otd and the office of security operations ( oso ) to capture tip data for the development of threat categories to assess individual tso's performance and asking tsa's office of acquisitions for a contract modification that will provide for more frequent report updates .

tsa estimates they will complete these actions to address the second recommendation by may 31 , 2017 .

with regard to the third recommendation to track implementation by airports of asap recommendations to ensure that corrective actions are being applied , dhs concurred and stated that tsa has taken actions to formalize asap reporting .

for example , tsa has reported developing a standard format for corrective action plans , which are submitted and implemented after an asap failure .

this should help tsa track corrective actions and their effectiveness in addressing findings from asap tests .

further , tsa plans to conduct reassessments within 30-60 days after a corrective action plan has been submitted to ensure corrective actions have been implemented .

tsa also reported that the standard format for caps deliberately maps corrective actions to their identified issues .

according to tsa , as of august 2016 , oso has conducted more than 55 post - headquarters evaluation team testing calls and more than 50 effectiveness calls to review caps .

oso has extracted common themes from high performing airports and distributed this “best practice” information to all its regional directors and federal security directors .

tsa also stated that oso is reassessing those previously - tested airports to ensure that corrective actions are implemented and detection performance is improving at or above the national average .

these efforts by tsa to ensure that corrective actions identified through asap testing are being applied , if continued in future testing cycles , should address the intent of this recommendation .

these completed actions for the third recommendation along with the planned actions for the first and second recommendations , if fully implemented , should address the intent of the three recommendations contained in this report .

we are sending copies of this report to the appropriate congressional committees , the secretary of homeland security , the attorney general of the united states , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-7141 or groverj@gao.gov .

key contributors to this report are listed in appendix iii .

this report answers the following questions: 1 .

how does the transportation security administration ( tsa ) train transportation security officers ( tso ) , and to what extent does tsa evaluate the training ? .

2 .

how does tsa measure the performance of tsos , and what do the performance data show ? .

3 .

to what extent does tsa use tso performance data to enhance tso performance ? .

to address our first objective regarding how tsa trains tsos and to what extent tsa evaluates the training , we reviewed relevant tsa policies and procedures for training , including management directives and the national training plan ( ntp ) , which prescribes the annual training curriculum for tsos .

we also reviewed documentation on training requirements , including those contained in the aviation and transportation security act , as well as documents on tsa's training development and completion .

we interviewed tsa headquarters officials responsible for developing and monitoring tso training , including officials from tsa's office of training and development ( otd ) , office of human capital ( ohc ) , and the office of security operations ( oso ) .

further , we interviewed staff from a total of 10 airports — including federal security directors ( fsd ) , transportation security managers , instructors , training managers , tsos , and other tsa staff , such as explosives experts , to determine how training is carried out in the field and to learn what tsa employees in the field thought about training .

specifically , we conducted site visits to six airports , including three airports in category x , and one airport each in categories i , ii , and iii .

further , we conducted phone interviews with officials at one airport each in categories i , ii , iii , and iv to obtain additional perspectives on how airport officials carry out training requirements locally — particularly at airports with smaller numbers of flights and passenger boardings .

we selected the airports to visit in person based on factors such as airport category , geographic proximity to one another , and our analysis of the airports' tso performance on annual screening certification tests from calendar years 2009 through 2014 .

for example , we calculated the average first time pass rates for screeners taking their annual proficiency review ( apr ) exams for each airport in each calendar year from 2009 to 2014 and sorted the scores by airport risk category .

apr assessments are annual certification tests tsos must pass to remain employed as a screener .

we then selected at least one airport from the high , low , and middle of the performance distribution and made sure to cover at least one airport in every risk category .

to assess the extent to which tsa evaluates tso training , we reviewed tsa documents used for evaluating training courses , including end - of - course surveys administered to learners .

further , we reviewed draft documents on tsa's training evaluation plan , including a draft management directive and draft standard operating procedures for evaluating training courses .

we compared the training evaluation documentation to the kirkpatrick model for training evaluation , which is the model tsa uses as guidance for its evaluations of tso training .

we also interviewed tsa headquarters officials responsible for evaluating tso training and for developing and implementing the tsa training evaluation plan .

for example , we interviewed tsa officials from otd , oso , and ohc to determine the extent to which they evaluated training courses and used this information to refine future training .

further , we interviewed management officials at each of the airports we visited to further understand how , if at all , training at individual airports is evaluated locally .

for our second objective , to determine how tsa measures the performance of tsos and what the performance data show , we analyzed data from four different performance evaluation programs and we interviewed tsa officials responsible for collecting and analyzing the data .

first , we reviewed and analyzed data on aprs , including analyzing apr pass rates from calendar year 2009 ( the first year for which data were available ) through 2015 .

for example , we calculated the average first time pass rate for screeners taking the apr assessments for each airport and sorted the results by year , airport category , and by each individual apr assessment .

see table 4 for a description of the apr assessments we analyzed .

we then conducted a trend analysis to observe overall apr first - time pass rates over time , and we compared apr first - time pass rates for screeners across airport risk categories to determine whether there were any differences in pass rates across airport categories .

in addition , we interviewed officials in charge of the apr testing process , including officials from oso , ohc , otd from tsa headquarters , as well as local airport officials in charge of overseeing the tests .

second , we reviewed threat image projection ( tip ) system data from fiscal year 2009 to fiscal year 2014 , the last year available at the time of our data request .

the tip system is intended to help tsa measure whether operators correctly identify threat items that are electronically superimposed on the x - ray monitor during the screening of passenger property at the checkpoint .

specifically , we analyzed the average percentage of tip images correctly identified during screening by screeners at different airport categories over time to determine whether there were differences in average tip scores between airport categories .

further , we interviewed tsa officials in charge of the tip image library from the office of security capabilities to understand how tip data are recorded and collected , and how the tip images are selected for use .

third , we reviewed data from tsa's presence , advisement , communication , and execution ( pace ) testing program , which tsa uses to measure whether tsos are adhering to standard operating procedures while screening at the passenger checkpoint .

we reviewed pace data from calendar year 2011 , the year the program was started , until 2014 and charted pace scores by airport category across time .

we also interviewed appropriate tsa officials regarding the pace program to understand how the program worked and how the scores were calculated .

finally , we analyzed data from the aviation screening assessment program ( asap ) , a covert testing program used to evaluate screeners' ability to properly follow tsa's standard operating procedures for screening and keep prohibited items from being taken through the checkpoint .

we analyzed asap data from fiscal years 2013 through 2015 because tsa made adjustments to the asap testing program in 2013 , and therefore the pre - 2013 testing data are not comparable to the 2013 through 2015 data .

results of asap testing are classified at the secret level and are not included in this report .

additionally we interviewed tsa officials from oso responsible for the asap program to gain their perspectives on the program .

we also interviewed officials responsible for conducting asap tests at each of the airports we visited to understand how the tests worked in practice , what happened after a test was passed or failed , and to learn about any challenges officials faced in running the tests .

we assessed the reliability of the apr , tip , pace , and asap data by ( 1 ) interviewing agency officials responsible for maintaining the data about how the data were collected and entered into the respective databases , how the data were used , and what procedures were in place to ensure the data were complete ; and ( 2 ) testing the data for missing data , duplicates , or entries that otherwise appeared to be unusual .

we found the apr and pace data to be sufficiently reliable to present in this report .

however , we found that the tip data were incomplete for the years we were analyzing and therefore not sufficiently reliable to include in this report .

specifically , tsa officials in charge of the tip data stated they were uncertain how complete the tip data were nationwide at any point in time , but added that it is likely never fully complete .

officials stated that this was due to two reasons .

first , when tsa first deployed new x - ray machines between 2009 and 2012 , the tip software was not activated on them due to technical issues .

as a result , no tip data were reported for those machines over this period .

second , the newer x - ray machines coming online are equipped to upload tip data to tsa headquarters automatically over a network .

however , not all machines in the field are equipped to do this , and tsa temporarily stopped implementation of the automatic upload capability on the newer machines in 2015 because of network security concerns .

instead , tsa personnel must manually download the tip data for these machines on a monthly basis as it does for older machines without this automatic upload capability .

as a result , tsa headquarters has not received tip data from every airport for every month over the time period of our review resulting in the database being incomplete .

tsa could not provide us with information on the extent of the missing data and we were not able to determine based on the data provided how many x - ray machines were unaccounted for between 2009 and 2014 .

for our third objective , to determine the extent to which tsa uses tso performance data to enhance screening performance , we reviewed tsa's processes and actions for using screener testing results to inform its operations and training , and assessed these processes against standards in standards for internal control in the federal government .

further , we interviewed program officials from several offices at tsa headquarters about how they analyze performance data such as apr , tip , and pace data , and how , if at all , they use the results to adjust training or take any other actions .

similarly , we interviewed officials from each of the airports we visited about how the collected , reported , monitored , and used the performance data they collected .

we conducted this performance audit from february 2015 to september 2016 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

jennifer grover ( 202 ) 512-7141 or groverj@gao.gov .

in addition to the contact named above , christopher e. ferencik , assistant director ; mike harmond , analyst in charge ; and brendan kretzschmar made key contributions to this report .

also contributing to the report were , eric d. hauswirth , susan hsu , thomas f. lombardi , heidi nielson , ying long , amanda miller , ruben montes de oca , dae park , and christine san .

