billions of taxpayer dollars are spent on information technology ( it ) investments each year ; federal it spending reported to the office of management and budget ( omb ) totaled approximately $79 billion in fiscal year 2011 .

during the past several years , we have issued multiple reports and testimonies and made numerous recommendations to omb to improve the transparency , oversight , and management of the federal government's it investments .

in june 2009 , omb deployed a public website , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 800 major federal it investments .

the dashboard aims to improve the transparency and oversight of these investments .

in july 2010 , we completed our first review of the dashboard and reported that the cost and schedule ratings were not always accurate because of limitations with omb's calculations .

we recommended that omb report to congress on the effect of its planned dashboard calculation changes on the accuracy of performance information and provide guidance to agencies that standardizes activity reporting .

in march 2011 , we completed our second review of the dashboard and again reported that the cost and schedule ratings were not always accurate .

specifically , this was due to weaknesses in agencies' practices and limitations with omb's calculations .

we recommended that selected agencies take steps to improve the accuracy and reliability of dashboard information and that omb improve how it rates investments relative to current performance and schedule variance .

this is the third report in our series of dashboard reviews and responds to your request that we examine the accuracy of the cost and schedule performance ratings on the dashboard for selected investments .

to accomplish this objective , we reviewed 4 agencies with large it budgets — the departments of commerce , the interior , and state , as well as the general services administration ( gsa ) — after excluding the 10 agencies included in the first two dashboard reviews .

the 4 agencies account for about 7 percent of it spending for fiscal year 2011 .

we then selected eight major investments undergoing development , which represent about $486 million in total spending for fiscal year 2011 .

we analyzed monthly cost and schedule performance reports , program management documents , and operational analyses for the eight investments to assess program performance .

we then compared our analyses of investment performance against the corresponding ratings on the dashboard to determine if the ratings were accurate .

additionally , we interviewed officials from omb and the agencies to obtain further information on their efforts to ensure the accuracy of the data used to rate investment performance on the dashboard .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on the documentation the agencies provided .

we conducted this performance audit from february 2011 to november 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

further details of our objective , scope , and methodology are provided in appendix i .

each year , omb and federal agencies work together to determine how much the government plans to spend on it investments and how these funds are to be allocated .

in fiscal year 2011 , government it spending reported to omb totaled approximately $79 billion .

omb plays a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cio ) to advise and assist agency heads in carrying out this responsibility .

the clinger - cohen act strengthened the requirements of the paperwork reduction act of 1995 , which established agency responsibility for maximizing value and assessing and managing the risks of major information systems initiatives .

the paperwork reduction act also requires that omb develop and oversee policies , principles , standards , and guidelines for federal agency it functions , including periodic evaluations of major information systems .

another key law is the e - government act of 2002 , which requires omb to report annually to congress on the status of e - government .

in these reports , referred to as implementation of the e - government act reports , omb is to describe the administration's use of e - government principles to improve government performance and the delivery of information and services to the public .

to help carry out its oversight role , in 2003 , omb established the management watch list , which included mission - critical projects that needed to improve performance measures , project management , it security , or overall justification for inclusion in the federal budget .

further , in august 2005 , omb established a high - risk list , which consisted of projects identified by federal agencies , with the assistance of omb , as requiring special attention from oversight authorities and the highest levels of agency management .

over the past several years , we have reported and testified on omb's initiatives to highlight troubled it projects , justify investments , and use project management tools .

we have made multiple recommendations to omb and federal agencies to improve these initiatives to further enhance the oversight and transparency of federal projects .

among other things , we recommended that omb develop a central list of projects and their deficiencies and analyze that list to develop governmentwide and agency assessments of the progress and risks of the investments , identifying opportunities for continued improvement .

in addition , in 2006 we also recommended that omb develop a single aggregate list of high - risk projects and their deficiencies and use that list to report to congress on progress made in correcting high - risk problems .

as a result , omb started publicly releasing aggregate data on its management watch list and disclosing the projects' deficiencies .

furthermore , omb issued governmentwide and agency assessments of the projects on the management watch list and identified risks and opportunities for improvement , including in the areas of risk management and security .

more recently , to further improve the transparency and oversight of agencies' it investments , in june 2009 , omb publicly deployed a website , known as the it dashboard , which replaced the management watch list and high - risk list .

it displays federal agencies' cost , schedule , and performance data for the approximately 800 major federal it investments at 27 federal agencies .

according to omb , these data are intended to provide a near - real - time perspective on the performance of these investments , as well as a historical perspective .

further , the public display of these data is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold the government agencies accountable for results and progress .

the dashboard was initially deployed in june 2009 based on each agency's exhibit 53 and exhibit 300 submissions .

after the initial population of data , agency cios have been responsible for updating cost , schedule , and performance fields on a monthly basis , which is a major improvement from the quarterly reporting cycle omb previously used for the management watch list and high - risk list .

for each major investment , the dashboard provides performance ratings on cost and schedule , a cio evaluation , and an overall rating , which is based on the cost , schedule , and cio ratings .

as of july 2010 , the cost rating is determined by a formula that calculates the amount by which an investment's total actual costs deviate from the total planned costs .

similarly , the schedule rating is the variance between the investment's planned and actual progress to date .

figure 1 displays the rating scale and associated categories for cost and schedule variations .

each major investment on the dashboard also includes a rating determined by the agency cio , which is based on his or her evaluation of the performance of each investment .

the rating is expected to take into consideration the following criteria: risk management , requirements management , contractor oversight , historical performance , and human capital .

this rating is to be updated when new information becomes available that would affect the assessment of a given investment .

last , the dashboard calculates an overall rating for each major investment .

this overall rating is an average of the cost , schedule , and cio ratings , with each representing one - third of the overall rating .

however , when the cio's rating is lower than both the cost and schedule ratings , the cio's rating will be the overall rating .

figure 2 shows the overall performance ratings of the 797 major investments on the dashboard as of august 2011 .

we have previously reported that the cost and schedule ratings on omb's dashboard were not always accurate for selected agencies .

in july 2010 , we reviewed investments at the departments of agriculture , defense , energy , health and human services , and justice , and found that the cost and schedule ratings on the dashboard were not accurate for 4 of 8 selected investments and that the ratings did not take into consideration current performance ; specifically , the ratings calculations factored in only completed activities .

we also found that there were large inconsistencies in the number of investment activities that agencies report on the dashboard .

in the report , we recommended that omb report on the effect of planned changes to the dashboard and provide guidance to agencies to standardize activity reporting .

we further recommended that the selected agencies comply with omb's guidance to standardize activity reporting .

omb and the department of energy concurred with our recommendations , while the other selected agencies provided no comments .

in july 2010 , omb updated the dashboard's cost and schedule calculations to include both ongoing and completed activities .

in march 2011 , we reported that agencies and omb need to do more to ensure the dashboard's data accuracy .

specifically , we reviewed investments at the departments of homeland security , transportation , the treasury , and veterans affairs , and the social security administration .

we found that cost ratings were inaccurate for 6 of 10 selected investments and schedule ratings were inaccurate for 9 of 10 .

we also found that weaknesses in agency and omb practices contributed to the inaccuracies on the dashboard ; for example , agencies had uploaded erroneous data , and omb's ratings did not emphasize current performance .

we therefore recommended that the selected agencies provide complete and accurate data to the dashboard on a monthly basis and ensure that the cios' ratings of investments disclose issues that could undermine the accuracy of investment data .

further , we recommended that omb improve how it rates investments related to current performance and schedule variance .

the selected agencies generally concurred with our recommendation .

omb disagreed with the recommendation to change how it reflects current investment performance in its ratings because dashboard data are updated on a monthly basis .

however , we maintained that current investment performance may not always be as apparent as it should be ; while data are updated monthly , the ratings include historical data , which can mask more recent performance .

most of the cost and schedule ratings on the dashboard were accurate , but did not provide sufficient emphasis on recent performance to inform oversight and decision making .

performance rating discrepancies were largely due to missing or incomplete data submissions from the agencies .

however , we generally found fewer such discrepancies than in previous reviews , and in all cases the selected agencies found and corrected these inaccuracies in subsequent submissions .

in the case of gsa , officials did not disclose that performance data on the dashboard were unreliable for one investment because of an ongoing baseline change .

without proper disclosure of pending baseline changes , the dashboard will not provide the appropriate insight into investment performance needed for near - term decision making .

additionally , because of the dashboard's ratings calculations , the current performance for certain investments was not as apparent as it should be for near - real - time reporting purposes .

if fully implemented , omb's recent and ongoing changes to the dashboard , including new cost and schedule rating calculations and updated investment baseline reporting , should address this issue .

these dashboard changes could be important steps toward improving insight into current performance and the utility of the dashboard for effective executive oversight .

in general , the number of discrepancies we found in our reviews of selected investments has decreased since july 2010 .

according to our assessment of the eight selected investments , half had accurate cost ratings and nearly all had accurate schedule ratings on the dashboard .

table 1 shows our assessment of the selected investments during a 6- month period from october 2010 through march 2011 .

as shown above , the dashboard's cost ratings for four of the eight selected investments were accurate , and four did not match the results of our analyses during the period from october 2010 through march 2011 .

specifically ,  state's global foreign affairs compensation system and interior's land satellites data system investments had inaccurate cost ratings for at least 5 months ,  gsa's system for tracking and administering real property / realty services was inaccurate for 3 months , and interior's financial and business management system was inaccurate for 2 months .

in all of these cases , the dashboard's cost ratings showed poorer performance than our assessments .

for example , state's global foreign affairs compensation system investment's cost performance was rated “yellow” ( i.e. , needs attention ) in october and november 2010 , and “red” ( i.e. , significant concerns ) from december 2010 through march 2011 , whereas our analysis showed its cost performance was “green” ( i.e. , normal ) during those months .

additionally , gsa's system for tracking and administering real property / realty services investment's cost performance was rated “yellow” from october 2010 through december 2010 , while our analysis showed its performance was “green” for those months .

regarding schedule , the dashboard's ratings for seven of the eight selected investments matched the results of our analyses over this same 6-month period , while the ratings for one did not .

specifically , interior's land satellites data system investment's schedule ratings were inaccurate for 2 months ; its schedule performance on the dashboard was rated “yellow” in november and december 2010 , whereas our analysis showed its performance was “green” for those months .

as with cost , the dashboard's schedule ratings for this investment for these 2 months showed poorer performance than our assessment .

there were three primary reasons for the inaccurate cost and schedule dashboard ratings described above: agencies did not report data to the dashboard or uploaded incomplete submissions , agencies reported erroneous data to the dashboard , and the investment baseline on the dashboard was not reflective of the investment's actual baseline ( see table 2 ) .

 missing or incomplete data submissions: four selected investments did not upload complete and timely data submissions to the dashboard .

for example , state officials did not upload data for one of the global foreign affairs compensation system investment's activities from october 2010 through december 2010 .

according to a state official , the department's investment management system was not properly set to synchronize all activity data with the dashboard .

the official stated that this issue was corrected in december 2010 .

 erroneous data submissions: one selected investment — interior's land satellites data system — reported erroneous data to the dashboard .

specifically , interior officials mistakenly reported certain activities as fully complete rather than partially complete in data submissions from september 2010 through december 2010 .

agency officials acknowledged the error and stated that they submitted correct data in january and february 2011 after they realized there was a problem .

inconsistent investment baseline: one selected investment — gsa's system for tracking and administering real property / realty services — reported a baseline on the dashboard that did not match the actual baseline tracked by the agency .

in june 2010 , omb issued new guidance on rebaselining , which stated that agencies should update investment baselines on the dashboard within 30 days of internal approval of a baseline change and that this update will be considered notification to omb .

the gsa investment was rebaselined internally in november 2010 , but the baseline on the dashboard was not updated until february 2011 .

gsa officials stated that they submitted the rebaseline information to the dashboard in january 2011 and thought that it had been successfully uploaded ; however , in february 2011 , officials realized that the new baseline was not on the dashboard .

gsa officials successfully uploaded the rebaseline information in late february 2011 .

additionally , omb's guidance states that agency cios should update the cio evaluation on the dashboard as soon as new information becomes available that affects the assessment of a given investment .

during an agency's internal process to update an investment baseline , the baseline on the dashboard will not be reflective of the current state of the investment ; thus , investment cio ratings should disclose such information .

however , the cio evaluation ratings for gsa's system for tracking and administering real property / realty services investment did not provide such a disclosure .

without proper disclosure of pending baseline changes and resulting data reliability weaknesses , omb and other external oversight groups will not have the appropriate information to make informed decisions about these investments .

in all of the instances where we identified inaccurate cost or schedule ratings , agencies had independently recognized that there was a problem with their dashboard reporting practices and taken steps to correct them .

such continued diligence by agencies to report accurate and timely data will help ensure that the dashboard's performance ratings are accurate .

according to omb , the dashboard is intended to provide a near - real - time perspective on the performance of all major it investments .

furthermore , our work has shown cost and schedule performance information from the most recent 6 months to be a reliable benchmark for providing this perspective on investment status .

this benchmark for current performance provides information needed by omb and agency executive management to inform near - term budgetary decisions , to obtain early warning signs of impending schedule delays and cost overruns , and to ensure that actions taken to reverse negative performance trends are timely and effective .

the use of such a benchmark is also consistent with omb's exhibit 300 guidelines , which specify that project activities should be broken into segments of 6 months or less .

in contrast , the dashboard's cost and schedule ratings calculations reflect a more cumulative view of investment performance dating back to the inception of the investment .

thus , a rating for a given month is based on information from the entire history of each investment .

while a historical perspective is important for measuring performance over time relative to original cost and schedule targets , this information may be dated for near - term budget and programmatic decisions .

moreover , combining more recent and historical performance can mask the current status of the investment .

as more time elapses , the impact of this masking effect will increase because current performance becomes a relatively smaller factor in an investment's cumulative rating .

in addition to our assessment of cumulative investment performance ( as reflected in the dashboard ratings ) , we determined whether the ratings were also reflective of current performance .

our analysis showed that two selected investments had a discrepancy between cumulative and current performance ratings .

specifically ,  state's global foreign affairs compensation system investment's schedule performance was rated “green” on the dashboard from october 2010 through march 2011 , whereas our analysis showed its current performance was “yellow” for most of that time .

from a cumulative perspective , the dashboard's ratings for this investment were accurate ( as previously discussed in this report ) ; however , these take into account activities dating back to 2003 .

interior's financial and business management system investment's cost performance was rated “green” on the dashboard from december 2010 through march 2011 ; in contrast , our analysis showed its current performance was “yellow” for those months .

the dashboard's cost ratings accurately reflected cumulative cost performance from 2003 onward .

further analysis of the financial and business management system's schedule performance ratings on the dashboard showed that because of the amount of historical performance data factored into its ratings as of july 2011 , it would take a minimum schedule variance of 9 years on the activities currently under way in order to change its rating from “green” to “yellow,” and a variance of more than 30 years before turning “red.” we have previously recommended to omb that it develop cost and schedule dashboard ratings that better reflect current investment performance .

at that time , omb disagreed with the recommendation , stating that real - time performance is always reflected in the ratings since current investment performance data are uploaded to the dashboard on a monthly basis .

however , in september 2011 , officials from omb's office of e - government & information technology stated that changes designed to improve insight into current performance on the dashboard have either been made or are under way .

if omb fully implements these actions , the changes should address our recommendation .

specifically ,  new project - level reporting: in july 2011 , omb issued new guidance to agencies regarding the information that is to be reported to the dashboard .

in particular , beginning in september 2011 , agencies are required to report data to the dashboard at a detailed project level , rather than at the investment level previously required .

further , the guidance emphasizes that ongoing work activities should be broken up and reported in increments of 6 months or less .

 updated investment baseline reporting: omb officials stated that agencies are required to update existing investment baselines to reflect planned fiscal year 2012 activities , as well as data from the last quarter of fiscal year 2011 onward .

omb officials stated that historical investment data that are currently on the dashboard will be maintained , but plans have yet to be finalized on how these data may be displayed on the new version of the dashboard .

 new cost and schedule ratings calculations: omb officials stated that work is under way to change the dashboard's cost and schedule ratings calculations .

specifically , officials said that the new calculations will emphasize ongoing work and reflect only development efforts , not operations and maintenance activities .

in combination with the first action on defining 6-month work activities , the calculations should result in ratings that better reflect current performance .

omb plans for the new version of the dashboard to be fully viewable by the public upon release of the president's budget for fiscal year 2013 .

once omb implements these changes , they could be significant steps toward improving insight into current investment performance on the dashboard .

we plan to evaluate the new version of the dashboard once it is publicly available in 2012 .

since our first review in july 2010 , the accuracy of investment ratings on the dashboard has improved because of omb's refinement of its cost and schedule calculations , and the number of discrepancies found in our reviews has decreased .

while rating inaccuracies continue to exist , for the discrepancies we identified , the dashboard's ratings generally showed poorer performance than our assessments .

reasons for inaccurate dashboard ratings included missing or incomplete agency data submissions , erroneous data submissions , and inconsistent investment baseline information .

in all cases , the selected agencies detected the discrepancies and corrected them in subsequent dashboard data submissions .

however , in gsa's case , officials did not disclose that performance data on the dashboard were unreliable for one investment because of an ongoing baseline change .

additionally , the dashboard's ratings calculations reflect cumulative investment performance — a view that is important but does not meet omb's goal of reporting near - real - time performance .

our it investment management work has shown a 6-month view of performance to be a reliable benchmark for current performance , as well as a key component of informed executive decisions about the budget and program .

omb's dashboard changes could be important steps toward improving insight into current performance and the utility of the dashboard for effective executive oversight .

to better ensure that the dashboard provides accurate cost and schedule performance ratings , we are recommending that the administrator of gsa direct its cio to comply with omb's guidance related to dashboard data submissions by updating the cio rating for a given gsa investment as soon as new information becomes available that affects the assessment , including when an investment is in the process of a rebaseline .

because we have previously made recommendations addressing the development of dashboard ratings calculations that better reflect current performance , we are not making additional recommendations to omb at this time .

we provided a draft of our report to the five agencies selected for our review and to omb .

in written comments on the draft , commerce's acting secretary concurred with our findings .

also in written comments , gsa's administrator stated that gsa agreed with our finding and recommendation and would take appropriate action .

letters from these agencies are reprinted in appendixes iii and iv .

in addition , we received oral comments from officials from omb's office of e - government & information technology and written comments via e - mail from an audit liaison from interior .

these comments were technical in nature and we incorporated them as appropriate .

omb and interior neither agreed nor disagreed with our findings .

finally , an analyst from education and a senior management analyst from state indicated via e - mail that they had no comments on the draft .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to interested congressional committees ; the director of omb ; the secretaries of commerce , education , the interior , and state ; the administrator of gsa ; and other interested parties .

in addition , the report will be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions on the matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix v .

our objective was to examine the accuracy of the cost and schedule performance ratings on the dashboard for selected investments .

we selected 5 agencies and 10 investments to review .

to select these agencies and investments , we used the office of management and budget's ( omb ) fiscal year 2011 exhibit 53 to identify 6 agencies with the largest information technology ( it ) budgets , after excluding the 10 agencies included in our first two dashboard reviews .

we then excluded the national aeronautics and space administration because it did not have enough investments that met our selection criteria .

as a result , we selected the departments of commerce , education , the interior , and state , as well as the general services administration ( gsa ) .

in selecting the specific investments at each agency , we identified the largest investments that , according to the fiscal year 2011 budget , were spending at least 25 percent of their budget on it development , modernization , and enhancement work .

to narrow this list , we excluded investments that , according to the fiscal year 2011 budget , were in the planning phase or were infrastructure - related .

we then selected the top 2 investments per agency .

the 10 final investments were commerce's geostationary operational environmental satellite — series r ground segment project and advanced weather interactive processing system , education's integrated partner management system and national student loan data system , interior's financial and business management system and land satellites data system , state's global foreign affairs compensation system and integrated logistics management system , and gsa's regional business application and system for tracking and administering real property / realty services .

to assess the accuracy and currency of the cost and schedule performance ratings on the dashboard , we evaluated , where available , agency or contractor documentation related to cost and schedule performance for 8 of the selected investments to determine their cumulative and current cost and schedule performance and compared our ratings with the performance ratings on the dashboard .

the analyzed investment performance - related documentation included program management reports , internal performance management system performance ratings , earned value management data , investment schedules , system requirements , and operational analyses .

 to determine cumulative cost performance , we weighted our cost performance ratings based on each investment's percentage of development spending ( represented in our analysis of the program management reports and earned value data ) and steady - state spending ( represented in our evaluation of the operational analysis ) , and compared our weighted ratings with the cost performance ratings on the dashboard .

to evaluate earned value data , we determined cumulative cost variance for each month from october 2010 through march 2011 .

to assess the accuracy of the cost data , we electronically tested the data to identify obvious problems with completeness or accuracy , and interviewed agency and program officials about the earned value management systems .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on what we were told by each agency and the information it could provide .

 to determine cumulative schedule performance , we analyzed requirements documentation to determine whether investments were on schedule in implementing planned requirements .

to perform the schedule analysis of the earned value data , we determined the investment's cumulative schedule variance for each month from october 2010 through march 2011 .

 to determine both current cost and schedule performance , we evaluated investment data from the most recent 6 months of performance for each month from october 2010 through march 2011 .

we were not able to assess the cost or schedule performance of 2 selected investments , education's integrated partner management investment and national student loan data system investment .

during the course of our review , we determined that the department did not establish a validated performance baseline for the integrated partner management investment until march 2011 .

therefore , the underlying cost and schedule performance data for the time frame we analyzed were not sufficiently reliable .

we also determined during our review that the department recently rescoped development work on the national student loan data system investment and did not have current , representative performance data available .

further , we interviewed officials from omb and the selected agencies to obtain additional information on agencies' efforts to ensure the accuracy of the data used to rate investment performance on the dashboard .

we used the information provided by agency officials to identify the factors contributing to inaccurate cost and schedule performance ratings on the dashboard .

we conducted this performance audit from february 2011 to november 2011 at the selected agencies' offices in the washington , d.c. , metropolitan area .

our work was done in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

below are descriptions of each of the selected investments that are included in this review .

the advanced weather interactive processing system is used to ingest , analyze , forecast , and disseminate operational weather data .

enhancements currently being implemented to the system are intended to improve the system's infrastructure and position the national weather service to meet future requirements in the years ahead .

the geostationary operational environmental satellite — series r ground segment includes the development of key systems needed for the on - orbit operation of the next generation of geostationary operational environmental satellites , receipt and processing of information , and distribution of satellite data products to users .

the integrated partner management investment is to replace five legacy applications and provide , in one solution , improved eligibility , enrollment , and oversight processes for schools , lenders , federal and state agencies , and other entities that administer financial aid to help students pay for higher education .

the national student loan data system includes continued operations and maintenance of an application that manages the integration of data regarding student aid applicants and recipients .

the investment also includes a development portion that is intended to ensure that reporting and data collection processes are in place to efficiently determine partner eligibility to participate in higher education financial aid programs , and ensure only eligible students receive loans , grants , or work study awards .

the financial and business management system is an enterprisewide system that is intended to replace most of the department's administrative systems , including budget , acquisitions , financial assistance , core finance , personal and real property , and enterprise management information systems .

the land satellites data system investment includes the continued operation of landsat satellites and the it - related costs for the ground system that captures , archives , processes , and distributes data from land - imaging satellites .

the development efforts under way are intended to enable the u.s. geological survey to continue to capture , archive , process , and deliver images of the earth's surface to customers .

the global foreign affairs compensation system is intended to enable the department to replace six obsolete legacy systems with a single system better suited to support the constant change of taxation and benefits requirements in more than 180 countries , and to help the department make accurate and timely payments to its diverse workforce and retired foreign service officers .

the integrated logistics management system is the department's enterprisewide supply chain management system .

it is intended to be the backbone of the department's logistics infrastructure and provide for requisition , procurement , distribution , transportation , receipt , asset management , mail , diplomatic pouch , and tracking of goods and services both domestically and overseas .

the regional business application includes three systems that are intended to provide a means to transition from a semi - automated to an integrated acquisition process , and provide tools to expedite the processing of customer funding documents and vendor invoices .

the system for tracking and administering real property / realty services investment includes continued operations of a transaction processor that supports space management , revenue generation , and budgeting .

the investment also includes development of a new system that is intended to simplify user administration and reporting , and improve overall security .

table 3 provides additional details for each of the selected investments in our review .

in addition to the contact named above , the following staff also made key contributions to this report: carol cha , assistant director ; emily longcore ; lee mccracken ; karl seifert ; and kevin walsh .

