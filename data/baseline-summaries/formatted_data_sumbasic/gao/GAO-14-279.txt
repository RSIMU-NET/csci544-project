increasingly , health care payers — including medicare — are rethinking the way they reimburse providers in an attempt to shift away from paying solely for the volume of care delivered and toward paying them for the value of their care .

one such approach — known as value - based payment ( vbp ) — links a portion of physician compensation to achieving specified levels of performance .

vbp can be used as a means to improving quality and efficiency in the traditional health care delivery environment by encouraging providers to address gaps in patient care and consider the likely costs and benefits of care .

it can also be used with newer care delivery models , such as accountable care organizations ( aco ) and patient - centered medical homes.hold teams of providers responsible for all of a patient's care .

they reward those who coordinate services across providers and make cost - effective referral decisions , among other practices .

under these arrangements , payers while physicians and other providers may intend to furnish consistently high - quality , efficient care , they may not always know how well they do or where practice changes are needed .

therefore , a key element of the vbp approach is for payers to develop performance feedback reports to indicate specific opportunities for improvement .

entails collecting data on measures of quality and cost of care , assessing performance against benchmarks , and communicating results to providers .

for example , periodic feedback reports can make providers aware of the percentage of their patients receiving appropriate screening tests , or those with potentially avoidable emergency department visits .

the expectation is that giving detailed , timely feedback to providers will enhance their ability to take actions that improve performance .

private feedback reports are generally designed to identify differences between providers' current practices and desired performance and may be combined with financial incentives to encourage improvement .

other performance reporting makes provider information available to the public through recognition programs or websites , thus using professional reputation to promote high - quality care .

in this report , performance feedback reports refer to the private reports sent to providers from a payer or other entity .

we conducted briefings for congressional staff on our preliminary findings in september 2013 .

this report contains information we provided during those briefings , updated with additional information , addressing 1. how and when private entities — such as health insurers — report performance data to physicians , and what information they report ; and 2. how the timing and approach cms uses to report performance data to physicians compare to that of private entities .

in addition , appendix i contains information on how private entities and medicare provide performance feedback to hospitals .

to examine how and when private entities report performance data to physicians , and to identify what information they provide , we contacted nine private entities that had experience with vbp programs or that had innovative features in their performance feedback programs .

to make our selection , we asked representatives of america's health insurance plans , blue cross blue shield association , and network for regional healthcare improvement to suggest leading organizations that met those criteria .

we also considered programs profiled in peer - reviewed literature , as well as those operating in varying geographic areas across the country .

we chose six health insurers , and three statewide health care collaboratives — organizations comprising providers , payers , and employers that focus on quality improvement activities — as follows: aetna blue shield of california blue cross and blue shield of florida , inc. highmark blue cross blue shield horizon blue cross blue shield of new jersey maine health management coalition oregon health care quality corporation we interviewed representatives of these private entities regarding the feedback report recipients , data sources used , types of performance measures and benchmarks , frequency of reporting , and efforts to enhance the utility of their performance reports .

we also requested sample physician feedback reports to learn how the data were presented .

in some cases , entities had multiple performance reporting initiatives .

we focused on the reports that were most similar to the type of reporting to physicians that cms provided to medical groups .

our findings regarding private entity performance reporting to physicians are limited to the entities we interviewed and cannot be generalized to other health insurers and health care collaboratives .

in this report , we describe private entities' feedback programs in operation in 2013 , although performance reporting continues to evolve as organizations adopt newer payment and delivery models .

we did not evaluate the effectiveness of the feedback in altering physician practice patterns .

also , we did not gather information on the payment incentives , if any , associated with these entities' reporting initiatives , as that issue was because nationwide interest in vbp has been largely beyond our scope.aimed at physician care , we primarily focused our review on performance reporting to physicians , and as noted , present additional information on our methodology and findings related to hospital feedback reporting in appendix i .

to learn how the timing and approach cms uses to report performance data to physicians compare to that of private entities , we obtained cms documentation similar to that received from the selected entities .

we analyzed information regarding cms report recipients , data sources used , types of performance measures and benchmarks , frequency of reporting , and efforts to enhance the utility of the reports .

we also examined a prototype of the report cms provided to physicians in september 2013 .

we spoke with cms officials about their report preparation process and about components of the feedback program that differ from those of private entities .

we conducted this performance audit from april 2013 to march 2014 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

laws enacted since 2006 have directed cms to collect performance information on providers and eventually reward quality and efficiency of care rather than reimburse for volume of services alone .

the tax relief and health care act of 2006 required the establishment of the physician quality reporting system ( pqrs ) to encourage physicians to successfully report data needed for certain quality measures .

pqrs applies payment adjustments to promote reporting by eligible medicare professionals ( ep ) — including physicians , nurses , physical therapists , and others .

in 2013 , eps could report data to pqrs using claims , electronic health records ( ehr ) or a qualified registry , or opt for cms to calculate quality measures using administrative claims data .

under its group practice reporting option , cms allows eps to report to pqrs as a group , either through a registry or a web - based interface .

the medicare improvements for patients and providers act of 2008 established the physician feedback program , under which cms was required , beginning in 2009 , to distribute confidential feedback reports , known as quality and resource use reports ( qrur ) , to show physicians their performance on quality and cost measures .

the patient protection and affordable care act required hhs to coordinate the physician feedback program with a value modifier ( vm ) that will adjust fee - for - service ( ffs ) physician payments for the relative quality and cost of care provided to beneficiaries .

in implementing the vm , cms's center for medicare intends to use pqrs and cost data from groups of eps defined at the taxpayer identification number level to calculate the vm and then report the payment adjustments in the qrurs .

as required in the act , cms plans to apply the vm first to select physicians in 2015 and to all physicians in 2017 .

as required by law , cms implemented a performance feedback program for medicare physicians , which serves as the basis for eventual payment adjustments .

 ( see fig .

1. ) .

in our december 2012 report on physician payment incentives in the vm program , we found that cms had yet to develop a method of reliably measuring the performance of physicians in small practices , that cms planned to reward high performers and penalize poor performers using absolute performance benchmarks , and that cms intended to annually adjust payments 1 year after the performance measurement period ends .

we recommended that cms develop a strategy to reliably measure the performance of small physician practices , develop benchmarks that reward physicians for improvement as well as for meeting absolute performance benchmarks , and make the vm adjustments more timely , to better reflect recent physician performance .

cms agreed with our recommendations , but noted that it was too early to fully implement these changes .

private entities we reviewed provided feedback mostly to groups of primary care physicians practicing within newer delivery models .

each entity decided which measures to report and which performance benchmarks to use , leading to differences in report content across entities .

largely relying on claims data , health insurers spent from 4 to 6 months to produce the annual reports .

to meet the information needs of physicians , they all provided feedback throughout the year .

the entities also generally offered additional report detail and other resources to help physicians improve their performance .

the private entities in our review had discretion in determining the number and type of physicians to be included in their performance reporting initiatives , and their feedback programs generally included only physician groups participating in newer delivery models — medical homes and acos — with which they contract .

within this set of providers , the entities used various approaches to further narrow the physician groups selected to receive performance feedback .

for example , one entity told us that only physician groups accredited by a national organization focused on quality were eligible for participation in its medical home program , which included physician feedback reports .

private entities' feedback programs were generally directed toward primary care physician practices .

one entity defined primary care as family medicine , internal medicine , geriatrics , and pediatrics ; and included data on the services furnished by nurse practitioners and physician assistants in its medical group reports .

the entities indicated that they rarely provided reports directly to specialty care physician groups .

among those that did , the programs typically focused on practice areas considered significant cost drivers — obstetrics / gynecology , cardiology , and orthopedics .

entities further limited their physician feedback programs to groups participating in medical homes with a sufficient number of attributed enrollees to ensure the reliability of the reported measures .

in medical home models , enrollees are attributed to a physician ( or physicians ) responsible for their care , who is held accountable for the quality and cost of care , regardless of by whom or where the services are provided .

among those entities we spoke with , the minimum enrollment size for feedback reporting varied widely , with most requiring a minimum of between 200 and 1,000 attributed enrollees to participate in the program .

for example , one entity had two levels of reporting in its medical home program , differentiated by the number of attributed enrollees .

in one medical home model , the entity required more than 2,000 attributed enrollees for participation and rewarded the practices through shared savings .

in a second medical home model , the entity included practices with fewer than 1,000 attributed enrollees , but these practices did not share in any savings .

according to the entities in our study , small physician practices ( including solo practitioners ) typically received performance reports for quality improvement purposes only .

because smaller practices may not meet minimum enrollment requirements needed for valid measurement , private entities generally did not link their performance results to payment or use them for other purposes .

for example , one entity provided feedback to practices of one to three primary care physicians upon request , but did not publicly report these practices' data on its website .

to increase the volume of patient data needed for reliable reporting , some entities pooled data from several small groups and solo practitioners and issued aggregate reports for those small practices .

most of the entities that used this method said they applied their discretion in forming these “virtual” provider groups ; however , another entity commented that allowing small practices to voluntarily form such groups for measurement purposes would be advantageous .

because each private entity in our study determined the number and types of measures on which it evaluated physician performance , the measures used in each feedback program differed .

each entity decided on quality measures to include , and many also identified utilization or cost measures for inclusion.acos to choose 8 to 10 measures from among a set of about 18 measures .

to assess physicians' quality and utilization / cost results , the entities used absolute or relative performance benchmarks .

private entities generally report on physician quality using many more process of care measures than outcomes of care measures .

entities in our review commonly included indicators of clinical care in areas such as diabetes care , cardiovascular health , and prevention or screening services for both their adult and pediatric patients .

the most common measure reported by all entities was breast cancer screening , followed by hemoglobin a1c measures , a service used to monitor diabetes .

we found wide variation in the number and type of measures in private entities' quality measure sets .

the total number of quality measures used in the feedback reports ranged from 14 to 51 .

measures typically fell into one of several measurement areas , each with as few as one or as many as 20 individual measures .

for example , in the quality measurement areas for pulmonary and respiratory conditions , one private entity reported on a single measure ( appropriate use of medications for asthma ) , while another reported three measures ( appropriate use of medications for asthma , appropriate testing for pharyngitis , and avoidance of antibiotic treatment for adults with acute bronchitis ) .

although primarily focused on clinical quality measures , entities also included nonclinical measures , such as patient safety and patient satisfaction .

 ( see app .

ii for more information on the number and types of quality measures included in sample reports provided by the entities we reviewed. ) .

even when entities appeared to report on similar types of measures in common areas , we found considerable variability in each measure's definition and specification .

for this reason , results shown in physician feedback reports may not be comparable across entities .

as shown in figure 2 , the diabetes hemoglobin a1c measure was defined and used in different ways in our selected entities' reports .

in some cases , entities calculated the percentage of enrollees with diabetes within a certain age range that received the test .

in other cases , the entities calculated the percentage of enrollees with diabetes within a certain age range that had either good or poor control of the condition , as determined from a specified hemoglobin a1c result .

in addition , some entities defined their diabetic patient population as enrollees from 18 to 75 years of age , while another did not indicate the age range , and one entity set the age range from 18 to 64 years of age .

some , but not all , private entities in our review included utilization or cost measures in their performance reports to physicians .

total cost of care per enrollee was the most commonly used measure , but cost measures disaggregated by type of service — facility , pharmacy , primary care physician , and specialty — were also used .

some entities described how they limited their reporting of a total cost of care measure to those medical groups with a large number of enrollees .

in one case the minimum enrollment size was 20,000 enrollees and in another it was 2,500 enrollees .

officials from one entity also told us that they allowed smaller physician practices to combine their data in order to meet the required number of enrollees for receiving feedback on cost of care .

in addition to feedback on the total cost of care per enrollee , some reports given to groups of primary care physicians contained information on the cost of care provided by specialists in the entity's network .

for example , one entity provided trend data that included the number of specialist visits ( total and by type ) and the number of patients with one or more visits for these specialty areas .

 ( see fig .

3. ) .

for the two specialties with the most enrollee visits during the measurement period — orthopedic surgery and dermatology — the entity also provided the medical group with data on which specialists were seen most frequently and their cost per visit .

this information was intended to encourage cost - efficient referrals .

another entity said it was focused on a program in july 2013 to provide feedback to primary care physicians on cardiologists' performance showing where care was being delivered most efficiently .

by providing such information , the entity expected primary care physicians to take cost differences into account when making referrals , rather than basing referrals solely on historical habits .

disseminating information to primary care physicians about the relative cost of specialty care providers is a key aspect of medical home and aco programs .

the entities were fairly consistent in the number and types of utilization measures they selected for feedback reporting .

the most common utilization measures reported by our private entities were physicians' generic drug prescribing rates , followed by emergency department visits , inpatient visits , hospital readmissions , and specialist visits .

one entity provided additional detail under the emergency department visits measure to show the number of patients that repeatedly seek care at emergency departments .

officials from the entity told us that this measure was included to alert physicians of potentially avoidable hospital visits so that they can encourage patients to use office - based care before seeking care in more costly settings .

 ( see examples of this measure as presented by private entities in their sample reports in fig .

4. ) .

to evaluate physician performance , the selected private entities compared the measures data to different types of benchmarks .

some entities compared each physician group's performance results to that of a peer group ( eg , others in the entity's network or others in the collaborative's state or region ) ; some entities compared physician groups' results to a pre - established target ; and others gauged physician groups' progress relative to their past performance .

 ( see fig .

5. ) .

entities generally used two or three such benchmarks in their feedback reports .

for example , one entity separately displayed results for the medical home's commercially insured , medicare insured , and composite patient population .

within each of these population groups , it compared the practice's performance to the average for nonmedical home practices , as well as to the practice's performance in the prior measurement year .

the entity also gave narrative detail to indicate favorable or unfavorable performance .

the most common benchmark for the entities in our study was a physician group's performance relative to the previous measurement period .

however , some entities used this benchmark only for utilization / cost measures and not for quality measures .

private entity officials told us they relied on claims as their primary data source for performance reporting .

however , several private entities noted shortcomings in relying solely on claims data — the billing codes that describe a patient's diagnoses , procedures , and medications — for performance reporting .

some entities supplemented their claims data by obtaining information from ehrs , patient satisfaction surveys , or chart extractions .

entities noted that using ehr data was resource - intensive for both providers and payers , because they depended on physician groups to submit the information .

the entities we spoke to have had limited success in using ehr data as a primary data source , although many saw it as complementary to claims data .

another entity supplemented its claims data with data from registries that compile information from administrative data sets , patient medical records , and patient surveys , and thus have the capacity to track trends in quality over time .

the health insurers in our review typically spent from 4 to 6 months to produce and distribute annual performance reports ; in contrast , the health care collaboratives spent 9 to 10 months .

 ( see illustrations of these timelines in fig .

6. ) .

as is common in the health insurance industry , payers require a 3-month interval after the performance period ends — referred to as the claims run - out — to allow claims for the services furnished late in the measurement period to be submitted and adjudicated for the report .

the claims run - out was followed by 1 to 3 months to prepare the data , a period that allowed for provider attribution , risk - adjustment , measure calculation , and quality assurance.collaborative stated that the quality assurance process is helpful in increasing physician trust because the group is able to compare its own data with the collaborative's data before results are final .

the statewide health care collaboratives we spoke with required additional time to collect and aggregate data from multiple health insurers , and their final reports were issued at least 9 months after the end of the performance period .

the time needed for some or all of these report production steps varied depending on the entity and the types of measures included .

collaboratives often used all - payer claims databases — centralized data collection where each payer submits claims data on that state's health care providers — for aggregate reporting to providers .

officials from entities told us that all - payer claims databases are helpful because they provide physicians with a better picture of their entire patient panel , not just results determined by individual payers for limited sets of patients .

one entity noted that it aggregates its quality data with other payers in its commercial market through a statewide organization , and no one payer can provide statistically meaningful data to a physician group on its own .

officials from one entity with all - payer claims database experience told us that the addition of medicare data into these databases would improve the information available for measurement and feedback .

in addition , one entity suggested that a multipayer database could help with feedback to physicians in groups of all sizes , including small practices , because the higher number of patients would generate sufficient data for calculating reliable measures .

however , one entity acknowledged that using all - payer databases requires more time for merging data from different payers in different formats , and another entity noted the challenges of customizing reports for each medical groups' patient population .

private entities told us that physicians valued frequent feedback on their performance so that they have time to make practice changes that may result in better performance by the end of the measurement period .

in response , these entities typically provided feedback reports on an interim basis throughout the measurement period .

interim reports typically covered a 1-year performance period , and were commonly issued on a rolling monthly , quarterly , or semiannual schedule .

entities also noted that frequent reporting throughout the period updated physicians on their performance so that year - end results were better expected and understood .

some entities in our study elected to issue interim reports that build up to the 12-month performance period by continually adding data from month to month .

those that used preliminary data that may not account for all final claims in building reports told us that such data starts to become useful about 3 to 6 months into the performance year .

they also stated that , although the interim reports may be limited by the use of rolling or incomplete data , providers generally seek this information for early identification of gaps in care .

private entities generally offered additional report detail intended to enhance physicians' understanding of the information contained in their reports or in response to physician requests for more data .

private entity officials told us that , because physicians prefer dynamic reports with as much detail as possible , they generally sent reports that can be expanded to show individual physician or patient - level data .

some entities formatted their reports to include summary - level information on quality and cost measures in labeled sections , with supplemental information following the summary data .

other entities provided additional reports or supplemental data through a web portal that allowed providers to see individual physician or patient - level detail .

private entities sent reports in multiple file formats , such as in a spreadsheet , some of which allowed report recipients to sort their data .

entities in our study also offered resources designed to assist physician groups with actionable steps they can take to improve in the next performance period .

most entities told us they offered resources to physician groups , such as consultations with quality improvement professionals , forums for information - sharing , and documents on best practices .

for example , one entity's staff worked directly with practices to improve their results by distributing improvement guidelines for each performance measure included in the feedback report .

in addition , the entity's officials told us they also convened workgroups to review trend information and paid particular attention to differences between medical homes and nonmedical homes .

cms has provided feedback to increasing numbers of physician practices each year in order to eventually reach all physicians .

each medical group's chosen method of quality data submission determined the quality measures included in its report , to which cms added health care costs and certain outcomes measures .

cms's report generation process took slightly longer than that of most private entities in our study , and the agency did not provide interim performance data during the measurement period .

cms feedback reports have included information to assist providers in interpreting their performance results .

unlike the private entities we contacted , which selected a limited set of physicians to receive feedback reports , cms is mandated to apply the vm to all physicians by 2017 .

therefore , the agency faces certain challenges not faced by private entities as it has expanded its feedback program to reach increasing numbers of physicians .

in preparation for implementation of the vm , cms provided performance reports to nearly 4,000 medical groups in september 2013 .

in 2014 , cms plans to disseminate reports to physicians in practices of all sizes .

as of september 2013 , cms had not yet determined how to report to smaller groups and physicians in solo practices .

according to cms , the decision not to present vm information to smaller groups stemmed from concerns regarding untested cost metrics and administrative complexity .

cms agreed with a 2012 gao recommendation to develop a strategy to reliably measure the performance of solo and small physician practices , but has not yet finalized such a strategy .

under the cms approach to performance reporting , the content of feedback reports related to quality measures may vary across providers .

unlike our selected private entities , the agency has allowed physician groups to select the method by which they will submit quality - of - care data , which , in turn , determines the measures on which they receive feedback .

cms used claims data for a consistent set of measures in all of its feedback reports for performance on cost and outcomes .

for the cms 2013 reports , medical groups submitted data on quality measures to cms via a web interface or through a qualified registry ; if a group did not select either of these options , the agency calculated quality measures based on claims data .

both cms and private entities focused on preventive care and management of specific diseases .

web interface .

quality measures under this method pertain to care coordination , disease management , and preventive services .

cms required groups reporting via the web interface to submit data on 17 quality measures — such as hemoglobin a1c levels for control of diabetes — for a patient sample of at least 218 beneficiaries .

registries .

some groups submitted data for quality measures via qualified registries — independent organizations , typically serving a particular medical specialty , that collect and report these data to cms .

cms required groups reporting to a qualified registry to submit at least three measures — such as whether cardiac rehabilitation patients were referred to a prevention program — for at least 80 percent of patients .

administrative claims .

as a default , if a group did not report via web interface or qualified registry , cms calculated quality measures using claims data .

in september 2013 , the majority of groups with 25 or more eps — nearly 90 percent — received quality scores based on claims data .

cms calculated performance on a set of 17 quality indicators , including several composite measures .

for example , the diabetes composite measure included several different measures of diabetes control .

regardless of the method a group selected to submit quality - of - care data , cms used claims to calculate three outcomes measures — two ambulatory care composite measures and hospital readmission .

one ambulatory care composite included hospitalization rates for three acute conditions: bacterial pneumonia , urinary tract infections , and dehydration .

another composite included hospitalization rates for three chronic conditions: diabetes , chronic obstructive pulmonary disease ( copd ) , and heart failure .

cms included cost measures — several of which differed from the measures private entities in our study reported to physicians — in all 2013 feedback reports ( see fig .

7 ) .

using claims data , cms calculated an overall measure of the cost of care as the total per capita costs for all beneficiaries attributed to each physician group.separately reported total per capita costs for attributed beneficiaries with any of four chronic conditions: diabetes , heart failure , copd , or coronary artery disease .

this contrasts with the private entities that typically measured a more limited set of measures focused on physicians' generic drug prescribing rates and hospital utilization .

cms's report generation process took longer than that of most private entities in our study because it required more steps .

while most health insurers generated performance reports in 4 to 6 months , cms issued reports about 9 months after the end of the january to december 2012 reporting period .

to produce its 2013 physician feedback reports using administrative claims , cms began with the standard claims run - out period followed by intervals for provider attribution , measure calculation , risk - adjustment , and quality assurance .

 ( see fig .

10. ) .

cms officials said they allowed a 3-month run - out interval to account for providers' late - year claims submissions .

after the run - out period , cms required 5 to 6 months for a series of additional tasks needed to prepare the data for reporting .

for groups that submitted data to cms via the web interface or registry options , cms gave these groups 3 months to submit such data after the end of the 12-month performance period .

cms then calculated the measures for these options over a period of the next several months .

although ffs beneficiaries see multiple physicians , cms attributed each beneficiary to a single medical group through its yearly attribution process .

it used the claims for the 12-month reporting period to determine which groups provided the beneficiary the most primary care and then assigned responsibility for performance on quality and cost measures to that group .

following attribution , the agency risk - adjusted the cost measures to account for differences in beneficiary characteristics and complexity , and standardized the cost measures by removing all geographic payment adjustments .

finally , cms officials said they performed data checks to ensure accuracy before the reports were disseminated .

according to health insurers and collaboratives , physicians find that frequent feedback enables them to improve their performance more quickly ; however , cms did not provide physicians interim performance feedback .

however , with only annual feedback from cms , physicians may be missing an opportunity to improve their performance on a more frequent basis .

asked if more frequent reporting was considered , cms officials cited concerns about the time it would take to generate each set of reports .

with each round , the agency would need to attribute all beneficiaries to a medical group , risk - adjust and standardize the cost measures , and compute the benchmarks for each measure .

in addition , providing interim reports on quality data would require certain providers to report more frequently .

for example , providers who submit via registry would need to finalize their data more often than annually .

however , experts and cms officials have stated that , with continued adoption of advanced data reporting technology , cms may be able to generate reports more frequently .

cms provided general information on its website and through the medicare learning network , to assist providers in understanding the performance feedback and vm .

unlike private entities , cms has not provided tailored guidance or action steps to help providers improve their scores .

however , cms resources included steps to access reports , a review of methodology , suggested ways to use the data in reports , and contact information for technical support .

a representative acting on behalf of a medical group could access the group's qrur .

in addition , cms's web - based reports allowed providers to access further detail on the medicare beneficiaries attributed to the group .

for example , physicians could view their patients' percentage of total cost by type of service and hospital admission data .

cms included explanatory information within the reports for providers .

in addition to comparative performance data , reports made available in september 2013 included a description of the attribution methods , the number of providers billing in each medical group , information about each attributed patient's hospitalizations during the year , and other details about the group's performance .

in addition , cms included within the qrur a glossary of terms used in the feedback report .

payers have been refining their performance reports for physicians , a key component of their vbp initiatives .

private entities have selectively rolled out their feedback programs , generally applying them to relatively large groups of primary care physicians participating in medical homes and acos .

although they are not uniform in their approaches , the entities in our study used their discretion to select a limited number of quality and utilization / cost measures , calculated them using claims data , and used them to assess performance against a variety of benchmarks .

in response to physicians' needs , their feedback reports tended to be frequent , timely , and dynamic .

cms's approach to performance reporting faces some unique challenges .

first , it is driven by the statutory requirement that , by 2017 , medicare pay ffs physicians in groups of all sizes , including specialists , using a vm .

second , the agency has had to develop the feedback program in the context of pre - existing incentive programs , such as pqrs .

cms finalized several key changes to the feedback program for future reporting periods , as it expands the application of the vm to all physicians .

specifically , cms continues to modify program components such as measures and reporting mechanisms as it works to align the reporting and feedback aspects of multiple programs .

despite these program modifications , we found that certain features of private entities' feedback programs , which are lacking in cms's program , could enhance the usefulness of the reports in improving the value of physician care .

cms's use of a single nationwide benchmark to compare performance on quality and cost ignores richer benchmarking feedback that could benefit physicians .

private entities in our study measured provider performance against several benchmarks .

cms's reliance on a national average as the sole benchmark precludes providers from gauging their performance relative to their peers in the same geographic area .

without such contextual information , providers lack the feedback to better manage their performance and target improvement efforts .

additionally , cms disseminates feedback reports only once a year ( for example , september 2013 ) .

this gives physicians little time ( october through december ) to analyze the information and make changes in their practices to score better in the next measurement period .

the private entities we reviewed sent reports more than once a year , and reported that greater frequency of reporting enabled more frequent improvements .

without interim performance reports , providers may not be able to make needed changes to their performance in advance of their annual vm payment modifications .

our findings also support past gao recommendations that cms reward physicians for improvement as well as performance against absolute benchmarks , and develop a strategy to reliably measure solo and small practices , such as by aggregating data .

as cms implements and refines its physician feedback and vm programs , the administrator of cms should consider taking the following two actions to help ensure physicians can best use the feedback to improve their performance: develop performance benchmarks that compare physicians' performance against additional benchmarks such as state or regional averages ; and disseminate performance reports more frequently than the current annual distribution — for example , semiannually .

we provided a draft of this report to hhs for comment .

in its written response , reproduced in appendix iii , the department generally agreed with our recommendations , and reiterated our observation that the agency faces unique challenges with its mandate to report to medicare ffs providers in groups of all sizes that encompass all specialty care areas .

hhs conditionally agreed with our recommendation that reporting physician performance using multiple benchmarks would be beneficial , but asked for further information on private entities' practices and their potential use for medicare providers .

as we stated in the report , private entities generally use two or three different types of benchmarks to provide a variety of performance assessments .

we found alternative benchmarks that could enhance medicare feedback reporting by allowing physicians to track their performance in their own historical and geographic context .

for example , some entities' reports included physician group performance on certain measures relative to their past performance , a recommendation we previously made to hhs in december 2012 .

although it agreed to consider developing benchmarks for performance improvement , hhs has yet to do so .

a comparison to past performance allows a medical group to see how much , if at all , it has improved regardless of where it stands relative to its peers .

in this way , cms can motivate physicians to continuously improve their performance .

in addition , some entities in our review compared physician performance data to statewide or regional - level benchmarks .

because of the number of medicare physicians , cms has extensive performance data , which could enable more robust localized peer benchmarks than any individual health plan could generate .

as we noted , such benchmarks reflect more local patterns of care that may be more relevant to physicians than comparisons to national averages alone .

hhs further asserted that , because the physician feedback program's key purpose is to support the national vm program , it is appropriate to limit reporting to a single national benchmark .

hhs expressed concern that displaying other benchmarks could be misleading and confusing for the purposes of the vm .

however , cms's reports provide a group's vm payment adjustment in a concise , one - page summary , as shown in figure 9 .

we do not believe that additional benchmark data , displayed separately , would detract from the information provided on the summary page , and could enhance the value of the reports for physicians .

hhs agreed with our second recommendation to disseminate feedback reports more frequently than on an annual basis .

as seen in the private entity practices of using rolling or preliminary data for interim reporting , disseminating reports more frequently can assist physicians in making improvements to their performance before cms determines their vm payment adjustment .

hhs commented that producing more frequent reports would first require modifying the pqrs data collection schedules .

for example , groups of eps that use the web interface and registry options currently are only required to submit data to cms once a year .

the registry option will eventually require groups to submit data to cms on a quarterly or semiannual basis , and hhs noted that these requirements would have to be synchronized with the timing of data submission through the web interface and ehr options .

the agency also provided technical comments , which we incorporated as appropriate .

we are sending copies of this report to the appropriate congressional committees and the administrator of cms .

the report also is available at no charge on gao's website at http: / / www.gao.gov .

if you or your staffs have any questions regarding this report , please contact me at ( 202 ) 512- 7114 or cosgrovej@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

this appendix contains information on the similarities and differences between private entities' and medicare's performance reporting to hospitals .

the private entities in our study provided feedback through a variety of value - based payment ( vbp ) initiatives and several entities have made accountable care organizations the focus of their feedback programs .

payers' efforts to provide feedback to hospitals on their performance are centered on rewarding higher - quality and lower - cost providers of care .

we followed the same methodology for comparing how private entities and the centers for medicare & medicaid services ( cms ) conduct performance feedback reporting for hospitals as we did for examining physician - focused feedback programs .

we interviewed representatives of the nine selected private entities about their feedback reporting to hospitals , if any , with regard to report recipients , data sources used , types of performance measures and benchmarks , frequency of reporting , and efforts to enhance the utility of performance reports .

one statewide health care collaborative in our review was established through a partnership between the state medical society and hospital association , and only provides feedback reports to hospitals .

we similarly requested sample feedback reports for hospitals .

we interviewed cms officials and obtained cms documentation on its hospital feedback reporting activities , and compared these to private entity efforts .

we also reviewed a sample cms hospital feedback report from july 2013 .

cms's hospital vbp efforts over the past decade have evolved to provide performance feedback to a range of hospital types , with a focus on acute care hospitals .

in 2003 the agency began with a quality incentive demonstration program designed to see whether financial incentives to hospitals were effective at improving the quality of inpatient care , and to publicly report that information .

since then , a number of laws have required cms to conduct both feedback reporting and vbp programs for hospitals .

these included the following: the medicare prescription drug , improvement , and modernization act of 2003 , which required the establishment of the hospital inpatient quality reporting program , a pay - for - reporting initiative.required cms to make downward payment adjustments to hospitals the act also that did not successfully report certain quality measures .

that downward payment adjustment percentage was increased by the deficit reduction act of 2005 .

the patient protection and affordable care act established medicare's hospital vbp program for inpatient care provided in acute care hospitals .

under this program , cms withholds a percentage of all eligible hospitals' payments and distributes those funds to high - performing hospitals .

in reviewing current feedback reporting practices , we found that private entities and cms report to hospitals on similar performance measures and that entities' feedback generally contains publicly available data .

table 1 compares features of the hospital feedback produced by those private entities in our study that report to hospitals through a vbp initiative and cms's hospital vbp program .

table 2 summarizes the number of quality measures included in sample physician feedback reports we received from private entities in our study .

these entities used their discretion to determine which measures to include in their reports .

we analyzed the measures focused on quality of care and categorized them into common areas .

in addition to the contact named above , individuals making key contributions to this report include rosamond katz , assistant director ; sandra george ; katherine perry ; and e. jane whipple .

electronic health record programs: participation has increased , but action needed to achieve goals , including improved quality of care .

gao - 14-207 .

washington , d.c.: march 6 , 2014 .

clinical data registries: hhs could improve medicare quality and efficiency through key requirements and oversight .

gao - 14-75 .

washington , d.c.: december 16 , 2013 .

medicare physician payment: private - sector initiatives can help inform cms quality and efficiency incentive efforts .

gao - 13-160 .

washington , d.c.: december 26 , 2012 .

medicare program integrity: greater prepayment control efforts could increase savings and better ensure proper payment .

gao - 13-102 .

washington , d.c.: november 13 , 2012 .

medicare physician feedback program: cms faces challenges with methodology and distribution of physician reports .

gao - 11-720 .

washington , d.c.: august 12 , 2011 .

value in health care: key information for policymakers to assess efforts to improve quality while reducing costs .

gao - 11-445 .

washington , d.c.: july 26 , 2011 .

medicare: per capita method can be used to profile physicians and provide feedback on resource use .

gao - 09-802 .

washington , d.c.: september 25 , 2009 .

medicare: focus on physician practice patterns can lead to greater program efficiency .

gao - 07-307 .

washington , d.c.: april 30 , 2007 .

