billions of taxpayer dollars are spent on information technology ( it ) investments each year ; federal it spending has now risen to an estimated $79 billion for fiscal year 2011 .

during the past several years , we have issued multiple reports and testimonies and made numerous recommendations to the office of management and budget ( omb ) to improve the transparency , oversight , and management of the federal government's it investments .

as part of its response to our prior work , omb deployed a public web site in june 2009 , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 800 major federal it investments .

the dashboard aims to improve the transparency and oversight of these investments .

in july 2010 , we completed our first review of the dashboard and reported that the cost and schedule ratings on omb's dashboard were not always accurate because of limitations with omb's calculations .

we recommended that omb report to congress on the effect of its planned dashboard calculation changes on the accuracy of performance information and provide guidance to agencies that standardizes milestone reporting .

this is the second report in our series of dashboard reviews and responds to your request that we ( 1 ) determine what efforts omb has under way to improve the dashboard and the ways in which it is using data from the dashboard to improve it management and ( 2 ) examine the accuracy of the cost and schedule performance ratings on the dashboard for selected investments .

to address our first objective , we interviewed omb officials and analyzed supporting omb guidance and documentation to determine the efforts omb has under way to improve the dashboard and the ways in which omb is using the data to improve it management .

to address our second objective , we selected five agencies — the departments of homeland security ( dhs ) , transportation ( dot ) , the treasury ( treasury ) , and veterans affairs ( va ) , as well as the social security administration ( ssa ) — and 10 investments to review .

the five agencies account for 22 percent of the planned it spending for fiscal year 2011 .

the 10 investments selected for case study represent about $1.27 billion in total planned spending in fiscal year 2011 .

we analyzed monthly cost and schedule performance reports and program management documents for the 10 investments to assess program performance against planned cost and schedule targets .

we then compared our analyses of investment performance against the corresponding ratings on the dashboard to determine if the ratings were accurate .

additionally , we interviewed officials from omb and the agencies to obtain further information on their efforts to ensure the accuracy of the data used to rate investment performance on the dashboard .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on the documentation the agencies provided .

we conducted this performance audit from july 2010 to march 2011 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are provided in appendix i .

each year , omb and federal agencies work together to determine how much the government plans to spend on it investments and how these funds are to be allocated .

according to the president's budget for fiscal year 2011 , the total planned spending on it in fiscal year 2011 is an estimated $79.4 billion , a 1.2 percent increase from the fiscal year 2010 budget level of $78.4 billion .

omb plays a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cio ) to advise and assist agency heads in carrying out this responsibility .

another key law is the e - government act of 2002 , which requires omb to report annually to congress on the status of e - government .

in these reports , referred to as implementation of the e - government act reports , omb is to describe the administration's use of e - government principles to improve government performance and the delivery of information and services to the public .

to help carry out its oversight role , in 2003 , omb established the management watch list , which included mission - critical projects that needed to improve performance measures , project management , it security , or overall justification for inclusion in the federal budget .

further , in august 2005 , omb established a high - risk list , which consisted of projects identified by federal agencies , with the assistance of omb , as requiring special attention from oversight authorities and the highest levels of agency management .

over the past several years , we have reported and testified on omb's initiatives to highlight troubled it projects , justify investments , and use project management tools .

we have made multiple recommendations to omb and federal agencies to improve these initiatives to further enhance the oversight and transparency of federal projects .

among other things , we recommended that omb develop a central list of projects and their deficiencies and analyze that list to develop governmentwide and agency assessments of the progress and risks of the investments , identifying opportunities for continued improvement .

in addition , in 2006 we also recommended that omb develop a single aggregate list of high - risk projects and their deficiencies and use that list to report to congress on progress made in correcting high - risk problems .

as a result , omb started publicly releasing aggregate data on its management watch list and disclosing the projects' deficiencies .

furthermore , omb issued governmentwide and agency assessments of the projects on the management watch list and identified risks and opportunities for improvement , including risk management and security .

more recently , to further improve the transparency and oversight of agencies' it investments , and to address data quality issues , in june 2009 , omb publicly deployed a web site , known as the it dashboard , which replaced the management watch list and high - risk list .

it displays federal agencies' cost , schedule , and performance data for the approximately 800 major federal it investments at 27 federal agencies .

according to omb , these data are intended to provide a near real - time perspective on the performance of these investments , as well as a historical perspective .

further , the public display of these data is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold the government agencies accountable for results and progress .

the dashboard was initially deployed in june 2009 based on each agency's exhibit 53 and exhibit 300 submissions .

after the initial population of data , agency cios have been responsible for updating cost , schedule , and performance fields on a monthly basis , which is a major improvement from the quarterly reporting cycle omb previously used for the management watch list and high - risk list .

for each major investment , the dashboard provides performance ratings on cost and schedule , a cio evaluation , and an overall rating , which is based on the cost , schedule , and cio ratings .

as of july 2010 , the cost rating was determined by a formula that calculates the amount by which an investment's total actual costs deviate from the total planned costs .

similarly , the schedule rating is the variance between the investment's planned and actual progress to date .

figure 1 displays the rating scale and associated categories for cost and schedule variations .

each major investment on the dashboard also includes a rating determined by the agency cio , which is based on his or her evaluation of the performance of each investment .

the rating is expected to take into consideration the following criteria: risk management , requirements management , contractor oversight , historical performance , and human capital .

this rating is to be updated when new information becomes available that would affect the assessment of a given investment .

last , the dashboard calculates an overall rating for each major investment .

this overall rating is an average of the cost , schedule , and cio ratings , with each representing one - third of the overall rating .

however , when the cio's rating is lower than both the cost and schedule ratings , the cio's rating will be the overall rating .

figure 2 shows the overall performance ratings of the 805 major investments on the dashboard as of march 2011 .

to better manage it investments , omb issued guidance directing agencies to develop comprehensive policies to ensure that their major it investments and high - risk development projects use earned value management to manage their investments .

earned value management is a technique that integrates the technical , cost , and schedule parameters of a development contract and measures progress against them .

during the planning phase , a performance measurement baseline is developed by assigning and scheduling budget resources for defined work .

as work is performed and measured against the baseline , the corresponding bud value is “earned.” using this earned value metric , cost and schedule variances , as well as cost a nd time to complete estimates , can be determined and analyzed .

without knowing the planned cost of completed work and work in progress ( i.e. , the earned value ) , it is difficult to determine a program's true status .

earned value allows for this key information , which provides an objective view of program status and is necessary for understandin health of a program .

as a result , earned value management can alert program managers to potential problems sooner than using expenditures alone , thereby reducing the chance and magnitude of cost overruns and schedule slippages .

moreover , earned value management directly supp the institutionalization of key processes for acquiring and developing systems and the ability to effectively manage investments — areas that are often found to be inadequate on the basis of our assessments of major it investments .

in july 2010 , we reported that the cost and schedule ratings on omb's dashboard were not always accurate for selected agencies .

specifically , we found that several selected investments had notable discrepancies in their cost or schedule ratings , the cost and schedule ratings did not takeinto consideration current performance , and the number of milestones ( activities ) reported by agencies varied widely .

we made a number of recommendations to omb to better ensure that the dashboard prov meaningful ratings and accurate investment data .

in particular , we recommended that omb report on its planned dashboard changes to improve the accuracy of performance information and provide guidance agencies that standardizes activity reporting .

omb agreed with the two recommendations and reported it had initiated work to address them .

since our last report , omb has initiated multiple efforts to increase the dashboard's value as a management and oversight tool , and has used data in the dashboard to improve the management of federal it investments .

specifically , omb is focusing its efforts in four main areas: streamlining key omb investment reporting tools , eliminating manual monthly submissions , coordinating with agencies to improve data , and improving the user interface .

omb's plan to reform federal it management commits omb to streamlining two of the dashboard's sources of information — specifically , the omb exhibits 53 and 300 .

omb has committed , by may 2011 , to reconstruct the exhibits around distinct data elements that drive value for agencies and provide the information necessary for meaningful oversight .

omb anticipates that these changes will also alleviate the reporting burden and increase data accuracy , and that the revised exhibits will serve as its authoritative management tools .

according to omb officials , the dashboard no longer accepts manual data submissions .

instead , the dashboard allows only system - to - system submissions .

officials explained that this update allows the dashboard to reject incomplete submissions and those that do not meet the dashboard's data validation rules .

by eliminating direct manual submissions , this effort is expected to improve the reliability of the data shown on the dashboard .

further , omb officials stated that they work to improve the dashboard through routine interactions with agencies and it portfolio management tool vendors , training courses , working groups , and data quality letters to agencies .

specifically , omb officials stated that they held 58 techstat reviews ( discussed later in this report ) , hosted four online training sessions ( recordings of which omb officials stated are also available online ) , collaborated with several dashboard working groups , and sent letters to agency cios identifying specific data quality issues on the dashboard that their agencies could improve .

further , omb officials explained that in december 2010 , omb analysts informed agency cios about specific data quality issues and provided analyses of agency data , a comparison of agency dashboard performance with that of the rest of the government , and expected remedial actions .

omb anticipates these efforts will increase the dashboard's data reliability by ensuring that the agencies are aware of and are working to address issues .

finally , omb continues to improve the dashboard's user interface .

for instance , in november 2010 , omb updated the dashboard to provide new views of historical data and rating changes and provide new functionality allowing agencies to make corrections to activities and performance metrics ( conforming to rebaselining guidance ) .

officials also described a planned future update , which is intended to contain updated budget data , display corrections and changes made to activities , and reflect increased validation of agency - submitted data .

omb anticipates these efforts will increase the transparency and reliability of investment information on the dashboard by providing agencies and users additional ways to view investment information and by improving validation of submitted data .

additionally , omb uses the dashboard to improve the management of it investments .

specifically , omb analysts are using the dashboard's investment trend data to track changes and identify issues with investments' performance in a timely manner .

omb analysts also use the dashboard to identify data quality issues and drive improvements to the data .

the federal cio stated that the dashboard has greatly improved oversight capabilities compared with those of previously used mechanisms , such as the annual capital asset plan and business case ( exhibit 300 ) process .

additionally , according to omb officials , the dashboard is one of the key sources of information that omb analysts use to identify it investments that are experiencing performance problems and to select them for a techstat session — a review of selected it investments between omb and agency leadership that is led by the federal cio .

as of december 2010 , omb officials stated that 58 techstat sessions have been held with federal agencies .

according to omb , these sessions have enabled the government to improve or terminate it investments that are experiencing performance problems .

information from the techstat sessions and the dashboard was used by omb to identify , halt , and review all federal financial it systems modernization projects .

furthermore , according to omb , these sessions and other omb management reviews have resulted in a $3 billion reduction in life - cycle costs , as of december 2010 .

omb officials stated that , as of december 2010 , 11 investments have been reduced in scope and 4 have been terminated as a result of these sessions .

for example , the techstat on the department of housing and urban development's transformation initiative investment found that the department lacked the skills and resources necessary and would not be positioned to succeed .

as a result , the department agreed to reduce the number of projects from 29 to 7 and to limit fiscal year 2010 funds for these 7 priority projects to $85.7 million ( from the original $138 million ) .

the techstat on the national archives and records administration's electronic records archives investment resulted in six corrective actions , including halting fiscal year 2012 development funding pending the completion of a strategic plan .

according to omb officials , omb and agency cios also used the dashboard data and techstat sessions , in addition to other forms of research ( such as reviewing program documentation , news articles , and inspector general reports ) , to identify 26 high - risk it projects and , in turn , coordinate with agencies to develop corrective actions for these projects at techstat sessions .

for example , the department of the interior is to establish incremental deliverables for its incident management analysis and reporting system , which will accelerate delivery of services that will help 6,000 law enforcement officers protect the nation's natural resources and cultural monuments .

while the efforts previously described are important steps to improving the quality of the information on the dashboard , cost and schedule performance data inaccuracies remain .

the dashboard's cost and schedule ratings were not always reflective of the true performance for selected investments from the five agencies in our review .

more specifically , while the dashboard is intended to present near real - time performance , the ratings did not always reflect the current performance of these investments .

dashboard rating inaccuracies were the result of weaknesses in agency practices , such as the dashboard not reflecting baseline changes and the reporting of erroneous data , as well as limitations of the dashboard's calculations .

until the agencies submit complete , reliable , and timely data to the dashboard and omb revises its dashboard calculations , performance ratings will continue to be inaccurate and may not reflect current program performance .

most of the dashboard's cost ratings of the nine selected investments did not match the results of our analyses over a 3-month period .

specifically , four investments had inaccurate ratings for 2 or more months , and two were inaccurate for 1 month , while three investments were accurately depicted for all 3 months .

for example , intelligent disability's cost performance was rated “red” on the dashboard for july 2010 and “green” for august 2010 , whereas our analysis showed its current cost performance was “yellow” for those months .

further , medical legacy's cost ratings were “red” on the dashboard for june through august 2010 , while the department's internal rating showed that the cost performance for 105 of the 107 projects that constitute the investment was “green” in august 2010 ; similar ratings were also seen for june and july 2010 .

overall , the dashboard's cost ratings generally showed poorer performance than our assessments .

figure 3 shows the comparison of the selected investments' dashboard cost ratings with gao's ratings based on analysis of agency data for the months of june 2010 through august 2010 .

regarding schedule , most of the dashboard's ratings of the nine selected investments did not match the results of our analyses over a 3-month period .

specifically , seven investments had inaccurate ratings for 2 or more months , and two were inaccurate for 1 month .

for example , automatic dependent surveillance - broadcast's schedule performance was rated “green” on the dashboard in july 2010 , but our analysis showed its current performance was “yellow” that month .

additionally , the “green” schedule ratings for en route automation modernization did not represent how this program is actually performing .

specifically , we recently reported that the program is experiencing significant schedule delays , and the cio evaluation of the program on the dashboard has indicated schedule delays since february 2010 .

as with the cost ratings , the dashboard's schedule ratings generally showed poorer performance than our assessments .

figure 4 shows the comparison of the selected investments' dashboard schedule ratings with gao's ratings based on analysis of agency data for the months of june 2010 through august 2010 .

omb guidance , as of june 2010 , states that agencies are responsible for maintaining consistency between the data in their internal systems and the data on the dashboard .

furthermore , the guidance states that agency cios should update their evaluation on the dashboard as soon as new information becomes available that affects the assessment of a given investment .

according to our assessment of the nine selected investments , agencies did not always follow this guidance .

in particular , there were four primary weaknesses in agency practices that resulted in inaccurate cost and schedule ratings on the dashboard: the investment baseline on the dashboard was not reflective of the investment's actual baseline , agencies did not report data to the dashboard , agencies reported erroneous data , and unreliable earned value data were reported to the dashboard .

in addition , two limitations of omb's dashboard calculations contributed to ratings inaccuracies: a lack of emphasis on current performance and an understatement of schedule variance .

table 1 shows the causes of inaccurate ratings for the selected investments .

inconsistent program baseline: three of the selected investments reported baselines on the dashboard that did not match the actual baselines tracked by the agencies .

agency officials responsible for each of these investments acknowledged this issue .

for example , according to modernized e - file officials , the investment was in the process of a rebaseline in june 2010 ; thus , officials were unable to update the baseline on the dashboard until july 2010 .

for another investment — healthevet core — officials stated that it was stopped in august , and thus the healthevet core baseline on the dashboard is incorrect .

as such , the cio investment evaluation should have been updated to reflect that the investment was stopped .

in june 2010 , omb issued new guidance on rebaselining , which stated that agencies should update investment baselines on the dashboard within 30 days of internal approval of a baseline change and that this update will be considered notification to omb .

however , agencies still must go through their internal processes to approve a new baseline , and during this process the baseline on the dashboard will be inaccurate .

as such , investment cio ratings should disclose that performance data on the dashboard are unreliable because of baseline changes .

however , the cio evaluation ratings for these investments did not include such information .

without proper disclosure of pending baseline changes and resulting data reliability weaknesses , omb and other external oversight groups will not have the appropriate information to make informed decisions about these investments .

missing data submissions: three investments did not upload complete and timely data submissions to the dashboard .

for example , dhs officials did not submit data to the dashboard for the c4isr investment from june through august 2010 .

according to dhs officials , c4isr investment officials did not provide data for dhs to upload for these months .

further compounding the performance rating issues of this investment is that in march 2010 , inaccurate data were submitted for nine of its activities ; these data were not corrected until september 2010 .

until officials submit complete , accurate , and timely data to the dashboard , performance ratings may continue to be inaccurate .

erroneous data submissions: seven investments reported erroneous data to the dashboard .

for example , ssa submitted start dates for intelligent disability and disability case processing system activities that had not actually started yet .

ssa officials stated that , because of ssa's internal processes , their start dates always correspond to the beginning of the fiscal year .

in addition , according to a treasury official , internal revenue service officials for the modernized e - file investment provided inaccurate data for the investment's “actual percent complete” fields for some activities .

until officials submit accurate data to the dashboard , performance ratings may continue to be inaccurate .

unreliable source data: treasury's payment application modernization investment used unreliable earned value data as the sole source of data on the dashboard .

as such , this raises questions about the accuracy of the performance ratings reported on the dashboard .

investment officials stated that they have taken steps to address weaknesses with the earned value management system and are currently evaluating other adjustments to investment management processes .

however , without proper disclosure about data reliability in the cio assessment , omb and other external oversight groups will not have the appropriate information to make informed decisions about this investment .

additionally , two limitations in the dashboard method for calculating ratings contributed to inaccuracies: current performance calculation: the dashboard is intended to represent near real - time performance information on all major it investments , as previously discussed .

to omb's credit , in july 2010 , it updated the dashboard's cost and schedule calculations to include both ongoing and completed activities in order to accomplish this .

however , the performance of ongoing activities is combined with the performance of completed activities , which can mask recent performance .

as such , the cost and schedule performance ratings on the dashboard may not always reflect current performance .

until omb updates the dashboard's cost and schedule calculations to focus on current performance , the performance ratings may not reflect performance problems that the investments are presently facing , and omb and agencies are thus missing an opportunity to identify solutions to such problems .

schedule variance calculation: another contributing factor to certain schedule inaccuracies is that omb's schedule calculation for in - progress activities understates the schedule variance for activities that are overdue .

specifically , omb's schedule calculation does not recognize the full variance of an overdue activity until it has actually completed .

for example , as of september 13 , 2010 , the dashboard reported a 21-day schedule variance for an en route automation modernization activity that was actually 256 days overdue .

until omb updates its in - progress schedule calculation to be more reflective of the actual schedule variance of ongoing activities , schedule ratings for these activities may be understated .

the dashboard has enhanced omb's and agency cios' oversight of federal it investments .

among other things , performance data from the dashboard are being used to identify poorly performing investments for executive leadership review sessions .

since the establishment of the dashboard , omb has worked to continuously refine it , with multiple planned improvement efforts under way for improving the data quality and dashboard usability .

however , the quality of the agency data reported to the dashboard continues to be a challenge .

specifically , the cost and schedule ratings on the dashboard were not always accurate in depicting current program performance for most of the selected investments , which is counter to omb's goal to report near real - time performance .

the dashboard rating inaccuracies were due , in part , to weaknesses in agencies' practices and limitations in omb's calculations .

more specifically , the agency practices — including the inconsistency between dashboard and program baselines , reporting of erroneous data , and unreliable source data — and omb's formulas to track current performance have collectively impaired data quality .

until agencies provide more reliable data and omb improves the calculations of the ratings on the dashboard , the accuracy of the ratings will continue to be in question and the ratings may not reflect current program performance .

to better ensure that the dashboard provides accurate cost and schedule performance ratings , we are making eleven recommendations to the heads of each of the five selected agencies .

specifically , we are recommending that: the secretary of the department of homeland security direct the cio to ensure that investment data submissions include complete and accurate investment information for all required fields ; comply with omb's guidance on updating the cio rating as soon as new information becomes available that affects the assessment of a given investment , including when an investment is in the process of a rebaseline ; and work with c4isr officials to comply with omb's guidance on updating investment cost and schedule data on the dashboard at least monthly .

the secretary of the department of transportation direct the cio to work with automatic dependent surveillance - broadcast officials to comply with omb's guidance on updating investment cost and schedule data on the dashboard at least monthly .

the secretary of the department of the treasury direct the cio to comply with omb's guidance on updating the cio rating as soon as new information becomes available that affects the assessment of a given investment , including when an investment is in the process of a rebaseline ; work with modernized e - file officials to report accurate actual percent complete data for each of the investment's activities ; and work with payment application modernization officials to disclose the extent of this investment's data reliability issues in the cio rating assessment on the dashboard .

the secretary of the department of veterans affairs direct the cio to comply with omb's guidance on updating the cio rating as soon as new information becomes available that affects the assessment of a given investment , including when an investment is in the process of a rebaseline ; work with medical legacy officials to comply with omb's guidance on updating investment cost and schedule data on the dashboard at least monthly ; and ensure medical legacy investment data submitted to the dashboard are consistent with the investment's internal performance information .

the commissioner of the social security administration direct the cio to ensure that data submissions to the dashboard include accurate investment information for all required fields .

in addition , to better ensure that the dashboard provides meaningful ratings and reliable investment data , we are recommending that the director of omb direct the federal cio to take the following two actions: develop cost and schedule rating calculations that better reflect current update the dashboard's schedule calculation for in - progress activities to more accurately represent the variance of ongoing , overdue activities .

we provided a draft of our report to the five agencies in our review and to omb .

in commenting on the draft , four agencies generally concurred with our recommendations .

one agency , the department of transportation , agreed to consider our recommendation .

omb agreed with one of our recommendations and disagreed with the other .

in addition , omb raised concerns about the methodology used in our report .

agencies also provided technical comments , which we incorporated as appropriate .

each agency's comments are discussed in more detail below .

in e - mail comments on a draft of the report , dhs's departmental audit liaison stated that the department concurred with our recommendations .

in e - mail comments , dot's director of audit relations stated that dot would consider our recommendation ; however , he also stated that the department disagreed with the way its investments were portrayed in the draft .

specifically , department officials stated that our assessment was not reasonable because our methodology only incorporated the most recent 6 months of performance rather than using cumulative investment performance .

as discussed in this report , combining the performance of ongoing and completed activities can mask recent performance .

as such , we maintain that our methodology is a reasonable means of deriving near real - time performance , which the dashboard is intended to represent .

in oral comments , treasury's chief architect stated that the department generally concurred with our recommendations and added that the department would work to update its dashboard ratings for the two selected investments .

in written comments , va's chief of staff stated that the department generally concurred with our recommendations and agreed with our conclusions .

further , he outlined the department's planned process improvements to address the weaknesses identified in this report .

va's comments are reprinted in appendix iii .

in written comments , ssa's deputy chief of staff stated that the administration agreed with our recommendation and had taken corrective actions intended to prevent future data quality errors .

ssa's comments are reprinted in appendix iv .

officials from omb's office of e - government & information technology provided the following oral comments on the draft: omb officials agreed with our recommendation to update the dashboard's schedule calculation for in - progress activities to more accurately represent the variance of ongoing , overdue activities .

these officials stated that the agency has long - term plans to update the dashboard's calculations , which they believe will provide a solution to the concern identified in this report .

omb officials disagreed with our recommendation to develop cost and schedule rating calculations that better reflect current investment performance .

according to omb , real - time performance is always reflected in the ratings since current investment performance data are uploaded to the dashboard on a monthly basis .

regarding omb's comments , our point is not that performance data on the dashboard are infrequently updated , but that the use of historical data going back to an investment's inception can mask more recent performance .

for this reason , current investment performance may not always be as apparent as it should be , as this report has shown .

until the agency places less emphasis on the historical data factored into the dashboard's calculations , it will be passing up an opportunity to more efficiently and effectively identify and oversee investments that either currently are or soon will be experiencing problems .

omb officials also described the agency's plans for enhancing dashboard data quality and performance calculations .

according to omb , plans were developed in february 2011 with stakeholders from other agencies to standardize the reporting structure for investment activities .

further , omb officials said that their plans also call for the dashboard's performance calculations to be updated to more accurately reflect activities that are delayed .

in doing so , omb stated that agencies will be expected to report new data elements associated with investment activities .

additionally , omb officials noted that new agency requirements associated with these changes will be included in key omb guidance ( circular a - 11 ) no later than september 2011 .

omb officials also raised two concerns regarding our methodology .

specifically , omb stated that our reliance on earned value data as the primary source for determining investment performance was questionable .

these officials stated that , on the basis of their experience collecting earned value data , the availability and quality of these data vary significantly across agencies .

as such , according to these officials , omb developed its dashboard cost and schedule calculations to avoid relying on earned value data .

we acknowledge that the quality of earned value data can vary .

as such , we took steps to ensure that the data we used were reliable enough to evaluate the ratings on the dashboard , and discounted the earned value data of one of the selected investments after determining its data were insufficient for our needs .

while we are not critical of omb's decision to develop its own method for calculating performance ratings , we maintain that our use of earned value data is sound .

furthermore , earned value data were not the only source for our analysis ; we also based our findings on other program management documentation , such as inspector general reports and internal performance management system performance ratings , as discussed in appendix i. omb also noted that , because we used earned value data to determine investment performance , our ratings were not comparable to the ratings on the dashboard .

specifically , omb officials said that the dashboard requires reporting of all activities under an investment , including government resources or operations and maintenance activities .

omb further said that this is more comprehensive than earned value data , which only account for contractor - led development activities .

we acknowledge and support the dashboard's requirement for a comprehensive accounting of investment performance .

further , we agree that earned value data generally only cover development work associated with the investments ( thus excluding other types of work , such as planning and operations and maintenance ) .

for this reason , as part of our methodology , we specifically selected investments for which the majority of the work being performed was development work .

we did this because earned value management is a proven technique for providing objective quantitative data on program performance , and alternative approaches do not always provide a comparable substitute for such data .

additionally , as discussed above , we did not base our analysis solely upon earned value data , but evaluated other available program performance documentation to ensure that we captured performance for the entire investment .

as such , we maintain that the use of earned value data ( among other sources ) and the comparison of selected investments' dashboard ratings with our analyses resulted in a fair assessment .

we are sending copies of this report to interested congressional committees ; the secretaries of the departments of homeland security , transportation , the treasury , and veterans affairs , as well as the commissioner of the social security administration ; and other interested parties .

in addition , the report will be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have any questions on the matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix v .

our objectives were to ( 1 ) determine what efforts the office of management and budget ( omb ) has under way to improve the dashboard and the ways in which it is using data from the dashboard to improve information technology ( it ) management and ( 2 ) examine the accuracy of the cost and schedule performance ratings on omb's dashboard .

to address the first objective , we examined related omb guidance and documentation to determine the ongoing and planned improvements omb has made to the dashboard and discussed these improvements with omb officials .

additionally , we evaluated omb documentation of current and planned efforts to oversee and improve the management of it investments and the dashboard , such as memos detailing the results of investment management review sessions , and interviewed omb officials regarding these efforts .

to address the second objective , we selected 5 agencies and 10 investments to review .

to select these agencies and investments , we first identified the 12 agencies with the largest it budgets as reported in omb's fiscal year 2011 exhibit 53 .

this list of agencies was narrowed down to 10 because 2 agencies did not have enough investments that met our criteria ( as defined in the following text ) .

we then excluded agencies that were assessed in our previous review of the dashboard .

as a result , we selected the departments of homeland security ( dhs ) , transportation ( dot ) , the treasury , and veterans affairs ( va ) , and the social security administration ( ssa ) .

in selecting the specific investments at each agency , we identified the 10 largest investments that , according to the fiscal year 2011 budget , were spending more than half of their budget on it development , modernization , and enhancement work .

to narrow this list , we excluded investments whose four different dashboard ratings ( overall , cost , schedule , and chief information officer ) were generally “red” because they were likely already receiving significant scrutiny .

we then selected 2 investments per agency .

as part of this selection process , we considered the following: investments that use earned value management techniques to monitor cost and schedule performance , and investments whose four different dashboard ratings appeared to be in conflict ( eg , cost and schedule ratings were “green,” yet the overall rating was “red” ) .

the 10 final investments were dhs's u.s .

citizenship and immigration service ( uscis ) - transformation program and u.s. coast guard - command , control , communications , computers , intelligence , surveillance & reconnaissance ( c4isr ) program ; dot's automatic dependent surveillance - broadcast system and en route automation modernization system ; treasury's modernized e - file system and payment application modernization investment ; va's healthevet core and medical legacy investments ; and ssa's disability case processing system and intelligent disability program .

the 5 agencies account for 22 percent of the planned it spending for fiscal year 2011 .

the 10 investments selected for case study represent about $1.27 billion in total planned spending in fiscal year 2011 .

to assess the accuracy of the cost and schedule performance ratings on the dashboard , we evaluated earned value data of 7 of the selected investments to determine their current cost and schedule performances and compared them with the performance ratings on the dashboard .

the investment earned value data were contained in contractor earned value management performance reports obtained from the programs .

to perform the current performance analysis , we averaged the cost and schedule variances over the last 6 months and compared the averages with the performance ratings on the dashboard .

to assess the accuracy of the cost data , we compared them with data from other available supporting program documents , including program management reports and inspector general reports ; electronically tested the data to identify obvious problems with completeness or accuracy ; and interviewed agency and program officials about the earned value management systems .

for the purposes of this report , we determined that the cost data for these 7 investments were sufficiently reliable .

for the 3 remaining investments , we did not use earned value data because the investments either did not measure performance using earned value management or the earned value data were determined to be insufficiently reliable .

instead , we used other program documentation , such as inspector general reports and internal performance management system performance ratings , to assess the accuracy of the cost and schedule ratings on the dashboard .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on what we were told by each agency and the information it could provide .

we also interviewed officials from omb and the selected agencies and reviewed omb guidance to obtain additional information on omb's and agencies' efforts to ensure the accuracy of the data used to rate investment performance on the dashboard .

we used the information provided by omb and agency officials to identify the factors contributing to inaccurate cost and schedule performance ratings on the dashboard .

we conducted this performance audit from july 2010 to march 2011 at the selected agencies' offices in the washington , d.c. , metropolitan area .

our work was done in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

below are descriptions of each of the selected investments that are included in this review .

uscis - transformation is a bureauwide program to move from a paper - based filing system to a centralized , consolidated , electronic adjudication filing system .

the c4isr common operating picture collects and fuses relevant information for coast guard commanders to allow them to efficiently exercise authority , while directing and monitoring all assigned forces and first responders , across the range of coast guard operations .

the automatic dependent surveillance - broadcast system is intended to be an underlying technology in the federal aviation administration's plan to transform air traffic control from the current radar - based system to a satellite - based system .

the automatic dependent surveillance - broadcast system is to bring the precision and reliability of satellite - based surveillance to the nation's skies .

the en route automation modernization system is to replace the current computer system used at the federal aviation administration's high - altitude en route centers .

the current system is considered the backbone of the nation's airspace system and processes flight radar data , provides communications , and generates display data to air traffic controllers .

the current modernized e - file system is a web - based platform that supports electronic tax returns and annual information returns for large corporations and certain tax - exempt organizations , as well as individual form 1040 and other schedules and supporting forms .

this system is being updated to include the electronic filing of the more than 120 remaining 1040 forms and schedules .

combining these efforts is intended to streamline tax return filing processes and reduce the costs associated with paper tax returns .

the payment application modernization investment is an effort to modernize the current mainframe - based software applications that are used to disburse approximately 1 billion federal payments annually .

the existing payment system is a configuration of numerous software applications that generate check , wire transfer , and automated clearing house payments for federal program agencies , including the social security administration , internal revenue service , department of veterans affairs , and others .

healthevet core was a set of initiatives to improve health care delivery , provide the platform for health information sharing , and update outdated technology .

the investment was to support veterans , their beneficiaries , and providers by advancing the use of health care information and leading edge it to provide a patient - centric , longitudinal , computable health record .

according to department officials , the healthevet core investment was “stopped” in august 2010 .

the medical legacy program is an effort to provide software applications necessary to maintain and modify the department's veterans health information systems and technology architecture .

the disability case processing system is intended to provide common functionality and consistency to support the business processes of each state's disability determination services .

ultimately , it is to provide analysis functionality , integrate health it , improve case processing , simplify maintenance , and reduce infrastructure growth costs .

the intelligent disability program is intended to reduce the backlog of disability claims , develop an electronic case processing system , and support efficiencies in the claims process .

table 2 provides additional details for each of the selected investments in our review .

in addition to the contact named above , the following staff also made key contributions to this report: carol cha , assistant director ; shannin o'neill , assistant director ; alina johnson ; emily longcore ; lee mccracken ; and kevin walsh .

