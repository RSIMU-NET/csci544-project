for a number of years , the department of defense has been researching and developing defenses against ballistic missile attacks on the united states , its deployed forces , friends , and allies .

in 1990 , the department awarded research and development contracts to three contractors to develop and test exoatmospheric kill vehicles .

the department planned to use the best of the three vehicles in a follow - on missile defense program .

one of the contractors , rockwell international , subcontracted a portion of its kill vehicle design work to trw .

trw was tasked with developing software that could operate on a computer onboard the kill vehicle .

the software was to analyze data collected in flight by the kill vehicle's sensor ( which collects real - time information about threat objects ) , enabling the kill vehicle to distinguish an enemy warhead from accompanying decoys .

the three contractors proceeded with development of the kill vehicle designs and built and tested key subsystems ( such as the sensor ) until 1994 .

in 1994 , the department of defense eliminated martin marietta from the competition .

both rockwell — portions of which in december 1996 became boeing north american — and hughes — now raytheon — continued designing and testing their kill vehicles .

in 1997 and 1998 , the national missile defense joint program office conducted tests , in space , of the sensors being developed by the contractors for their competing kill vehicles .

boeing's sensor was tested in june 1997 ( integrated flight test 1a ) and raytheon's sensor was tested in january 1998 ( integrated flight test 2 ) .

program officials said these tests were not meant to demonstrate that the sensor met performance requirements , nor were they intended to be the basis for any contract award decisions .

rather , they were early research and development tests that the program office considered experiments to primarily reduce risk in future flight tests .

specifically , the tests were designed to determine if the sensor could operate in space ; to examine the extent to which the sensor could detect small differences in infrared emissions ; to determine if the sensor was accurately calibrated ; and to collect target signature data for post - mission discrimination analysis .

after the two sensor tests , the program office planned another 19 flight tests from 1999 through 2005 in which the kill vehicle would attempt to intercept a mock warhead .

initially , boeing's kill vehicle was scheduled for testing in integrated flight test 3 and raytheon's in integrated flight test 4 .

however , boeing became the lead system integrator for the national missile defense program in april 1998 and , before the third flight test was conducted , selected raytheon as the primary kill vehicle developer .

meanwhile , in september 1995 , trw had hired a senior staff engineer , dr. nira schwartz , to work on various projects , including the company's effort to develop the exoatmospheric kill vehicle's discrimination software .

the engineer helped evaluate some facets of a technology known as the extended kalman filter feature extractor , which trw planned to add as an enhancement to its discrimination software .

the engineer reported to trw in february 1996 that tests revealed that the filter could not extract the key characteristics , or features , from various target objects that an enemy missile might deploy and demanded that the company inform rockwell and the department of defense .

trw fired the engineer in march 1996 .

in april 1996 , the engineer filed a lawsuit under the false claims act alleging that trw falsely reported or hid information to make the national missile defense joint program office believe that the extended kalman filter feature extractor met the department's technical requirements .

the engineer has amended the lawsuit several times , including adding allegations that trw misled the department of defense about the ability of its discrimination software to distinguish a warhead from decoys and that trw's test reports on integrated flight test 1a falsely represented the discrimination software's performance .

the false claims act allows a person to bring a lawsuit on behalf of the u.s. government if he or she has knowledge that a person or company has made a false or fraudulent claim against the government .

if the suit is successful , the person bringing the lawsuit may share in any money recovered .

the department of justice reviews all lawsuits filed under the act before deciding whether to join them .

if it does , it becomes primarily responsible for prosecuting the case .

to determine whether it should join the engineer's lawsuit against trw , justice asked the defense criminal investigative service , a unit within the department of defense inspector general's office , to examine the allegations .

the engineer cooperated with the investigative service for more than 2 years .

during the course of the department of defense's investigation into the allegations of contractor fraud , two groups examined the former employee's specific allegations regarding the performance of trw's basic discrimination software and performed limited evaluations of the extended kalman filter feature extractor .

the first was nichols research corporation , a contractor providing technical assistance to the ground based interceptor project management office for its oversight of the exoatmospheric kill vehicle contracts .

 ( this office within the national missile defense joint program office is responsible for the exoatmospheric kill vehicle contracts. ) .

because an investigator for the defense criminal investigative service was concerned about the ability of nichols to provide a truly objective assessment , the national missile defense joint program office asked an existing advisory group , known as the phase one engineering team , to undertake another review of the specific allegations of fraud with respect to the software .

this group is comprised of scientists from federally funded research and development centers who were selected for the review team because of their knowledge of the national missile defense system .

in addition , both nichols and the phase one engineering team assessed the feasibility of using the extended kalman filter feature extractor to extract additional features from target objects that an enemy missile might deploy .

the department of justice and the defense criminal investigative service investigated the engineer's allegations until march 1999 .

at that time , the department of justice decided not to intervene in the lawsuit .

the engineer has continued to pursue her lawsuit without justice's intervention .

because you were the principal sponsors of the 1986 amendments to the false claims act and are interested in eliminating fraud in federal government programs , you asked that we review the former trw employee's allegations .

as agreed with your offices , we focused our review on integrated flight test 1a .

we also examined the evaluations of trw's discrimination software and extended kalman filter feature extractor conducted by nichols research corporation and the phase one engineering team .

to answer your question regarding whether the phase one engineering team's evaluation was objective and unbiased , we examined how the national missile defense joint program office addressed the team's potential conflicts of interest .

we also reviewed the basis for the department of justice's decision not to intervene in the former trw employee's lawsuit .

specifically , this report addresses the following questions: 1 .

did boeing and trw disclose the key results and limitations of the flight test to the national missile defense joint program office ? .

2 .

what were the methodology , findings , and limitations of the evaluations conducted by nichols research corporation and the phase one engineering team of trw's discrimination software and the extended kalman filter feature extractor ? .

3 .

what was the basis for the department of justice's decision not to join the lawsuit ? .

4 .

how did the national missile defense joint program office assure itself that the phase one engineering team could provide an independent and objective review ? .

boeing and trw disclosed the key results and limitations of integrated flight test 1a in written reports released between august 13 , 1997 , and april 1 , 1998 .

the contractors explained in a report issued 60 days after the june 1997 test that the test achieved its primary objectives , but that some sensor abnormalities were noted .

for example , while the report explained that the sensor detected the deployed targets and collected some usable target signals , the report also stated that some sensor components did not operate as desired and the sensor often detected targets where there were none .

in december 1997 , the contractors documented other test anomalies .

according to briefing charts prepared for a december meeting , the boeing sensor tested in integrated flight test 1a had a low probability of detection ; the sensor's software was not always confident that it had correctly identified some target objects ; the software significantly increased the rank of one target object toward the end of the flight ; and in - flight calibration of the sensor was inconsistent .

additionally , on april 1 , 1998 , the contractors submitted an addendum to an earlier report that noted two more problems .

in this addendum , the contractors disclosed that their claim that trw's software successfully distinguished a mock warhead from decoys during a post - flight analysis was based on tests of the software using about one - third of the target signals collected during integrated flight test 1a .

the contractors also noted that trw reduced the software's reference data so that it would correspond to the collected target signals being analyzed .

project office and nichols research officials said that in late august 1997 , the contractors orally communicated to them all problems and limitations that were subsequently described in the december 1997 briefing and the april 1998 addendum .

however , neither project officials nor contractors could provide us with documentation of these communications .

although the contractors reported the test's key results and limitations , they described the results using some terms that were not defined .

for example , one written report characterized the test as a “success” and the sensor's performance as “excellent.” we found that the information in the contractors' reports , in total , enabled officials in the ground based interceptor project management office and nichols research to understand the key results and limitations of the test .

however , because such terms are qualitative and subjective rather than quantitative and objective , their use increased the likelihood that test results would be interpreted in different ways and might even be misunderstood .

as part of our ongoing review of missile defense testing , we are examining the need for improvements in test reporting .

appendix i provides details on the test and the information disclosed .

two groups — nichols research corporation and the phase one engineering team — evaluated trw's basic discrimination software .

nichols evaluated the software by testing it against simulated warheads and decoys similar to those that the contractors were directed to design their software to handle .

the evaluation concluded that although the software had some weaknesses , it met performance requirements established by boeing in nearly all cases .

however , nichols explained that the software was successful because the simulated threat was relatively simple .

nichols said that trw's software was highly dependent on prior knowledge about the threat and that the test conditions that nichols' engineers established for the evaluation included providing perfect knowledge of the features that the simulated warhead and decoys would display during the test .

nichols' evaluation was limited because it did not test trw's software using actual flight data from integrated flight test 1a .

nichols told us that it had planned to assess the software's performance using real target signals collected during integrated flight test 1a , but did not do so because resources were limited .

because it did not perform this assessment , nichols cannot be said to have definitively proved or disproved trw's claim that its software discriminated the mock warhead from decoys using data collected from integrated flight test 1a .

the phase one engineering team was tasked by the national missile defense joint program office to assess the performance of trw's software and to complete the assessment within 2 months using available data .

the team's methodology included determining if trw's software was based on sound mathematical , engineering , and scientific principles and testing the software's critical modules using data from integrated flight test 1a .

the team reported that although the software had weaknesses , it was well designed and worked properly , with only some changes needed to increase the robustness of the discrimination function .

further , the team reported that the results of its test of the software using integrated flight test 1a data produced essentially the same results as those reported by trw .

based on its analysis , team members predicted that the software would perform successfully in a future intercept test if target objects deployed as expected .

because the phase one engineering team did not process the raw data from integrated flight test 1a or develop its own reference data , the team cannot be said to have definitively proved or disproved trw's claim that its software successfully distinguished the mock warhead from decoys using data collected from integrated flight test 1a .

a team member told us its use of boeing - and trw - provided data was appropriate because the former trw employee had not alleged that the contractors tampered with the raw test data or used inappropriate reference data .

in assessing trw's extended kalman filter feature extractor , both nichols and the phase one engineering team tested whether the filter could be used to extract an additional feature ( key characteristic ) from a target object's signal to help identify that object .

nichols tested the filter's ability against a number of simulated target signals and found that it was generally successful .

the phase one engineering team tested the filter's ability using the signals of one simulated target and one collected during integrated flight test 1a .

both groups concluded that the filter could feasibly provide additional information about target objects , but neither group's evaluation allowed it to forecast whether the filter would improve the basic software's discrimination capability .

appendix ii provides additional details on the nichols and phase one engineering team evaluations .

the department of justice relied primarily on scientific reports , but considered information from two army legal offices when it determined in march 1999 that it would not intervene in the false claims lawsuit brought by the former trw employee .

the scientific reports were prepared by nichols research corporation and the phase one engineering team .

justice's attorneys said they also considered an opinion of the army space and missile defense command's legal office that said it did not consider vouchers submitted by boeing for work performed by its subcontractor , trw , as being false claims .

in addition , the attorneys said a recommendation from the army legal services agency that justice not intervene was a factor in their decision .

it is not clear how the army legal services agency came to that decision as very little documentation is available and agency officials told us that they remember very little about the case .

appendix iii provides additional information on factors that were considered in justice's decision .

when the national missile defense joint program office determined that another assessment of trw's software should be undertaken , it tasked an existing advisory group , known as the phase one engineering team , to conduct this review .

comprised of various federally funded research and development centers , this group was established in 1988 by the strategic defense initiative organization as a mechanism to provide the program office with access to a continuous , independent , and objective source of technical and engineering expertise .

since the federally funded research and development centers are authorized , established , and operated for the express purpose of providing the government with independent and objective advice , program officials determined that making use of this existing advisory group would be sufficient to assure an independent and objective review .

program officials said that they relied upon the centers' adherence to requirements contained in both the federal acquisition regulation and their contracts and agreements with their sponsoring federal agencies to assure themselves that the review team could provide an independent , unbiased look at trw's software .

appendix iv provides a fuller explanation of the steps taken by the national missile defense joint program office to assure itself that the phase one engineering team would provide an independent and objective review .

in commenting on a draft of this report , the department of defense and the department of justice concurred with our findings .

the department of defense also suggested technical changes , which we incorporated as appropriate .

the department of defense's comments are reprinted in appendix vii .

the department of justice provided its concurrence via e - mail and had no additional comments .

we conducted our review from august 2000 through february 2002 in accordance with generally accepted government auditing standards .

appendix vi provides details on our scope and methodology .

the national missile defense joint program office's process for releasing documents significantly slowed our work .

for example , the program office took approximately 4 months to release key documents , such as nichols research corporation's 1996 and 1998 evaluations of the extended kalman filter feature extractor and nichols' 1997 evaluation of trw's discrimination software .

we requested these and other documents on september 14 , 2000 , and received them on january 9 , 2001 .

as arranged with your staff , unless you publicly announce its contents earlier , we plan no further distribution of this report until 30 days from its issue date .

at that time , we plan to provide copies of this report to the chairmen and ranking minority members of the senate committee on armed services ; the senate committee on appropriations , subcommittee on defense ; the house committee on armed services ; and the house committee on appropriations , subcommittee on defense ; and the secretary of defense ; the attorney general ; and the director , missile defense agency .

we will make copies available to others upon request .

if you or your staff have any questions concerning this report , please contact bob levin , director , acquisition and sourcing management , on ( 202 ) 512-4841 ; jack brock , managing director , on ( 202 ) 512-4841 ; or keith rhodes , chief technologist , on ( 202 ) 512-6412 .

major contributors to this report are listed in appendix viii .

boeing and trw disclosed the key results and limitations of an early sensor flight test , known as integrated flight test 1a , to the ground based interceptor project management office .

the contractors included some key results and limitations in written reports submitted soon after the june 1997 test , but others were not included in written reports until december 1997 or april 1998 .

however , according to project office and nichols officials , all problems and limitations included in the written reports were communicated orally to the project management office in late august 1997 .

the deputy project office manager said his office did not report these verbal communications to others within the program office or the department of defense because the project office was the office within the department responsible for the boeing contract .

one problem that was included in initial reports to program officials was a malfunctioning cooling mechanism that did not lower the sensor's temperature to the desired level .

boeing characterized the mechanism's performance as somewhat below expectations but functioning well enough for the sensor's operation .

we hired experts to determine the extent to which the problem could affect the sensor's performance .

the experts found that the cooling problem degraded the sensor's performance in a number of ways , but would not likely result in extreme performance degradation .

the experts studied only how increased noise affected the sensor's performance regarding comparative strengths of the target signals and the noise ( signal to noise ratio ) .

the experts did not evaluate discrimination performance , which is dependent on the measurement accuracy of the collected infrared signals .

the experts' findings are discussed in more detail later in this appendix .

integrated flight test 1a , conducted in june 1997 , was a test of the boeing sensor — a highly sensitive , compact , infrared device , consisting of an array of silicon detectors , that is normally mounted on the exoatmospheric kill vehicle .

however , in this test , a surrogate launch vehicle carried the sensor above the earth's atmosphere to view a cluster of target objects that included a mock warhead and various decoys .

when the sensor detected the target cluster , its silicon detectors began to make precise measurements of the infrared radiation emitted by the target objects .

over the tens of seconds that the target objects were within its field of view , the sensor continuously converted the infrared radiation into an electrical current , or signal , proportional to the amount of energy collected by the detectors .

the sensor then digitized the signal ( converted the signals into numerical values ) , completed a preliminary part of the planned signal processing , and formatted the signal so that it could be transmitted via a data link to a recorder on the ground .

after the test , boeing processed the signals further and formatted them so that trw could input the signals into its discrimination software to assess its capability to distinguish the mock warhead from decoys .

in post - flight ground testing , the software analyzed the processed data and identified the key characteristics , or features , of each signal .

the software then compared the features it extracted to the expected features of various types of target objects .

based on this comparison , the software ranked each item according to its likelihood of being the mock warhead .

trw reported that the highest - ranked object was the mock warhead .

the primary objective of integrated flight test 1a was to reduce risk in future flight tests .

specifically , the test was designed to determine if the sensor could operate in space ; to examine the extent to which the sensor could detect small differences in infrared emissions ; to determine if the sensor was accurately calibrated ; and to collect target signature data for post - mission discrimination analysis .

in addition , boeing established quantitative requirements for the test .

for example , the sensor was expected to acquire the target objects at a specified distance .

according to a nichols' engineer , boeing established these requirements to ensure that its exoatmospheric kill vehicle , when fully developed , could destroy a warhead with the single shot precision ( expressed as a probability ) required by the ground based interceptor project management office .

the engineer said that in integrated flight test 1a , boeing planned to measure its sensor's performance against these lower - level requirements so that boeing engineers could determine which sensor elements , including the software , required further refinement .

however , the engineer told us that because of the various sensor problems , of which the contractor and project office were aware , boeing determined before the test that it would not use most of these requirements to judge the sensor's performance .

 ( although boeing did not judge the performance of its sensor against the requirements as it originally planned , boeing did , in some cases , report the sensor's performance in terms of these requirements .

for a summary of selected test requirements and the sensor's performance as reported by boeing and trw in their august 22 , 1997 , report , see app .

v. ) .

table 1 provides details on the key results and limitations of integrated flight test 1a that contractors disclosed in various written reports and briefing charts .

although the contractors disclosed the key results and limitations of the flight test in written reports and in discussions , the written reports described the results using some terms that were not defined .

for example , in their august 22 , 1997 , report , boeing and trw described integrated flight test 1a as a “success” and the performance of the boeing sensor as “excellent.” we asked the contractors to explain their use of these terms .

we asked boeing , for example , why it characterized its sensor's performance as “excellent” when the sensor's silicon detector array did not cool to the desired temperature , the sensor's power supply created excess noise , and the sensor detected numerous false targets .

boeing said that even though the silicon detector array operated at temperatures 20 to 30 percent higher than desired , the sensor produced useful data .

officials said they knew of no other sensor that would be capable of producing any useful data under those conditions .

boeing officials went on to say that the sensor continuously produced usable , and , much of the time , excellent data in “real - time” during flight .

in addition , officials said the sensor component responsible for suppressing background noise in the silicon detector array performed perfectly in space and the silicon detectors collected data in more than one wave band .

boeing concluded that the sensor's performance allowed the test to meet all mission objectives .

based on our review of the reports and discussions with officials in the ground based interceptor project management office and nichols research , we found that the contractors' reports , in total , contained information for those officials to understand the key results and limitations of the test .

however , because terms such as “success” and “excellent” are qualitative and subjective rather than quantitative and objective , we believe their use increases the likelihood that test results would be interpreted in different ways and could even be misunderstood .

as part of our ongoing review of missile defense testing , we are examining the need for improvements in test reporting .

this report , sometimes referred to as the 45-day report , was a series of briefing charts .

in it , contractors reported that integrated flight test 1a achieved its principal objectives of reducing risks for subsequent flight tests , demonstrating the performance of the exoatmospheric kill vehicle's sensor , and collecting target signature data .

in addition , the report stated that trw's software successfully distinguished a mock warhead from accompanying decoys .

the august 22 report , known as the 60-day report , was a lengthy document that disclosed much more than the august 13 report .

as discussed in more detail below , the report explained that some sensor abnormalities were observed during the test , that some signals collected from the target objects were degraded , that the launch vehicle carrying the sensor into space adversely affected the sensor's ability to collect target signals , and that the sensor sometimes detected targets where there were none .

these problems were all noted in the body of the report , but the report summary stated that review and analysis subsequent to the test confirmed the “excellent” performance and nominal operation of all sensor subsystems .

boeing disclosed in the report that sensor abnormalities were observed during the test and that the sensor experienced a higher than expected false alarm rate .

these abnormalities were ( 1 ) a cooling mechanism that did not bring the sensor's silicon detectors to the intended operating temperature , ( 2 ) a power supply unit that created excess noise , and ( 3 ) software that did not function as designed because of the slow turnaround of the surrogate launch vehicle .

in the report's summary , boeing characterized the cooling mechanism's performance as somewhat below expectations but functioning well enough for the sensor's operation .

in the body of the report , boeing said that the fluctuations in temperature could lead to an apparent decrease in sensor performance .

additionally , boeing engineers told us that the cooling mechanism's failure to bring the silicon detector array to the required temperature caused the detectors to be noisy .

because the discrimination software identifies objects as a warhead or a decoy by comparing the features of a target's signal with those it expects a warhead or decoy to display , a noisy signal may confuse the software .

boeing and trw engineers said that they and program office officials were aware that there was a problem with the sensor's cooling mechanism before the test was conducted .

however , boeing believed that the sensor would perform adequately at higher temperatures .

according to contractor documents , the sensor did not perform as well as expected , and some target signals were degraded more than anticipated .

boeing disclosed in the report that sensor abnormalities were observed during the test and that the sensor experienced a higher than expected false alarm rate .

these abnormalities were ( 1 ) a cooling mechanism that did not bring the sensor's silicon detectors to the intended operating temperature , ( 2 ) a power supply unit that created excess noise , and ( 3 ) software that did not function as designed because of the slow turnaround of the surrogate launch vehicle .

in the report's summary , boeing characterized the cooling mechanism's performance as somewhat below expectations but functioning well enough for the sensor's operation .

in the body of the report , boeing said that the fluctuations in temperature could lead to an apparent decrease in sensor performance .

additionally , boeing engineers told us that the cooling mechanism's failure to bring the silicon detector array to the required temperature caused the detectors to be noisy .

because the discrimination software identifies objects as a warhead or a decoy by comparing the features of a target's signal with those it expects a warhead or decoy to display , a noisy signal may confuse the software .

boeing and trw engineers said that they and program office officials were aware that there was a problem with the sensor's cooling mechanism before the test was conducted .

however , boeing believed that the sensor would perform adequately at higher temperatures .

according to contractor documents , the sensor did not perform as well as expected , and some target signals were degraded more than anticipated .

the report also referred to a problem with the sensor's power supply unit and its effect on target signals .

an expert we hired to evaluate the sensor's performance at higher than expected temperatures found that the power supply , rather than the temperature , was the primary cause of excess noise early in the sensor's flight .

boeing engineers told us that they were aware that the power supply was noisy before the test , but , as shown by the test , it was worse than expected .

the report explained that , as expected before the flight , the slow turnaround of the massive launch vehicle on which the sensor was mounted in integrated flight test 1a caused the loss of some target signals .

engineers explained to us that the sensor would eventually be mounted on the lighter , more agile exoatmospheric kill vehicle , which would move back and forth to detect objects that did not initially appear in the sensor's field of view .

the engineers said that boeing designed software that takes into account the kill vehicle's normal motion to remove the background noise , but the software's effectiveness depended on the fast movement of the kill vehicle .

boeing engineers told us that , because of the slow turnaround of the launch vehicle used in the test , the target signals detected during the turnaround were particularly noisy and the software sometimes removed not only the noise but the entire signal as well .

the report mentioned that the sensor experienced more false alarms than expected .

a false alarm is a detection of a target that is not there .

according to the experts we hired , during integrated flight test 1a , the boeing sensor often mistakenly identified noise produced by the power supply as signals from actual target objects .

in a fully automated discrimination software program , a high false alarm rate could overwhelm the tracking software .

because the post - flight processing tools were not fully developed at the time of the august 13 and august 22 , 1997 , reports , boeing did not rely upon a fully automated tracking system when it processed the integrated flight test 1a data .

instead , a boeing engineer manually tracked the target objects .

the contractors realized , and reported to the ground based interceptor project management office , that numerous false alarms could cause problems in future flight tests , and they identified software changes to reduce their occurrence .

on december 11 , 1997 , boeing and trw briefed officials from the ground based interceptor project management office and one of its support contractors on various anomalies observed during integrated flight test 1a .

the contractors' briefing charts explained the effect the anomalies could have on integrated flight test 3 , the first planned intercept test for the boeing exoatmospheric kill vehicle , identified potential causes of the anomalies , and summarized the solutions to mitigate their effect .

while some of the anomalies included in the december 11 briefing charts were referred to in the august 13 and august 22 reports , others were being reported in writing for the first time .

the anomalies referenced in the briefing charts included the sensor's high false alarm rate , the silicon detector array's higher - than - expected temperature , the software's low confidence factor that it had correctly identified two target objects correctly , the sensor's lower than expected probability of detection , and the software's elevation in rank of one target object toward the end of the test .

in addition , the charts showed that an in - flight attempt to calibrate the sensor was inconsistent .

according to the charts , actions to prevent similar anomalies from occurring or impacting integrated flight test 3 had in most cases already been implemented or were under way .

the contractors again recognized that a large number of false alarms occurred during integrated flight test 1a .

according to the briefing charts , false alarms occurred during the slow turnarounds of the surrogate launch vehicle .

additionally , the contractors hypothesized that some false alarms resulted from space - ionizing events .

by december 11 , engineers had identified solutions to reduce the number of false alarms in future tests .

as they had in the august 22 , 1997 , report , the contractors recognized that the silicon detector array did not cool properly during integrated flight test 1a .

the contractors reported that higher silicon detector array temperatures could cause noisy signals that would adversely impact the detector array's ability to estimate the infrared intensity of observed objects .

efforts to eliminate the impact of the higher temperatures , should they occur in future tests , were on - going at the time of the briefing .

contractors observed that the confidence factor produced by the software was small for two target objects .

the software equation that makes a determination as to how confident the software should be to identify a target object correctly , did not work properly for the large balloon or multiple - service launch vehicle .

corrections to the equation had been made by the time of the briefing .

the charts state that the integrated flight test 1a sensor had a lower than anticipated probability of detection and a high false alarm rate .

because a part of the tracking , fusion , and discrimination software was designed for a sensor with a high probability of detection and a low false alarm rate , the software did not function optimally and needed revision .

changes to prevent this from happening in future flight tests were under way .

the briefing charts showed that trw's software significantly increased the rank of one target object just before target objects began to leave the sensor's field of view .

although a later integrated flight test 1a report stated the mock warhead was consistently ranked as the most likely target , the charts show that if in integrated flight test 3 the same object's rank began to increase , the software could select the object as the intercept target .

in the briefing charts , the contractors reported that trw made a software change in the model that is used to generate reference data .

when reference data was generated with the software change , the importance of the mock warhead was increased , and it was selected as the target .

tests of the software change were in progress as of december 11 .

the boeing sensor measures the infrared emissions of target objects by converting the collected signals into intensity with the help of calibration data obtained from the sensor prior to flight .

however , the sensor was not calibrated at the higher temperature range that was experienced during integrated flight test 1a .

to remedy the problem , the sensor viewed a star with known infrared emissions .

the measurement of the star's intensity was to have helped fill the gaps in calibration data that was essential to making accurate measurements of the target object signals .

boeing disclosed that the corrections based on the star calibration were inconsistent and did not improve the match of calculated and measured target signatures .

boeing subsequently told us that the star calibration corrections were effective for one of the wavelength bands , but not for another , and that the inconsistency referred to in the briefing charts was in how these bands behaved at temperatures above the intended operating range .

efforts to find and implement solutions were in progress .

on april 1 , 1998 , boeing submitted a revised addendum to replace an addendum that had accompanied the august 22 , 1997 , report .

this revised addendum was prepared in response to comments and questions submitted by officials from the ground based interceptor project management office , nichols research corporation , and the defense criminal investigative service concerning the august 22 report .

in this addendum , the contractors referred in writing to three problems and limitations that had not been addressed in earlier written test reports or the december 11 briefing .

contractors noted that a gap - filling module , which was designed to replace noisy or missing signals , did not operate as designed .

they also disclosed that trw's analysis of its discrimination software used target signals collected during a selected portion of the flight timeline and used a portion of the integrated flight test 1a reference data that corresponded to this same timeline .

the april 1 addendum reported that a gap - filling module that was designed to replace portions of noisy or missing target signals with expected signal values did not operate as designed .

trw officials told us that the module's replacement values were too conservative and resulted in a poor match between collected signals and the signals the software expected the target objects to display .

the april 1 , 1998 , addendum also disclosed that the august 13 and august 22 reports , in which trw conveyed that its software successfully distinguished the mock warhead from decoys , were based on tests of the software using about one - third of the target signals collected during integrated flight test 1a .

we talked to trw officials who told us that boeing provided several data sets to trw , including the full data set .

the officials said that boeing provided target signals from the entire timeline to a trw office that was developing a prototype version of the exoatmospheric kill vehicle's tracking , fusion , and discrimination software , which was not yet operational .

however , trw representatives said that the test bed version of the software that trw was using so that it could submit its analysis within 60 days of integrated flight test 1a could not process the full data set .

the officials said that shortly before the august 22 report was issued , the prototype version of the tracking , fusion , and discrimination software became functional and engineers were able to use the software to assess the expanded set of target signals .

according to the officials , this assessment also resulted in the software's selecting the mock warhead as the most likely target .

in our review of the august 22 report , we found no analysis of the expanded set of target signals .

the april 1 , 1998 , report , did include an analysis of a few additional seconds of data collected near the end of integrated flight test 1a , but did not include an analysis of target signals collected at the beginning of the flight .

most of the signals that were excluded from trw's discrimination analysis were collected during the early part of the flight , when the sensor's temperature was fluctuating .

trw told us that their software was designed to drop a target object's track if the tracking portion of the software received no data updates for a defined period .

this design feature was meant to reduce false tracks that the software might establish if the sensor detected targets where there were none .

in integrated flight test 1a , the fluctuation of the sensor's temperature caused the loss of target signals .

trw engineers said that boeing recognized that this interruption would cause trw's software to stop tracking all target objects and restart the discrimination process .

therefore , boeing focused its efforts on processing those target signals that were collected after the sensor's temperature stabilized and signals were collected continuously .

some signals collected during the last seconds of the sensor's flight were also excluded .

the former trw employee alleged that these latter signals were excluded because during this time a decoy was selected as the target .

the phase one engineering team cited one explanation for the exclusion of the signals .

the team said that trw stopped using data when objects began leaving the sensor's field of view .

our review did not confirm this explanation .

we reviewed the target intensities derived from the infrared frames covering that period and found that several seconds of data were excluded before objects began to leave the field of view .

boeing officials gave us another explanation .

they said that target signals collected during the last few seconds of the flight were streaking , or blurring , because the sensor was viewing the target objects as it flew by them .

boeing told us that streaking would not occur in an intercept flight because the kill vehicle would have continued to approach the target objects .

we could not confirm that the test of trw's discrimination software , as explained in the august 22 , 1997 , report , included all target signals that did not streak .

we noted that the april 1 , 1998 , addendum shows that trw analyzed several more seconds of target signals than is shown in the august 22 , 1997 , report .

it was in these additional seconds that the software began to increase the rank of one decoy as it assessed which target object was most likely the mock warhead .

however , the april 1 , 1998 , addendum also shows that even though the decoy's rank increased the software continued to rank the mock warhead as the most likely target .

but , because not all of the integrated flight test 1a timeline was presented in the april 1 addendum , we could not determine whether any portion of the excluded timeline might have been useful data and if there were additional seconds of useful data whether a target object other than the mock warhead might have been ranked as the most likely target .

the april 1 addendum also documented that portions of the reference data developed for integrated flight test 1a were also excluded from the discrimination analysis .

nichols and project office officials told us the software identifies the various target objects by comparing the target signals collected from each object at a given point in their flight to the target signals it expects each object to display at that same point in the flight .

therefore , when target signals collected during a portion of the flight timeline are excluded , reference data developed for the same portion of the timeline must be excluded .

officials in the national missile defense joint program office's ground based interceptor project management office and nichols research told us that soon after integrated flight test 1a the contractors orally disclosed all of the problems and limitations cited in the december 11 , 1997 , briefing and the april 1 , 1998 , addendum .

contractors made these disclosures to project office and nichols research officials during meetings that were held to review integrated flight test 1a results sometime in late august 1997 .

the project office and contractors could not , however , provide us with documentation of these disclosures .

the current ground based interceptor project management office deputy manager said that the problems that contractors discussed with his office were not specifically communicated to others within the department of defense because his office was the office within the department responsible for the boeing contract .

the project office's assessment was that these problems did not compromise the reported success of the mission , were similar in nature to problems normally found in initial developmental tests , and could be easily corrected .

because we questioned whether boeing's sensor could collect any usable target signals if the silicon detector array was not cooled to the desired temperature , we hired sensor experts at utah state university's space dynamics laboratory to determine the extent to which the sub - optimal cooling degraded the sensor's performance .

these experts concluded that the higher temperature of the silicon detectors degraded the sensor's performance in a number of ways , but did not result in extreme degradation .

for example , the experts said the higher temperature reduced by approximately 7 percent the distance at which the sensor could detect targets .

the experts also said that the rapid temperature fluctuation at the beginning and at the end of data acquisition contributed to the number of times that the sensor detected a false target .

however , the experts said the major cause of the false alarms was the power supply noise that contaminated the electrical signals generated by the sensor in response to the infrared energy .

when the sensor signals were processed after integrated flight test 1a , the noise appeared as objects , but they were actually false alarms .

additionally , the experts said that the precision with which the sensor could estimate the infrared energy emanating from an object based on the electrical signal produced by the energy was especially degraded in one of the sensor's two infrared wave bands .

in their report , the experts said that the massachusetts institute of technology's lincoln laboratory analyzed the precision with which the boeing sensor could measure infrared radiation and found large errors in measurement accuracy .

the utah state experts said that their determination that the sensor's measurement capability was degraded in one infrared wave band might partially explain the errors found by lincoln laboratory .

although boeing's sensor did not cool to the desired temperature during integrated flight test 1a , the experts found that an obstruction in gas flow rather than the sensor's design was at fault .

these experts said the sensor's cooling mechanism was properly designed and boeing's sensor design was sound .

nichols research corporation and the phase one engineering team tested trw's discrimination software and a planned enhancement to that software , known as the extended kalman filter feature extractor .

nichols concluded that although it had weaknesses , the discrimination software met performance requirements established by boeing when it was tested against a simple threat and given near perfect knowledge about the key characteristics , or features , that the target objects would display during flight .

the phase one engineering team reported that despite some weaknesses , trw's discrimination software was well designed and worked properly .

like nichols , the team found that the software's performance was dependent upon prior knowledge of the target objects .

because nichols did not test the software's capability using data collected from integrated flight test 1a and the phase one engineering team did not process the raw data from integrated flight test 1a or develop its own reference data , neither group can be said to have definitively proved or disproved trw's claim that its software successfully identified the mock warhead from decoys using data collected from integrated flight test 1a .

from their assessments of trw's extended kalman filter feature extractor , both groups concluded that it was feasible that the filter could provide additional information about target objects , but neither group determined to what extent the filter would improve the software's discrimination performance .

nichols research corporation evaluated trw's discrimination software to determine if it met performance requirements developed by boeing .

boeing established discrimination performance requirements to ensure that its exoatmospheric kill vehicle , when fully developed , could destroy a warhead with the single shot precision ( expressed as a probability ) required by the ground based interceptor project management office.the kill vehicle must perform a number of functions successfully to accurately hit - to - kill its target , such as acquiring the target cluster , discriminating the warhead from other objects , and diverting to hit the warhead .

boeing believed that if it met the performance requirements that it established for each function , including the discrimination function , the exoatmospheric kill vehicle should meet the required single shot probability of kill .

to determine if trw's software performed as required , nichols' engineers obtained a copy of trw's software ; verified that the software was based on sound scientific and engineering principles ; validated that it operated as designed ; and tested its performance in 48 simulated scenarios that included countermeasures , such as decoys , that the system might encounter before 2010 .

nichols validated the software by obtaining a copy of the actual source code from trw and installing the software in a nichols computer .

engineers then examined the code line - by - line ; verified its logic , data flow , and input and output ; and determined that the software accurately reflected trw's baseline design .

nichols next verified that the software performed exactly as reported by trw .

engineers ran 13 trw - provided test cases through the software and compared the results to those reported by trw .

nichols reported that their results were generally consistent with trw's results with only minor performance differences in a few cases .

after analyzing the 13 reference cases , nichols generated additional test cases by simulating a wide - range of enemy missiles with countermeasures that included decoys .

including the 13 reference cases , nichols analyzed the software's performance in a total of 48 test scenarios .

because the software performed successfully in 45 of 48 simulated test cases , nichols concluded that the system met the performance requirements established by boeing .

however , nichols explained that the software met its requirement because it was tested against a simple threat .

in addition , nichols said that the software was given nearly perfect knowledge of the features the simulated warhead and any decoys included in each test would display .

nichols found anomalies when it simulated the performance of trw's software .

nichols' december 2 , 1997 , report identified anomalies that prevented the software from meeting its performance requirement in 3 of the 48 cases .

first , nichols found that a software module did not work properly .

 ( trw used this gap - filling software module to replace missing or noisy target signals. ) .

second , nichols found that the software's target selection logic did not always work well .

as a result , the probability that the software would select the simulated warhead as the target was lower than required in three of the test cases .

nichols reported inconsistencies in trw's software code .

engineers found that in some cases the software did not extract one particular feature from the target signals , and , in other cases , the results improved substantially when this feature was excluded .

the nichols report warned that in cases where this feature was the most important in the discrimination process , the software's performance could be significantly degraded .

evaluation parameters .

in its 1997 report , nichols cautioned that trw's software met performance requirements because the countermeasures included in the 48 tests were relatively simple .

nichols' testing also assumed perfect knowledge about the warhead and decoys included in the simulations .

engineers told us that all 48 test cases were constructed to test the software against the simple threats that the department of defense believed “nations of concern” might deploy before 2010 .

the engineers said that their evaluation did not include tests of the software against the number and type of decoys deployed in integrated flight test 1a because that threat cluster was more complex than the simple threat that contractors were required to design their software to handle .

in addition , nichols reported that in all 48 test cases perfect reference data was used — that is , the software was told what features the warhead and decoys would display during the simulations .

nichols engineers said trw's software is sensitive to prior knowledge about the threat and the ground based interceptor project management office was aware of this aspect of trw's design .

nichols' evaluation was limited because it did not test trw's software using actual flight data from integrated flight test 1a .

nichols told us that in addition to testing trw's discrimination software using simulated data it had also planned to assess the software's performance using real target signals collected during integrated flight test 1a .

because it did not perform this assessment , nichols can not be said to have definitively proved or disproved trw's claim that its software discriminated the mock warhead from decoys using data collected from integrated flight test 1a .

officials said they did not complete this aspect of the evaluation because their resources were limited .

however , we noted that nichols' engineers had already verified trw's software and obtained the raw target signals collected during integrated flight test 1a .

these engineers told us that this assessment could be done within two weeks after nichols received all required information .

 ( nichols said it did not have some needed information. ) .

in 1998 , the national missile defense joint program office asked the phase one engineering team to conduct an assessment , using available data , of trw's discrimination software , even though nichols research corporation had already concluded that it met the requirements established by boeing .

the program office asked for the second evaluation because the defense criminal investigative service lead investigator expressed concern about the ability of nichols to provide a truly objective evaluation .

the phase one engineering team developed a methodology to ( 1 ) determine if trw's software was consistent with scientific , mathematical , and engineering principles ; ( 2 ) determine whether trw accurately reported that its software successfully discriminated a mock warhead from decoys using data collected from integrated flight test 1a ; and ( 3 ) predict the performance of trw's basic discrimination software against integrated flight test 3 scenarios .

the key results of the team's evaluation were that the software was well designed ; the contractors accurately reported the results of integrated flight test 1a ; and the software would likely perform successfully in integrated flight test 3 .

the primary limitation was that the team used boeing - and trw - processed target data and trw - developed reference data in determining the accuracy of trw reports for integrated flight test 1a .

the team began its work by assuring itself that trw's discrimination software was based on sound scientific , engineering , and mathematical principles and that those principles had been correctly implemented .

it did this primarily by studying technical documents provided by the contractors and the program office .

next , the team began to look at the software's performance using integrated flight test 1a data .

the team studied trw's august 13 and august 22 , 1997 , test reports to learn more about discrepancies that the defense criminal investigative service said it found in these reports .

team members also received briefings from the defense criminal investigative service , boeing , trw , and nichols research corporation .

team members told us that they did not replicate trw's software in total .

instead , the team emulated critical functions of trw's discrimination software and tested those functions using data collected during integrated flight test 1a .

to test the ability of trw's software to extract the features of each target object's signal , the team designed a software routine that mirrored trw's feature - extraction design .

unlike nichols , the team did not obtain target signals collected during the test and then process those signals .

rather , the team received integrated flight test 1a target signals that had been processed by boeing and then further processed by trw .

these signals represented about one - third of the collected signals .

team members input the trw - supplied target signals into the team's feature - extraction software routine and extracted two features from each target signal .

the team then compared the extracted features to trw's reports on these same features and concluded that trw's software - extraction process worked as reported by trw .

next , the team acquired the results of 200 of the 1,000 simulations that trw had run to determine the features that target objects deployed in integrated flight test 1a would likely display .

using these results , team members developed reference data that the software could compare to the features extracted from integrated flight test 1a target signals .

finally , the team wrote software that ranked the different observed target objects in terms of the probability that each was the mock warhead .

the results produced by the team's software were then compared to trw's reported results .

the team did not perform any additional analysis to predict the performance of the boeing sensor and its software in integrated flight test 3 .

instead , the team used the knowledge that it gained from its assessment of the software's performance using integrated flight test 1a data to estimate the software's performance in the third flight test .

in its report published on january 25 , 1999 , the phase one engineering team reported that even though it noted some weaknesses , trw's discrimination software was well designed and worked properly , with only some refinement or redesign needed to increase the robustness of the discrimination function .

in addition , the team reported that its test of the software using data from integrated flight test 1a produced essentially the same results as those reported by trw .

the team also predicted that the boeing sensor and its software would perform well in integrated flight test 3 if target objects deployed as expected .

the team's assessment identified some software weaknesses .

first , the team reported that trw's use of a software module to replace missing or noisy target signals was not effective and could actually hurt rather than help the performance of the discrimination software .

second , the phase one engineering team pointed out that while trw proposed extracting several features from each target - object signal , only a few of the features could be used .

the phase one engineering team also reported that it found trw's software to be fragile because the software was unlikely to operate effectively if the reference data — or expected target signals — did not closely match the signals that the sensor collected from deployed target objects .

the team warned that the software's performance could degrade significantly if incorrect reference data were loaded into the software .

because developing good reference data is dependent upon having the correct information about target characteristics , sensor - to - target geometry , and engagement timelines , unexpected targets might challenge the software .

the team suggested that very good knowledge about all of these parameters might not always be available .

the phase one engineering team reported that the results of its evaluation using integrated flight test 1a data supported trw's claim that in post - flight analysis its software accurately distinguished a mock warhead from decoys .

the report stated that trw explained why there were differences in the discrimination analysis included in the august 13 , 1997 , integrated flight test 1a test report and that included in the august 22 , 1997 , report .

according to the report , one difference was that trw mislabeled a chart in the august 22 report .

another difference was that the august 22 discrimination analysis was based on target signals collected over a shorter period of time ( see app .

i for more information regarding trw's explanation of report differences ) .

team members said that they found trw's explanations reasonable .

the phase one engineering team predicted that if the targets deployed in integrated flight test 3 performed as expected , trw's discrimination software would successfully identify the warhead as the target .

the team observed that the targets proposed for the flight test had been viewed by boeing's sensor in integrated flight test 1a and that target - object features collected by the sensor would be extremely useful in constructing reference data for the third flight test .

the team concluded that given this prior knowledge , trw's discrimination software would successfully select the correct target even in the most stressing integrated flight test 3 scenario being considered , if all target objects deployed as expected .

however , the team expressed concern about the software's capabilities if objects deployed differently , as had happened in previous flight tests .

the phase one engineering team's conclusion that trw's software successfully discriminated is based on the assumption that boeing's and trw's input data were accurate .

the team did not process the raw data collected by the sensor's silicon detector array during integrated flight test 1a or develop their own reference data by running hundreds of simulations .

instead , the team used target signature data extracted by boeing and trw and developed reference data from a portion of the simulations that trw ran for its own post - flight analysis .

because it did not process the raw data from integrated flight test 1a or develop its own reference data , the team cannot be said to have definitively proved or disproved trw's claim that its software successfully discriminated the mock warhead from decoys using data collected from integrated flight test 1a .

a team member told us its use of boeing - and trw - provided data was appropriate because the former trw employee had not alleged that the contractors tampered with the raw test data or used inappropriate reference data .

nichols research corporation and the phase one engineering team evaluated trw's extended kalman filter feature extractor and determined that it could provide additional information to trw's discrimination software .

however , nichols research told us that its evaluation was not an exhaustive analysis of the filter's capability , but an attempt to determine if a kalman filter — which is frequently used to estimate such variables as an object's position or velocity — could extract a feature from an infrared signal .

the phase one engineering team reported that because of the limited time available to assess both trw's discrimination software and the extended kalman filter feature extractor , it did not rigorously test the filter .

its analysis was also aimed at determining whether the filter could extract a feature from target objects .

nichols engineers assessed trw's application of the kalman filter in 1996 and again in 1998 .

for both evaluations , nichols engineers constructed a stand - alone version of the filter ( the filter is comprised of mathematical formulas converted into software code ) that the engineers believed mirrored trw's design .

however , nichols designed its 1996 version of the filter from information extracted and pieced together from multiple documents and without detailed design information from trw engineers .

nichols research corporation and ground based interceptor project management office officials said the nichols' engineers did not talk with trw's engineers about the filter's design because the project office was limiting communication with the contractors in order to prevent disclosure of contractors' proprietary information during the source selection for the exoatmospheric kill vehicle .

in 1996 , nichols engineers tested the filter's ability to extract the features of simulated signals representative of threat objects .

engineers said that under controlled conditions they attempted to determine from which signals the filter could extract features successfully and from which signals it could not .

also , because the filter could not begin to extract features from the target objects unless it had some advance knowledge about the signal , engineers conducted tests to determine how much knowledge about initial conditions the filter needed .

in its november 1996 report , nichols concluded that the filter was unlikely to enhance the capability of trw's discrimination software .

the assessment showed that the filter could not extract the features of a signal unless the filter had a great deal of advance knowledge about the signal .

it also showed that the filter was sensitive to “noise” ( undesirable energy that degrades the target signal ) .

by 1998 , the competitive phase of the exoatmospheric kill vehicle contracts was over .

based on additional understanding of the filter's implementation , coupled with its proposed candidacy as an upgrade to the discrimination software , the ground based interceptor project management office asked nichols to test the filter again .

nichols engineers were now able to hold discussions with trw engineers regarding their respective filter designs .

from these discussions , nichols learned that it had designed two elements of the filter differently from trw .

the primary difference was in the number of filters that nichols and trw used to preprocess the infrared signals before the feature extraction began .

nichols' design included only one pre - processing filter , while trw's included several .

there was also one less significant difference , which was the difference in a delay time before feature extraction began .

nichols modified its version to address these differences .

in its second assessment , nichols again examined the feature extraction capability of the filter .

engineers pointed out that in both assessments the filter was tested as stand - alone software , not as an integrated part of trw's discrimination software program .

the new tests showed that the redesigned filter could perform well against the near - term threat .

however , in its report , nichols expressed reservations that unless the target and specifics of the target's deployment were well defined , the filter's performance would likely be sub - optimal .

nichols also pointed out that the filter was unlikely to perform well against targets that exhibited certain characteristics .

nichols tested the ability of the extended kalman filter feature extractor to extract features over a wide range of object dynamics and characteristics , including elements of the far - term threat .

nichols demonstrated the filter's ability to extract information ( features ) , but did not assess the filter's potential impact on the trw discrimination design .

because it did not assess the discrimination capability of the extended kalman filter , nichols could not predict how the filter would have performed against either the target complex for integrated flight test 1a or the target complex proposed for integrated flight test 3 .

target sets for integrated flight test 1a and initially proposed for integrated flight test 3 were more complex than the near - term threat that nichols tested the filter against .

in their discussions with us , nichols' engineers stressed that their assessments should be viewed as an evaluation of a technology concept , not an evaluation of a fully integrated component of the discrimination software .

engineers admitted that their approach to this assessment was less thorough than the evaluation they conducted of trw's discrimination software and that engineers did not fully understand why the additional bank of pre - processing filters improved the filter's performance .

they said a more systematic analysis would be needed to fully evaluate the filter's performance .

the national missile defense joint program office did not originally ask the phase one engineering team to evaluate trw's application of the kalman filter .

however , the team told us that program officials later asked them to do a quick assessment as an addition to their evaluation of trw's software .

team members designed an extended kalman filter feature extractor similar to trw's .

like nichols first design , the phase one engineering team's design was not identical to trw's filter .

in fact , the team did not include any filters to preprocess the infrared signals before the feature extraction began .

the phase one engineering team tested the capability of its filter against one simulated target object and one of the objects whose signal was collected during integrated flight test 1a .

the team reported that the filter did stabilize and extract the features of the objects' infrared signals .

however , the team added the caveat that the filter would need good initial knowledge about the target object before it could begin the extraction process .

the team reported that its evaluation of the filter was limited .

it said it did not evaluate the filter's sensitivity to noise , the information the filter needed to begin operation , or the extent to which the filter would improve the performance of the discrimination software .

before deciding in march 1999 not to intervene in the false claims lawsuit brought by the former trw employee , the department of justice considered scientific reports and information from two army sources .

specifically , justice relied upon evaluations of trw's software conducted by the nichols research corporation and the phase one engineering team ( see appendix ii for more information on these evaluations ) , information provided by the army space and missile defense command , and a recommendation made by the army legal services agency .

justice officials told us that the input of the space and missile defense command carried more weight in the decision - making process than the recommendation by the army legal services agency because the command is the contracting agency for the kill vehicle and is therefore more familiar with the contractors involved as well as the technical details of the lawsuit .

the army space and missile defense command was brought into this matter in response to an inquiry by the department of justice concerning the vouchers that were submitted for cost reimbursement by boeing for work performed by its subcontractor , trw .

specifically , justice asked whether the army would have paid the contractor's vouchers if boeing and trw had misrepresented the capabilities of the software in the vouchers .

in a letter to justice , dated february 24 , 1999 , the command stated that the army did not consider the vouchers submitted by boeing for trw's work to be false claims .

the letter cited the nichols' and phase one engineering team's reports as support for its conclusions and noted that a cost - reimbursement research - and - development contract only requires that the contractor exercise its “best efforts. .

there is some uncertainty about how the army legal services agencycame to recommend in february 1999 that justice not intervene in the lawsuit .

army legal services had very little documentation to explain the recommendation , and agency officials told us that they remember very little about the case .

the agency's letter stated that it was basing its recommendation on conversations with investigators handling the case and on the former trw engineer's wishes .

however , the lead investigator in the case ( from the defense criminal investigative service ) stated that he and his team had not recommended to the army that the case not proceed .

the little documentation available shows only that the case attorney's predecessor spoke with the lead investigator shortly after the case was opened .

officials said they could not remember why they cited conversations with case investigators in the letter and agreed that there were no other investigators aside from those in the defense criminal investigative service .

one official stressed that the letter does not explicitly say that the investigators recommended nonintervention .

as for the engineer's wishes , army legal services has no record of direct contacts with the engineer , and agency officials acknowledged that they probably obtained information about the engineer's wishes from justice .

agency officials also said they could not remember why they cited the engineer's wishes in their letter .

the engineer told us that she did tell justice that if it was not going to help , it should not hinder the case .

the engineer also told us that this may have been misinterpreted by the agency as a refusal of any help .

justice officials agreed that the engineer consistently wanted justice to take up the case .

legal services agency officials noted that it would be very unusual for someone not to want help from justice , especially considering that less than 10 percent of false claims cases succeed when justice is not involved .

army legal services agency officials said that the case was one of several hundred the agency handles at any one time and that their involvement in a case like this one is usually minimal , unless the agency is involved in the prosecution .

the officials stated that the army space and missile defense command letter likely would have influenced their own letter because the command's deputy counsel was recognized for his expertise in matters of procurement fraud .

they also said that they relied on justice to provide information about technical details of the case .

the case attorney stated that he had not reviewed the phase one engineering team or nichols studies .

the defense criminal investigative service , which was investigating the allegations against boeing and trw , asked the national missile defense joint program office to establish an independent panel to evaluate the capability and performance of trw's discrimination software .

although nichols research corporation , a support contractor overseeing boeing's work , had already conducted such an assessment and reported that the software met requirements , the case investigator was concerned about the ability of nichols to provide a truly objective assessment .

in response to the investigator's request , the program office utilized an existing advisory group , known as the phase one engineering team , to conduct the second assessment .

comprised of various federally funded research and development centers , this group had been established by the strategic defense initiative organization in 1988 in order to provide the program office access to a continuous , independent and objective source of technical and engineering expertise .

since federally funded research and development centers are expressly authorized , established and operated to provide the government with independent and objective advice , the joint program office officials determined that making use of such a group would be sufficient to assure an independent and objective review .

scientific associations , however , said that there are alternative ways of choosing a panel to review contentious issues .

nonetheless , program officials said that establishing a review team using such methods would likely have increased the time the reviewers needed to complete their work and could have increased the cost of the review .

when the national missile defense joint program office determined that it should undertake a review of the trw discrimination software because of allegations that contractors had misrepresented their work , it turned to the phase one engineering team .

the phase one engineering team was established in 1988 by the strategic defense initiative organization — later known as the ballistic missile defense organization — as an umbrella mechanism to obtain technical and engineering support from federally funded research and development centers .

to ensure that the individual scientists who work on each review undertaken by the phase one engineering team have the requisite expertise , membership on each review team varies with each assignment .

when asked to advise a program , the director of the phase one engineering team determines which federally funded research and development centers have the required expertise .

the director then contacts officials at those centers to identify the appropriate scientists for the task .

according to the director , the national missile defense joint program office does not dictate the individuals who work on a phase one engineering team review .

when the director received the request to conduct a review of trw's discrimination software , he determined there were three federally funded research and development centers best suited to undertake this review .

a total of five scientists were then selected from these three centers to comprise the review team: one member from the aerospace corporation , sponsored by the u.s. air force ; two members from the massachusetts institute of technology's lincoln laboratory , also sponsored by the u.s. air force ; and two members from the lawrence livermore national laboratory , sponsored by the department of energy .

the federal government established the federally funded research and development centers to meet special or long - term research or development needs of the sponsoring federal government agencies that were not being met effectively by existing in - house or contractor resources .

the federal government enters into long - term relationships with the federally funded research and development centers in order to encourage them to provide the continuity that allows them to attract high quality personnel who will maintain their expertise , retain their objectivity and independence , preserve familiarity with the government's needs , and provide a quick response capability .

to achieve these goals , the federally funded research and development centers must have access , beyond that required in normal contractual relationships with the government , to government and supplier information , sensitive or proprietary data , and to employees and facilities .

because of this special access , the federally funded research and development centers are required by the federal acquisition regulation and agreements with their sponsoring agencies to operate in the public interest with objectivity and independence , to be free from organizational conflicts of interest , and to fully disclose their affairs to the sponsoring agency .

to further ensure that they are free from organizational conflicts of interest , federally funded research and development centers are operated , managed , and / or administered by a university or consortium of universities ; other not - for - profit or non - profit organization ; or an industrial firm , as an autonomous organization or as an identifiable separate operating unit of a parent organization .

all three of the federally funded research and development centers involved in this review had entered into sponsoring agreements and contracts with their respective sponsoring agencies that contain the requirements imposed on such centers by the federal acquisition regulation .

for example , the sponsoring agreement between the air force and lincoln laboratory requires that lincoln laboratory avoid any action that would put its personnel in perceived or actual conflicts of interest regarding either unfair competition or objectivity .

joint program office officials said they relied upon adherence to the governing regulations and sponsoring agreements to assure themselves that the members of this review team could provide a fresh , unbiased look at trw's software .

officials with whom we spoke expressed confidence in the team's independence .

justice officials said that they had no reason to doubt the objectivity or independence of the review team's members nor the seriousness and thoroughness of their effort .

the phase one engineering team director told us that independence is a program goal and that their reviews report the technical truth regardless of what the national missile defense joint program office might want to hear .

the director noted that the best way to ensure independence is to have the best scientists from different organizations discuss the technical merits of an issue .

at your request , we spoke with officials of the national academy of sciences and the american physical society who told us that there are alternative ways to choose a panel .

one method commonly used by these scientific organizations , which frequently conduct studies and evaluate reports or journal articles , is peer review .

according to a gao report that studied federal peer review practices , peer review is a process wherein scientists with knowledge and expertise equal to that of the researchers whose work they review make an independent assessment of the technical or scientific merit of that research .

according to the phase one engineering team director , the evaluation performed by the team assigned to review trw's software was a type of peer review .

however , national academy of sciences and american physical society officials told us that since individuals knowledgeable in a given area often have opinions or biases , an unbiased study team should include members who would , as a group , espouse a broad spectrum of opinions and interests .

such a team should include both supporters and critics of the issue being studied .

these officials told us that it was their opinion that the phase one engineering team members are “insiders” who are unlikely to be overly critical of the national missile defense program .

the national missile defense joint program office official who requested that the phase one engineering team conduct such a review said that he could have appointed a panel such as that suggested by the national academy of sciences and the american physical society .

but he said that he wanted a panel that was already knowledgeable about warhead discrimination in space and required little additional knowledge to complete its review .

the official noted that the team's report was originally intended to be a one - to - two - month effort , even though it eventually took about eight months to complete .

some additional time was required to address further issues raised by the defense criminal investigative service .

a team member said that the statement of work was defined so that the panel could complete the evaluation in a timely manner with the data available .

officials of the national academy of sciences and the american physical society acknowledged that convening a panel such as the type they suggested would likely have required more time and could have been more costly .

the table below includes selected requirements that boeing established before the flight test to evaluate sensor performance and the actual sensor performance characteristics that boeing and trw discussed in the august 22 report .

we determined whether boeing and trw disclosed key results and limitations of integrated flight test 1a to the national missile defense joint program office by examining test reports submitted to the program office on august 13 , 1997 , august 22 , 1997 , and april 1 , 1998 , and by examining the december 11 , 1997 , briefing charts .

we also held discussions with and examined various reports and documents prepared by boeing north american , anaheim , california ; trw inc. , redondo beach , california ; the raytheon company , tucson , arizona ; nichols research corporation , huntsville , alabama ; the phase one engineering team , washington , d.c. ; the massachusetts institute of technology / lincoln laboratory , lexington , massachusetts ; the national missile defense joint program office , arlington , virginia , and huntsville , alabama ; the office of the director , operational test and evaluation , washington d.c. ; the u.s. army space and missile defense command , huntsville , alabama ; the defense criminal investigative service , mission viejo , california , and arlington , virginia ; and the institute for defense analyses , alexandria , virginia .

we held discussions with and examined documents prepared by dr. theodore postol , massachusetts institute of technology , cambridge , massachusetts ; dr. nira schwartz , torrance , california ; and mr. roy danchick , santa monica , california .

in addition , we hired the utah state university space dynamics laboratory , logan , utah , to examine the performance of the boeing sensor because we needed to determine the effect the higher operating temperature had on the sensor's performance .

as agreed with your offices , we did not replicate trw's assessment of its software using target signals that the boeing sensor collected during the test .

this would have required us to make engineers and computers available to verify trw's software , format raw target signals for input into the software , develop reference data , and run the data through the software .

we did not have these resources available , and we , therefore , cannot attest to the accuracy of trw's discrimination claims .

we examined the methodology , key results , and limitations of evaluations completed by nichols research corporation and the phase one engineering team by analyzing nichols' report on trw's discrimination software dated december 1997 ; nichols' reports on the extended kalman filter dated november 1996 and november 1998 ; and the phase one engineering team's “independent review of trw discrimination techniques” dated january 1999 .

in addition , we held discussions with the nichols engineers and phase one engineering team members that conducted the assessments and with officials from the national missile defense joint program office .

we did not replicate the evaluations conducted by nichols and the phase one engineering team and cannot attest to the accuracy of their reports .

we examined the basis for the department of justice's decision not to intervene in the false claims lawsuit by holding discussions with and examining documents prepared by the department of justice , washington , d.c. we also held discussions with and reviewed documents at the u.s. army legal services agency , arlington , virginia , and the u.s. army space and missile defense command , huntsville , alabama .

we reviewed the national missile defense joint program office's efforts to address potential conflicts of interest that an expert panel might have in reviewing the results of integrated flight test 1a by holding discussions with national missile defense joint program office officials and with members of the expert panel , known as the phase one engineering team .

we also examined the federal regulations and support agreements agreed to by the federally funded research and development centers and national laboratory that employed the panel members .

last , as you requested , we discussed alternative methods of establishing an expert panel with the american physical society , ridge , new york ; and the national academy of sciences' national research council , washington , d.c. our work was conducted from august 2000 through february 2002 according to generally accepted government auditing standards .

the length of time the national missile defense joint program office required to release documents to us significantly slowed our review .

for example , the program office required approximately 4 months to release key documents such as nichols 1997 evaluation of trw's discrimination software and nichols 1996 and 1998 evaluations of the extended kalman filter feature extractor .

we requested these and other documents on september 14 , 2000 , and received them on january 9 , 2001 .

