securing the 6,000 miles of international borders that the contiguous united states shares with canada and mexico is a challenge and a mission imperative to the department of homeland security ( dhs ) .

although hundreds of thousands of illegal aliens are prevented from entering the country each year , many more are not detected .

to enhance border security and reduce illegal immigration , dhs launched its multiyear , multibillion dollar secure border initiative ( sbi ) program in november 2005 .

through sbi , dhs intends to enhance surveillance technologies , raise staffing levels , increase domestic enforcement of immigration laws , and improve the physical infrastructure along the nation's borders .

within sbi , secure border initiative network ( sbinet ) is a multibillion dollar program that includes the acquisition , development , integration , deployment , and operation and maintenance of surveillance technologies to create a “virtual fence” along the border , as well as command , control , communications , and intelligence ( c3i ) technologies to create a picture of the border in command centers and vehicles .

managed by dhs's customs and border protection ( cbp ) , sbinet is intended to strengthen the ability of cbp to detect , identify , classify , track , and respond to illegal breaches at and between land ports of entry .

in september 2008 , we reported that sbinet was at risk because of a number of acquisition management weaknesses , and we made recommendations to address them that dhs largely agreed with and committed to addressing .

because of the importance , high cost , and challenges facing sbinet , you subsequently asked us to continue to review dhs's management of sbinet .

as agreed , our objectives were to determine the extent to which dhs has ( 1 ) defined the scope of its proposed system solution , ( 2 ) developed a reliable schedule for delivering this solution , ( 3 ) demonstrated the cost - effectiveness of this solution , ( 4 ) acquired this solution in accordance with key life cycle management processes , and ( 5 ) addressed our recent recommendations .

to accomplish our objectives , we largely focused on the first increment of sbinet known as block 1 .

in doing so , we reviewed key program documentation , including guidance , plans , schedules , cost estimates , and artifacts related to system life cycle events , requirements , risks , and testing .

we also analyzed a random probability sample of requirements and their related verification methods .

in addition , we interviewed program officials about sbinet cost and schedule estimates , program commitments , the development and implementation of the sbinet system life cycle approach , requirements development and management , test management , and risk management .

we then compared this information to relevant federal guidance , leading industry practices , and the recommendations in our september 2008 report on sbinet to identify any deviations and interviewed program officials as to the reasons for any deviations .

to assess the reliability of the data that we relied on to support the findings in the report , we reviewed relevant program documentation to substantiate evidence obtained through interviews with knowledgeable agency officials , where available .

we determined that the data used in this report are sufficiently reliable .

we have also made appropriate attribution indicating the sources of the data used .

we conducted this performance audit from december 2008 to may 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are in appendix i .

sbinet includes the acquisition , development , integration , deployment , and operations and maintenance of a mix of surveillance technologies , such as cameras , radars , sensors , and c3i technologies .

the initial focus of sbinet has been on addressing the requirements of cbp's office of border patrol , which is responsible for securing the borders between the land ports of entry .

longer term , sbinet is to address requirements of cbp's office of field operations , which controls vehicle and pedestrian traffic at the ports of entry , and its office of air and marine operations , which operates helicopters , fixed - wing aircraft , and marine vessels used in securing the borders .

 ( see fig .

1 for the potential long - term sbinet concept of operations. ) .

surveillance technologies are to include a variety of sensor systems .

specifically , unattended ground sensors are to be used to detect heat and vibrations associated with foot traffic and metal associated with vehicles .

radar mounted on fixed and mobile towers is to detect movement , and cameras on fixed and mobile towers are to be used by operators to identify and classify items of interest detected and tracked by ground sensors and radar .

aerial assets are also to be used to provide video and infrared imaging to enhance tracking targets .

these technologies are generally to be acquired through the purchase of commercial off - the - shelf ( cots ) products .

c3i technologies ( software and hardware ) are to produce a common operating picture ( cop ) — a uniform presentation of activities within specific areas along the border .

together , the sensors , radar , and cameras are to gather information along the border and transmit this informatio cop terminals located in command centers and agents' vehicles , which in turn are to assemble it to provide cbp agents with border situational awareness .

among other things , cop hardware and software are to allow agents to ( 1 ) view data from radar and sensors that detect and track movement in the border areas , ( 2 ) control cameras to help identify and classify illegal entries , ( 3 ) correlate entries wit h the positions of nearby agents , and ( 4 ) enhance tactical decision making regarding the appropriate r esponse to apprehend an entry , if necessary .

to increase border security and decrease illegal immigration , dhs launched sbi more than 4 years ago after canceling its america's shield initiative program .

since fiscal year 2006 , dhs has received about $4.4 billion in appropriations for sbi , including about $2.5 billion for physical fencing and related infrastructure , about $1.5 billion for virtual fencing ( surveillance systems ) and related technical infrastructure ( towers ) , and about $300 million for program management .

the sbi program ex office , which is organizationally within cbp , is responsible for managing key acquisition functions associated with sbinet , including prime contractor tracking and oversight .

it is organized into four components: sbinet system program office ( referred to as the spo in this report ) , systems engineering , business management , and operational integration as of dece mber 31 , 2009 , the sbi program executive office was staffed with 188 people — 87 government employees , 78 contractor staff , and 13 detailees .

 .

in september 2006 , cbp awarded a 3-year prime contract to the boeing company , with three additional 1-year options for designing , producing , testing , deploying , and sustaining sbi .

in 2009 , cbp exercised the first option year .

under this contract , cbp has issued 10 task orders tha to sbinet , covering for example , cop design and development , system deployment , and system maintenance and logistics support .

as of december 2009 , 4 of the 10 task orders had been completed and 6 were ongoing .

 ( see table 1 for a summary of the sbinet task orders. ) .

one of the completed task orders is for an effort known as project 28 , which is a prototype system that covers 28 miles of the border in cbp's tucson sector in arizona , and has been operating since february 2008 .

however , its completion took 8 months longer than planned because of problems in integrating system components ( eg , cameras and radars ) with the cop software .

as we have reported , these problems were attributable to , among other things , limitations in requirements development and contractor oversight .

through the task orders , cbp's strategy is to deliver sbinet capabilities incrementally .

to accomplish this , the spo has adopted an evolutionary system life cycle management approach in which system capabilities are to be delivered to designated locations in a series of discrete subsets of system functional and performance capabilities that are referred to as blocks .

the first block , which has been designated as block 1 , includes the purchase of commercially available surveillance systems , development of customized cop systems and software , and use of existing cbp communications and network capabilities .

such an incremental approach is a recognized best practice for acquiring large - scale , complex systems because it allows users access to new capabilities and tools sooner , and thus permits both their early operational use and evaluation .

subsequent increments of sbinet capabilities are to be delivered based on feedback and unmet requirements , as well as the availability of new technologies .

in general , the sbinet life cycle management approach consists of four primary work flow activities: ( 1 ) planning activity , ( 2 ) system block activity , ( 3 ) project laydown activity , and ( 4 ) sustainment activity .

during the planning activity , the most critical user needs are to be identified and balanced against what is affordable and technologically available .

the outcome of this process is to be a set of capability requirements that are to be acquired , developed , and deployed as a specific block .

this set of capabilities , once agreed to by all stakeholders , is then passed to the system block activity , during which the baseline system solution to be fielded is designed and built .

also as part of this activity , the verification steps are to be conducted on the individual system components and the integrated system solution to ensure that they meet defined requirements .

the project laydown activity is performed to configure the block solution to a specific geographic area's unique operational characteristics .

this activity involves assessing the unique threats , terrain , and environmental concerns associated with a particular area , incorporating these needs into the system configuration to be deployed to that area , obtaining any needed environmental permits , and constructing the infrastructure and installing the configured system .

it also involves test and evaluation activities , including system acceptance testing , to verify that the installed block system was built as designed .

the final activity , sustainment , is focused on the operations and maintenance of the deployed block solution and supporting the user community .

associated with each of these activities are various milestone or gate reviews .

for example , a key review for the system block activity is the critical design review ( cdr ) .

at this review , the block design and requirements are baselined and formally controlled to approve and track any changes .

among other things , this review is to verify that the block solution will meet the stated requirements within the program's cost and schedule commitments .

an important review conducted during the project laydown activity is the deployment design review .

at this review , information such as the status of environmental reviews and land acquisitions for a specific geographic area is assessed , and the location - specific system configuration is determined .

the deployment readiness review is another key event during this activity .

during this review , readiness to begin site preparation and construction is assessed .

in addition to the four above described workflow activities are various key life cycle management processes , such as requirements development and management , risk management , and test management .

requirements development and management , among other things , involves defining and aligning a hierarchy of five types of sbinet requirements .

these five types begin with high - level operational requirements and are followed by increasingly more detailed lower - level requirements , to include system , component , c3i / cop software , and design requirements .

to help it manage the requirements , the spo relies on boeing's use of a database known as the dynamic object - oriented requirements system ( doors ) .

the various types of sbinet requirements are described in table 2 .

risk management entails taking proactive steps to identify and mitigate potential problems before they become actual problems .

the spo has defined a “risk” to be an uncertain event or condition that , if it occurs , will have a negative effect on at least one program objective , such as schedule , cost , scope , or technical performance .

the spo has defined an “issue” as a risk that has been realized ( i.e. , a negative event or condition that currently exists or has a 100 percent future certainty of occurring ) .

according to sbinet's risk management process , anyone involved in the program can identify a risk .

identified risks are submitted to the risk management team , which includes both the spo risk manager and boeing risk manager , for preliminary review .

if approved for further consideration , the risk is entered into the boeing - owned risk database , which is accessible by spo and boeing officials .

these risks are subsequently reviewed by the joint risk review board , which is composed of approximately 20 spo and boeing officials .

if a risk is approved , it is to be assigned an owner who will be responsible for managing its mitigation .

test management involves planning , conducting , documenting , and reporting on a series of test events that first focus on the performance of individual system components , then on the performance of integrated system components , followed by system - level tests that focus on whether the system ( or major system increments ) are acceptable and operationally suitable .

for sbinet , the program's formal test events fall into two major phases: developmental test and evaluation ( dt&e ) and operational test and evaluation ( ot&e ) .

dt&e is to verify and validate the systems engineering process and provide confidence that the system design solution satisfies the desired capabilities .

it consists of four test events — integration testing , component qualification testing , system qualification testing , and system acceptance testing .

ot&e is to ensure that the system is effective and suitable in its operational environment with respect to key considerations , including reliability , availability , compatibility , and maintainability .

sbinet defines three operational testing events — user assessment , operational test , and follow - on operational test and evaluation .

 ( see table 3 for each test event's purpose , responsible parties , and location. ) .

as of december 2009 , the program was in the project laydown activity .

specifically , the sbinet cdr was completed in october 2008 , and the block 1 design has been configured and is being tested and readied for deployment to the tucson border patrol station ( tus - 1 ) , and then to the ajo border patrol station ( ajo - 1 ) , both of which are located in the cbp's tucson sector of the southwest border .

more specifically , the deployment design review covering both tus - 1 and ajo - 1 was completed in june 2007 , the tus - 1 deployment readiness review was completed in april 2009 , and the ajo - 1 deployment readiness review was completed in december 2009 .

together , these two deployments are to cover 53 miles of the 1,989-mile - long southern border ( see fig .

2 ) .

once a deployed configuration has been accepted and is operational , the program will be in the sustainment activity .

as of november 2009 , program documentation showed that tus - 1 and ajo - 1 were to be accepted in january and july 2010 , respectively .

however , the sbi executive director told us in december 2009 that these and other sbinet scheduled milestones are currently being re - evaluated .

as of february 2010 , tus - 1 and ajo - 1 were proposed to be accepted in september 2010 and november 2010 , respectively .

however , this proposed schedule has yet to be approved by cbp .

since 2007 , we have identified a range of management weaknesses and risks facing sbinet and we have made a number of recommendations to address them that dhs has largely agreed with and , to varying degrees , taken actions to address .

for example , in february 2007 , we reported that dhs had not fully defined activities , milestones , and costs for implementing the program ; demonstrated how program activities would further the strategic goals and objectives of sbi ; and reported on the costs incurred , activities , and progress made by the program in obtaining operational control of the border .

further , we reported that the program's schedule contained a high level of concurrency among related tasks and activities , which introduced considerable risk .

accordingly , we recommended that dhs define explicit and measurable commitments relative to , among other things , program capabilities , schedules , and costs , and re - examine the level of concurrency in the schedule and adjust the acquisition strategy appropriately .

we are currently reviewing dhs's fiscal year 2010 sbi expenditure plan to , among other things , determine the status of dhs's actions to address these recommendations .

in october 2007 , we testified that dhs had fallen behind in implementing project 28 due to software integration problems , although program officials stated at that time that boeing was making progress in correcting the problems .

shortly thereafter , we testified that while dhs had accepted project 28 , it did not fully meet expectations .

to benefit from this experience , program officials stated that they identified a number of lessons learned , including the need to increase input from border patrol agents and other users in sbinet design and development .

in september 2008 , we reported that important aspects of sbinet were ambiguous and in a continued state of flux , making it unclear and uncertain what technological capabilities were to be delivered when .

we concluded that the absence of clarity and stability in key aspects of sbinet impaired the ability of congress to oversee the program and hold dhs accountable for results , and hampered dhs's ability to measure program performance .

as a result , we recommended that the spo establish and baseline the specific program commitments , including the specific system functional and performance capabilities that are to be deployed , and when they were to be deployed .

also , we reported that the spo had not effectively performed key requirements definition and management practices .

for example , it had not ensured that different levels of requirements were properly aligned , as evidenced by our analysis of a random probability sample of component requirements showing that a large percentage of them could not be traced to higher - level system and operational requirements .

also , some of sbinet's operational requirements , which are the basis for all lower - level requirements , were found by an independent dhs review to be unaffordable and unverifiable , thus casting doubt on the quality of lower - level requirements that were derived from them .

as a result of these limitations , we concluded that the risk of sbinet not meeting mission needs and performing as intended was increased , as were the chances of the program needing expensive and time - consuming system rework .

we recommended that the spo implement key requirements development and management practices to include ( 1 ) baselining requirements before system design and development efforts begin ; ( 2 ) analyzing requirements prior to being baselined to ensure that that they are complete , achievable , and verifiable ; and ( 3 ) tracing requirements to higher - level requirements , lower - level requirements , and test cases .

we also reported that sbinet testing was not being effectively managed .

for example , the spo had not tested the individual system components to be deployed to the initial deployment locations , even though the contractor had initiated integration testing of these components with other system components and subsystems .

further , while a test management strategy was drafted , it had not been finalized and approved , and it did not contain , among other things , a clear definition of testing roles and responsibilities ; a high - level master schedule of sbinet test activities ; or sufficient detail to effectively guide project - specific test planning , such as milestones and metrics for specific project testing .

we concluded that without a structured and disciplined approach to testing , the risk that sbinet would not satisfy user needs and operational requirements , thus requiring system rework , was increased .

we recommended that the spo ( 1 ) develop and document test practices prior to the start of testing ; ( 2 ) conduct appropriate component - level testing prior to integrating system components ; and ( 3 ) approve a test management strategy that , at a minimum , includes a relevant testing schedule , establishes accountability for testing activities by clearly defining testing roles and responsibilities , and includes sufficient detail to allow for testing and oversight activities to be clearly understood and communicated to test stakeholders .

in light of these weaknesses and risks , we further recommended that ( 1 ) the risks associated with planned sbinet acquisition , development , testing , and deployment activities be immediately assessed and ( 2 ) the results , including proposed alternative courses of action for mitigating the risks , be provided to the cbp commissioner and dhs's senior leadership , as well as to the department's congressional authorization and appropriations committees .

dhs agreed with all but one of the recommendations in our september 2008 report .

the status of dhs's efforts to implement these recommendations is summarized later in this report and discussed in detail in appendix iii .

in september 2009 , we reported that sbinet had continued to experience delays .

for example , deployment to the entire southwest border had slipped from early 2009 to 2016 , and final acceptance of tus - 1 and ajo - 1 had slipped from november 2009 and march 2010 to december 2009 and june 2010 , respectively .

we did not make additional sbinet recommendations at that time .

most recently , we reported in january 2010 that sbinet testing was not being effectively managed .

specifically , while dhs's approach to testing appropriately consisted of a series of progressively expansive developmental and operational test events , the test plans , cases , and procedures for the most recent test events were not defined in accordance with important elements of relevant guidance .

for example , none of the plans adequately described testing risks and only two of the plans included quality assurance procedures for making changes to test plans during their execution .

further , a relatively small percentage of test cases for these events described the test inputs and the test environment ( eg , facilities and personnel to be used ) , both of which are essential to effective testing .

in addition , a large percentage of the test cases for these events were changed extemporaneously during execution .

while some of the changes were minor , others were more significant , such as re - writing entire procedures and changing the mapping of requirements to test cases .

moreover , these changes to procedures were not made in accordance with documented quality assurance processes , but rather were based on an undocumented understanding that program officials said they established with the contractor .

compounding the number and significance of changes were questions raised by the spo and a support contractor about the appropriateness of some changes .

for example , the spo wrote to the prime contractor that changes made to system qualification test cases and procedures appeared to be designed to pass the test instead of being designed to qualify the system .

further , we reported that from march 2008 through july 2009 , that about 1,300 sbinet defects had been found , with the number of new defects identified during this time generally increasing faster than the number being fixed — a trend that is not indicative of a system that is maturing and ready for deployment .

while the full magnitude of these unresolved defects was unclear because the majority were not assigned a priority for resolution , some of the defects that had been found were significant .

although dhs reported that these defects had been resolved , they had nevertheless caused program delays , and related problems had surfaced that continued to impact the program's schedule .

further , an early user assessment of sbinet had raised significant concerns about the performance of key system components and the system's operational suitability .

in light of these weaknesses , we recommended that dhs ( 1 ) revise the program's overall test plan to include ( a ) explicit criteria for assessing the quality of test documentation , including test plans and test cases , and ( b ) a process for analyzing , prioritizing , and resolving program defects ; ( 2 ) ensure that test schedules , plans , cases , and procedures are adequately reviewed and approved consistent with the revised test plan ; ( 3 ) ensure that sufficient time is provided for reviewing and approving test documents prior to beginning a given test event ; and ( 4 ) triage the full inventory of unresolved system problems , including identified user concerns , and periodically report on their status to cbp and dhs leadership .

dhs fully agreed with the last three recommendations and partially agreed with the first .

for block 1 , functional and performance capabilities and the number of geographic locations to which they are to be deployed have continued to decrease .

we reported in september 2008 that the capabilities and deployment locations of sbinet were decreasing .

since that time , the number of component - level requirements to be deployed to tus - 1 and ajo - 1 has decreased by about 32 percent .

in addition , the number of sectors that the system is to be deployed to has been reduced from three to two , and the stringency of the system performance measures that the deployed system is to meet has been reduced .

according to program officials , the decreases are due to poorly defined requirements and limitations in the capabilities of commercially available system components .

the result will be a deployed and operational system that , like project 28 , does not live up to user expectations and provides less mission support than was envisioned .

since our september 2008 report , the number of requirements that block 1 is to meet has dropped considerably .

specifically , in september 2008 , dhs directed the spo to identify the operational requirements to be allocated to block 1 .

in response , 106 operational requirements were established , such as providing border surveillance , facilitating decision support and situational awareness , enabling communications , providing operational status and readiness metrics , and enabling system audits .

of the 106 requirements , 69 were to be included in the initial technology deployments planned for tus - 1 and ajo - 1 .

the remaining 37 were to be addressed in future blocks .

to implement the 69 operational requirements , the spo developed a system - level requirement specification and 12 component - level requirements specifications .

more specifically , as part of cdr , which concluded in october 2008 , the 69 operational requirements for tus - 1 and ajo - 1 were associated with 97 system - level requirements .

also during cdr , the 97 system - level requirements were associated with 1,286 component - level requirements .

however , between october 2008 and september 2009 , the number of component - level requirements was reduced from 1,286 to 880 , or by about 32 percent .

first , 281 requirements related to the specifications for three components — communications , network operations , and network security — were eliminated , leaving 1,005 baselined requirements .

examples of the 281 requirements that were eliminated include the following: the failure in a single piece of hardware or software would not affect mission critical functions which include detection and resolution of border incursions ; the failure of a network operations center / security operations center ( noc / soc ) workstation would not prevent the system from operating ; and the failure of one network power supply would be compensated for by additional backup power supplies .

in addition , another 125 component - level requirements were granted “waivers” or “deviations,” further reducing the number of block 1 requirements to be deployed to tus - 1 and ajo - 1 to 880 ( as of september 2009 ) .

for example , the unattended ground sensors were required to differentiate between human , vehicle , and animal targets .

however , because the sensors that are to be deployed to tus - 1 and ajo - 1 are only able to identify potential vehicles and are not able to differentiate between humans and animals , this requirement was deviated .

similarly , the radar was required to classify targets as humans or vehicles .

however , the radar also cannot differentiate between classes of targets ( eg , humans and vehicles ) .

as a result , the requirement in the radar specification was also deviated .

figure 3 summarizes the roughly 32 percent drop in requirements that has occurred over the last 15 months .

according to program officials , component requirements were eliminated because they were either poorly written or duplicative of other requirements , or because the capabilities of commercially available products were limited .

in addition , they attributed a significant number of eliminated requirements to a decision to not use a boeing designed and developed network and instead to use an existing dhs network .

to the spo's credit , this decision was made to align sbinet with dhs technical standards and to increase the use of cots products .

compounding this reduction in block 1 requirements is the likelihood that further requirements deviations and waivers will be granted based on the results of an early user assessment of the system .

according to the july 2009 assessment report , certain sbinet components did not meet requirements .

for example: the daytime cameras were judged to be operationally ineffective over 5 kilometers for identifying humans , while the requirement is that the cameras be usable to 10 kilometers .

the laser range finder was determined to have an effective range of less than 2 kilometers , while the requirement is for the effective range to be 10 kilometers .

program officials told us that many of the limitations found during the user assessment were previously known , and corrective actions were already under way or planned for future technology upgrades to address them .

however , the officials also stated they plan to issue a waiver or deviation for the camera and the laser range finder to address the two problems discussed above .

in addition , they stated that a previously known limitation of the range of the radar will also need to be addressed through a deviation .

in this case , the radar is required to have a range of 20 kilometers , but testing shows a maximum range of 10 kilometers .

beyond the requirement reductions , the geographic locations to receive the initial sbinet capabilities have also been reduced .

as of september 2008 , the initial block 1 deployment was to span three border patrol sectors: tucson , yuma , and el paso — a total of 655 miles .

according to program officials , deployment to these three areas was the expressed priority of the border patrol due to the high threat levels in these areas .

however , the acquisition program baseline , which was drafted in december 2008 , states that initial deployment will be to just the tucson and yuma sectors , which will cover only 387 miles .

according to program officials , deployment to the 268 miles of the el paso sector was dropped from the initial deployment in anticipation that the sector will instead receive the capabilities slated for the next sbinet increment ( i.e. , build ) .

however , plans for the next increment have not been developed .

according to the sbi executive director in december 2009 , the spo is re - evaluating where and when future deployments of sbinet will occur , and a date for when the revised deployment plans will be available has not been set .

system performance measures define how well a system is to perform certain functions , and thus are important in ensuring that the system meets mission and user needs .

according to program documentation , failure to meet a key performance parameter can limit the value of the system and render it unsuccessful .

in november 2008 , the spo re - evaluated its existing sbinet key performance parameters and determined that sbinet must meet three such parameters: ( 1 ) the probability of detecting items of interest between the border and the control boundary ; ( 2 ) the probability of correctly identifying items of interest as human , conveyance , or others ; and ( 3 ) the operational availability of the system .

according to program officials , subject matter experts and cbp staff concluded that these three were critical to determining whether the system successfully meets mission and user needs .

associated with each parameter is a threshold for acceptable performance .

in november 2008 , the spo re - evaluated the thresholds for its three key performance parameters , and it significantly relaxed each of the thresholds: the threshold for detecting items of interest dropped from 95 percent to 70 percent .

the threshold for identifying items of interest declined from 95 percent to 70 percent .

the threshold for operational availability decreased from 95 to 85 percent .

these threshold reductions significantly lower what constitutes acceptable system performance .

for example , the system will meet its detection and identification performance requirements if it identifies 70 percent of the 70 percent of items that it detects , thus producing a 49 percent probability of identifying items of interest that cross the border .

furthermore , the reduction in operational availability means that the time that the system can be unavailable for use has gone from 18.25 days per year to 54.75 days per year — or from approximately 2.5 weeks to about 7 weeks per year , excluding downtime for planned maintenance .

the sbi executive director attributed the performance reductions to program officials' limited understanding of needed operational capabilities at the time the parameters and thresholds were set .

the director further stated that once block 1 has been deployed and border patrol personnel gain experience operating it , decisions will be made as to what additional changes to make to the key performance parameters and associated thresholds .

until then , system performance relative to identifying items of interest and operational availability will remain as described above , which program officials agreed fall short of expectations .

the success of a large - scale system acquisition program like sbinet depends in part on having a reliable schedule of when the program's set of work activities and milestone events will occur , how long they will take , and how they are related to one another .

among other things , a reliable schedule provides a road map for systematic execution of a program and the means by which to gauge progress , identify and address potential problems , and promote accountability .

our research has identified nine best practices associated with developing and maintaining a reliable schedule .

these are ( 1 ) capturing all activities , ( 2 ) sequencing all activities , ( 3 ) assigning resources to all activities , ( 4 ) establishing the duration of all activities , ( 5 ) integrating activities horizontally and vertically , ( 6 ) establishing the critical path for all activities , ( 7 ) identifying reasonable float between activities , ( 8 ) conducting a schedule risk analysis , and ( 9 ) updating the schedule using logic and durations .

to be considered reliable , a schedule should meet all nine practices .

the august 2009 sbinet integrated master schedule , which was the most current version available for our review , is not reliable because it substantially complies with only two of the nine key schedule estimating practices and it does not comply with , or only partially or minimally complies with , the remaining seven practices ( see table 4 for a summary and app .

iv for the detailed results of our analysis of the extent to which the schedule meets each of the nine practices ) .

examples of practices that were either substantially , partially , minimally , or not met are provided below .

without having a reliable schedule , it is unlikely that actual program execution will track to plans , thus increasing the risk of cost , schedule , and performance shortfalls .

capturing all activities: the schedule does not capture all activities as defined in the program's work breakdown structure or integrated master plan .

first , 57 percent of the activities listed in the work breakdown structure ( 71 of 125 ) and 67 percent of the activities listed in the integrated master plan ( 46 of 69 ) were not in the integrated master schedule .

for example , the schedule is missing efforts associated with systems engineering , sensor towers , logistics , system test and evaluation , operations support , and program management .

second , the schedule does not include key activities to be performed by the government .

for example , while the schedule shows the final activity in the government process for obtaining an environmental permit in order to construct towers , it does not include the related government activities needed to obtain the permit .

sequencing all activities: the schedule identifies virtually all of the predecessor and successor activities .

specifically , only 9 of 1,512 activities ( less than 1 percent ) were missing predecessor links .

further , only 21 of 1,512 activities ( about 1 percent ) had improper predecessor and successor links .

while the number of unlinked activities is very small , not linking a given activity can cause problems because changes to the durations of these activities will not accurately change the dates for related activities .

more importantly , 403 of 1,512 activities ( about 27 percent ) are constrained by “start no earlier than” dates , which is significant because it means that these activities are not allowed to start earlier , even if their respective predecessor activities have been completed .

establishing the critical path for all activities: the schedule does not reflect a valid critical path for several reasons .

first , and as noted above , it is missing government and contractor activities , and is thus not complete .

second , as mentioned above , the schedule is missing some predecessor links , and improperly establishes other predecessor and successor links .

problems with the critical path were recognized by the defense contract management agency as early as november 2008 , when it reported that the contractor could not develop a true critical path that incorporates all program elements .

conducting a schedule risk analysis: an analysis of the schedule's vulnerability to slippages in the completion of tasks has not been performed .

further , program officials described the schedule as not sufficiently stable to benefit from a risk analysis .

reasons that these practices were not fully met vary and include the program's use of boeing to develop and maintain the integrated master schedule , even though boeing's processes and tools do not allow it to include in the schedule work that it does not have under contract to perform , as well as the constantly changing nature of the work to be performed .

without a reliable schedule that includes all activities necessary to complete block 1 , the spo cannot accurately determine the amount of time required to complete block 1 , and it does not have an adequate basis for guiding the program's execution and measuring progress , thus reducing the likelihood of meeting the program's completion dates .

collectively , the weaknesses in meeting the nine key practices for the program's integrated master schedule increase the risk of schedule slippages and related cost overruns and make meaningful measurement and oversight of program status and progress , as well as accountability for results , difficult to achieve .

in the case of block 1 , this risk has continued to be realized .

for example , the dates presented at the december 2008 to november 2009 monthly program review meetings for government acceptance of block 1 at tus - 1 and ajo - 1 showed a pattern of delays , with tus - 1 and ajo - 1 acceptance slipping by 4 months and 7 months , respectively .

 ( see fig .

4. ) .

moreover , these slipped dates have not been met , and the sbi executive director told us in december 2009 that when block 1 will be accepted and operational continues to change and remains uncertain .

as of february 2010 , tus - 1 and ajo - 1 were proposed to be accepted in september 2010 and november 2010 , respectively ; however , this proposed schedule has yet to be approved by cbp .

as we have previously reported , the decision to invest in any system or major system increment should be based on reliable estimates of costs and meaningful forecasts of quantifiable and qualitative benefits over the system's useful life .

for block 1 , dhs does not have a complete and current life cycle cost estimate .

moreover , it has not projected the mission benefits expected to accrue from block 1 over the same life cycle .

according to program officials , it is premature to project such benefits given the uncertainties surrounding the role that block 1 will ultimately play in overall border control operations .

without a meaningful understanding of sbinet costs and benefits , dhs lacks an adequate basis for knowing whether the initial system solution on which it plans to spend at least $1.3 billion is cost - effective .

moreover , dhs and congressional decision makers continue to lack a basis for deciding what investment in sbinet beyond this initial capability is economically prudent .

a reliable cost estimate is critical to successfully delivering large - scale information technology ( it ) systems , like sbinet , as well as major system increments , like block 1 .

such an estimate provides the basis for informed investment decision making , realistic budget formulation , meaningful progress measurement , and accountability for results .

according to the office of management and budget ( omb ) , federal agencies must maintain current and well - documented estimates of program costs , and these estimates must encompass the program's full life cycle .

among o things , omb states that a reliable life cycle cost estimate is critical to the capital planning and investment control process .

without such an estimate , agencies are at increased risk of making poorly informed investment decisions and securing insufficient resources to effectively execute defined program plans and schedules , and thus experiencing program cost , schedule , and performance shortfalls .

our research has identified a number of practices that form the basis of effective program cost estimating .

these practices are aligned with four characteristics of a reliable cost estimate .

to be reliable , a cost estimate should possess all four characteristics , each of which is summarized below .

 ( see app .

v for the key practices associated with each characteristic , including a description of each practice and our analysis of the extent to which the sbinet cost estimate meets each practice. ) .

comprehensive: the cost estimate should include all government and contractor costs over the program's full life cycle , from program inception through design , development , deployment , and operation and maintenance to retirement .

it should also provide sufficient detail to ensure that cost elements are neither omitted nor double counted , and it should document all cost - influencing ground rules and assumptions .

well - documented: the cost estimate should capture in writing things such as the source and significance of the data used , the calculations performed and their results , and the rationale for choosing a particular estimating method or reference .

moreover , this information should be captured in such a way that the data used to derive the estimate can be traced back to , and verified against , their sources .

finally , the cost estimate should be reviewed and accepted by management to demonstrate confidence in the estimating process and the estimate .

accurate: the cost estimate should not be overly conservative or optimistic , and should be , among other things , based on an assessment of most likely costs , adjusted properly for inflation , and validated against an independent cost estimate .

in addition , the estimate should be updated regularly to reflect material changes in the program and actual cost experience on the program .

further , steps should be taken to minimize mathematical mistakes and their significance and to ground the estimate in documented assumptions and a historical record of actual cost and schedule experiences on comparable programs .

credible: the cost estimate should discuss any limitations in the analysis due to uncertainty or biases surrounding the data and assumptions .

major assumptions should be varied and other outcomes computed to determine how sensitive the estimate is to changes in the assumptions .

risk and uncertainty inherent in the estimate should be assessed and disclosed .

further , the estimate should be properly verified by , for example , comparing the results with one or more independent cost estimates .

the spo's block 1 life cycle cost estimate includes the costs to complete those portions of block 1 that are to be deployed to the tucson and yuma sectors , which together cover about 387 miles of the southwest border ( 53 miles associated with both tus - 1 and ajo - 1 , which are in the tucson sector , as well as an additional 209 miles in the tucson sector and 125 miles in the yuma sector ) .

more specifically , this estimate , which is dated december 2008 , shows the minimum cost to acquire and deploy block 1 to the tucson and yuma sectors to be $758 million , with another $544 million to operate and maintain this initial capability , for a total of about $1.3 billion .

however , this block 1 cost estimate is not reliable because it does not sufficiently possess any of the above four characteristics .

specifically: the estimate is not comprehensive because it does not include all relevant costs , such as support contractor costs and costs associated with system and software design , development , and testing activities that were incurred prior to december 2008 .

moreover , it includes only 1 year of operations and maintenance costs rather than these costs over the expected life of the system .

further , the estimate does not document and assess the risks associated with all ground rules and assumptions , such as known budget constraints , staff and schedule variations , and technology maturity .

the estimate is not well - documented because , among other things , the sources and significance of key data have not been captured and the quality of key data , such as historical costs and actual cost reports , is limited .

for example , instead of identifying and relying on historical costs from similar programs , the estimate was based , in part , on engineering judgment .

further , the calculations performed and their results , while largely documented , did not document contingency reserves and the associated confidence level for the risk - adjusted cost estimate .

also , as noted above , assumptions integral to the estimate , such as those for budget constraints , and staff and schedule variances , were not documented .

the estimate is not accurate because it was not , for example , validated against an independent cost estimate .

further , it has not been updated to reflect material program changes since the estimate was developed .

for example , the estimate does not reflect development and testing activities that were added since the estimate was approved to correct problems discovered during testing .

further , the estimate has not been updated with actual cost data available from the contractor .

the estimate is not credible because its inherent risk and uncertainty were not adequately assessed , and thus the estimate does not address limitations associated with the assumptions used to create it .

for example , the risks associated with software development were not examined , even though such risks were known to exist .

in fact , the only risks considered were those associated with uncertainty in labor rates and hardware costs , and instead of being based on historical quantitative analyses , these risks were expressed by assigning them arbitrary positive or negative percentages .

in addition , and for the reasons mentioned above , the estimate did not specify contingency reserve amounts to mitigate known risks , and an independent cost estimate was not used to verify the estimate .

program officials attributed these limitations in the cost estimate's comprehensiveness , documentation , accuracy , and credibility to a range of factors , including competing program office priorities and the department's limited cost estimating capabilities .

for example , program officials stated that the dhs cost analysis division did not prepare an independent estimate because it did not have , among other things , the people and tools needed to do so .

in this regard , this division reports that as of july 2009 , dhs only had eight cost estimators ( six in headquarters and two in program offices ) for departmentwide needs .

because the estimate does not adequately display these four characteristics , it does not provide a reliable picture of block 1's life cycle costs .

as a result , dhs does not have complete information on which to base informed investment decision making , understand system affordability , and develop justifiable budget requests .

moreover , the block 1 cost estimate does not provide a meaningful standard against which to measure cost performance , is likely to show large cost overruns , and does not provide a good basis for informing future cost estimates .

the clinger - cohen act of 1996 and omb guidance emphasize the need to ensure that it investments actually produce tangible , observable improvements in mission performance .

as we have previously reported,to accomplish this , benefits that are expected to accrue from investments need to be forecast and their actual accrual needs to be measured .

in the case of block 1 , however , expected mission benefits have not been defined and measured .

for example , while program officials told us that system benefits are documented in the sbinet mission need statement dated october 2006 , this document does not include either quantifiable or qualitative benefits .

rather , it provides general statements such as “the lack of a program such as sbinet increases the risks of terrorist threats and other illegal activities.” congress recognized the importance of having a meaningful understanding of sbinet's value proposition when it required dhs in 2008 to provide in its border security , fencing , infrastructure , and technology fiscal year 2009 expenditure plan a description of how the department's planned expenditure of funds would be linked to expected sbi mission benefits and outcomes .

however , we reported that the plan dhs submitted only described links among planned activities , expenditures , and outputs .

it did not link these to outcomes associated with improving operational control of the border .

more recently , we reported that while sbi technology and physical infrastructure , along with increases in border patrol personnel , are intended to allow dhs to gain effective control of u.s. borders , cbp's measures of effective control are limited .

thus , we recommended that cbp conduct a cost - effectiveness evaluation of the sbi tactical infrastructure's impact on effective control of the border , and dhs agreed with this recommendation .

further , program officials noted that uncertainty about sbinet's role in and contribution to effective control of the border makes it difficult to forecast sbinet benefits .

rather , they said that operational experience with block 1 is first needed in order to estimate such benefits .

while we recognize the value of operationally evaluating an early , prototypical version of a system in order to better understand , among other things , its mission impact , and thus to better inform investment decisions , we question the basis for spending in excess of a billion dollars to gain this operational experience .

without a meaningful understanding and disclosure of sbinet benefits , to include the extent to which expected mission benefits are known and unknown , dhs did not have the necessary basis for justifying and making informed decisions about its sizeable investment in block 1 , as well as for measuring the extent to which the deployed block 1 will actually deliver mission value commensurate with costs .

successful management of large it programs , like sbinet , depends in large part on having clearly defined and consistently applied life cycle management processes .

our evaluations and research show that applying system life cycle management rigor and discipline increases the likelihood of delivering expected capabilities on time and within budget .

in other words , the quality of a system is greatly influenced by the quality of the processes used to manage it .

to the spo's credit , it has defined key life cycle management processes that are largely consistent with relevant guidance and associated best practices .

however , it has not effectively implemented these processes .

specifically , it has not consistently followed its systems engineering plan , requirements development and management plan , and risk management approach .

reasons cited by program officials for not implementing these processes include the decision by program officials to rely on contract task order requirements that were developed prior to the systems engineering plan , and competing spo priorities , including meeting an aggressive deployment schedule .

until the spo consistently implements these processes , it will remain challenged in its ability to successfully deliver sbinet .

each of the steps in a life cycle management approach serves an important purpose and has inherent dependencies with one or more other steps .

in addition , the steps used in the approach should be clearly defined and repeatable .

thus , if a life cycle management step is omitted or not performed effectively , later steps can be affected , potentially resulting in costly and time - consuming rework .

for example , a system can be effectively tested to determine whether it meets requirements only if these requirements have been completely and correctly defined .

to the extent that interdependent life cycle management steps or activities are not effectively performed , or are performed concurrently , a program will be at increased risk of cost , schedule , and performance shortfalls .

the spo's systems engineering plan documents its life cycle management approach for sbinet definition , development , testing , deployment , and sustainment .

as noted earlier , we reported in september 2008 on a number of weaknesses in the sbinet life cycle management approach and made recommendations to improve it.in response , the spo revised its systems engineering plan in november 2008 , and to its credit , the revised plan is largely consistent with dhs and other relevant guidance .

for example , it defines a number of key life cycle milestone or “gate” reviews that are important in managing the program , such as initial planning reviews , requirements reviews , system design reviews , and test reviews .

in addition , the revised plan requires most of the key artifacts and program documents that dhs guidance identified as important to each gate review , such as a concept of operations , an operational requirements document , a deployment plan , a risk management plan , a life cycle cost estimate , requirements documentation , and test plans .

to illustrate , the plan identifies cdr as the important milestone event where a design baseline is to be established , requirements traceability is to be demonstrated , and verification and testing plans are to be in place .

however , the systems engineering plan does not address the content of the key artifacts that it requires .

for example , it does not provide a sample document or content template for the concept of operations , the operational requirements document , or the deployment plan .

as a result , the likelihood of the developers and reviewers of these artifacts sharing and applying a consistent and repeatable understanding of their content is minimized , thus increasing the risk that they will require costly and time - consuming rework .

as we recently reported , the absence of content guidance or criteria for assessing the quality of the prime contractor's test - related deliverables was a primary reason that limitations were found in test plans .

beyond the content of the systems engineering plan , the spo has not consistently implemented key system life cycle management activities for block 1 that are identified by the plan .

for example , the following artifacts were not reviewed or considered during the cdr that concluded in october 2008: security test plan , which describes the process for assessing the robustness of the system's security capabilities ( eg , physical facilities , hardware , software , and communications ) in light of their vulnerabilities .

quality plan , which documents the process for verifying that the contractor deliverables satisfy contractual requirements and meet or exceed quality standards .

test plan , which describes the overall process for the test and evaluation , including the development of detailed test event plans , test procedure instructions , data collection methods , and evaluation reports .

block training plan , which outlines the objectives , strategy , and curriculum for training that are specific to each block , including the activities needed to support the development of training materials , coordination of training schedules , and reservation of personnel and facilities .

block maintenance plan , which lays out the policies and concepts to be used to maintain the operational availability of hardware and software .

to the spo's credit , it reviewed and considered all but one of the key artifacts for the tus - 1 deployment readiness review that concluded in april 2009 .

the omitted artifact was the site specific training plan , which outlines the objectives , strategy , and curriculum for training that are specific to each geographic site , including the activities needed to support the development of training materials , coordination of training schedules , and reservation of personnel and facilities .

according to program officials , even though the systems engineering plan cites the training plan as integral to the deployment readiness review , this training plan is to be reviewed as part of a later milestone review .

program officials stated that a reason that the artifacts were omitted is that they have yet to begin implementing the systems engineering plan .

instead , they have , for example , enforced the cdr requirements in the system task order that boeing was contractually required to follow .

to address this , they added that the spo intends to bring the task orders into alignment with the systems engineering plan , but they did not specify when this would occur .

as a result , key milestone reviews and decisions have not always benefited from life cycle management documentation that the spo has determined to be relevant and important to these milestone events .

more specifically , the systems engineering plan states that the gate reviews are intended to identify and address problems early and thus minimize future costs and avoid subsequent operational issues .

by not fully informing these gate reviews and associated decisions with key life cycle management documentation , the risk of block 1 design and deployment problems is increased , as is the likelihood of expensive and time - consuming system rework .

well - defined and managed requirements are essential to successfully acquiring large - scale systems , like sbinet .

according to relevant guidance,effective requirements development and management include establishing a baseline set of requirements that are complete , unambiguous , and testable .

it also includes ensuring that system - level requirements are traceable backwards to higher - level operational requirements and forward to design requirements and the methods used to verify that they are met .

among other things , this guidance states that such traceability should be used to verify that higher - level requirements have been met by first verifying that the corresponding lower - level requirements have been satisfied .

however , not all block 1 component requirements were sufficiently defined at the time that they were baselined , and operational requirements continue to be unclear and unverifiable .

in addition , while requirements are now largely traceable backwards to operational requirements and forward to design requirements and verification methods , this traceability has not been used until recently to verify that higher - level requirements have been satisfied .

program officials attributed these limitations to competing spo priorities , including aggressive schedule demands .

without ensuring that requirements are adequately defined and managed , the risks of block 1 not performing as intended , not meeting user needs , and costing more and taking longer than necessary to complete are increased .

the sbinet requirements development and management plan states that a baseline set of requirements should be established by the time of the cdr and that these requirements should be complete , unambiguous , and testable .

further , the program's systems engineering plan states that the cdr is intended to establish the final allocated requirements baseline and ensure that system development , integration , and testing can begin .

to the spo's credit , it established a baseline set of requirements for the tus - 1 and ajo - 1 system deployments at cdr .

however , the baseline requirements associated with the noc / soc were not adequately defined at this time , as evidenced by the fact that they were significantly changed 2 months later .

specifically , about 33 percent of the component - level requirements and 43 percent of the design specifications for noc / soc were eliminated from the block 1 design after cdr .

program officials attributed these changes to the noc / soc requirements to ( 1 ) requirements that were duplicative of another specification , and thus were redundant ; ( 2 ) requirements that were poorly written , and thus did not accurately describe needs ; and ( 3 ) requirements that related to the security of a system that sbinet would not interface with , and thus were unnecessary .

according to program officials , the noc / soc was a late addition to the program , and at the time of cdr , the component's requirements were known to need additional work .

further , they stated that while the requirements were not adequately baselined at the time of cdr , the interface requirements were understood well enough to begin system development .

without properly baselined requirements , system testing challenges are likely to occur , and the risk of system performance shortfalls , and thus cost and schedule problems , are increased .

in this regard , we recently reported that noc / soc testing was hampered by incorrect mapping of requirements to test cases , failure to test all of the requirements , and significant changes to test cases made during the testing events .

this occurred in part because ambiguities in requirements caused testers to rewrite test steps during execution based on interpretations of what they thought the requirements meant , and they required the spo to conduct multiple events to test noc / soc requirements .

according to the sbinet requirements development and management plan , requirements should be achievable , verifiable , unambiguous , and complete .

to ensure this , the plan contains a checklist that is to be used in verifying that each requirement possesses these characteristics .

however , not all of the sbinet operational requirements that pertain to block 1 possess these characteristics .

specifically , a november 2007 dhs assessmentdetermined that 19 operational requirements , which form the basis for the lower - level requirements used to design and build the system , were not complete , achievable , verifiable , or affordable .

further , our analysis of the 12 block 1 requirements that are included in these 19 operational requirements shows that they have not been changed to respond to the dhs findings .

according to the assessment , 6 of the 12 were unaffordable and unverifiable , and the other 6 were incomplete .

examples of these requirements and dhs's assessment follow: a requirement that the system should provide for complete coverage of the border was determined to be unverifiable and unaffordable because defining what complete coverage meant was too difficult and ensuring complete coverage , given the varied and difficult terrain along the border , was cost prohibitive .

a requirement that the system should be able to detect and identify multiple simultaneous events with different individuals or groups was determined to be incomplete because the requirement did not specify the number of events to be included , the scope of the area to be covered , and the system components to be involved .

as we have previously reported,these limitations in the operational requirements affect the quality of system , component , and software requirements .

this is significant because , as of september 2009 , these 12 operational requirements were associated with 16 system - level requirements , which were associated with 152 component - level requirements , or approximately 15 percent of the total number of component - level requirements .

according to program officials , these requirements were not updated because the spo planned to resolve the problems through the testing process .

however , we recently reported that requirements limitations actually contributed to testing challenges .

specifically , we reported that about 71 percent of combined system qualification and component qualification test cases had to be rewritten extemporaneously during test execution .

according to program officials , this was partly due to ambiguities in requirements , which led to differing opinions among the program and contractor staff about what was required to effectively demonstrate that the requirements were met .

further , program officials stated that a number of requirements have been granted deviations or waivers because they were poorly written .

for example: a requirement for camera equipment to “conform to the capabilities and limitations of the users to operate and maintain it in its operational environment and not exceed user capabilities” was determined to be subjective and unquantifiable and thus was waived .

a requirement for the tower design to accommodate the future integration of components “without causing impact on cost , schedule , and / or technical performance” was determined to have no specific criteria to objectively demonstrate closure decision and thus was also waived .

as a result of these deviations and waivers , the system capabilities that are to be delivered as part of block 1 will be less than originally envisioned .

consistent with relevant guidance,the sbinet requirements development and management plan provides for maintaining bidirectional traceability from high - level operational requirements through detailed low - level requirements to test plans .

more specifically , it states that operational requirements should trace to system requirements , which in turn should trace to component requirements that trace to design requirements , which further trace to v erification methods .

since september 2008 , the spo has worked with boeing to manually review each requirement and develop a bidirectional traceability matrix .

further , it has used this matrix to update the doors requirements database .

our analysis of the traceability of a random sample of block 1 component - level requirements in the doors database shows that they are largely traceable backwards to operational requirements and forward to design requirements and verification methods .

for example , we estimate that only 5 percent ( with a 95 percent confidence interval between 1 and 14 percent ) of a random sample of component requirements cannot be traced to the system requirements and then to the operational requirements .

in addition , we estimate that 0 percent ( with a 95 percent confidence interval between 0 and 5 percent ) of the component requirements in the same sample do not trace to a verification method .

 ( see table 5 for the results of our analysis along with the associated confidence intervals. ) .

by establishing this traceability , the spo is better positioned to know the extent to which the acquired and deployed system can meet operational requirements .

however , the spo has not used its requirements traceability in closing higher - level component requirements .

according to relevant guidance,all lower - level requirements ( i.e. , children ) should be closed in order to sufficiently demonstrate that the higher - level requirements ( i.e. , parents ) have been met .

consistent with this guidance , the sbinet requirements development and management plan states that ensuring the traceability of requirements from children to their parents is an integral part of ensuring that testing is properly planned and conducted .

however , 4 of 8 higher - level component requirements ( parents ) in the above cited random sample of system - level requirements were closed regardless of whether their corresponding lower - level design requirements ( children ) had been closed .

according to program officials , this is because their standard practice in closing parent requirements , until recently , was to sometimes close them before their children were closed .

further , they said that this was consistent with their verification criteria for closing higher - level requirements , which did not require closure of the corresponding lower - level requirements .

they also said that the reason parent verification criteria did not always reflect children verification criteria was that traceability was still being established when the verification criteria were developed and thus parent - child relationships were not always available to inform the closure criteria .

furthermore , they stated that schedule demands did not permit them to ensure that the verification criteria for requirements were aligned with the traceability information .

after we shared our findings on parent requirement closure with the spo , officials stated that they had changed their approach and will no longer close parent requirements without ensuring that all of the children requirements have first been closed .

however , they did not commit to reviewing previously closed parents to determine that all of the children were closed .

without fully ensuring traceability among requirements verification methods , the risks of delivering a system solution that does not fully meet user needs or perform as intended , and thus requires additional time and resources to deliver , are increased .

risk management is a continuous , forward - looking process that effectively anticipates and mitigates risks that may have a critical impact on a program's success .

in 2008 , the spo documented a risk management approach that largely complies with relevant guidance .

however , it has not effectively implemented this approach for all risks .

moreover , available documentation does not demonstrate that significant risks were disclosed to dhs and congressional decision makers in a timely fashion , as we previously recommended and , while risk disclosure to dhs leadership has recently improved , not all risks have been formally captured and thus shared .

as a result , the program will likely continue to experience actual cost , schedule , and performance shortfalls , and key decision makers will continue to be less than fully informed .

according to relevant guidance , effective risk management includes defining a process that , among other things , proactively identifies and analyzes risks on the basis of likelihood of occurrence and impact , assigns ownership , provides for mitigation , and monitors status .

to the spo's credit , it has developed an approach for risk management that is largely consistent with this guidance .

for example , the approach provides for continuously identifying risks throughout the program's life cycle before they develop into actual problems , including suggested methods for doing so , such as conducting brainstorming sessions and interviewing subject matter experts ; analyzing identified risks to determine their likelihood of occurring and assigning responsibility for risks ; developing a risk mitigation plan , to include a set of discrete , measurable actions or events which , if successfully accomplished , can avoid or reduce the likelihood of occurrence or severity of impact of the risk ; and executing and regularly monitoring risk mitigation plans to ensure that they are implemented and to allow for corrective actions if the desired results are not being achieved .

in february 2007 , we reported that the program's risk management approach was in the process of being established .

specifically , we noted that at that time the spo had drafted a risk management plan , established a governance structure , developed a risk management database , and identified 30 risks .

in april 2009 , we reported that the dhs chief information officer had certified that this approach provided for the regular identification , evaluation , mitigation , and monitoring of risks throughout the system life cycle , and that it provided for communicating high - risk conditions to dhs investment decision makers .

the spo has not adhered to key aspects of its defined process for managing program risks .

in particular , the program's risk management repository , which is the tool used for capturing and tracking risks and their mitigation , has not included key risks that have been identified by stakeholders .

for example , our analysis of reports from the repository showing all open and closed risks from april 2006 to september 2009 shows that the following program risks that have been identified by us and others were not captured in the repository: program cost and schedule risks briefed by the spo to senior sbinet officials in january 2009 , such as unplanned and unauthorized work impacting the credibility of the program cost data , and program costs and schedule plans lacking traceability ; program schedule and cost estimate risks identified by the defense contract management agency prior to march 2009 , such as contractor - provided documentation not permitting adequate assessment of critical path accuracy , and cost projections not including all applicable elements and thus lacking credibility ; and the risk of the spo's heavy reliance on contractors , reported by the dhs office of inspector general in june 2009 .

in addition , the sbi executive director told us that the program faces a number of other risks , all but one of which were also not in the repository .

these include the lack of well - defined acquisition management processes , staff with the appropriate acquisition expertise , and agreement on key system performance parameters .

according to program officials , some of these risks are not in the repository because boeing is responsible for operating and maintaining the repository , and the specifics surrounding the risks and their mitigation are considered acquisition sensitive , meaning that they should not be shared with boeing .

in this regard , the officials acknowledged that the spo needs a risk database independent of the contractor to manage these acquisition - sensitive risks .

further , the risk manager identified other limitations that have hindered the spo's risk management efforts , along with recent actions intended to address them .

for example: risk review meetings were only being held once a month , which was resulting in lost opportunities to mitigate risks that were to be realized as actual problems within 30 days .

as a result , the frequency of these meetings has been increased to twice a month .

risk information provided to senior sbi managers at monthly joint program management review meetingswas not sufficiently detailed , and thus has been expanded .

changes were being made to the risk management repository by contractor staff without sufficient justification and without the approval of the joint risk review board .

for example , program officials cited an instance in which a risk's severity was changed from medium to high and no board member knew the reason for the change .

as a result , the number of contractor staff authorized to modify data in the repository was reduced .

the repository did not include all requisite information for all identified risks .

for example , some risks were missing the rationale for the likelihood of occurrence and the potential impact .

as a result , the joint risk review board has adopted a policy of not accepting risks that are missing requisite information .

according to the risk manager , competing program priorities have resulted in insufficient resources devoted to risk management activities , which has contributed to the state of the spo's risk management efforts .

however , he added that the spo is taking steps to improve risk management by revising risk management guidance , implementing a cbp - approved database tool for managing government - only risks , and increasing risk management training and oversight .

until the program's risk management is strengthened and effectively implemented , the program will continue to be challenged in its ability to forestall cost , schedule , and performance problems .

as noted earlier , we recommended in september 2008 that the spo assess sbinet risks and that the results of these assessments , along with alternative courses of action to address them , be provided to dhs leadership and congressional committees .

according to program officials , shortly after receiving our draft report they briefed the dhs acquisition review board on , among other things , sbinet risks .

however , the briefing slides used for this meeting do not identify individual risks .

instead , the briefing contains one slide that only identifies “contributing factors” to changes in the program's schedule , including a reallocation sbinet funding to sbi physical infrastructure , concurrencies and delays that have occurred in testing , and the need for environmental studies .

the slides do not identify risks and alternative courses of action to address or mitigate them .

in addition , program officials told us that they briefed congressional committees during the fall of 2008 on the program's status , which they said included disclosure of program risks .

however , they did not have any documentation of these briefings to show which committees were briefed , when the briefings occurred , who was present , and what was discussed and disclosed .

further , house committee on homeland security staff stated that while program officials briefed them following our september 2008 report , specific program risks were not disclosed .

as a result , it does not appear that either dhs or congressional stakeholders received timely information on risks facing the program at a crucial juncture in its life cycle .

to the spo's credit , it has recently improved its disclosure of risks facing the program .

in particular , the sbi executive director briefed the dhs chief information officer in november 2009 on specific program risks .

however , this briefing states that the risks presented were the block 1 risks as captured in the contractor's risk repository and that additional risks have not yet been formalized ( see above discussion about repository limitations ) .

until all key risks are formally managed and regularly disclosed to department and congressional stakeholders , informed sbinet investment decision making will be constrained .

as noted earlier , we reported on a number of sbinet program management weaknesses in september 2008 , and we concluded that these weaknesses introduced considerable risk that the program would not meet expectations and would require time - consuming and expensive rework .

in summary , these problems included a lack of clarity and certainty surrounding what technological capabilities would be delivered when , and a lack of rigor and discipline around requirements definition and management and test management .

to address these problems and thereby reduce the program's exposure to cost , schedule , and performance risks , we made eight recommendations .

dhs concurred with seven of the recommendations and disagreed with one aspect of the remaining one .

in summary , the department has not implemented two of the recommendations and has partially implemented the remaining six .

see table 6 for a summary and appendix iii for a detailed discussion of the status of each recommendation .

dhs has yet to demonstrate that its proposed sbinet solution is a cost - effective course of action , and thus whether the considerable time and money being invested to acquire and deploy it is a wise and prudent use of limited resources .

given that the magnitude of the initial investment in sbinet spans more than 3 years of effort and totals hundreds of millions of dollars , coupled with the fact that the scope of the initial system's capabilities and areas of deployment have continued to shrink , the program is fraught with risk and uncertainty .

as a result , the time is now for dhs to thoughtfully reconsider its proposed sbinet solution , and in doing so , to explore ways to both limit its near - term investment in an initial set of operational capabilities and develop and share with congressional decision makers reliable projections of the relative costs and benefits of longer - term alternatives for meeting the mission goals and outcomes that sbinet is intended to advance , or reasons why such information is not available and the uncertainty and risks associated with not having it .

compounding the risks and uncertainty surrounding whether the department is pursuing the right course of action are a number of system life cycle management concerns , including limitations in the integrated master schedule ; shortcomings in the documentation available to inform key milestone decisions ; and weaknesses in how requirements have been developed and managed , risks have been managed , and tests have been conducted .

collectively , these concerns mean that the program is not employing the kind of acquisition management rigor and discipline needed to reasonably ensure that proposed system capabilities and benefits will be delivered on time and on budget .

because of sbinet's decreased scope , uncertain timing , unclear costs relative to benefits , and limited life cycle management discipline and rigor , in combination with its size and mission importance , the program represents a risky undertaking .

to minimize the program's exposure to risk , it is imperative for dhs to move swiftly to first ensure that sbinet , as proposed , is the right course of action for meeting its stated border security and immigration management goals and outcomes , and once this is established , for it to move with equal diligence to ensure that it is being managed the right way .

to this end , our prior recommendations to dhs relative to sbinet provide for strengthening a number of life cycle management processes , including requirements development and management and test management .

accordingly , we are not making additional recommendations that focus on these processes at this time .

to address the considerable risks and uncertainties facing dhs on its sbinet program , we are making 12 recommendations .

specifically , we recommend that the secretary of homeland security direct the commissioner of u.s. customs and border protection to limit future investment in the program to only work that meets one or both of the following two conditions: ( 1 ) is already under contract and supports deployment , acceptance , and operational evaluation of only those block 1 capabilities ( functions and performance levels ) that are currently targeted for tus - 1 and ajo - 1 ; or ( 2 ) provides the analytical basis for informing a departmental decision as to what , if any , expanded investment in sbinet , both in terms of capabilities ( functions and performance ) and deployment locations , represents a prudent , responsible , and affordable use of resources for achieving the department's border security and immigration management mission .

with respect to the first condition , we further recommend that the secretary of homeland security direct the commissioner of u.s. customs and border protection to have the sbi executive director make it a program priority to ensure that the integrated master schedule for delivering block 1 capabilities to tus - 1 and ajo - 1 is revised to address the key schedule estimating practices discussed in this report ; the currently defined block 1 requirements , including key performance parameters , are independently validated as complete , verifiable , and affordable and any limitations found in the requirements are addressed ; the systems engineering plan is revised to include or reference documentation templates for key artifacts required at milestone gate reviews ; all parent requirements that have been closed are supported by evidence of the closure of all corresponding and associated child requirements ; and all significant risks facing the program are captured , mitigated , tracked , and periodically reported to dhs and congressional decision makers .

also with respect to the first condition , we reiterate our prior recommendations , as stated in our september 2008 report,relative to establishing program commitments , implementing the systems engineering plan , defining and managing requirements , and testing .

with respect to the second condition , we further recommend that the secretary of homeland security direct the commissioner of u.s. customs and border protection to have the sbi executive director make it a program priority to ensure that a life cycle cost estimate for any incremental block of sbinet capabilities that is to include capabilities and cover locations beyond those associated with the tus - 1 and ajo - 1 deployments is developed in a manner that reflects the four characteristics of a reliable estimate discussed in this report ; a forecast of the qualitative and quantitative benefits to be derived from any such incremental block of sbinet over its useful life , or reasons why such forecasts are not currently possible , are developed and documented ; the estimated life cycle costs and benefits and associated net present value of any such incremental block of sbinet capabilities , or reasons why such an economic analysis cannot be performed , are prepared and documented ; and the results of these analyses , or the documented reasons why such analyses cannot be provided , are provided to the commissioner of u.s. customs and border protection and the dhs acquisition review board .

also with respect to this second condition , we recommend that the secretary of homeland security direct the deputy secretary of homeland security , as the chair of the dhs acquisition review board , to ( 1 ) decide , in consultation with the board and commissioner of u.s. customs and border protection , what , if any , expanded investment in sbinet , both in terms of capabilities ( functions and performance ) and deployment locations , represents a prudent , responsible , and affordable use of resources for achieving the department's border security and immigration management mission ; and ( 2 ) report the decision , and the basis for it , to the department's authorization and appropriations committees .

in written comments on a draft of this report , signed by the director , departmental gao / office of inspector general liaison , and reprinted in appendix ii , dhs stated that it agreed with ten of our recommendations and partially agreed with the remaining two .

in this regard , it described ongoing and planned actions to address each , and it provided milestones for completing these actions .

in addition , dhs provided technical comments , which we have incorporated in the report as appropriate .

in agreeing with our first recommendation , however , dhs commented that the words “one of” were omitted before the two conditions contained in the recommendation .

however , this interpretation is not correct .

rather , the intent of our recommendation is to limit future investment on the program to either of the conditions , meaning “one or both of.” notwithstanding dhs's interpretation , we believe that actions that it described to address this recommendation , which include freezing funding beyond the initial deployments to tus - 1 and ajo - 1 until it completes a comprehensive reassessment of the program that includes an analysis of the cost and mission effectiveness of alternative technologies , is consistent with the intent of the recommendation .

nevertheless , we have slightly modified the recommendation to avoid any further confusion .

regarding its partial agreement with our recommendation for revising the integrated master schedule in accordance with a range of best practices embodied in our cost and schedule estimating guide , dhs acknowledged the merits of employing these practices and stated that it is committed to adopting and deploying them .

however , it added that the current contract structure limits its ability to fully implement all the practices prior to completing the tus - 1 and ajo - 1 deployments .

we understand that program facts and circumstances create practical limitations associated with some of the practices , and believe that dhs's planned actions are consistent with the intent of our recommendation .

regarding its partial agreement with our recommendation that reiterated a number of the recommendations that we made in a prior report , dhs stated that , while these prior recommendations reflect program management best practices and it continues to make incremental improvements to address each , the scope of the program had narrowed since these recommendations were made .

as a result , dhs stated that these prior recommendations were not fully applicable until and unless a decision was made to move the program forward and conduct future deployments beyond tus - 1 and ajo - 1 .

we acknowledge that the facts and circumstances surrounding the program have recently changed and that these changes impact the nature and timing of actions appropriate for implementing them .

moreover , we believe that dhs's planned actions are consistent with the intent of our recommendation .

dhs also commented that it believed that it had implemented two of our recommendations and that these recommendations should be closed .

because closure of our recommendations requires evidentiary validation of described actions , and because many of the actions that dhs described were planned rather than completed , we are not closing any of our recommendations at this time .

as part of our recurring review of the status of all of our open recommendations , we will determine if and when the recommendations have been satisfied and thus can be closed .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to interested congressional committees and other parties .

we will also send copies to the secretary of homeland security , the commissioner of the u.s. customs and border protection , and the director of the office of management and budget .

in addition , this report will be available at no cost on the gao web site at http: / / www.gao.gov .

should you or your offices have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix vi .

our objectives were to determine the extent to which the department of homeland security ( dhs ) has ( 1 ) defined the scope of its proposed secure border initiative network ( sbinet ) solution , ( 2 ) developed a reliable schedule for delivering this solution , ( 3 ) demonstrated the cost - effectiveness of this solution , ( 4 ) acquired this solution in accordance with key life cycle management processes , and ( 5 ) addressed our recent sbinet recommendations .

to accomplish our objectives , we largely focused on the first increment of sbinet , known as block 1 .

to determine the extent to which dhs has defined the scope of its proposed system solution , we reviewed key program documentation related to the block 1 functional and performance requirements and deployment locations , such as the sbinet acquisition program baseline and related acquisition decision memorandums , the operational requirements document , the operational requirements document elements applicable to block 1 system , the requirements traceability matrix , the requirements verification matrix , and the sbinet block 1 user assessment .

in addition , we compared block 1 requirements that were baselined in october 2008 as part of the critical design review ( cdr ) to the block 1 requirements as defined as of september 2009 to identify what , if any , changes had occurred , and we interviewed program officials as to the reasons for any changes .

we also compared the locations , including the miles of border associated with these locations , that were to receive block 1 as of september 2008 to the locations specified in the program's march 2009 acquisition program baseline to identify any changes , and we interviewed program officials as to the reasons for any changes .

further , we compared the key performance parameters listed in the operational requirements document , dated march 2007 , to the key performance parameters in the program's acquisition program baseline dated march 2009 .

to determine the extent to which dhs has developed a reliable schedule for its proposed system solution , we analyzed the sbinet integrated master schedule as of june 2009 against the nine key schedule estimating practices in our cost estimating and assessment guide .

in doing so , we used commercially available software tools to determine whether it , for example , included all critical activities , a logical sequence of activities , and reasonable activity durations .

further , we observed a demonstration of the schedule in june 2009 provided by contractor officials responsible for maintaining the schedule and program officials responsible for overseeing the contractor .

in july 2009 , we observed a demonstration of the program office's efforts to reconcile the version of the integrated master schedule that is exported for the government's use with the version of the schedule that the prime contractor uses to manage the program .

during this demonstration , we discussed some of our concerns regarding the integrated master schedule with program officials and we inquired about deviations from some of the key practices .

subsequently , the program office provided us with a revised version of the integrated master schedule as of august 2009 , which we analyzed .

in doing so , we repeated the above described steps .

further , we characterized the extent to which the revised schedule met each of the practices as either not met , minimally met , partially met , substantially met , or met .

in addition , we analyzed changes in the scheduled block 1 deployment dates presented at each of the monthly program reviews for the 1-year period beginning in december 2008 and ending in november 2009 .

to determine the extent to which dhs has demonstrated the cost - effectiveness of the proposed solution , we evaluated the reliability of the block 1 life cycle cost estimate and the definition of expected system benefits , both of which are addressed below .

cost estimate: we first observed a demonstration of the cost model used to develop the estimate , which was provided by the contractor officials who are responsible for maintaining it and the program officials who are responsible for overseeing the contractor .

we then analyzed the derivation of the cost estimate relative to 12 key practices associated with four characteristics of a reliable estimate .

as defined in our cost estimating and assessment guide , these four characteristics are comprehensive , well - documented , accurate , and credible , and the practices address , for example , the methodologies , assumptions , and source data used .

we also interviewed program officials responsible for the cost estimate about the estimate's derivation .

we then characterized the extent to which each of the four characteristics was met as either not met , minimally met , partially met , substantially met , or met .

to do so , we scored each of the 12 individual key practices associated with the four characteristics on a scale of 1-5 ( not met = 1 , minimally met = 2 , partially met = 3 , substantially met = 4 , and met = 5 ) , and then averaged the individual practice scores associated with a given characteristic to determine the score for that characteristic .

benefits: we interviewed program officials to identify any forecasts of qualitative and quantitative benefits that the system was to produce .

in this regard , we were directed to the sbinet mission need statement dated october 2006 , which we analyzed .

in addition , we reviewed our prior reports on the secure border initiative ( sbi ) , including a report on the sbi expenditure plan , which is a plan that dhs has been required by statute to submit to the house and senate appropriations committees to , among other things , identify expected system benefits .

we also interviewed program officials to determine the extent to which the system's life cycle costs and expected benefits had been analyzed together to economically justify dhs's proposed investment in sbinet .

to determine the extent to which dhs has acquired its proposed system solution in accordance with key life cycle management processes , we focused on three key processes: the system engineering approach , requirements development and management , and risk management , each of which is addressed below .

systems engineering approach: we compared the program's defined system engineering approach , as defined in the sbinet systems program office's ( spo ) systems engineering plan , to dhs and other relevant guidance .

to determine the extent to which the defined systems engineering approach had been implemented , we focused on two major “gates” ( i.e. , life cycle milestone reviews ) — the cdr and the deployment readiness review .

for each of these reviews , we compared the package of documentation prepared for and used during these reviews to the program's defined system engineering approach as specified in the systems engineering plan to determine what , if any , deviations existed .

we also interviewed program officials as to the reason for any deviations .

requirements development and management: we compared relevant requirements management documentation , such as the requirements development and management plan , the requirements management plan , the configuration and data management plan , the operational requirements document , the system - level requirements specification , and the component - level requirements specifications , to relevant requirements development and management guidance to identify an variances , focusing on the extent to which requirements were properly baselined , adequately defined , and fully traced .

with respect to requirements baselining , we compared the component and system requirements as of september 2008 , which were approved during the cdr that concluded in october 2008 , to the component and system requirements as of november 2008 , and identified the number and percentage of requirements changes .

we also interviewed program officials as to the reasons for any changes .

for requirements definition , weassessed the extent to which operational requirements that were identified as poorly defined in november 2007 had been clarified in the operatio requirements document , elements applicable to block 1 system , dat november 2008 .

in doing so , we focused on those operational requirements that are associated with block 1 .

we also traced these blo 1 operational requirements to the lower - level system requirements ( i.e. , system and component requirements ) to determine how many of the lower - level requirements were associated with any unchanged operationa requirements .

for requirements traceability , we randomly selected a sample of 60 requirements from 1,008 component requirements in the program's requirements management tool , known as the dynamic object - oriented requirements system ( doors ) , as of july 2009 .

before doing so we reviewed the quality of the access controls for the database , and we interviewed program and contractor officials and received a door tutorial to understand their respective roles in requirements management and development and the use of doors .

once satisfied as to the reliability of the data in doors , we then traced each of the 60 requirements s backwards to the system requirements and then to the operational requirements and forward to design requirements and verification methods .

because we followed a probability procedure based on ra selection , we are 95 percent confident that each of the confidence intervals in this report will include the true values in the study population .

we used statistical methods appropriate for audit compliance testing to estimate 95 percent confiden r equirements in our sample .

ce intervals for the traceability of risk management: we reviewed relevant documentation , such as t sbinet risk / issue / opportunity management plan , the sbinet spo risk / issue / opportunity management process , and the sbinet risk management policy , as well as extracts from the sbinet risk management database and minutes of meetings and agendas from the risk management team and the joint risk review board .

in doing so , we compared the risk management process defined in these documents to relevant guidance todetermine the extent to which the program has defined an effective risk management approach .

further , we observed a demonstration of the r database , and we compared sbinet risks identified by us and others , including the sbi executive director , to the risks in the database to determine the extent to which all key risks were being actively managed .

further , we discussed actions recently taken and planned to improve risk management with the person responsible for sbinet risk management .

w also reviewed briefings and related material provided to dhs leadership during oversight reviews of sbinet and interviewed program officials to ascertain the extent to which program risks were disclosed at these reviews and at meetings with congressional committees .

in this regard , we also asked cognizant staff with the house homeland security commi about the extent to which pr o ogram risks were disclosed by program fficials in status briefings .

to determine the extent to which dhs has addressed our prior sbinet recommendations , we focused on the eight recommendations that we made in our september 2008 report .

for each recommendation , we leveraged the work described above , augmenting it as necessary to determine any plans or actions peculiar to a given recommendat example , to determine the status of efforts to address our prior recommendation related to sbinet testing , we reviewed key testing ion .

for documentation , such as the test and evaluation master plan ; sbinet component and system qualification test plans , test procedures , and test reports ; program management reviews ; program office briefings ; and d acquisition review p board decision memoranda .

we also interviewed rogram officials .

to support our work across the above objectives , we also interviewed officials from the department of defense's defense contract managemen agency , which provides contractor oversight services , to understand i reviews of the contractor's integrated master schedule , requirements development and management activities , risk management practices , and testing activities .

we also reviewed defense contract management agenc reports pertaining to documentation , such as monthly status reports and the integrated master schedule and cost reporting .

to assess the reliability of the data that we relied on to support the findings in the report , we reviewed relevant program documentation to substantiate evidence obtained through interviews with knowledgeable agency officials , where available .

we determined that the data used in this also made appropriate attribution report are sufficiently reliable .

we have indicating the sources of the data used .

we performed our work at the customs and border protection headquarters and contractor facilities in the washington , d.c. , metropolitan area and at a contractor facility and a defense contract management agency office in huntsville , alabama .

we conducted this performance audit from december 2008 to may 2010 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropria evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

 ( cbp ) .

in september 2008 , we reported on a number of sbinet program management weaknesses and associated risks related to establishing program commitments , developing an integrated master schedule , defining and implementing a life cycle management approach , developing and managing requirements , and testing .

to address these weaknesses and risks , we made a number of recommendations .

table 7 provides details on dhs efforts to address each recommendation .

our research has identified a range of best practices associated with effective schedule estimating .

these are ( 1 ) capturing all activities , ( 2 ) sequencing all activities , ( 3 ) assigning resources to all activities , ( 4 ) establishing the duration of all activities , ( 5 ) integrating activities horizontally and vertically , ( 6 ) establishing the critical path for all activities , ( 7 ) identifying reasonable float time between activities , ( 8 ) conducting a schedule risk analysis , and ( 9 ) updating the schedule using logic and durations .

we assessed the extent to which the sbinet integrated master schedule , dated august 2009 , met each of the nine practices as either not met ( the program provided no evidence that satisfies any portion of the criterion ) , minimally met ( the program provided evidence that satisfies less than one - half of the criterion ) , partially met ( the program provided evidence that satisfies about one - half of the criterion ) , substantially met ( the program provided evidence that satisfies more than one - half of the criterion ) , and met ( the program provided evidence that satisfies the entire criterion ) .

table 8 shows the detailed results of our analysis .

our research has identified 12 practices that are integral to effective program life cycle cost estimating .

these 12 practices in turn relate to four characteristics of a high - quality and reliable cost estimate: comprehensive: the cost estimate should include all government and contractor costs over the program's full life cycle , from program inception through design , development , deployment , and operation and maintenance to retirement .

it should also provide sufficient detail to ensure that cost elements are neither omitted nor double - counted , and it should document all cost - influencing ground rules and assumptions .

well - documented: the cost estimate should capture in writing things such as the source and significance of the data used , the calculations performed and their results , and the rationale for choosing a particular estimating method or reference .

moreover , this information should be captured in such a way that the data used to derive the estimate can be traced back to , and verified against , their sources .

finally , the cost estimate should be reviewed and accepted by management to demonstrate confidence in the estimating process and the estimate .

accurate: the cost estimate should not be overly conservative or optimistic , and should be , among other things , based on an assessment of most likely costs , adjusted properly for inflation , and validated against an independent cost estimate .

in addition , the estimate should be updated regularly to reflect material changes in the program and actual cost experience with the program .

further , steps should be taken to minimize mathematical mistakes and their significance and to ground the estimate in documented assumptions and a historical record of actual cost and schedule experiences with other comparable programs .

credible: the cost estimate should discuss any limitations in the analysis due to uncertainty or biases surrounding data or assumptions .

major assumptions should be varied and other outcomes computed to determine how sensitive the estimate is to changes in the assumptions .

risk and uncertainty inherent in the estimate should be assessed and disclosed .

further , the estimate should be properly verified by , for example , comparing the results with an independent cost estimate .

our analysis of the $1.3 billion sbinet life cycle cost estimate relative to each of the 12 best practices , as well as to each of the four characteristics , is summarized in table 9 .

a detailed analysis relative to the 12 practices is in table 10 .

in addition to the contact named above , deborah davis ( assistant director ) , david alexander , rebecca alvarez , carl barden , tisha derricotte , neil doherty , nancy glover , dan gordon , cheryl dottermusch , thomas j. johnson , kaelin p. kuhn , jason t. lee , lee mccracken , jamelyn payan , karen richey , karl w.d .

seifert , matt snyder , sushmita srikanth , jennifer stavros - turner , stacey l. steele , and karen talley made key contributions to this report .

