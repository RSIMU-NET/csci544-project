the department of defense's ( dod ) readiness assessment system was designed to assess the ability of units and joint forces to fight and meet the demands of the national security strategy .

for more than a decade ending in 1998 , various audit and oversight organizations questioned the thoroughness and reliability of dod reports on the readiness of u.s. forces .

since 1998 , congress has added reporting requirements to enhance its oversight of military readiness .

in doing so , congress expressed concern over contradictions between assessments of military unit readiness in reports and observations made by military personnel in the field .

dod provides congress a quarterly report that contains readiness information from several sources: the unit - level readiness assessment system ; results of scenario - based assessments ; and summaries of information briefed to senior dod officials .

we reviewed dod's readiness assessment and reporting system in 1998 and concluded that the readiness reports provided congress offered a vague description of readiness problems and remedial actions and therefore were not effective as oversight tools .

considering the concerns raised in our 1998 report , reporting requirements added by congress since 1998 , and the new national security strategy , you asked us to provide an updated assessment of dod's readiness reporting .

as agreed with your office , we examined ( 1 ) the progress dod has made in resolving issues raised in our prior report on both the unit level readiness reporting system and the lack of specificity in the department's quarterly readiness reports to the congress , ( 2 ) the extent to which dod has complied with legislative reporting requirements enacted since 1997 , and ( 3 ) dod's plans to improve readiness reporting .

in conducting this analysis , we compared current reported readiness data with legislative requirements and with data reported in 1998 .

we also identified dod initiatives for improving readiness reporting .

we conducted our review from june 2002 through january 2003 in accordance with generally accepted government auditing standards .

 ( for a complete description of our methodology , see app .

i. ) .

dod's readiness assessment and reporting system was designed to assess and report on military readiness at three levels — ( 1 ) the unit level ; ( 2 ) the joint force level ; and ( 3 ) the aggregate , or strategic , level .

unit - level readiness is assessed with the global status of resources and training system ( gsorts ) , which is an automated system that assesses the extent to which military units possess the required resources and training to undertake their wartime missions .

to address joint readiness , the chairman of the joint chiefs of staff established the joint monthly readiness review ( now called the joint quarterly readiness review or jqrr ) , that compiles readiness assessments from the combatant commands , the combat support agencies , and the military services .

the joint staff and the services use these assessments to brief dod's leadership on the senior readiness oversight council — an executive - level forum for monitoring emerging readiness issues at the strategic level .

the briefings to the council are intended to present a view of readiness at the aggregate force level .

from these briefings to the council , dod prepares a legislatively mandated quarterly readiness report to congress .

figure 1 provides an overview of dod's readiness assessment process .

we have issued several reports containing recommendations for improving readiness reporting .

in 1994 , we recommended dod develop a more comprehensive readiness system to include 26 specific readiness indicators .

in 1998 , we reported on shortcomings in dod's readiness assessment system .

at that time , we stated gsorts' limitations included lack of precision in measurements , late reporting , subjective input , and lack of standardization .

secondly , we reported that while the quarterly readiness reports to the congress accurately reflected briefs to the senior readiness oversight council , they lacked specific details on deficiencies and remedial actions and thus did not meet the requirements of 10 u.s.c .

482 ( b ) .

dod concurred with our recommendation that the secretary of defense take steps to better fulfill the legislative reporting requirements under 10 u.s.c .

482 by providing ( 1 ) supporting data on key readiness deficiencies and ( 2 ) specific information on planned remedial actions .

finally , we reported that deficiencies identified as a result of the joint monthly readiness reviews remained open because the solutions require funding over the long term .

in 2002 , we issued a classified report on dod's process for tracking the status of deficiencies identified in the joint monthly readiness reviews .

we made recommendations to improve dod's deficiency status reporting system and for dod to develop funding estimates for correcting critical readiness deficiencies .

in its comments , dod generally agreed with the report's findings and recommendations .

although dod has made progress in resolving readiness reporting issues raised in our 1998 report , we found that some of the same issues still exist today .

for example , dod has added information to its quarterly readiness reports to the congress ( hereafter referred to as the quarterly reports ) .

however , we found that the reports still contain vague descriptions of readiness problems and remedial actions .

even though some report annexes contain detailed data , the data as presented are not “user friendly” — it is largely unevaluated and is not linked to readiness issues mentioned in the report plus the report text does not explain how the data relates to units' readiness .

thus , as we reported in 1998 , these reports do not specifically describe readiness problems or remedial actions as required under 10 u.s.c .

482 ( b ) .

we believe that this kind of information would be useful for congress to understand the significance of the information in these reports for use in its oversight role .

dod has improved some aspects of its unit - level reporting system , the global status of resources and training system ( gsorts ) .

for example , in 1998 gsorts' data were maintained in multiple databases and data were not synchronized .

as of september 2002 , the data are reported to a central site , and there is one database of record .

also in 1998 , u.s. army gsorts review procedures delayed submission of army data , and all the services' data entry was manual .

as of september 2002 , army reporting procedures require reporting consistent with gsorts' requirements , and all the services have automated data entry , which reduces errors .

in 1998 , combat units only reported on readiness for wartime missions .

as of september 2002 , combat units report on assigned mission readiness in addition to wartime mission readiness .

conversely , dod has not resolved some issues we raised in 1998 .

for example , readiness ratings are still reported in broad bands and actual percentages of required resources are not externally reported .

these issues remain because the manual specifying readiness reporting rules has not changed in these areas .

the manual's definition of readiness levels for personnel has not changed since our 1998 report — it still defines readiness levels in bands of 10 percentage points or more and does not require external reporting of actual percentages .

for example , the highest personnel rating can range from 90 percent to 100 percent , and there is no requirement to report the actual percentage outside of dod .

we have also reported that gsorts does not always reflect training and equipment deficiencies .

for example , we reported in april and june 2002 that readiness data do not reflect the effect of training range restrictions on unit readiness .

we have also reported that gsorts does not include whether a unit's chemical / biological equipment is usable .

in commenting on our analysis , the ousd p&r office responsible for readiness reporting stated that it recognized the imprecision of the current measurements .

according to that office , an effort to develop the planned new readiness reporting system , which is discussed later in this report , includes working with the dod components to enhance and expand readiness reporting .

since our 1998 report , the quarterly reports improved in some areas , but degraded in others .

although some information was added , we found that some of the same quality issues remain — namely , that the reports do not specifically describe readiness problems , their effects on readiness , or remedial actions .

dod has added information to the quarterly reports in response to legislative direction .

for example , dod added information on the services' cannibalization rates .

also , dod added annual reports on infrastructure and institutional training readiness .

however , some information was eliminated from the quarterly reports .

for example , the law requires results of joint readiness reviews to be reported to congress .

dod included these results until the july - september 2001 quarterly readiness report to the congress .

since that report , four quarterly reports have been issued without the joint force assessments .

defense officials responsible for readiness reporting said that the joint readiness reviews were not included because the scenarios were based on the former national security strategy of two major wars .

the officials stated they plan to include results from the joint readiness reviews in future reports .

in commenting on our analysis the ousd p&r office responsible for readiness reporting stated that it continues to seek better ways to provide concise , quality information .

as we reported in 1998 , we found that the quarterly reports still contain broad statements of readiness issues and remedial actions , which are not supported by detailed examples and are not related to data in the reports' annexes .

among other things , the law requires the quarterly reports to specifically describe each readiness problem and deficiency as well as planned remedial actions .

the reports did not specifically describe the nature of each readiness problem or discuss the effects of each on unit readiness .

also , the reports included only broad statements of remedial actions that lacked details on timelines , objectives , or funding requirements .

for example , one report said that the air force continued to experience shortages in critical job skills that affected the service's ability to train .

the report did not refer the reader to data in annexes showing training readiness ratings ; it did not state which skills were short , which training was not accomplished , or whether this shortage had or was expected to affect units' readiness ratings .

further , the report did not explain the remedial actions taken or planned to reverse the skill shortage , how long it would take to solve this problem , or what funding was programmed to implement remedial actions .

defense readiness officials agreed , stating that information in the quarterly reports is summarized to the point that there are no details on readiness deficiencies , remedial actions , or funding programmed to implement remedial actions .

we believe the congress needs this type of information to understand the significance of the information reported .

although some of the quarterly report annexes contain voluminous data , the data are not adequately explained or related to units' readiness .

the law does not mandate specific explanations of these “readiness indicators,” but we believe it is essential for congress to understand the significance of the information in these reports for use in its oversight role .

for example , dod is required to report on the maintenance backlog .

although the report provides the quantity of the backlog , it does not explain the effect the backlog had on readiness .

specifically , the report did not explain whether units' readiness were affected because maintenance was not accomplished when needed .

in addition , dod is required to report on training commitments and deployments .

the expanded quarterly readiness report to congress implementation plan dated february 1998 stated that “either an excessive or a reduced level of commitment could be an indicator of potential readiness problems.” however , ousd p&r did not define what kind of “readiness problems” this data may indicate would occur as a result of “excessive or reduced” levels of training and deployments , such as degraded equipment or training .

the data reported are the amount of training away from home station and the amount of deployments .

however , these data are not explained or related to a unit's equipment or training ratings .

further , criteria have not been established to distinguish between acceptable and unacceptable levels of the training and deployment data reported .

as a result , the reader does not know whether the data reported indicate a problem or the extent of the problem .

in commenting on our analyses , ousd p&r acknowledged “the department would be better served by providing more information as to how various data relates to readiness.” generally , the quarterly reports also do not contain information on funding programmed to implement specific remedial actions .

for example , one quarterly report included the statement that budgets were revised “to address readiness and capabilities issues,” but no examples were provided .

also , the report lacked statements explaining how this “budget revision” would improve readiness .

although not required by law , we believe it would prove useful for congress to understand how dod addresses specific readiness problems .

in commenting on our analysis , ousd p&r officials stated that they would continue to work with the services to provide more fidelity with the information presented in the quarterly report annexes .

however , they also said that detailed examples require significant staff effort throughout dod and that the added time for more detailed analysis could render the report a historical document .

they further said that complete information would certainly be desired and agreed it is important for the congress to understand the significance of the information in the quarterly reports for use in its oversight role .

dod has complied with most , but not all , of the readiness reporting requirements added by congress in the national defense authorization acts for fiscal years 1998 through 2002 .

congress added readiness reporting requirements out of concern over contradictions between assessment of military unit readiness in official readiness reports and observations made by military personnel in the field .

in a review of these acts , we identified both recurring readiness reporting requirements that were added to existing law and one - time reporting requirements related to military readiness .

we compared current readiness reporting to the requirements in these acts to make an overall judgment on the extent of compliance .

we did not develop a total count of the number of reporting requirements because the acts included a series of sections and subsections that could be totaled in various ways .

because dod is not reporting on all the requirements added over the past several years , the congress is not receiving all the information mandated by law .

our analysis showed that dod has complied with most of the requirements added in the national defense authorization acts for fiscal years 1998-2002 .

for example , dod took the following actions in response to legislative requirements: dod is now reporting on the readiness of prepositioned equipment and is listing individual units that have reported low readiness as required by the national defense authorization act for fiscal year 1998 .

dod is reporting on infrastructure and institutional training readiness as required by the national defense authorization act for fiscal year 1999 .

dod contracted for an independent study of requirements for a comprehensive readiness reporting system and submitted the study report to the congress as required by the national defense authorization act for fiscal year 2000 .

dod has added quarterly information on the military services' cannibalization rates as required by the national defense authorization act for fiscal year 2001 .

dod is reporting on some , though not all , of the items congress required be added to the quarterly readiness reports .

for example , the national defense authorization act for fiscal year 1998 required 19 specific items be reported that are consistent with our previously cited 1994 report on readiness reporting .

the 1994 report included a list of 26 readiness indicators that dod commanders said were important for a more comprehensive assessment of readiness .

a 1994 dod - funded study by the logistics management institute found that 19 of the 26 indicators could help dod monitor critical aspects of readiness .

the 19 items listed in the national defense authorization act for fiscal year 1998 are very similar to those identified in the 1994 logistics management institute study .

dod is reporting on 11 of the 19 items and is not reporting on the other 8 .

the eight items are ( 1 ) historical personnel strength data and trends , ( 2 ) personnel status , ( 3 ) borrowed manpower , ( 4 ) personnel morale , ( 5 ) operations tempo , ( 6 ) training funding , ( 7 ) deployed equipment , and ( 8 ) condition of nonpacing equipment as required in the act .

in an implementation plan setting forth how it planned to comply with reporting on the 19 items , which was also required by the national defense authorization act for fiscal year 1998 , dod stated that it would not report on these eight indicators for the following reasons: deployed equipment was considered part of the status of prepositioned equipment indicator .

historical personnel strength data and trends were available from the annual defense manpower requirements report .

training funding and operations tempo were believed to be represented adequately in the budget requests as flying hours , steaming days , or vehicle miles and were not considered good measures of readiness output .

personnel strength status was considered to be part of the personnel rating , but dod agreed to investigate other ways to evaluate the effect of service personnel working outside the specialty and grade for which they were qualified .

borrowed manpower data was only captured in a limited sector of the deployable force and may not be meaningful until a better method is developed to capture the data .

personnel morale had no existing data sources .

the condition of nonpacing equipment had no reasonable measurement to use as an indicator .

notwithstanding the reasoning that dod stated , these eight indicators continue to be required by law , and we saw no indication in our work that dod is working to develop data for them .

also , dod is not complying with some of the requirements in the national defense authorization act for fiscal year 1999 .

examples are as follows: the act required dod to establish and implement a comprehensive readiness reporting system by april 2000 .

as of january 2003 , dod had not implemented a new system , and officials said it is not expected to be fully capable until 2007 or 7 years later than required .

the act also required dod to develop implementing regulations for the new readiness reporting system .

dod had not developed implementing regulations as of january 2003 .

the act required dod to issue regulations for reporting changes in the readiness of training or defense infrastructure establishments within 72 hours .

although dod has provided some guidance , officials stated they have not issued regulations because no mechanism exists for institutional training or defense infrastructure establishments to report changes and because these entities are not part of an established readiness reporting system .

in commenting on our analyses , dod officials acknowledged “the department is not in full compliance” and stated that they plan to achieve compliance with the new readiness reporting system under development .

ousd p&r officials said that the shortfalls in reporting are unwieldy under the current system ; ousd p&r intends to correct these shortfalls when the new system is functional .

however , as noted above , dod does not plan to implement its new system until 2007 .

as of january 2003 , dod also had not targeted incremental improvements in readiness reporting during the period in which the new system is being developed .

until then , congress will receive less readiness information than it mandated by law .

dod issued a directive in june 2002 to establish a new readiness reporting system .

the undersecretary of defense for personnel and readiness is to oversee the system to ensure the accuracy , completeness , and timeliness of its information and data , its responsiveness , and its effective and efficient use of modern practices and technologies .

officials in the ousd p&r readiness office responsible for developing the new system said that they plan to use the new system to comply with the requirements in the national defense authorization acts and to address many of the recommendations contained in a congressionally directed independent study .

however , as of january 2003 , there are few details of what the new system would include .

although the new system may have the potential to improve readiness reporting , as of january 2003 , it is only a concept without detailed plans to guide development and monitor implementation .

as a result , the extent to which the new system will address existing shortcomings is unknown .

the national defense authorization act for fiscal year 1999 required dod to establish a comprehensive readiness reporting system .

in doing so , the congress expressed concern about dod's lack of progress in developing a more comprehensive readiness measurement system reflective of operational realities .

the congress also noted that past assessments have suffered from dod's inability to create and implement objective and consistent readiness reporting criteria capable of providing a clear picture to senior officials and the congress .

subsequently , the august 2001 defense planning guidance for fiscal years 2003-2007 called for the development of a strategic plan for transforming dod readiness reporting .

in june 2002 , dod issued a directive establishing the department of defense readiness reporting system .

the system will measure and report on the readiness of military forces and the supporting infrastructure to meet missions and goals assigned by the secretary of defense .

all dod components will align their readiness reporting processes in accordance with the directive .

the directive assigns oversight and implementing responsibility to the undersecretary of defense for personnel and readiness .

the undersecretary is responsible for developing , fielding , maintaining , and funding the new system and scenario assessment tools .

the undersecretary — in collaboration with the joint chiefs of staff , services , defense agencies , and combatant commanders — is to issue implementing instructions .

the chairman of the joint chiefs of staff , the service secretaries , the commanders of the combatant commands , and the heads of other dod components are each assigned responsibilities related to readiness reporting .

ousd p&r established a timetable to implement the new readiness reporting system .

ousd p&r plans to achieve initial capability in 2004 and reach full capability in 2007 .

ousd p&r officials involved in developing the system said that they have been briefing the concept for the new reporting system since october 2002 .

as of january 2003 these officials stated that they are continuing what they have termed the “concept demonstration” phase , which began in october 2002 .

this phase consists of briefing various offices within dod , the joint staff , and the services to build consensus and refine the new system's concept .

these officials also said that the new system will incorporate many , but not all , of the recommendations contained in a legislatively mandated independent study of readiness reporting , which concluded that improvements were needed to meet legislatively mandated readiness reporting requirements and included numerous recommendations for what a new system should include .

for example , the study recommended that ( 1 ) dod report on all elements essential to readiness , such as depots , combat support agencies , and defense agencies ; ( 2 ) reporting should be in terms of mission essential tasks ; and ( 3 ) dod should measure the capability to carry out the full range of national security strategy requirements — not just a unit's wartime mission .

we believe that successfully developing and implementing a large - scale effort , such as dod's new readiness reporting system , requires an implementation plan that includes measurable performance goals , identification of resources , performance indicators , and an evaluation plan .

as discussed earlier , full implementation of dod's new readiness reporting system is several years away , and much remains to be done .

in january 2003 the ousd p&r office responsible for developing the new system said that the new readiness reporting system is a large endeavor that requires buy - in from many users and that the development of the system will be challenging .

this office also wrote that it had just been given approval to develop the new readiness reporting system , was targeting development of an implementing instruction in the march 2003 time frame , and had not developed an implementation plan to assess progress in developing and implementing the new reporting system .

the directive establishing the new reporting system requires the undersecretary of defense for personnel and readiness , in collaboration with others , to issue implementing instructions for the new system .

dod has experienced delays in implementing smaller readiness improvements than envisioned in the new readiness reporting system .

one such effort involved development of an interface to query the existing readiness data base ( gsorts ) .

in a july 2002 report , the dod inspector general reported that the planned implementation of this interface slipped 44 months , or just over 3.5 years .

also , history has shown it takes dod time to make changes in the readiness reporting system .

as illustrated in figure 2 , dod began reporting on specific readiness indicators 4 years after it agreed with gao recommendations to include them in readiness reporting ( see fig .

2 ) .

other dod development efforts recognize the need for effective planning to guide development .

for example , dod is working to transform military training as directed by the defense planning guidance for fiscal years 2003-07 .

a march 2002 strategic plan for transforming dod training developed by a different office within ousd p&r discusses a training transformation road map with major tasks subdivided into near - , mid - , and long - term actions .

the plan includes a list of near - term actions to be completed by october 2003 and definition of mid - and long - term actions in a comprehensive implementation plan that will identify specific tasks , responsibilities , timelines , resources , and methods to assess completion and measure success .

the may 2002 defense planning guidance update for fiscal years 2004-2009 directs ousd p&r , working with other dod components , to develop a comprehensive program to implement the strategic training transformation plan and provide it to the deputy secretary of defense by april 1 , 2003 .

since the directive for creating a new readiness reporting system established broad policy with no specifics and since dod has not developed an implementation plan , the extent to which the new system will address the current system's shortcomings will remain unknown until the new system is fully capable in 2007 .

until then , readiness reporting will continue to be based on the existing system .

commenting on its plans for the new system , ousd p&r said that it is in the process of creating an advanced concept technology demonstration ( actd ) structure for the new system and will produce all necessary planning documents required within the established actd process .

however , this process is intended to provide decision makers an opportunity to understand the potential of a new concept before an acquisition decision .

we do not believe the actd process will necessarily result in an implementation plan to effectively monitor development and assess whether the new system is being implemented on schedule and achieving desired results .

dod's actd guidelines state the principal management tool for actds is a management plan , which provides a top - level description of the objectives , critical events , schedule , funding , and measures of evaluation for the project .

we reported in december 2002 that these guidelines contain advice and suggestions as opposed to formal directives and regulations .

dod's guidelines state that the actd should plan exercises or demonstrations to provide an adequate basis for utility assessment .

we also reported in december 2002 that dod lacks specific criteria to evaluate demonstration results , which may cause acquisition decisions to be based on too little knowledge .

therefore , we still believe an implementation plan is necessary since the actd process does not require a detailed implementation plan and does not always include specific criteria to evaluate effectiveness .

while dod has made some improvements in readiness reporting since 1998 , some of the same issues remain unresolved today .

although dod is providing congress more data than in 1998 , the voluminous data are neither evaluated nor explained .

the quarterly reports do not link the effects of “readiness issues” or deficiencies to changes in readiness at the unit level .

also , as in 1998 , the reports contain vague descriptions of remedial actions not linked to specific deficiencies .

finally , the quarterly reports do not discuss funding that is programmed to implement specific remedial actions .

as a result , the information available to congress is not as effective as it could be as an oversight tool .

even though dod directed development of a new readiness reporting system , it has not yet developed an implementation plan identifying objective and measurable performance goals , the resources and personnel needed to achieve the goals , performance indicators , and an evaluation plan to compare program results with goals , and milestones to guide overall development of the new readiness system .

even though the new system may have the potential to improve readiness reporting , without an implementation plan little assurance exists that the new system will actually improve readiness assessments by the time full capability is planned in 2007 .

without such a plan , it will also remain difficult to gauge progress toward meeting the 2007 target date .

this concern is reinforced in light of the ( 1 ) years - long delays in implementing other readiness reporting improvements and ( 2 ) the deficiencies in existing reporting that ousd p&r plans to rectify with the new system .

furthermore , without an implementation plan neither senior dod leadership nor the congress will be able to determine if the resources spent on this system are achieving their desired results .

to improve the information available to congress for its use in its oversight role , we recommend that the secretary of defense direct the ousd p&r to improve the quality of information contained in the quarterly reports .

specifically , we recommend that dod's reports explain ( in the unclassified section ) the most critical readiness issues that are of greatest concern to the department and the services .

for each issue , we recommend that dod's reports include an analysis of the readiness deficiencies , including a clear explanation of how the issue affects units' readiness ; a statement of the specific remedial actions planned or implemented ; and clear statements of the funding programmed to implement each remedial action .

to be able to assess progress in developing the new readiness system , we recommend that the secretary of defense direct the ousd p&r to develop an implementation plan that identifies performance goals that are objective , quantifiable , and measurable ; the cost and personnel resources needed to achieve the goals , including an identification of the new system's development and implementation costs in the president's budget beginning in fiscal year 2005 and future years defense plan ; performance indicators to measure outcomes ; an evaluation plan to compare program results with established goals ; and milestones to guide development to the planned 2007 full capability date .

to assist congress in its oversight role , we recommend that the secretary of defense give annual updates to the congress on the new readiness reporting system's development to include performance measures , progress toward milestones , comparison of progress with established goals , and remedial actions , if needed , to maintain the implementation schedule .

in written comments on a draft of this report , which are reprinted in appendix ii , the department of defense did not agree with our recommendations .

in response to our recommendation that dod improve the quality of information contained in its quarterly readiness reports , dod said that the quarterly readiness report to the congress is one of the most comprehensive and detailed reports submitted to the congress that discusses serious readiness issues and ways in which these issues are being addressed .

dod further stated that the department presents briefings on specific readiness issues to the congress and that spending more time and resources expanding the existing written report would be counterproductive .

we recognize that the quarterly readiness reports to the congress contain voluminous data .

however , as discussed in this report , we found that the quarterly reports' annexes are large and mostly consist of charts or other data that are not adequately explained and are not related to units' readiness .

in some cases , criteria have not been established to enable the reader to distinguish between acceptable and unacceptable levels of the data reported .

as a result , the reader cannot assess the significance of the data because it is not at all clear whether the data reported indicate a problem or the extent of the problem .

considering that the quarterly reports contain inadequately explained data and that much of the information is not “user friendly,” we continue to believe the quality of information in the quarterly reports can be improved .

in fact , we reviewed all the quarterly reports provided to congress since 1998 and found that through the january - june 2001 report the reports did include an unclassified summary of readiness issues for each service addressing four topics — personnel , equipment , training , and enablers ( critical units or capabilities , such as specialized aircraft , essential to support operations ) .

however , the reports did not include supporting data or a discussion of remedial actions .

since that time , these summaries have been eliminated from the quarterly reports .

for example , the unclassified narrative of the last two reports available at the time we performed our work — january - march 2002 and april - june 2002 — were less than two pages long and neither discussed readiness issues nor ways in which these issues are being addressed .

one report discussed the new readiness reporting system , and the other discussed a review of seven environmental laws .

given that dod has highlighted key issues in the past , we believe that improving the quarterly reports would be beneficial if dod were to focus on the most critical readiness issues that are of greatest concern to the services and includes supporting data and a discussion of remedial actions .

therefore , we have modified our recommendation that dod improve the quality of readiness reporting to focus on readiness issues deemed to be critical by the secretary and the military services and to provide more detailed data and analyses of those issues and the remedial actions planned for each one .

dod did not agree with our recommendations that it ( 1 ) develop an implementation plan with , among other things , performance goals that are objective , quantifiable , and measurable and ( 2 ) provide annual updates to the congress on the new readiness reporting system's development .

dod said that it had undertaken an initiative to develop better tools for assessing readiness and that it intended to apprise congress on its efforts to develop tools for readiness assessment .

dod further stated that the effort to improve readiness reporting is in its infancy , but that it has established milestones , cost estimates , functional responsibilities , and expected outcomes .

dod believes that further planning and a prescriptive annual update to the congress is unnecessary .

we agree that the new readiness reporting system may have the potential to improve readiness reporting .

however , as discussed in this report , the directive establishing the new system contains very broad , high - level statements of overall functional responsibilities and outcomes , but no details on how these will be accomplished .

further , dod has established two milestones — initial capability in 2004 and full capability in 2007 .

dod does not have a road map explaining the steps needed to achieve full capability by 2007 , which is seven years after congress mandated a new system be in place .

in addition , as discussed earlier in this report , dod has experienced delays in implementing much smaller readiness improvements .

while dod has undertaken an initiative to develop better tools for assessing readiness and intends to routinely and fully apprise the congress on its development efforts , tools are the mechanics for evaluating readiness data .

as such , tools are not the same thing as the comprehensive readiness reporting system mandated by congress that dod has said will include new metrics and will evaluate entities within dod that currently do not report readiness .

considering that congress expressed concern about dod's lack of progress in developing a comprehensive system , that developing and implementing dod's planned new system is scheduled to take 4 more years , and that delays have been experienced in earlier efforts to make small improvements in readiness reporting , we continue to believe that it is important for dod to develop an implementation plan to gauge progress in developing and implementing the new readiness reporting system and to provide annual updates to the congress .

such a plan would be consistent with dod's approach to other major initiatives such as transforming training .

we have therefore retained these two recommendations .

dod also provided technical corrections and we have modified the report where appropriate .

we are sending copies of this report to the ranking minority member , subcommittee on readiness , house committee on armed services ; the chairman and ranking minority member , subcommittee on readiness and management support , senate committee on armed services ; other interested congressional committees ; secretary of defense ; and the director , office of management and budget .

we will also make copies available to others on request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions , please call me on ( 757 ) 552-8111 or by e - mail at curtinn@gao.gov .

major contributors to this report were steven sternlieb , brenda waterfield , james lewis , dawn godfrey , and herbert dunn .

to assess the progress the department of defense ( dod ) has made in resolving issues raised in our prior report concerning both the unit level readiness reporting system and the lack of specificity in dod's quarterly readiness reports to the congress , we met with dod officials and reviewed regulations and quarterly reports .

specifically , we met with officials of the office of the undersecretary of defense for personnel and readiness ( ousd p&r ) responsible for readiness reporting , the joint staff , and the military services to discuss their individual progress in each of these areas .

to assess progress regarding unit level readiness reporting , we reviewed the chairman of the joint chiefs of staff manual governing this system and the related service implementing instructions to determine if these documents had changed since our 1998 report or if the manual and service instructions continued to allow reporting in the same manner as reflected in our earlier report .

through a comparison of the current and prior documents , discussions with pertinent officials , and our analysis , we determined whether the readiness reporting issues we raised in 1998 had been resolved .

we also reviewed the content of quarterly reports to assess their quality and usefulness , and assess whether the problems we reported in 1998 had been rectified .

we discussed our analysis with ousd p&r officials and provided them with our analyses in order that they could fully consider and comment on our methodology and conclusions .

we did not assess the accuracy of reported readiness data .

to determine the extent to which dod has complied with legislative reporting requirements enacted since our prior report , we compared a complete listing of these requirements to dod's readiness reporting .

first , we identified the legislatively mandated readiness reporting requirements enacted since our 1998 report .

to accomplish this , we reviewed the national defense authorization acts for fiscal years 1998-2002 to list the one - time and recurring reporting requirements related to military readiness .

we also requested congressional staff and ousd p&r to review the list , and officials from both offices agreed it was accurate .

we did not develop a total count of the number of reporting requirements because the acts included a series of sections and subsections that could be totaled in various ways .

once we obtained concurrence that this listing was complete and accurate , we compared this list to current readiness reporting to make an overall judgment on the extent of compliance .

to assess how dod plans to improve readiness reporting , we reviewed the june 2002 dod directive establishing a new readiness reporting system and a progress update briefing on the new system .

we also obtained readiness briefings from each of the services , ousd p&r , and joint staff officials .

we performed several electronic searches of the deputy under secretary of defense ( readiness ) electronic web site to determine the status of readiness reporting .

to assess how smoothly other readiness improvements progressed , we reviewed dod audit reports .

we discussed our findings with ousd p&r officials and worked proactively with them in conducting our analyses .

specifically , we provided them drafts of our analyses for their comments and corrections .

we conducted our review from june 2002 through january 2003 in accordance with generally accepted government auditing standards .

