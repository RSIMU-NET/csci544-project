americans rely on more than 51,000 community water systems for safe drinking water .

even though this drinking water supply is generally considered among the safest in the world , 11 states had 20 outbreaks of illness associated with drinking water in 2005 and 2006 that resulted in 612 illnesses and 4 deaths , according to data published by the centers for disease control and prevention ( cdc ) in 2008 .

in part to safeguard against such outbreaks , the environmental protection agency ( epa ) , which is generally responsible for the regulation of the nation's drinking water , requires public water systems to comply with regulations it established under the safe drinking water act ( sdwa ) .

among other things , these regulations establish ( 1 ) health - based requirements , including limitations , and treatment techniques for controlling contaminants that could harm human health and ( 2 ) monitoring requirements to determine whether drinking water meets the health - based requirements .

epa authorizes and assists state , territorial , and tribal regulatory agencies — referred to as states in this report — to administer sdwa through epa's public water system supervision ( pwss ) program .

states that have accepted “primacy” responsibility for the pwss program collect and review data from community water systems to determine their compliance with sdwa ; all states except wyoming and the district of columbia have received primacy .

with the exception of the navajo nation , epa maintains primacy for community water systems in indian country .

primacy states are responsible for , among other things , determining when systems have violated sdwa , taking timely and appropriate enforcement action , and reporting those actions to epa .

epa's regions and headquarters oversee the states to ensure they meet their primacy responsibilities ; the epa regions also act as the primacy agency in nonprimacy states , where a state has not yet received primacy for a particular drinking water regulation , and on tribal lands where the tribe has not assumed primacy .

to determine water systems' compliance with federal standards for safe drinking water , epa must have access to reliable data on the inventory of community water systems , which , along with other public water systems are subject to these standards ; the quality of drinking water ; and violations of sdwa's requirements including those to monitor drinking water to ensure the water meets health standards .

epa also needs reliable data regarding the status of enforcement actions to inform its oversight role .

these data play a critical role in helping epa manage the pwss program by identifying , for example , systems' return to compliance after committing violations of the safe drinking water standards for microbiological and chemical contaminants .

the states collect and manage relevant data ( including violations and enforcement information ) in either a database provided by epa — known as the safe drinking water information system / state ( sdwis / state ) — or in a data system of their own design .

the states also periodically transfer from their database information on violations and enforcement actions to the epa headquarters version of sdwis known as sdwis / fed .

epa generally uses the data in sdwis / fed — along with other documentation provided on request — to review state determinations of when water systems are complying with the act .

epa also uses these data to determine whether water systems , in the aggregate , are achieving the agency's national targets for compliance .

additionally , epa can use enforcement data to determine whether the states or epa regions have taken actions consistent with epa's drinking water enforcement response policy .

the policy calls for states or epa regions to take enforcement actions that are timely and appropriate for returning the water system to compliance with safe drinking water standards .

the quality of drinking water data in sdwis / fed was called into question in the late 1990s and was the subject of a 2004 report by epa's office of inspector general .

in this context , you asked us to review the sdwis / fed data .

our objectives were to examine the ( 1 ) quality of the sdwis / fed data that epa uses to measure community water systems' compliance with the health - based and monitoring requirements in sdwa and the status of the states' and epa regions' enforcement actions , ( 2 ) ways in which sdwis data quality could affect epa's management of the pwss program , and ( 3 ) actions epa and the states have been taking to improve the quality of data in sdwis / fed .

to address the first objective , we examined the results of audits epa conducted from 1996 through 2009 to assess the completeness and accuracy of the data that states submitted to sdwis / fed ( data verification audits ) .

epa's most recent published analysis of its audits was released in 2008 and covered audits done in 2002 through 2004 .

we evaluated the methods that epa used to conduct those audits to test the methods' validity and determined that these methods produced audit data that were sufficiently reliable for the purposes of our review .

epa also conducted audits in 2005 through 2009 , but it had not published its analysis of those audits at the time of our review .

we therefore obtained the results of the 2007 , 2008 , and 2009 audits from epa and conducted our own analysis of data quality using the methods that the agency used in its 2008 report .

to identify factors that affected the quality of the data , we surveyed all 44 members of three joint epa - state work groups that were created to address various aspects of data management ; we received the views of all of the members .

we examined epa's national sdwis / fed data from 2005 through 2009 to determine the percentage of violations that the states identified as returned to compliance , addressed through an enforcement action , or not addressed .

because epa's recent audits of state data did not assess the completeness and accuracy of these data , we interviewed epa and state officials to obtain their views on the completeness and accuracy of those data and analyzed relevant comments from our survey respondents .

to address the second objective , we examined the potential impact data quality could have on epa's drinking water enforcement response policy , which uses a scoring system that identifies community water systems that are a high priority for enforcement action because of unresolved violations .

we examined the impact that using data from the data verification audits could have on the scoring system compared with using data from sdwis / fed .

we also examined the views of the survey respondents on the impact that data quality may have on implementation of the enforcement response policy .

further , we examined the impact data quality could have on the agency's ability to inform the public and congress about water systems' compliance with drinking water standards relative to strategic targets it has set under the government performance and results act .

to address the third objective , we examined the survey respondents' views on steps that epa and the states could take to address data reliability — including the adoption of particular data management tools — and ways in which the three epa - state work groups could be more effective .

we also examined information epa provided on recent actions it has taken to improve data quality , including its current proposal for modifying the sdwis data management system .

we did not evaluate the merits of that proposal .

a more detailed description of our scope and methodology can be found in appendixes i and ii .

we conducted this performance audit from february 2010 through june 2011 in accordance with generally accepted auditing standards .

those standards require that we plan and perform the audit to obtain sufficient and appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

this section provides information on the risks posed by unsafe drinking water , the authority epa gives to states under sdwa , differences among community water systems , epa and the states' processes for entering water systems' data into sdwis / fed , epa's enforcement response policy and enforcement targeting tool , and epa's strategic targets for compliance with sdwa .

while the nation's public drinking water supplies are much less prone to outbreaks of waterborne diseases such as cholera and typhoid than they were in the 19th and early 20th centuries , waterborne - disease outbreaks caused by microorganisms do still occur .

for example , according to a 2006 study , an estimated 4.3 million to 11.7 million annual cases of acute gastrointestinal illnesses in the united states are attributable to drinking water from community drinking water systems supplied by surface - water and ground - water sources .

other contaminants found in drinking water may also pose a threat to human health from long - term exposure at certain levels .

for example: long - term exposure to disinfectants — such as chlorine — that are added to water to control microorganisms and the byproducts of disinfectants may cause anemia , stomach discomfort , and eye or nose irritations .

in small children and infants , inappropriate exposure to disinfectants could lead to nervous system problems .

in addition , long - term ingestion of water with disinfection byproducts may increase the risk of cancer and may affect the nervous system , liver , and kidneys .

arsenic , which occurs naturally and in industrial waste , may cause skin damage and circulatory system problems and increase the risk of cancer if it is not treated .

lead and copper introduced into drinking water from the corrosion of household plumbing systems or the erosion of natural deposits may cause liver or kidney damage .

long - term exposure to lead may delay physical or mental development in infants and children .

nitrate , which comes from fertilizer runoff , septic tanks , and erosion of natural deposits , is especially harmful to infants below the age of 6 months , and exposure may cause shortness of breathe , a serious illness known as blue - baby syndrome , and , if left untreated , death .

under sdwa , epa may authorize states meeting specified requirements to implement the pwss program — referred to as primacy authority .

for example , states must have regulations for contaminants that are no less stringent than those promulgated by epa , adequate record keeping and reporting requirements , and adequate enforcement authority to compel water systems to comply with drinking water requirements .

epa has approved primacy authority for 49 states , 5 territories , and the navajo nation .

epa's regions administer the programs in wyoming and the district of columbia , and for most tribes .

epa provides annual grants through the pwss program to the states using a formula that takes into account population , geographical area , and the number of water systems covered .

 ( epa may also consider other relevant factors in its allocation formula. ) .

in recent years , total epa allocations to these grants have averaged about $100 million per year .

states must provide matching funds ; under the act , the pwss grant can provide no more than 75 percent of the costs expended by a state to carry out its pwss program .

epa's drinking water program guidance instructs epa regions to work with the states to develop grant workplans that include the states' commitments to report key activities .

for example , the state of washington's workplan includes a commitment to assure complete and accurate identification and reporting of public water system compliance .

as of july 2009 , more than 51,000 community water systems supplied water to the same populations year - round .

community water systems vary widely in the number of people they serve , from 25 to over a million .

as figure 1 shows , small systems are the most common .

however , the 8 percent of community water systems that serve more than 10,000 people supply approximately 82 percent of all community water system users .

figure 1 shows the number of community water systems in 2009 categorized by the number of people they serve .

community water systems obtain their water from groundwater reserves or from surface water sources .

they may obtain , treat , and distribute their water entirely on their own , or they may purchase treated water from another system .

treatment generally consists of filtration , sedimentation , and other processes to remove impurities and harmful agents , and disinfection processes such as chlorination to eliminate biological contaminants .

community water systems must meet a variety of health - based requirements under sdwa .

these include providing drinking water that meets numerical limits for some contaminants , using treatment techniques for other contaminants , and using laboratory testing to monitor and report on the quality of the drinking water that they provide .

under sdwa , epa may establish an enforceable standard — called a maximum contaminant level , or mcl — that limits the amount of a contaminant that may be present in drinking water .

if epa determines it is not economically or technically feasible to ascertain the level of a contaminant , the agency may instead establish a treatment technique to prevent known or anticipated health effects .

in total , epa has set mcl or treatment technique standards — known as the national primary drinking water regulations — for 89 regulated contaminants .

we refer to violations of these standards as health - based violations .

epa has also established monitoring , reporting , and other requirements for each of the 89 regulated contaminants .

in this report we refer to these requirements collectively as monitoring requirements and refer to violations of these requirements as monitoring violations .

these requirements may vary depending on several factors .

for example , the frequency of monitoring may depend on whether the system obtains its water from ground water or surface water sources or upon the size of the water system .

additionally , if the water system detects certain contaminants above a specified amount , it may need to increase the frequency of its monitoring .

community water systems must also notify the public within specified times about the occurrence of health - based or monitoring violations and provide their customers with an annual consumer confidence report containing data on the presence and concentrations of the 89 regulated contaminants .

most of the states enter data they collect and generate on community water systems into a version of sdwis designed for use by the states known as sdwis / state .

as epa promulgates new or revised regulations for particular contaminants , it develops new sdwis versions to capture data associated with those regulations .

epa encourages the states to place their water systems' data into sdwis / state but the states may choose not to if they have an alternative database that meets their needs while also complying with epa recordkeeping requirements .

the data include inventory information about each system , such as its name , owner , address , and the size of the population it serves .

the data also include the results of the water monitoring conducted according to contaminant - specific schedules by each system , the state's determination of whether the system has committed violations , and a record of enforcement actions taken .

according to epa , every 3 months , the states must transfer certain information from either sdwis / state or their alternative data system to sdwis / fed .

specifically , the states transfer to sdwis / fed data on the health and monitoring violations identified and enforcement actions taken , and whether the state has determined that the system has returned to compliance .

sdwis / fed is the data system epa uses to gauge community water systems' compliance with sdwa .

in 2006 , epa and the association of state drinking water administrators set a goal that 90 percent of health - based drinking water violations be completely and accurately reported to sdwis / fed .

epa , however , does not have a goal for the completeness and accuracy of data on monitoring violations .

under its 2009 drinking water enforcement response policy , epa is to identify water systems with the most serious compliance problems and direct enforcement resources to these systems .

an important component of epa's enforcement policy is its enforcement targeting tool for identifying water systems with the most serious compliance problems .

the enforcement targeting tool assigns a score to each water system based on an accounting of unresolved violations over a 5-year period .

because some violations may have more serious health consequences than others , the tool assigns each violation a “weight” or number of points based on the potential threat to public health: acute health violations are worth 10 points , other health violations and some major monitoring violations are worth 5 points , and all other monitoring violations are worth 1 point .

additional points are added for each year a violation remains unresolved .

points for each violation at a water system are summed to generate the system's score .

water systems whose scores meet or exceed a certain threshold — epa has set the threshold at 11 points — are considered to have serious compliance problems and are placed on a priority list of water systems that the states and epa are to target for enforcement .

using this approach , epa and the states target resources to address those water systems that epa determines have the most significant problems complying with sdwa's requirements .

epa's enforcement response policy also provides guidance on the amount of time in which states and epa regions should address violations at priority water systems .

once systems have been targeted as a priority , states and the regions have 6 months to work with them in whatever manner they deem appropriate to resolve violations and return the system to compliance .

enforcement and compliance assistance may include a range of actions such as providing violation notification letters to systems , offering them technical assistance , conducting site visits to resolve violations , entering into compliance agreements such as consent orders , and additional formal actions such as issuing administrative orders , assessing fines or penalties , and filing or referring judicial cases .

in situations where the system is unlikely to return to compliance within the 6-month time frame , epa's policy calls for states or the regions to take a formal enforcement action within 6 months that will put the system “on the path” to compliance by laying out future actions and time frames the system needs to follow .

according to epa , states and the regions are required to enter information on enforcement actions , including violation resolution , into sdwis / state or the equivalent system as they occur and send those data to sdwis / fed every 3 months .

epa's two most recent strategic plans issued in 2006 and 2010 have included the strategic objective of protecting human health by reducing exposure to contaminants in drinking water .

these strategic plans are required by the government performance and results act ( gpra ) , which calls for related annual performance plans to outline the process for communicating goals and strategies throughout the agency , and for assigning accountability to managers and staff for goal achievement .

as we have previously reported , a clear relationship should exist between an agency's long - term strategic goals and the performance goals in the annual performance plan .

successful organizations try to link performance measures to the organization's strategic goals and , to the extent possible , have performance measures that will show annual progress toward achieving their long - term strategic goals .

gpra also requires that the agency publish an annual performance report communicating to managers , policymakers , and the public what was actually accomplished for the resources expended .

to help gauge its progress relative to its objective of reducing exposure to contaminants , epa uses annual performance measures and strategic targets to track national rates of drinking water compliance , including the: percentage of community water systems that meet all applicable health - based standards — the strategic target for 2009 was 90 percent ; percentage of population served by community water systems that will receive drinking water that meet all applicable health - based drinking water standards — the strategic target for 2009 was 90 percent ; and percentage of person months during which community water systems provide drinking water that meets all applicable health - based standards — the strategic target for 2009 was 95 percent .

epa uses the data on violations that the states report to sdwis / fed to gauge the performance of community water systems in relation to these gpra strategic targets .

according to epa , the sdwis / fed data indicated that community water systems either met , or came close to meeting , these strategic targets in 2007 through 2009 .

as part of its effort to achieve the objective in its 2010 strategic plan to reduce exposure to contaminants in drinking water , epa has also adopted performance indicators that it will use to track the number and percentage of small water systems with repeat health - based violations and the average time for those systems to return to compliance .

the data that states provided to epa did not reliably reflect the frequency of community water systems' violations of sdwa's health - based standards , according to our analysis of epa's audit data for 2007 and 2009 and past epa audit reports .

in addition , the data did not reliably reflect the frequency of monitoring violations , which are a predictor of health - based violations .

survey respondents support the concept of epa setting a numerical goal for the percentage of monitoring violations accurately reported in order to increase the reliability of data in sdwis / fed .

furthermore , data provided by the states on the status of enforcement actions taken against systems with violations were incomplete , according to epa and state officials we interviewed .

officials identified several factors , such as inadequate training , staffing , and guidance , as contributing to errors in data on violations and enforcement .

using epa's 2007 and 2009 audits of the data that the states provided to sdwis / fed , we found that the states did not completely and accurately report health - based violations committed by community water systems .

for example , we estimate that the 19 states epa audited in 2007 did not report or reported inaccurately 20 percent , or 543 , of the health - based violations that epa determined should have been reported .

for 2009 , we estimate that the 14 states epa audited in that year did not report or reported inaccurately 26 percent , or 778 , of the health - based violations that epa determined should have been reported .

figure 2 shows our estimates of the percentage of health - based violations the states did not report or inaccurately reported .

epa's audits from 1996 through 2004 also found that violations had been unreported .

for example , on the basis of its 2002 through 2004 audits , epa reported that the 37 states it audited did not report or inaccurately reported about 49 percent of health - based violations committed by community water systems to sdwis / fed , as shown in figure 3 .

it is not possible to infer a trend between 2002 and 2009 because epa's 2002-2004 results are not directly comparable to the results of our analysis of 2007 and 2009 audit data .

that is because , among other things , epa's analysis combined 3 years of audits and because the audits were of different states than were audited in 2007 and 2009 .

in its analysis of the completeness and accuracy of state data for all types of public water systems , epa found that the reliability of state data on violations varied for different health - based standards .

for example , data on violations of the total coliform rule and surface water treatment rules were more reliable than data on violations of the lead and copper treatment technique standards .

epa's audits have shown two types of errors in the data the states submitted .

the first type , known as a compliance determination error , occurs when a violation occurs but the state ( or epa region acting as a primacy agency ) does not issue a violation notice to the water system and does not report that violation to sdwis / fed .

the second type , known as a data flow error , occurs when the state or region issues a violation notice to the water system and is reported to the state data system but information about the violation is not correctly transferred to sdwis / fed .

compliance determination errors , according to our analysis of epa's data , are much more common than data flow errors .

for example , using epa's audit data from 2009 , we estimate that 91 percent of the errors in health - based violations between the audited data and sdwis / fed were compliance determination errors and 9 percent were data flow errors .

among these errors were some state - reported violations to sdwis / fed that epa determined had not occurred ( eg , false positives ) .

according to our analysis of epa's audit data from 2007 through 2009 , the states did not report or inaccurately reported the number of monitoring violations .

for example , we estimate that the 14 states audited in 2009 did not report or inaccurately reported about 54,600 — or 84 percent — of the monitoring violations committed by community water systems to sdwis / fed .

on the basis of these audit results , we conclude that the total nationwide number of actual monitoring violations had to have been considerably higher than the 82,000 reported in sdwis / fed .

monitoring violations , as we have defined them in this report , include a variety of situations , ranging from instances in which a water system did not do required monitoring , did not report the results to the state on time , or did not issue public notice of a health - based violation in a timely fashion .

it is important to note that the underreporting of monitoring violations may affect what is known about health - based violations .

some unknown percentage of both reported and unreported monitoring violations may have hidden the presence of a health violation , particularly when the violation was that required monitoring was not done at all .

our analysis found that having a monitoring violation was a strong and statistically significant predictor of whether a system had a health - based violation , among systems sampled in epa's audit data for 2007 to 2009 .

furthermore , the number of monitoring violations was positively and statistically significantly related to the rate of health - based violations .

in its 2010 report on 2007 and 2008 national drinking water compliance rates , epa noted that monitoring violations were a concern because “if a water system does not monitor and report on the quality of its water it is impossible to know if there are health - based violations.” therefore , the presence of monitoring violations may “mask” the presence of health - based violations .

the total number of “masked” health - based violations is unknown , but may be affected by the total number of monitoring violations .

as we have shown , the total number of monitoring violations is much higher than indicated by the sdwis / fed data , suggesting that the total number of health - based violations is also larger than indicated .

regarding the low quality of sdwis / fed data on monitoring violations , a majority of survey respondents who expressed an opinion ( 20 of 34 ) indicated they thought epa should — as it has for health - based violations — establish a numerical data quality goal for the percentage of monitoring violations that are completely and accurately reported .

these respondents had a range of views on what a numerical goal should be , from a low of 41 percent to a high of 100 percent ; the average was about 83 percent .

when asked to describe the actions they thought epa and the states need to take to achieve their preferred goal , the most common responses focused on increasing management prioritization , training , and information system technology .

when respondents who indicated they thought epa should not establish a data quality goal were asked to explain their answers , the most common response was that the quality of data on monitoring violations was not a high priority .

for example , one respondent said that unless the states and epa can address inadequate staffing , monitoring violations will continue to be the lowest priority .

epa's data verification audits in 2005 through 2009 did not include any analysis of the accuracy of the data the states reported to sdwis / fed on their enforcement actions .

according to epa's audits for 2002 through 2004 , the audited states did not accurately report to epa 27 percent of the enforcement actions they took against community water systems .

however , epa arrived at this estimate by comparing data in sdwis / fed with the data in sdwis / state to determine whether they matched .

epa did not examine original source documents that could have shown whether the data in sdwis / state accurately represented the status of the states' enforcement actions .

the approach epa used in its audits to estimate the accuracy of the reporting of enforcement actions by states differed from the approach it took in its audits of violations data , in which epa examined the sampling data that community water systems provided to the states .

consequently , epa's estimates of the completeness and accuracy of enforcement data were less likely to be as reliable as its audits of violations data .

epa has not conducted recent audits of enforcement data , but officials we spoke with from epa's drinking water and enforcement offices and three regions — as well as survey respondents — stated that current sdwis / fed data underreport the percentage of water systems where enforcement actions have been taken .

they also indicated that the sdwis / fed data do not accurately report the percentage of water systems that have returned to compliance .

for example , state officials told us that when they have quarterly discussions with the regions about the status of enforcement actions as shown in sdwis / fed they discover that the database is not accurate because the states have not consistently entered the data on enforcement actions into sdwis / state .

we examined violations that occurred from 2005 through 2009 to determine what the states have reported to epa .

according to our analysis of sdwis / fed data on enforcement , the states reported that less than half of these health - based and monitoring violations were resolved as of march 31 , 2010 ( see fig .

4 ) .

specifically , we found that about 59 percent of health - based violations and about 49 percent of monitoring violations committed by community water systems had not been resolved .

however , given that the enforcement data have not been audited for several years , as well as the concerns of officials we spoke with , we cannot be certain that the results of our analysis accurately reflect the status of enforcement actions .

epa and state officials responding to our survey or in interviews cited several factors as contributing to inaccuracies in sdwis / fed data on health - based and monitoring violations and the status of enforcement .

for the violations data , some factors were cited as contributing to both compliance determination and data flow errors — such as inadequate training and guidance — but the importance of these factors varied by the type of error .

for enforcement data problems , other factors were often cited , such as higher priorities , inadequate guidance , and information system flaws .

we asked survey respondents whether they thought any of five factors ( information system structure ; training by state or federal agencies ; funding from state or federal agencies ; state staffing levels ; or guidance from epa ) contributed to incorrect compliance determinations and to the less common data flow errors ; we also asked them to indicate if other factors were important .

as figure 5 shows , at least half of the 41 respondents identified each of the factors as contributing to compliance determination errors , with training and staffing cited most often .

and , as the figure shows , more than two - thirds of the 41 respondents cited the information system structure as contributing to data flow errors and more than half of respondents cited state staffing and training as contributing factors .

respondents also provided more detailed information on the factors they identified as contributing to incorrect compliance determinations and data flow errors .

for example , with regard to incorrect compliance determinations , one respondent said that training for new drinking water rules was limited and training for old drinking water rules was virtually nonexistent .

another respondent said that staffing levels were at an all - time low while another said that states had always experienced a revolving door for compliance staff .

an epa official responded that a state might not issue a violation because of “sympathy” for a water system if the state viewed the violation as not being a major health problem .

with regard to data flow errors , several respondents said that sdwis does not have adequate quality control features to clearly identify errors that might occur when the states transfer violations data from sdwis / state to sdwis / fed .

when we asked survey respondents to identify the most important steps they believe that epa could take to address compliance determination errors , the most frequent suggestions — from 18 of the 41 respondents — concerned training and guidance .

for example , 9 respondents said that epa needs to improve the timeliness of guidance on how to make compliance determinations to ensure that the guidance does not come after the date that a drinking water rule takes effect .

with respect to actions states should take , the most frequent comments related to management and training , from 25 and 22 respondents , respectively .

many of the comments regarding management called for states to conduct more thorough oversight or to hold their staff accountable .

survey respondents also identified the most important steps they believe that epa could take to address data flow errors .

the most frequent suggestions for lowering the error rate concerned information system structures and management , from 16 and 15 respondents , respectively .

with respect to information systems , respondents said that epa should take action to address the quality , complexity , or ease of use of sdwis .

most who commented on management called for more oversight and accountability .

according to epa and state officials we interviewed , as well as survey respondents , the factors that contributed to concerns about incomplete data on enforcement actions and water systems' return to compliance are similar to those that contributed to unreliable data on violations .

for example , epa and state officials told us that some state agencies have not routinely and thoroughly entered data on enforcement actions or returns to compliance into sdwis / fed because it is a low priority for their limited staff .

officials from epa regions said this is particularly the case for monitoring violations that states may have considered less serious than violations of health standards .

state and epa officials also cited a lack of guidance from epa on what conditions must exist for a system with a violation to be recorded as returned to compliance as having been a factor contributing to incomplete data on enforcement .

recognizing the importance of these definitions , epa collaborated with states and the association of state drinking water administrators on guidance it issued in 2010 .

epa regional officials told us the new definitions would likely lead to improvements in the states' reporting on returns to compliance .

in addition , officials we spoke with stated that sdwis / fed used to have an automated function that would categorize some common violations as returned to compliance if certain subsequent conditions existed .

however , that function is no longer available , meaning that state officials need to enter the information manually .

in its comments on a draft of this report , epa said that the function was removed because it did not work correctly .

see appendix ii for more details on the results of our survey .

incomplete and inaccurate data on violations and enforcement actions limit epa's ability to identify water systems with the most serious compliance problems and ensure its enforcement goals are met .

unreported violations and unreliable enforcement data also impede epa's ability to monitor and fully communicate to congress and the public the agency's progress toward its strategic objective of reducing the public's exposure to contaminants in drinking water .

incomplete and inaccurate data on violations and enforcement actions reduce epa's ability to ensure that it is achieving its goal of targeting for enforcement those systems with the most serious compliance problems .

specifically , the lack of reliable data in sdwis / fed reduces the usefulness of epa's enforcement targeting tool for identifying water systems with the most serious compliance problems .

that is , water systems without a complete violations record in sdwis / fed could receive a lower enforcement targeting score indicating a higher level of compliance than other systems whose violation record is complete .

conversely , systems whose return to compliance has not been recorded in sdwis / fed could receive a score that is higher , or worse , than warranted .

according to epa's current enforcement policy , water systems whose scores equal or exceed 11 points are considered to have serious compliance problems and are targeted for enforcement actions .

to demonstrate the effect that unreported health and monitoring violations have on the implementation of the enforcement targeting tool , we calculated two scores for each community water system audited by epa in 2007 , 2008 , and 2009 — a total of 1,225 systems over the period .

one score was based on more complete data incorporating the violations found in the data verification audits , and the other score was based on violations in sdwis / fed .

because the audited data are a more complete dataset , we expected to see , and indeed found , differences between the two scores for each system .

for 16 percent of the systems , the point difference between the two scores alone equaled or exceeded the 11-point threshold .

another 14 percent had scores that were 6 to 10 points higher , which would increase the likelihood that these systems would have been prioritized for enforcement under epa's targeting tool .

the results of our analysis are shown in figure 6 .

overall , according to our analysis , 73 percent of the water systems ( or 892 ) had a different score using the two sets of data .

twenty - seven percent ( 333 ) of the water systems showed no difference between the scores calculated using the two sets of data .

we found that the majority of score differences were the result of unreported monitoring violations .

the enforcement targeting tool assigns a much lower weight to monitoring violations than to health - based violations , but , as previously discussed , the number of monitoring violations plays an important role in limiting epa's ability to identify systems with serious compliance problems .

while most of the 1,225 water systems had a higher score with audited data than with sdwis / fed data , 2 percent ( 21 systems ) had lower scores because epa found in its audit that the violations had not occurred .

when the sdwis / fed data are incomplete , epa's ability to identify and set priorities for enforcement in water systems is compromised .

for example , because the enforcement targeting tool uses sdwis / fed data that may be missing violations , some systems may not be assigned enough points to exceed epa's threshold of 10 points for priority enforcement action .

for some of these systems , one or two additional points may be all that are needed to exceed the threshold and in other cases , as described below , the point difference for a particular system exceeded the threshold by an extraordinary amount .

for example , we calculated the following for three systems: a 170 point difference: the score we calculated was 3 points for one water system in vermont using sdwis / fed data , but 173 when we accounted for unreported health and monitoring violations that epa found in its 2009 audit .

a 138 point difference: the score we calculated was 0 using sdwis / fed data for a tribal system in new york that epa's region 2 office oversees as the primacy agency , but 138 when we accounted for unreported health and monitoring violations found during epa's 2009 audit .

a 95 point difference: the score we calculated was 0 using sdwis / fed data for a system in utah , but 95 when we used data from epa's audit .

the difference was entirely attributable to unreported monitoring violations .

our analysis echoes concerns voiced by respondents to our survey ; 22 of 41 respondents indicated that the usefulness of epa's enforcement targeting tool is affected by limitations in the sdwis / fed database .

one respondent said “missing data will significantly affect the usefulness of the results.” another respondent said the tool “is hinging on the information recorded in sdwis / fed” and that “the tool is as good as the data provided.” survey respondents and state and epa officials also reported that incomplete or inaccurate data on the resolution of violations could result in a water system receiving a higher score for enforcement priority than it merits .

epa and state officials told us that states do not always indicate in sdwis / fed that a violation is resolved , perhaps causing the enforcement targeting tool to mistakenly place the system on the targeted enforcement list .

according to one survey respondent , this condition will “confuse states and lead to continued poor quality data.” another respondent said that use of the enforcement targeting tool “is a waste of time” without steps taken to fix this issue .

state officials told us that in their regular review of the targeted list with epa regional officials , they can recognize when a system has been erroneously included on the targeting list because resolved violations were not recorded and they can correct the discrepancy .

however , epa officials have told us this data correction process is a time - consuming one that places additional demands on limited state and epa enforcement staff .

incomplete sdwis / fed data can also limit epa's ability to ensure that the states meet the agency's enforcement goal that targeted systems have returned , or are returning , to compliance in a timely fashion .

epa's enforcement response policy calls for states to work with systems to resolve violations or put the system on a “path to compliance” within 6 months of when the system becomes a priority system on an enforcement targeting tool list .

however , unreported data on enforcement actions can hamper epa's ability to determine whether states have met that goal .

for instance , while states might take an enforcement response that leads , or will lead , a water system to resolve the violation , states frequently do not enter this information into the sdwis / fed database or enter the information months or years later , according to epa and state officials we spoke with .

either situation hampers epa's ability to track the timeliness of enforcement responses .

unreported violations and enforcement data impede epa's ability to fully measure and communicate its progress toward meeting the strategic objective of reducing human exposure to contaminants in drinking water .

the agency has established a number of indicators and targets that it uses to measure its progress toward meeting that objective .

however , the unreliable quality of the violations data and concerns about the accuracy of enforcement data in sdwis / fed make it difficult for epa to reliably communicate the relative public health risk posed by community water systems' noncompliance with sdwa and the progress made in resolving noncompliance in a timely manner .

for example: epa's 2011 national water program guidance contains an indicator for the number and percentage of systems serving less than 10,000 people with certain repeated health - based violations .

epa's ability to set and reliably use this type of indicator requires complete and accurate data on violations , but as we have shown , the sdwis / fed data on violations are not reliable .

epa's 2011 national water program guidance also contains an indicator for the average time taken for systems serving less than 10,000 people to return to compliance after committing certain health - based violations .

however , the ability to set and reliably use an indicator of this type requires complete and accurate data on enforcement actions .

as we have previously indicated , epa and state officials we interviewed told us the enforcement data in sdwis / fed are not reliable .

unreliable data quality also limits epa's ability to introduce or modify targets to manage its program and communicate progress in meeting the program's goals .

quality data are necessary to accurately measure performance relative to strategic targets .

two key epa strategic targets associated with the agency's strategic objective of reducing exposure to contaminants in drinking water — the percentage of community water systems that met all health - based standards and the percentage of the population served by community water systems that received drinking water that met all applicable health - based drinking water standards — are broad measures of compliance .

however , these measures do not provide information on the relative severity of the violations or account for systems that have multiple health - based violations , offering the public a narrow view of the quality of the nation's water systems and not clearly communicating the public health risk posed by these systems' noncompliance with sdwa .

for example , a water system with multiple health - based violations is effectively “counted” the same as a system with one health - based violation .

thus , the relative health risk posed by different systems' noncompliance is not apparent .

without complete and accurate sdwis / fed data it is difficult to develop a new measure or modify these strategic targets .

similarly , without complete and accurate data from the states , epa will be unable to establish reliable measures or targets regarding the rate of reduction in health - based violations or compliance with monitoring requirements or further epa's core value of transparency .

epa and the states have taken actions over many years to identify and address the causes of incomplete and inaccurate violations data , but those efforts have not been fully successful , according to those we surveyed .

epa has conducted audits to assess the quality of state violation data in sdwis / fed and developed recommendations for improving data quality .

survey respondents generally reported that those audits have contributed to improvement , but epa has discontinued them .

epa and the states also established work groups to address data management and quality .

in addition epa has emphasized the importance of specific data quality management tools , although it has not required states or regions to use them .

more recent epa initiatives include a new strategy for data sharing , plans to redesign sdwis , and a new tool to help the states make and report compliance determinations and enforcement actions .

as described earlier , epa used its data verification audits to assess the quality of the violations data and , to a lesser extent , the enforcement data the states have submitted to sdwis / fed .

the agency also used the audits it conducted from 1996 through 2004 to develop state - specific and national recommendations for improving data quality .

epa and state officials we surveyed had mixed , but generally favorable , views about the value of the audits' recommendations with regard to improving data quality .

eight respondents said the recommendations were very effective in improving data quality , while most respondents ( 26 of 39 ) said the recommendations were only slightly or moderately effective .

according to respondents , the audits pointed out states' inefficiencies and poor practices .

for example , one respondent said that states are able to use the results as a guide to improve training for staff and improve data quality .

despite the recommendations offered to help states , six respondents indicated that the states or regions did not adequately change their practices in response to the audit findings .

for example , one epa headquarters manager commented that states may incorrectly interpret systemic problems identified through the audit as isolated problems to be corrected only at the water systems covered by the individual audits .

nonetheless , seven respondents stated that the audits' scope or methodology was not adequate to determine data quality .

epa discontinued the audits of violations data in 2010 due to funding constraints .

according to the director of the office of ground water and drinking water , epa may be able to resume the audits in 2011 , but at a much reduced number .

epa conducted an average of about 17 audits of states , regions , and other primacy agencies in 2007 through 2009 , but the director told us in december 2010 the agency may be able to do 4 or 5 in 2011 .

epa had not done any 2011 audits as of june 2011 .

in its comments on a draft of this report , epa said that the office of water will conduct six to eight audits in 2011 .

in the 1990s , epa and the states jointly established two work groups charged with providing analysis and recommendations on various aspects of data management and formed a third such group in 2010 .

according to most of their members , the two older groups — the data management steering committee and data technical advisory committee — were effective at helping epa and the states improve data quality .

on the other hand , the members of the newer data quality work group were almost evenly divided on whether this new group has been effective or not effective .

the data management steering committee is charged with supporting epa and the states in their cooperative efforts to enhance management of drinking water data .

in explaining their answers to a question about the group's effectiveness , one - third of the members said the committee had helped epa and the states understand the nature of the data quality problem and about one - half said it provided direction .

however , one - third of the members commented on the lack of implementation of the committee's recommendations .

the data technical advisory committee is responsible for recommending ways to obtain the data epa needs to carry out its pwss program responsibilities .

about three - fifths of the members commented that the committee had helped epa and the states understand data problems and had provided direction .

however , similar to the steering committee , advisory committee members had concerns about epa's implementation of recommendations , with close to half saying that the agency's implementation had been inadequate .

the data quality work group met several times in 2010 and outlined draft recommendations for improving data quality , including additional training , standard operating procedures for staff managing any new drinking water rules , checklists of rule milestone dates for states , and quality assurance / quality control checks for sdwis data .

however , the office of ground water and drinking water diverted the work group staff in 2010 to focus their attention on implementing the administrator's drinking water strategy before the group could issue final recommendations , according to senior office staff .

perhaps in light of that , among the most common comments from members of the group was that the work group was too new to evaluate or that its activity level had been inadequate .

the office of ground water and drinking water staff noted they would consider the work group's draft recommendations in its redesign of sdwis .

in march 2008 , epa reported the results of the audits it conducted of state data in 2002 through 2004 .

as it had done in its prior reports on state audits , epa included recommendations for improving data quality in this report ; these recommendations took the form of an action plan in its 2008 report .

the 2008 action plan was a joint effort of epa and the association of state drinking water administrators to provide recommendations for achieving the goal set in 2006 of 90 percent complete and accurate data for health - based violations , as well as improving the quality of monitoring violations data .

according to the action plan , the largest challenge was ensuring that all data reflecting determinations of violations were entered into sdwis / fed .

the plan called for , among other things , the development of new data management tools as well as the implementation of tools that epa has developed over the years to improve compliance determinations .

however , as discussed below , widespread implementation of those tools has not yet occurred .

in april 2009 , the director of epa's office of ground water and drinking water issued a memorandum to epa's regional water management directors calling attention to ( 1 ) the incomplete implementation of the 2008 action plan and ( 2 ) the importance of increasing oversight and accountability of the states for the quality of drinking water data .

noting that the quality of drinking water data in sdwis / fed was called into question in the media in the late 1990s and was the subject of a 2004 report by epa's office of inspector general , the director stated that the quality of data continued to be too low .

she also cited the 2006 agreement between epa and the association of state drinking water administrators to set a goal that 90 percent of health - based drinking water violations be completely and accurately reported to sdwis / fed and said that more than 10 states had met the goal , but the overall goal had not been met .

in her memorandum , the director noted that one of the conditions of primacy the states must meet is to report all violations .

to improve data quality , the director called upon the regions to increase their efforts to implement the 2008 action plan .

her memorandum and the action plan call for the states to increase their use of several data management tools that epa has developed over the years to improve accuracy , including sdwis / state , electronic data verification ( edv ) , and electronic reporting from laboratories to states .

although epa has developed these tools to improve compliance determinations and data flow , the states are not required to use them as a condition of primacy or their pwss grant agreements .

we gathered information from epa regarding the current status of these tools and asked survey respondents to comment on the factors that have limited the use of these tools and the steps they believed should be taken to increase the tools' use .

epa reported to us in march 2011 that eight states , one tribe , and one territory were not using sdwis / state at all .

of the states that do use sdwis / state , some report using the database only for particular drinking water rules .

for example , an epa survey of states in mid - 2010 found that 20 of the 55 primacy agencies were using sdwis / state to make compliance determination decisions for the surface water treatment rule and 35 were using it for the total coliform rule .

due to limitations in the data available to us and inherent difficulties in establishing a cause - and - effect relationship , we could not determine whether a state's use of sdwis / state leads to more reliable data on violations of particular sdwa rules .

however , 26 of the 41 respondents indicated that more widespread use of sdwis / state would improve data quality ; 5 respondents indicated it would not ; and 10 indicated they did not know .

of the 26 respondents who provided detail on why they thought more widespread use of sdwis / state would improve data quality , 17 indicated it would promote more consistent and accurate compliance determinations through automation .

the most common theme among the respondents' suggestions for what epa should do to address the factors that have prevented full use of sdwis related to the quality , complexity , and ease of use of the system .

for example , one respondent said that epa should be aware of the needs of drinking water managers , who are the principal users of sdwis / state , rather than database managers .

according to epa officials , as of march 2011 , only seven states had done pilot tests of epa's electronic quality control tool for sdwis / state , known as electronic data verification , or edv .

epa officials told us that the edv tool needs additional refinement to be fully compatible with sdwis / state .

edv could assist states in making compliance determinations according to 18 of the 24 survey respondents who were familiar with it .

they said that using the tool would improve data quality by improving states' oversight or auditing capability .

however , epa and state officials told us this tool can be used only by states that use sdwis / state to manage all of the drinking water rules , and survey respondents noted that the need to fully use sdwis / state was a factor that prevented more states from using edv .

the survey respondents' most common recommendation for epa was to improve the quality and ease the use of the tool .

for example , one respondent said that epa needs to update the tool to keep pace with changes in regulations .

at the same time , many respondents suggested that the states need to make a greater commitment to using sdwis / state and edv more .

epa has developed a tool that testing laboratories can use to electronically transmit the results of community and other public water systems' monitoring directly to the state .

however , according to epa's mid - 2010 survey , only 19 states were using this tool .

of the 40 survey respondents who expressed an opinion , 39 believed that more widespread use of this tool would improve data quality , and 25 of these respondents stated the tool would reduce data entry errors and increase accuracy .

another 11 respondents said the tool would increase the speed of data exchange between the states and epa , and 10 said it would free up state resources that could be used to improve data quality in other ways .

for example , one respondent said that , in the long run , electronic reporting should save state resources by reducing the need for data entry staff but that in the short run , switching to electronic reporting requires technical support for the laboratories and additional resources .

according to our survey respondents , the two leading barriers to having more states require electronic reporting are ( 1 ) laboratories' inadequate capability to implement the reporting technology and ( 2 ) the states' lack of legal authority to make the tool's use a requirement .

specifically , many survey respondents said that laboratories , particularly small ones , are not always adequately equipped or staffed to adopt electronic reporting .

several survey respondents said that epa needs to provide support to laboratories to make it easier to adopt the tool .

some respondents also said that as epa's current sdwa regulations do not require electronic reporting and some state laws prohibit state agencies from including requirements in their pwss programs that are more stringent than what is required by sdwa , the state agencies are unable to require electronic reporting .

nine respondents said that epa should require electronic reporting .

several respondents also identified epa's cross - media electronic reporting rule as a regulatory barrier to more widespread use of electronic reporting .

this rule provides the legal framework for electronic reporting under all of the agency's environmental regulations .

the rule requires states , tribes , and local governments that wish to use electronic reporting for implementing authorized federal environmental programs to obtain epa approval , which may require modifications to electronic reporting systems to meet epa requirements .

one state respondent said that many states do not have the financial resources to build a system to receive data electronically because of the constraints of this rule , noting that the rule places very tight requirements on the security required for receiving data electronically .

several respondents called on epa to review the need for the rule or abolish it .

citing the need for strong regional oversight of the states' pwss programs , the director also requested in her april 2009 memorandum that the regions take specific actions over and above those identified in the 2008 action plan .

specifically , she called for the regions to provide documentation of good standard operating procedures and lessons learned that may enable epa to improve sdwis / fed data quality , among other things ; discuss with states annually ( or more frequently ) the completeness and accuracy of the violations data reported to sdwis / fed , including a review of the state's implementation of recommendations contained in previous data verification reports ; and include language in future pwss grant agreements indicating that the state must make compliance determinations that are consistent with applicable drinking water regulations , report all violations and enforcement actions to sdwis / fed in a timely fashion , and otherwise comply with 40 cfr §142.15 .

also include any corrective action steps identified by data verification audits or program reviews in the state's annual work plan .

although the director of the office of ground water and drinking water requested these actions , she told us her office does not have the authority to require the regions — which report directly to the epa administrator — to do so .

the director and other drinking water program managers we interviewed told us the regions had responded to her request , but the office had not assessed and could not document the extent to which the regions had complied with the requests to discuss data quality with the states or add relevant language to grant agreements .

in its comments on a draft of this report , epa stated that all of the regions have incorporated data quality into their discussions with states and data quality has been incorporated into grant agreements or state workplans .

however , we were unable to verify these statements ; in response to our request during the comment process , epa said that documentation was not available .

according to epa , it made the statements on the basis of e - mail communications and discussions between the managers and staff in the office of ground water and drinking water and regional management and staff .

the director also requested comments from the regions' water management directors on four proposed measures that would assist the office in monitoring the regions' oversight of the states' performance , including several directly related to the steps discussed above: the percentage of states within a region with which the region has an annual discussion regarding data quality ; the percentage of states that have an action plan to correct deficiencies relating to drinking water compliance determinations or data reporting that were noted in the most recent epa data verification audit ; the percentage of a region's annual pwss grants that include grant conditions requiring the states to make compliance determinations that are consistent with drinking water regulations ; and the extent to which a region is achieving epa's goal that 90 percent of health - based violations are completely and accurately reported to sdwis / fed , when a region is acting as the primacy authority for a particular rule in a state .

according to office of ground water and drinking water officials , the regions responded to the director's request for comments but did not fully support the proposed performance measures , and none of these have been implemented as of march 2011 .

in march 2010 , the administrator of epa issued a drinking water strategy that called for , among other goals , the agency and the states to increase data sharing on water systems .

as part of this strategy , epa announced it will redesign sdwis to help to meet the administrator's goals for data sharing .

according to the director of the office of ground water and drinking water , software for the next generation of sdwis is only at the beginning stages of development ; she anticipates it will be ready by 2014 , depending upon the availability of funding .

to help achieve the goals of the strategy , epa signed a memorandum of understanding in november 2010 with three associations that represent state agencies and officials .

epa also formed an implementation work group comprising agency and state officials to further the data sharing goals spelled out in the memorandum .

the november 2010 memorandum of understanding on data sharing — which is a voluntary agreement among the parties — outlines the vision , goals , terms , and conditions under which drinking water monitoring data are to be exchanged between the states and epa .

the anticipated benefits of data sharing include allowing states to more readily compare their water system monitoring results with epa regulations and with the sdwis / fed data before the states submit the data to epa .

epa intends to use the shared data for a variety of purposes , including calculating national and state data completeness , accuracy , and timeliness ; evaluating differences in state interpretations of epa regulations ; and conducting national program oversight .

the mission of the implementation working group is to recommend ways for states to share appropriate compliance monitoring data that eventually will be housed in the next generation of sdwis .

epa officials said they expect this redesign of sdwis — and accompanying revisions to state data submission requirements — to expand the amount of data that epa receives electronically from the states .

with sdwis / fed , epa generally only receives data from the states on inventories , violations , and enforcement actions .

according to the director of epa's office of ground water and drinking water , the next generation of sdwis would give epa access to the compliance monitoring and enforcement data now collected by the states .

she told us that having direct access to the states' raw monitoring data would improve epa's ability to better understand national patterns of compliance and to diagnose problems faced by states .

for instance , according to the director , the data could reveal that particular rules are hampered by a misunderstanding of the requirements , and epa could use that information to write regulations that are easier to understand and report .

the director said that a redesigned sdwis could also reveal and address instances when a state has made a compliance determination error .

however , she also said that some compliance determination errors can be addressed with a redesigned sdwis , but others might need to be addressed through better training or writing regulations more clearly so that state staff understand what constitutes a violation .

according to the director , under the agency's current position , states will continue to have the option to use the next generation of sdwis .

epa will continue to provide ways for states that do not use sdwis to transfer their data to the agency .

however , epa will expect those states , like those using sdwis , to share their compliance monitoring data with epa .

in november 2010 , the office of ground water and drinking water unveiled an initial version of another tool for improving compliance determinations and data quality — the compliance determination and violation / enforcement reporting tool .

according to the office , the tool was designed by region 5 staff to consolidate , update , and supplement epa guidance on sdwa violations of specific requirements in one electronic document .

the tool includes violation descriptions , compliance determinations , violation reporting instructions , common discrepancies from data verifications , related epa memos , enforcement tracking instructions , and return to compliance definitions .

the target audience for the tool is state and regional compliance , enforcement , and data staff .

the tool is being developed in modules for each national primary drinking water regulation .

the first module of the tool was for the lead and copper rule .

epa officials said they anticipate all modules will be developed by the end of fiscal year 2011 if funding is available .

epa relies on the soundness of state - reported data to ensure that community water systems are complying with sdwa and that the states and regions are taking appropriate enforcement actions against noncompliant water systems .

as our analysis of epa's audit data for 2007 through 2009 shows , however , states continue to fall short in providing accurate and complete data on health - based and monitoring violations .

epa's data verification audits after 2004 did not examine the quality of data on enforcement actions and systems' return to compliance , but epa officials and survey respondents told us that current state data on such actions are also incomplete or inaccurate .

in addition to discontinuing its audits of enforcement data after 2004 , epa also discontinued its audits of violation data after 2009 because of budget constraints .

the agency hopes to resume these audits of violation data in 2011 , but the number of states to be audited would be greatly reduced from an annual average of about 17 from 2007 through 2009 to 6 to 8 .

conducting fewer audits of state - reported data — both violations and enforcement data — will hamper the effectiveness of epa's oversight of the states and its ability to assess its efforts to improve data quality .

epa and the association of state drinking water administrators established a goal of completely and accurately reporting 90 percent of health - based violations , but epa has not set a similar goal for monitoring violations .

monitoring violations may reflect a wide range of circumstances , such as instances in which monitoring was done but was not reported to the state in a timely fashion or the potentially more serious situation in which required monitoring was not done at all .

we found , however , that the number of monitoring violations was positively and statistically significantly related to the rate of health - based violations .

recognizing the importance of having complete and accurate data on monitoring violations , a majority of those state and epa officials we surveyed who voiced an opinion supported the idea of having a goal for the quality of data on monitoring violations .

as called for by gpra , epa has established several performance measures with associated targets and indicators for community water systems to assess progress toward the agency's strategic objective of reducing exposure to contaminants in drinking water .

each year , epa publicly reports systems' performance levels relative to the targets using data from sdwis / fed .

to be useful and appropriate , these performance measures should clearly reflect conditions that directly relate to human exposure to contaminants .

however , we found that some of the measures that epa relies upon to gauge national compliance levels measure how many systems are out of compliance but not the extent to which they are out of compliance , which does not clearly communicate the public health risk posed by these systems' noncompliance with sdwa .

in addition , some measures that epa uses depend on data on violations and the status of enforcement actions that are unreliable .

we found that incomplete and inaccurate data could impede epa's ability to monitor and report progress toward its strategic targets for those measures , including having those systems return to compliance in a timely manner .

recognizing its long - standing problem of receiving incomplete and inaccurate state data on violations , epa has made efforts to improve the quality of data reported to sdwis / fed .

however , many of those efforts , including those to implement epa's 2008 action plan , have not been fully successful .

in light of the need for more improvement , the director of the office of ground water and drinking water requested in her 2009 memorandum that the regional water managers take numerous steps to implement the action plan — including encouraging the states to increase their use of sdwis / state , electronic data verification , and electronic reporting from laboratories to states .

both the action plan and additional steps requested in the memorandum have the potential to address the factors epa and state officials identified as contributing to unreliable data quality .

according to the director , however , the states currently are not required to use the data management tools that epa has developed as a condition of primacy or their pwss grant agreements , and many have chosen not to do so .

in response to our survey , epa and state drinking water officials generally said these tools would help improve data quality but noted barriers they believe have prevented more widespread use of them .

for example , the most common theme among the survey respondents' suggestions for increasing the use of the sdwis / state and electronic data verification tools was to address their quality , complexity , and ease of use .

the director of the office of ground water and drinking water also asked the regional water management directors in her memorandum to increase their oversight of state programs .

for example , she asked the regions to include language in future grant agreements indicating that the state , among other things , must make compliance determinations that are consistent with applicable drinking water regulations .

however , the director also indicated that her office does not have the authority to require the regions to take the actions requested in the memorandum , and could not document the extent to which the regions had done so .

epa's plan to develop a next generation of sdwis and to increase its access to state data might help the agency ensure that it receives higher quality violations and enforcement data from the states .

however , it is uncertain if and when the new system or increased access to data will be available .

in the meantime , further efforts to overcome the barriers to implementation of the 2008 action plan and the director's 2009 memorandum are needed to improve state data .

to improve epa's ability to oversee the states' implementation of the safe drinking water act and provide congress and the public with more complete and accurate information on compliance , we recommend that the administrator of epa take the following four actions: resume data verification audits to routinely evaluate the quality of selected drinking water data on health - based and monitoring violations that the states provide to epa .

these audits should also evaluate the quality of data on the enforcement actions that states and other primacy agencies have taken to correct violations .

work with the states to establish a goal , or goals , for the completeness and accuracy of data on monitoring violations .

in setting these goals , epa may want to consider whether certain types of monitoring violations merit specific targets .

for example , the agency may decide that a goal for the states to completely and accurately report when required monitoring was not done should differ from a goal for reporting when monitoring was done but not reported on time .

consider whether epa's performance measures for community water systems could be constructed to more clearly communicate the aggregate public health risk posed by these systems' noncompliance with sdwa and progress in having those systems return to compliance in a timely manner .

work with the epa regions and states to assess the progress made in implementing the steps called for by the 2008 action plan and the director of the office of ground water and drinking water's 2009 memorandum ; identify the barriers that have prevented more widespread implementation of the action plan and memorandum ; and develop and publish a strategy for overcoming those barriers .

we provided a draft of this report to epa for review and comment .

the agency provided written comments , which are reproduced in appendix iii .

epa partially agreed with two of our recommendations , disagreed with one , and neither agreed nor disagreed with another .

our responses to epa's comments on our recommendations follow , and our responses to epa's attachment of substantive comments are in appendix iii .

epa also provided technical comments that we have incorporated as appropriate .

in its overall comments , epa said that it recognizes the importance and value of high quality data to complement the activities that comprise its oversight of primacy agencies .

epa also acknowledged that gao found data quality problems similar to those previously found by the agency during data verification audits and that underreporting violations data and enforcement actions may limit the public's full knowledge of the status of public water system compliance .

epa noted that it has implemented a number of activities to improve data quality and its ability to oversee the drinking water program .

we agree that epa has taken steps to improve data quality and describe many of them in our report .

epa also noted that complete and accurate data are important in order to effectively target enforcement to those systems with the most serious compliance problems .

the agency added that its 2009 enforcement targeting tool provides an incentive to the states to keep their enforcement data current to ensure that the tool yields accurate scores .

we agree that the tool underscores the importance to the states of keeping enforcement data current .

however , the scores generated by the tool will also be incorrect if data on the existence of violations are incomplete or inaccurate .

we believe that whereas the use of the tool provides an incentive to the states to improve the accuracy of their enforcement data , it does not necessarily provide them an incentive to improve the accuracy of their violations data .

epa partially agreed with our first recommendation that it resume data verification audits of violations and enforcement actions .

the agency stated that it has found that data verification audits provide valuable information on data completeness and accuracy and that it plans to conduct six to eight audits during calendar year 2011 .

however , epa did not commit to conducting data verification audits beyond 2011 .

instead , epa said that until the next generation of sdwis is deployed , thus enabling the agency to view compliance monitoring data and compliance determinations directly , it will consider using data verification audits to evaluate data quality .

epa did not comment on how it would evaluate data on enforcement actions taken to correct violations .

we understand that the next generation of sdwis may enable epa to more directly monitor water systems data and oversee the states' compliance determinations .

we note that the director of the office of ground water and drinking water said that the new system would not be available until 2014 , depending on the availability of funding .

we continue to believe that epa should commit to , not merely consider , conducting data verification audits until the new system is available , and that those audits should also evaluate the completeness and accuracy of enforcement data .

epa did not clearly indicate its agreement or disagreement with our second recommendation that it work with the states to establish a goal , or goals , for the completeness and accuracy of data on monitoring violations .

the agency stated that it appreciates the need for improved data quality for those types of violations .

however , epa neither indicated that it would adopt a goal nor offered any reasons for why a goal — such as the one it has for the quality of data on health - based violations — would be inappropriate .

instead , the agency suggested that along with technology enhancements as part of the next generation of sdwis , ( 1 ) it will consider changes to its approach to reporting violations data , and ( 2 ) will explore the possibility of revising the enforcement targeting tool , which could improve its oversight capabilities .

we are not able to evaluate these changes given their speculative nature , and it is not clear how they might be relevant to achieving a higher degree of data quality .

epa also stated that the regions' annual incorporation of data quality in state grants and workplans will improve epa's oversight capabilities .

we agree that increased emphasis from the regions is necessary and could lead to improved data quality .

however , we continue to believe that setting a goal for the quality of data on monitoring violations would emphasize its importance and encourage the states to make and report correct compliance determinations .

epa disagreed with our third recommendation that it consider whether its performance measures could be constructed to more clearly communicate the aggregate public health risk posed by systems' noncompliance with sdwa .

the agency noted that its program guidance currently includes a measure that attempts to address the duration ( in “person months” ) of time consumers may be exposed to health - based violations .

we describe this measure in the background section of our report .

however , we believe this measure has the same limitation as other epa strategic targets , in that it does not distinguish between water systems with multiple health - based violations in a particular month and those systems with a single violation in that month .

epa also stated that it uses a variety of tools that may convey information on risks associated with noncompliance and show progress toward returning systems to compliance better than a new performance measure would .

among the tools epa identified is a web site containing detailed information about the violations for individual water systems .

epa also said that it recently posted drinking water data to its enforcement and compliance history online tool .

similarly , epa said water systems directly convey to their customers information on public health risks associated with violations through public notifications and consumer confidence reports .

we acknowledge these tools provide the public with details on the violations that states and water systems have reported for individual water systems .

however , our recommendation encourages epa to consider changes to its performance measures to provide congress and the public a clearer understanding of noncompliance with sdwa at a national level and not elicit more information about the performance of individual water systems .

we continue to believe that it is important for epa to develop a national performance measure that helps gauge epa's overall management of the drinking water program .

regarding our fourth recommendation that epa work with the regions and states to assess progress in , and develop a strategy for overcoming barriers to implementing the 2008 action plan and the director's 2009 memorandum , the agency expressed partial agreement by saying it will continue to assess the progress of improving data quality .

epa also noted that since these documents were issued , the office has worked with state and regional staff to understand data quality challenges and opportunities for improvement .

specifically , epa commented that all of the regions have incorporated data quality into their discussions with states and that data quality has been incorporated into grant agreements or state workplans .

we agree that those actions would signal progress toward implementing the director's memorandum .

however , we were unable to verify these statements because epa told us that documentation was not available .

epa told us it made the statements on the basis of e - mail communications and discussions between the managers and staff in the office of ground water and drinking water and regional management and staff .

epa also stated that the data quality work group formed by the director developed a list of recommendations to address underlying data quality problems and that it will continue to evaluate those recommendations .

as we note in the report , the recommendations were in draft , and epa's comments did not provide evidence that they have been adopted .

epa identified other future actions that it believes will lead to improved data quality .

for example , epa emphasized the effect that a next generation of sdwis could have on improving data quality .

we do not disagree that these actions , if taken , may contribute to improved data quality .

however , we point out that epa has already developed tools that states could use to improve the quality of their data on violations but that those tools have not been widely used .

there is no requirement that the states use the next generation of sdwis , if and when it is available .

furthermore , epa did not directly address the recommendation to identify the barriers that have prevented more widespread implementation of the action plan and memorandum and develop and publish a strategy for overcoming those barriers .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution for 30 days from the report date .

at that time , we will send copies to the appropriate congressional committees , the administrator of epa , and other interested parties .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff members have any questions about this report , please contact me at ( 202 ) 512-3841 or trimbled@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

to examine the quality of the safe drinking water information system / federal ( sdwis / fed ) data that the environmental protection agency ( epa ) uses to measure community water systems' compliance with the health - based and monitoring requirements in the safe drinking water act ( sdwa ) , we examined the results of audits epa conducted from 1996 through 2009 in which it assessed — for a sample of states — the completeness and accuracy of violations data those states submitted to sdwis / fed .

we evaluated the methods that epa used to conduct those audits to test the methods' validity and determined that , while limited , these methods were sufficiently reliable for the purposes of our review .

we also conducted our own analysis of epa's audit data from 2007 through 2009 in order to arrive at estimates of the quality of the data that states reported to sdwis / fed .

we focused our analysis on community water systems only .

the sample design for the epa data verification audits consists of a nonprobability sample of primacy agencies within a given year and a probability sample of community water systems within each selected primacy agency .

based on our review of the sample design , we determined that it is not appropriate for our purposes to make quantitative statements or inferences about the entire nation from the selected primacy agencies or comparisons with sampled primacy agency data quality results from the previous years .

as such , we only generated estimates to the states audited within a given year .

table 1 provides a description of the number of primacy agencies that were included in the sample for each year .

we classified violations into one of two types: health - based violations and monitoring violations .

by definition , monitoring violations include “other” violations such as public notification and consumer confidence report violations .

we reviewed and decided to use definitions of violation types provided by epa to make these classifications .

we included lead and copper treatment technology violations as health - based violations in our analysis .

we defined three separate measures of data quality: accuracy , completeness , and overall quality ( a combination of accuracy and completeness ) .

these measures are consistent with the measures used by epa in previous years .

we reviewed and decided to use definitions provided by epa to calculate these measures .

to estimate the percentage of violations that were accurate and complete , we first created a data set with one observation per violation , and then we used a procedure in statistical software that appropriately accounts for the stratified cluster sample design .

we calculated point estimates and 95 percent confidence intervals .

we did not report estimates that have margins of error that exceed plus or minus 20 percentage points at the 95 percent confidence level .

to determine whether there was a relationship between monitoring violations and health - based violations , we used audit data to estimate regression models controlling for size , source and administrative control .

we tested various specifications of three types of statistical models to ensure that the significance and magnitude of our estimates were consistent across statistical models .

we conducted a subpopulation analysis to estimate the percentage of incomplete ( not reported ) violations that were either compliance determination or data flow reporting discrepancies .

we calculated point estimates and 95 percent confidence intervals .

we did not report estimates that have margins of error that exceed plus or minus 20 percentage points at the 95 percent confidence level .

the second component of our first objective was to examine the quality of sdwis / fed data on the status of the states' and epa regions' enforcement actions .

because epa's recent audits of state data did not assess the completeness and accuracy of enforcement data in sdwis / fed , we examined epa's national sdwis / fed data from 2005 through 2009 to determine the percentage of violations the states have identified as either resolved ( known as returned to compliance ) , addressed through an enforcement action but not yet resolved , or not addressed .

we then interviewed epa officials to obtain their views on the completeness and accuracy of those data , and also analyzed relevant comments from survey respondents .

the survey was of epa and state drinking water officials to obtain their views on a range of issues related to data quality .

 ( see app .

ii for more details on our survey methodology ) .

to identify factors that have affected data quality , we analyzed 41 survey responses representing the views of all 44 members of three joint epa - state work groups that were created to address various aspects of data management .

to examine the ways in which sdwis data quality could affect epa's management of the public water system supervision ( pwss ) program , we examined the importance of data quality for two aspects of epa's management of the pwss program .

first , we examined the potential impact data quality could have on epa's enforcement response policy .

this policy uses a targeting tool that assigns scores to community water systems that are a high priority for enforcement action because of unresolved violations .

to demonstrate the effect that underreported health and monitoring violations can have on the enforcement targeting tool , we calculated two scores for each of the approximately 1,200 water systems audited by epa in 2007 , 2008 , and 2009 .

one score was based on violations found in the data verification audits , and the second score was based on violations found in sdwis / fed .

we then subtracted the two scores for each system to obtain a point difference , a result that we used to illustrate the impact of incomplete data on the scoring process .

we used the same methodology epa uses to create the enforcement score: we assigned point values to unresolved violations ( acute health violations are worth 10 points , nonacute health violations and some monitoring violations are worth 5 points , and 1 point for all other monitoring and reporting violations ) and then added these points together to produce an overall score for each system .

however , the enforcement scores we calculated cannot be considered a water system's actual score for three reasons: 1 .

epa's targeting tool scores 5 years of violation data , whereas the audits that epa conducted reviewed state files to identify violations that had occurred in shorter periods of time .

those time periods typically ranged from 1 to 3 years , depending on the drinking water regulation .

2 .

due to limited data in epa's audit database , our enforcement scores only include underreported violation discrepancies and do not include any discrepancies related to accuracy .

however , underreported violations accounted for nearly 97 percent of the discrepancies .

3 .

epa's enforcement score includes an additional penalty that is tied to the year of the oldest unaddressed violation .

for instance , if a violation is 5 years old , epa adds an additional 5 points to the score .

because our analysis considered violations from a shorter period of time , we could not duplicate this additional penalty .

while these three limitations prevent us from duplicating epa's exact targeting tool , our analysis presents a conservative estimate of the effect that poor data quality has on the enforcement scoring process .

it is likely that additional years of underreported violations , plus other enforcement penalties , would reveal further distortions of the scoring process .

in addition , we examined survey respondents' views on the impact that data quality may have on implementation of the enforcement response policy .

we also interviewed epa and state officials to obtain their views on the matter .

second , we examined the impact data quality could have on the agency's ability to inform the public and congress about water systems' compliance with drinking water standards .

in particular , we examined whether epa's claims about community water systems' performance relative to government performance and reporting act ( gpra ) goals were affected by the use of incomplete and inaccurate sdwis / fed data .

two key gpra measures are the percentage of community water systems and the percentage of population served by those systems that meet all health - based drinking water standards ( i.e. , had no health - based violations ) in a fiscal year .

we used epa's data verification audit data to estimate the percentage of community water systems that met all health - based drinking water standards for selected states within each audit year .

based on our review of epa's data verification audit sample design , we determined that the sample was not designed to produce reliable estimates of the percentage of the population served by systems with health - based violations .

therefore , we focused our analysis on the percentage of community water systems that met all health - based drinking water standards .

to estimate the percentage of community water systems that met all health - based drinking water standards from the data verification audit data , we counted the number of health - based violations for each community water system in the sample and calculated point estimates and 95 percent confidence intervals of the percentage of systems that met all health - based drinking water standards after accounting for the results of the data validation audits .

to examine the actions epa and the states have been taking to improve the quality of data in sdwis / fed , we interviewed epa officials and obtained and reviewed documentation on steps the agency has taken , or is considering , to modernize sdwis and improve data quality .

we also examined survey respondents' views on steps that epa and the states could take to address data quality — including the adoption of particular data management tools — and ways in which the three epa - state work groups could be more effective .

for more information on our survey and content analysis , see appendix ii .

we conducted this performance audit from february 2010 through june 2011 in accordance with generally accepted auditing standards .

those standards require that we plan and perform the audit to obtain sufficient and appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

to obtain the views of knowledgeable epa and state drinking water officials about sdwis data management and data quality , we surveyed the members of three epa - state work groups that were formed to address various aspects of drinking water data: the data management steering committee , the data technical advisory committee , and the data quality work group .

epa and the association of state drinking water administrators provided us with the names and e - mail addresses of the 46 members of these groups .

the work group members come from epa headquarters , epa regions , states , and the association of state drinking water administrators .

the data management steering committee had 18 members , the data technical advisory committee had 15 members , and the data quality work group had 25 members .

nine officials served on more than one of the committees .

our survey asked a range of questions related to the drinking water violations data that states and other primacy agencies provide to epa .

we asked the respondents to comment on the factors that have contributed to data errors , the steps that should be taken to correct those errors , the impact that data errors could have on epa's enforcement response policy , and various data management tools .

we also asked the respondents to evaluate the effectiveness of the work group or groups on which they served .

the practical difficulties of conducting any survey may introduce errors , commonly referred to as nonsampling errors .

for example , respondents may have difficulty in interpreting a particular question or may lack information necessary to provide valid and reliable responses .

in order to minimize these errors , we conducted pretests of the draft survey with one epa headquarters official , one epa regional official , and one state official by telephone .

the chief of the infrastructure branch of epa's office of ground water and drinking water suggested that we conduct the pretest with those individuals .

during these pretests , we checked whether ( 1 ) questions were clear and unambiguous , ( 2 ) terminology was used correctly , ( 3 ) the questionnaire did not place undue burden on respondents , ( 4 ) the information could feasibly be obtained , and ( 5 ) the survey was comprehensive and unbiased .

in addition , the survey was peer reviewed by a gao senior survey methodologist .

we made changes to the content and the format of the survey based on the feedback we received .

survey administration we administered our survey in august and september of 2010 .

we first phoned each work group member to alert them to our plan to send the survey and to request their participation .

through those phone calls , we learned that one of the work group members had retired and another had transferred to a different position and was no longer a member of a work group .

as a result , our survey population decreased to 44 .

prior to fielding the survey , we sent an e - mail to each member of the work groups to further explain its purpose .

we notified work group members electronically when the survey was available , and sent e - mail reminders prior to our requested deadline of september 13 , 2010 .

we also made phone calls to several survey recipients during the extension period to request their participation .

in total we received 41 completed surveys .

however , the three work group members from the association of state drinking water administrators collaborated to prepare one response and two epa regional officials collaborated to prepare one response .

therefore , the 41 completed surveys represent the views of all 44 members of the survey population .

the survey contained closed - ended questions that asked respondents to select from a finite number of options .

for example , some questions asked respondents to select “yes,” “no,” or “don't know.” others asked respondents to select from a list of factors that may have contributed to drinking water violation data errors .

our analysis of the responses to these questions simply involved counting the number of responses for each option .

in the report , there are instances in which we identify all of the responses and other instances in which we identify the most common response .

we also asked respondents to evaluate the effectiveness of certain data management tools or the epa - state work groups .

for these questions , we offered the respondents a four - point range of answers: very effective , moderately effective , slightly effective , and not effective .

respondents could also answer “don't know” to these questions .

in the report , we sometimes identified the most common response while in other instances we combined the number of “moderately effective” and “slightly effective” responses because each was relatively common .

several survey questions asked for opinions on the creation of a goal for the percentage of monitoring violations that are completely and accurately reported to sdwis / fed .

we asked those respondents who supported the idea of having such a goal to state what they think the percentage should be .

we summed those percentages and divided by the number of respondents who answered that question to arrive at an average .

the responses to the closed - ended questions are provided in this appendix .

the survey also contained open - ended questions that asked respondents to provide a narrative response .

in order to succinctly summarize the open - ended responses , we performed a content analysis in which we grouped the responses into a coding structure that represented common themes .

we decided that the responses to each open - ended question would have a coding structure with two dimensions .

to explain this , it is useful to discuss the link between closed - ended and open - ended questions .

for example , one closed - ended question asked the respondents to select a factor — such as training by epa or the states or guidance from epa — that they believe has contributed to compliance determination errors .

the subsequent open - ended question asked them to elaborate on why they thought the factor or factors they selected have contributed to compliance determination errors .

note that the respondents did not necessarily elaborate on each of the factors they selected in the prior question .

as part of our content analysis , we sought to first identify the first dimension code for the response .

in this example , the first dimension codes mirrored the factors ( eg , training or guidance ) that the respondent selected to write about .

a second dimension code provided a more detailed description of what the respondent said about the first order code .

for example , second dimension codes for that question included amount or quality , timing , and targeting .

a team of three gao analysts jointly reviewed several completed surveys to develop an initial draft of a structure for coding the open - ended responses .

to further identify meaningful first and second dimension codes for the coding structure , the three gao analysts independently reviewed the open - ended responses for four completed surveys .

each analyst made a judgment about appropriate codes that described the themes in the open - ended responses .

the analysts compared their decisions and reconciled any disagreements regarding appropriate codes by refining the criteria used to categorize the responses .

after the team agreed upon the coding structure , it continued its analysis of the responses to the closed - ended questions .

the three gao analysts were each assigned to independently review the responses to specific sets of questions .

for example , analyst a and analyst b independently reviewed and coded the open - ended answers to questions 1 through 4 .

analysts a and b then compared their coding decisions and reconciled any disagreements .

if they could not reconcile a disagreement , analyst c was consulted to achieve agreement .

the three analysts rotated assignments so that each performed the role of “tiebreaker” when the other two could not agree on a coding decision .

our survey of epa and state drinking water officials contained numerous closed - ended questions about various data management issues .

table 2 presents those questions and the respondents' answers .

many of the questions in our survey of epa and state drinking water officials asked for open - ended responses .

we developed several “coding schemes” to categorize the responses to those questions .

our schemes generally had first and second dimension codes .

for example , question 2 asked the respondents to explain how factors they selected contributed to compliance determination errors .

the first dimension codes for that question included information system structure , training , funding , staffing , and guidance , among others .

the second dimension codes included amount ; targeting ; timeliness ; quality , complexity , or ease of use ; automation ; and others .

a response to question 2 might have included a comment that related to information system structures and , more specifically , a comment about the quality , complexity , or ease of use of information systems .

in that situation , we would have coded the response as falling into those first and second dimension codes .

because some sets of questions generated answers that could be similarly coded , we used some coding schemes for multiple questions .

on the other hand , we used some schemes for only one question .

the following are gao's comments responding to the comments in appendix a of the environmental protection agency's letter dated june 8 , 2011 .

1 .

epa commented that there are over 152,000 public water systems in the united states that are also subject to the requirements of the national primary drinking water regulations .

we do not disagree , but did not modify the report in response to epa's comment .

footnote 11 of the report describes the universe of public water systems .

2 .

epa said that violations that are reported as “other” should not be included with monitoring and reporting violations and that it would be beneficial to explain how reporting violations differ from monitoring violations .

our analysis is consistent with epa's 2008 analysis of data quality which also combined “other” violations , such as violations of consumer confidence reporting and public notification requirements — with monitoring and reporting violations .

therefore , we did not modify our analysis or the report in response to epa's comment .

we also did not modify the background section to further explain the difference between monitoring and reporting violations because we provide examples of different violations in a subsequent section of the report .

3 .

epa requested that we provide a footnote in the report describing new performance indicators for small water systems .

specifically , epa requested that we explain the agency's intent behind the indicators and its plan to evaluate their utility .

we did not modify the report in response to this comment since we describe two of these indicators in the section of the report that addresses epa's ability to monitor and report progress toward its strategic objective of reducing exposure to contaminants in drinking water .

4 .

epa said it was unsure of the basis for our statement that monitoring violations are predictors of health - based violations .

epa also noted some of the variations between monitoring and reporting violations and asked for more details regarding our analysis .

our statement was based on aggregate regression analyses ( negative binomial and zero - inflated poisson models ) with limited controls .

it does not take into account which type of monitoring and reporting violation occurred , and cannot differentiate between lack of monitoring and monitoring that was not reported or was delivered late .

the regression was intended to illustrate the link between overall counts of monitoring and reporting violations and counts of health - based violations , and does not provide insight into the nature of the link or the reasons that monitoring and reporting violations might condition the number of health - based violations .

we realize the implications of our statement are limited , but believe the correlation between overall counts of monitoring violations and health - based violations offers useful insight .

5 .

epa requested that we present an analysis of the quality of violations data for the lead and copper rule separately from other national primary drinking water regulations , as it has done .

we acknowledge that there may be value in conducting data quality analyses for specific drinking water regulations , as epa did , for example , in its 2008 report on data quality .

that report showed that the data quality for the lead and copper rule was lower than for other types of drinking water regulation .

however , because the data verification audit data we analyzed was from a sample of community water systems from a sample of states , our results included margins of error .

analyzing data quality for particular drinking water regulations results in larger margins of error than analyzing date quality for all health - based violations .

in light of that circumstance , we decided to conduct our analysis of all health - based violations .

we did not modify our analysis or report in response to this comment .

6 .

epa commented that it intends to conduct six to eight data verification audits in calendar year 2011 .

we have modified the report to reflect that comment , but note that epa's statement concerns audits it has yet to conduct .

epa also said that it would appreciate our including any specific suggestions made by survey respondents on how the data verification audits can be improved .

we have added a footnote with examples of comments from survey respondents .

7 .

epa requested that we update the report to reflect the status of work done by the office of ground water and drinking water to assess regional responses and document the extent to which the regions had complied with requests to discuss data quality with states .

epa went on to say it understands that all the regions include data quality as an issue for discussions with their states and as part of their grant agreements or state work plans .

we have modified the report to include epa's statements .

however , we were not able to verify the accuracy of those statements because epa did not have supporting documentation .

8 .

epa said that it agrees that the regions did not fully support new measures for tracking regional oversight suggested by headquarters but that the lack of those measures has not prevented the regions or headquarters from continuing to work with the states to address data quality challenges .

we note that the proposed performance measures would assist the office of ground water and drinking water to monitor the regions' oversight of the states , not the states' performance .

we assume that epa headquarters proposed these performance measures because it thought they would help encourage the regions to increase their oversight .

without them , epa headquarters may find it more difficult to oversee the regions .

we did not modify the report in response to this comment .

in addition to the individual named above , diane b. raynes , assistant director ; james ashley ; elizabeth beardsley ; mark braza ; ross campbell ; anna maria ortiz ; carla d. rojas paz ; kelly rubin ; jerome sandau ; jeffrey sanders ; carol herrnstadt shulman ; and vasiliki theodoropoulos made significant contributions to this report .

robert alarapon , mick ray , and lisa vojta also made important contributions to this report .

