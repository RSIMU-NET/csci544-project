congress established the small business innovation research ( sbir ) program in 1982 to enable federal agencies to support research and development ( r&d ) projects carried out by small businesses .

for example , an sbir award from the department of health and human services helped a small business develop glasses that allow people with color vision deficiency to see the full color palette , and this business has made more than $20 million in annual sales , according to information on the sbir website .

congress established the related small business technology transfer ( sttr ) program in 1992 .

the sbir and sttr programs are similar in that participating agencies identify topics and make awards for r&d projects led by small businesses .

the sttr program further requires the small business to partner with a nonprofit research institution , such as a college , university , or federally funded research and development center .

each year , small businesses may apply for sbir and sttr awards to develop and commercialize innovative technologies .

the awards generally do not exceed $150,000 for the initial award and $1 million for subsequent follow - on awards .

federal agencies with obligations of $100 million or more for extramural r&d are required to participate in the sbir program , and those with such obligations of $1 billion or more are also required to participate in the sttr program .

since the sbir and sttr programs began , federal agencies have awarded at least 162,000 contracts and grants totaling around $46 billion to small businesses to help them develop and commercialize new technologies .

according to the small business administration ( sba ) , which oversees the sbir and sttr programs , 11 federal agencies participated in the sbir program in fiscal year 2016 and obligated around $2.4 billion for sbir awards , and five agencies also participated in the sttr program in fiscal year 2016 and obligated around $314 million for sttr awards , the most recent year for which data are available .

within these 11 agencies , a number of component agencies provide sbir or sttr awards .

for example , within the department of defense , the air force and navy provide sbir and sttr awards , as do nine other component agencies within the department .

in addition , the army has separate program offices for its sbir and sttr awards .

similarly , the department of commerce has two component agencies that provide sbir and sttr awards — the national institute of standards and technology and the national oceanic and atmospheric administration .

table 1 lists the 28 component agencies we reviewed .

sba provides policy directives on the general operation of the sbir and sttr programs .

according to the sba policy directive for sbir and sttr , at least once a year , each participating agency issues a solicitation requesting proposals on a variety of topics .

each agency reviews the proposals it receives to determine which small businesses should receive awards , then negotiates contracts , grants , or cooperative agreements to issue the awards to the selected small businesses .

the small business act and sba directive state that most agencies are required to review proposals and to notify applicants of the agency's decision no more than 90 calendar days after the closing date of the solicitation .

further , the directive recommends that most agencies issue awards — that is , finalize the funding agreement with the selected small businesses — no more than 180 calendar days after the closing date of the solicitation .

according to an sba official , these time periods apply to both the initial phase i award and the follow - on phase ii awards .

the john s. mccain national defense authorization act for fiscal year 2019 included provisions requiring the department of defense to institute a pilot program to reduce the award issuance time for sbir and sttr programs .

the act also included provisions for gao to report annually for 4 years on the timeliness of agencies' sbir and sttr proposal review and award issuance , and to identify best practices for shortening proposal review and award times , among other things .

this report — the first of the annual reports required by the act — covers fiscal years 2016 through 2018 and describes: ( 1 ) the amount of time agencies spent reviewing sbir and sttr proposals and notifying awardees , and the factors that affect the time spent ; and ( 2 ) the amount of time agencies spent issuing sbir and sttr awards , and the factors that affect the time spent .

to describe the time agencies spent reviewing proposals and issuing awards , we collected information on awards made by 28 component agencies from fiscal years 2016 through 2018 , the 3 most recent years for which data were available .

for every award issued in these years , we asked each component agency to report certain dates , including the proposal submission date , the solicitation close date , the date the awardee was notified that they were selected for an award , the date the award was issued , and the award's period of performance — the period of time during which the awardee is expected to complete the award activities .

we calculated the time spent reviewing a proposal and notifying the awardee starting from the solicitation close date and ending at the notification of the awardee .

we calculated the time spent issuing an award starting from the solicitation close date and ending at either the award issuance date or the first day of the period of performance if the issuance date was not available .

we took several steps to assess the reliability of the data provided .

first , we evaluated the data for potential outliers by looking for awards with particularly long or short notification or issuance periods .

we also looked for potential duplicates by identifying awards with identical award numbers .

we then asked the component agencies to review the data on these specific awards and make any necessary corrections , which we then included in our data .

in addition , we compared our data for fiscal year 2016 awards to information in sba's fiscal year 2016 annual report to congress on the sbir and sttr programs , the most recent report available .

to verify the accuracy of the data provided , we also compared the dates reported by each of the component agencies to dates in grant or contract documents , emails , and other relevant agency documentation — for between two and six awards per component agency — and assessed any discrepancies .

after updating the data as needed based on these steps , we used the resulting data set of 15,453 awards to calculate the proposal review and notification time and award issuance time for each award .

as part of our data collection , we also asked each component agency about the processes they used to record the data and to provide it to us .

we found the data to be sufficiently reliable for the purpose of describing the time spent reviewing proposals and issuing awards at each component agency .

we performed additional analysis to corroborate our proposal review and notification time and award issuance time results and found that they were robust when controlling for selected characteristics .

we created regression models of notification and issuance times and , using these models , we estimated how awards' timeliness varied across five factors: the dollar value of each award , whether each award was phase i or phase ii , the award's fiscal year , whether each award was through the sbir or sttr program , and each award's type of grant or contract .

further , we used the model to estimate how mean notification or issuance times ( reported below ) changed with interagency differences in these factors .

the models' timeliness predictions for a theoretical award with government - wide - average characteristics were generally quite similar to the actual mean at the agency .

for example , for agencies with a mean issuance time that was longer than recommended , the model almost always predicted that the agency would also take longer than the recommended period to handle an award with government - wide - average characteristics .

these results suggest that factors in addition to the five that we modelled explain differences in review time .

to describe the factors that affect the time agencies spent reviewing proposals and issuing awards , we interviewed officials at 24 of the 28 component agencies and received written responses to our questions from two other offices .

these officials included the program manager of the sbir or sttr program , and in some cases also included officials from the component agency's contracting or grants management offices .

we also interviewed an official from sba responsible for overseeing the sbir and sttr programs .

we reviewed their responses and summarized the factors they identified .

we did not evaluate the effect or relative importance of the factors on the amount of time spent reviewing proposals and issuing awards .

we conducted this performance audit from february 2019 to september 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

overall , component agencies reviewed proposals and notified awardees within the required time for 12,890 of the 15,453 sbir and sttr awards that we reviewed ( 84 percent ) , for fiscal years 2016 through 2018 .

the small business act and sbir / sttr policy directive require most agencies notify applicants of the agency's decision within 90 calendar days and require nih and nsf do so in 1 year .

agencies notified awardees after the required time period for 2,533 of 15,453 awards ( 16 percent ) .

three of the 28 component agencies met the notification requirement for every award in the data we reviewed , and nine additional component agencies did so for at least 90 percent of their awards .

the remaining 16 component agencies met the notification requirement for less than 90 percent of their awards .

table 2 lists the mean and median notification times and the percentage of awardees notified within the required time period for each component agency .

some notifications occurred within days after the required time period , while others occurred months later .

for example , all of the notifications by the department of education from fiscal year 2016 through 2018 that took longer than 90 days occurred in 91 days .

department officials attributed the one day difference to interpreting the 90-day requirement as a 3- month requirement .

similarly , all of the notifications for army sttr awards that occurred after the 90-day requirement occurred within 92 days .

of the 2,533 awards with notifications after the required time , notifications occurred on average about 1 ½ months later .

during the 3 fiscal years that we reviewed , some component agencies had substantial changes from year to year in the percentage of awardees that they notified within the required time period , while other component agencies consistently notified about the same percentage of awardees .

for example , the department of energy's office of science and the army sbir program each had a single fiscal year during which they notified less than 50 percent of awardees within the required time period , substantially less than during the other fiscal years we examined .

table 3 describes the percent of awardees notified within the required time by each component agency for each of the 3 fiscal years we examined .

agency officials described several factors that affect the time spent reviewing proposals and notifying awardees , including ( 1 ) the availability of reviewers , ( 2 ) the number of proposals to review , and ( 3 ) other agency - specific factors .

availability of reviewers .

officials from some component agencies we interviewed said the availability of agency staff or external reviewers affected the time they spent reviewing proposals .

for example , usda officials told us that the agency cannot notify awardees within 90 days because they need additional time to identify and recruit experts for their external peer review system .

usda officials compared their review process to that of the nsf and nih , the two agencies that are directed to complete proposal review and notification within 1 year .

similarly , navy officials said that the availability of reviewers was the biggest variable in completing their proposal review and notification process .

these reviewers are navy employees who contribute part of their time to reviewing sbir and sttr proposals while continuing to perform their regular duties .

according to navy officials , although they give reviewers deadlines based on the number of proposals they have to review , conflicts with their regular duties or higher priority tasks may cause reviewers to miss their deadlines .

in contrast , department of education officials said that they identify and train reviewers before the agency receives proposals so that the reviews may begin as soon as possible .

other agencies , however , may not know what areas of expertise reviewers will need until the agency has examined the proposals it received .

number of proposals .

officials from some component agencies we interviewed said the number of proposals they receive affected the time spent reviewing proposals and notifying awardees .

for example , officials from the department of transportation said that the number of proposals they receive can range between two and 40 , which makes it difficult to predict the workload of agency evaluators who perform the proposal reviews .

similarly , national institute of standards and technology officials said that the number of proposals they receive fluctuates from year to year .

because agencies must review all proposals that meet the minimum requirements , an increase in the number of proposals directly increases the workload of proposal reviewers .

other agency - specific factors .

some component agency officials identified factors specific to their agency or process as factors affecting the time needed .

for example: two component agencies within the department of health and human services — the centers for disease control and prevention ( cdc ) and food and drug administration ( fda ) — notified none of their awardees within the required time in fiscal years 2016 through 2018 .

cdc and fda participate in the solicitation and review process led by the nih .

however , while the nih has 1 year to notify awardees , these agencies are required to notify awardees within 90 calendar days .

cdc officials said that participating in the longer nih program is more efficient than creating their own review process and allows them to leverage additional programs at nih that support small business awardees .

environmental protection agency officials told us that their review process includes three consecutive reviews , which leads the agency to regularly request waivers to exceed the 90-day notification requirement .

these reviews include an administrative review for responsiveness to the solicitation , an external peer review process , and an internal review by the sbir program office .

some agency officials also identified continuing resolutions , sequestration , or government shutdowns as factors that could slow proposal review .

proposal review and notification activities could be affected because the availability or amount of funds for agency activities is uncertain in these instances .

for example , a defense microelectronics activity official told us that their agency generally completes its proposal review process within 90 days , but does not notify awardees until it has determined funding availability for awards later in the fiscal year .

national institute of standards and technology officials described a delay notifying one awardee , a replacement awardee , due to the initial awardee being determined ineligible during a pre - award assessment .

the agency made a replacement selection immediately , but this replacement awardee was notified approximately 20 days after the 90-day requirement .

overall , component agencies issued 11,710 of the 15,453 awards we reviewed ( 76 percent ) within the recommended time period , for fiscal years 2016 through 2018 .

the sbir / sttr policy directive recommends that most agencies issue an award within 180 days and recommends that nih and nsf do so in 15 months .

agencies issued 3,743 of the 15,453 awards ( 24 percent ) after the recommended time period .

three of 28 component agencies issued every award in the data we reviewed within the recommended time , and five additional component agencies did so for at least 90 percent of their awards .

the remaining 20 component agencies issued less than 90 percent of their awards within the recommended time period .

for the 3,743 awards that agencies issued after the recommended time period , the average award was issued about two and a half months after the recommended time .

table 4 lists the mean and median award issuance times and the percent of awards issued within the recommended time for each component agency .

during the 3 fiscal years that we reviewed , some component agencies had substantial changes from year to year in the percentage of awards they issued within the recommended time period , while other component agencies consistently issued about the same percentage of awards within the recommended time period .

for example , the department of energy's advanced research projects agency - energy issued no awards within the recommended time in each of the three years we examined .

table 5 describes the percent of awards issued within the recommended time period by each component agency for each of the 3 fiscal years we examined .

agency officials described several factors that increased the time spent issuing awards , including ( 1 ) additional time needed to issue certain types of contracts , ( 2 ) the availability of grants and contracting officers , ( 3 ) delays coordinating among agency officials , ( 4 ) the responsiveness of awardees , and ( 5 ) the availability of funding for the awards .

cost reimbursement contracts .

officials from some component agencies we interviewed said that the contract type was a factor that affected the time needed to issue sbir and sttr awards .

specifically , officials said cost reimbursement contracts took longer to issue because of the need to review the awardee's accounting system in accordance with federal acquisition regulations .

for example , officials from the defense advanced research projects agency ( darpa ) said cost reimbursement contracts routinely take more time to award than fixed - price contracts because of this accounting system review .

according to darpa officials , this review can add 45 days or more to the awards process .

in february 2019 , we found that the department of defense does not have a mechanism to monitor and ensure that contractor business system reviews and audits are conducted in a timely manner and recommended that the department develop such a mechanism .

our analysis of the sbir and sttr award data confirmed that component agencies spent more time issuing awards identified as cost reimbursement contracts than issuing fixed price contracts .

we found that sbir and sttr awards identified as cost reimbursement contracts in the fiscal year 2016 through 2018 data took significantly longer to issue than those identified as fixed - price , as shown in figure 1 .

fixed - price contracts took on average 152 days and cost reimbursement contracts took 231 days ( 79 days longer ) .

cost reimbursement contracts also took on average 40 days longer than contracts that were not specified as fixed or cost reimbursement .

availability of grants or contracting officers .

the availability or experience of agency staff to negotiate the contract or grant can be a factor , according to some component agency officials .

first , some officials said limited availability of grants or contracting officers was a factor in the time to issue awards and may result in delays .

for example , officials from both army program offices said that the workload for contracting officers is high , and sbir and sttr awards are part of a larger contracting backlog .

similarly , officials from the national institute of standards and technology and national oceanic and atmospheric administration also said that the availability of grants and contracting officers is a pervasive issue for federal agencies that can affect award timeliness .

second , officials from some component agencies said that the contracting officer's level of experience with small business awards affects the time needed to issue sbir and sttr awards .

coordination among agency officials .

air force officials said that the need for coordination among agency officials , such as between the contracting officer and proposal evaluators , can create delays .

because the proposal review and award process can require coordination among multiple officials who are not always immediately available , delays may occur as one official waits for input or information from another .

beginning in fiscal year 2018 , the air force made changes to its proposal review and award process for a subset of awards that included scheduling dedicated time for reviewers , contracting officers , and other agency officials to jointly evaluate proposals and process awards .

this change guaranteed the availability of agency officials and reduced the time needed for coordination among these officials .

overall , it allowed the agency to issue awards within a few days or weeks .

according to agency officials , the air force awarded about 150 awards in 2018 through this process and they expect about one - third of air force awards in fiscal year 2019 and half of awards in fiscal year 2020 will use this expedited process .

responsiveness of awardees .

some component agency officials said that the responsiveness of the small business was a factor in delays .

for example , officials from usda said that the majority of sbir grantees at usda are first - time grantees who have never worked with the federal government , and this can extend the time it takes to issue the award .

in order to receive an sbir or sttr award , the small business must , among other things , submit a certification that it meets size , ownership , and other requirements .

delays in providing these certifications or other information required by the awarding agency can therefore delay award issuance .

in our july 2018 report that reviewed dod's weapon - systems - related contracts awarded from fiscal year 2014 through fiscal year 2016 , contracting officials stated that quicker contractor responses to requests for additional information could help reduce the time between when a solicitation is issued to when a contract is awarded .

availability of funding .

some component agency officials said that delays in determining the amount of funding available for small business awards due to continuing resolutions or delays in intradepartmental fund transfers may delay the issuance of awards .

for example , nasa officials said that they estimate the agency's r&d budget at the start of the fiscal year to calculate the amount required for sbir and sttr awards .

according to these officials , if nasa is operating under a continuing resolution at the start of the fiscal year , the estimate may be smaller than the final appropriated amounts .

in this case , nasa would go back to its proposals to make additional awards from the pool of proposals that were rejected under the original estimate , and this would lead to longer issuance times for some awards .

we provided a draft of this report to sba and the 11 agencies that participated in the sbir and sttr programs in fiscal years 2016 through 2018 for their review and comment .

the sba , department of defense , and department of education provided written comments that are reproduced in appendix ii , iii , and iv .

in addition , the department of energy , the nih within the department of health and human services , department of transportation , and the national institute of standards and technology within the department of commerce provided technical comments , which we incorporated as appropriate .

the remaining agencies told us they had no comment .

in its formal comments , the department of education stated that it has taken steps to ensure that future awardees will be notified within the required period .

in their comments , sba and the department of defense suggested phase i and ii awards should be evaluated separately in future reports .

in this report , we combined phase i and ii awards because we did not find a statistically significant difference in notification time between phase i and ii awards in the fiscal year 2016 through 2018 data that we examined .

however , some analyses showed that phase ii awards took longer to issue .

we may further examine differences between phase i and phase ii awards in subsequent reports .

sba also described the importance of minimizing delays between phase i and phase ii awards .

we did not evaluate the time between phase i and subsequent phase ii awards in this report , but agree that the time between awards may be of interest in future reports because , as noted by sba , the time between awards may affect small businesses' ability to retain key personnel .

sba also sought explanations for various dates and figures used in our analysis and we updated the report to include the definitions used when collecting award data and to describe our figures in more detail .

the department of defense also stated that the sbir and sttr policy directive does not explicitly include phase ii awards in its 90 and 180-day timeliness requirements .

however , we confirmed with sba — the agency that issues the directive — that the 90-day requirement for notification of selection and the 180-day recommendation for award issuance apply to both phase i and phase ii awards .

the department of defense further stated that subsequent phase ii awards could occur several years after the end of the initial phase ii award and should not be included in the analysis of phase ii awards .

in this report , we took steps to eliminate these outliers from the data .

we are sending copies of this report to the appropriate congressional committees , the acting administrator of the sba , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6888 or neumannj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iii .

this appendix describes the awards made by agencies participating in the small business innovation research ( sbir ) and small business technology transfer ( sttr ) programs , based on the data provided to gao for fiscal years 2016 through 2018 .

these data include figures showing the ( 1 ) proposal review and notification time , ( 2 ) award issuance time , and ( 3 ) distribution of awards by fiscal year and phase .

the fiscal year and phase figure describes the number of phase i and phase ii awards issued in fiscal years 2016 through 2018 and is based on the first year of the award activities .

for example , if an agency obligated funding to a phase ii award in fiscal years 2017 and 2018 , the award is counted among the fiscal year 2017 phase ii awards .

nist participated in sbir only .

noaa participated in sbir only .

air force participated in sbir and sttr .

small business award timeliness ( fiscal year 2016-2018 awards ) small business award timeliness ( fiscal year 2016-2018 awards ) navy participated in sbir and sttr .

mda participated in sbir and sttr .

darpa participated in sbir and sttr .

dha participated in sbir and sttr .

socom participated in sbir and sttr .

dla participated in sbir and sttr .

dtra participated in sbir and sttr .

cbd participated in sbir and sttr .

nga participated in sbir and sttr .

dmea participated in sbir and sttr .

education participated in sbir only .

office of science participated in sbir and sttr .

arpa - e participated in sbir and sttr .

nih participated in sbir and sttr .

cdc participated in sbir only .

fda participated in sbir only .

dhs s&t participated in sbir only .

dndo participated in sbir only .

dot participated in sbir only .

epa participated in sbir only .

nasa participated in sbir and sttr .

nsf participated in sbir and sttr .

usda participated in sbir only .

in addition to the contact named above , rob marek ( assistant director ) , tind shepper ryen ( analyst - in - charge ) , nora adkins , david aja , jenny chanley , robert letzler , anika mcmillon , amanda postiglione , and ben shouse made key contributions to this report .

