the complex and crosscutting challenges facing the federal government highlight the need for agencies to leverage the expertise and capacity of those in the private , nonprofit , and academic sectors , and other levels of government .

in recent years federal agencies have increasingly used open innovation strategies to directly engage and collaborate with citizens and external stakeholders .

these strategies help agencies harness ideas , expertise , and resources to address issues and achieve goals .

for example , agencies have used open innovation strategies to , among many other things , measurably improve the detection of asteroids that could threaten earth , develop plans to rebuild communities following hurricane sandy , and deploy improved air and water pollution sensors .

acknowledging the benefits of using open innovation strategies , congress and the executive branch have taken actions to clarify agency authorities to use these strategies , and provide implementation guidance and access to online platforms .

for instance , the american innovation and competitiveness act , enacted in january 2017 , authorizes agencies to use crowdsourcing and citizen science to advance their missions and stimulate broader public participation , among other things .

the act also directs the general services administration ( gsa ) , in coordination with the office of science and technology policy ( ostp ) and office of personnel management , to develop relevant products and training , and identify contract vehicles and platforms to enhance the ability of agencies to carry out these projects .

these new resources , once developed , will complement resources already developed by these agencies and the office of management and budget ( omb ) to support the use of crowdsourcing and citizen science and other types of open innovation strategies .

as part of the federal performance management framework originally put into place by the government performance and results act of 1993 ( gpra ) , and updated and expanded by the gpra modernization act of 2010 ( gprama ) , agencies are to identify the strategies and resources they will use to achieve their goals .

gprama also includes a provision for us to periodically review how implementing its requirements is affecting agency performance .

this report is part of our response to that mandate , and builds on our october 2016 report that described how agencies are using open innovation strategies and identified practices and key actions to ensure that open innovation initiatives are implemented effectively .

specifically , this report ( 1 ) identifies key government - wide resources omb , ostp , and gsa put in place to support the use of open innovation strategies ; ( 2 ) examines the extent to which key government - wide guidance reflects practices for effectively implementing these strategies ; and ( 3 ) identifies resources that selected agencies have developed to support the use of open innovation strategies .

to identify the various government - wide resources that omb , ostp , and gsa have developed to support the use of open innovation strategies , we reviewed relevant policy and guidance documents and websites , and interviewed staff from those agencies .

to determine the extent to which key government - wide guidance reflects practices for effectively implementing open innovation initiatives , we evaluated relevant guidance and compared the contents to the practices and related key actions identified in our october 2016 report .

we identified key guidance for each open innovation strategy in consultation with omb , ostp , and gsa staff .

in assessing the guidance , we considered it to fully reflect a key action when it suggested steps in line with those outlined in our report .

if guidance suggested some but not all steps , we considered it to partially reflect the key action .

finally , when guidance for a strategy did not suggest any steps in line with a key action , we considered the guidance to not reflect that action .

lastly , to identify the resources that selected agencies have put in place , we reviewed relevant documentation and websites , and interviewed staff from six agencies: the departments of energy ( doe ) , health and human services ( hhs ) , housing and urban development ( hud ) , and transportation ( dot ) ; the environmental protection agency ( epa ) ; and the national aeronautics and space administration ( nasa ) .

we selected these agencies based on various criteria , including the number and variety of open innovation strategies outlined in their individual agency open government plans .

these selections also were in line with suggestions from knowledgeable staff at omb , ostp , and gsa familiar with agencies that have actively used such strategies .

see appendix i for additional details about our scope and methodology .

we conducted this performance audit from february 2016 to june 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

as identified and described in figure 1 , we reported in october 2016 that agencies have frequently used five open innovation strategies to collaborate with citizens and other external parties , and encourage their participation in agency initiatives .

as we described in our october 2016 report , agencies can use these open innovation strategies singularly , or in combination as part of a larger open innovation initiative , to achieve a number of different purposes .

agencies can use them to efficiently engage a broad range of citizens and external stakeholders in developing new ideas , solutions to specific problems , or new products ranging from software applications to physical devices .

for example , doe's wave energy prize competition was designed to achieve a ground - breaking advancement in technology that produces electricity by capturing energy from ocean waves .

doe's goals for the competition were to engage a diverse collection of developers to create more efficient devices that would double the energy captured from ocean waves .

according to the wave energy prize website , the competition , which was concluded in november 2016 , produced a significant advancement in this technology .

ultimately , 4 of 9 finalist teams — selected from 92 that registered to participate — developed devices that surpassed doe's goal , while the winning team achieved a five - fold improvement .

agencies can also use these strategies to enhance collaboration among citizens and external stakeholders or organizations interested in an issue , and leverage the resources , knowledge , and expertise of citizens and stakeholders to supplement that which is available within the agency .

these contributions enhance the agency's capacity and its ability to achieve goals that would be more difficult to reach without this additional capacity or expertise .

in another example highlighted in our october 2016 report , the federal highway administration's ( fhwa ) every day counts ideation initiative helps states implement innovations that improve the efficiency and quality of their highway transportation and construction projects .

every two years , fhwa engages a range of stakeholders to identify innovative technologies and practices that merit more widespread deployment .

once innovations are identified , fhwa creates deployment teams comprised of experts from fhwa , state transportation agencies , and industry to help states and other stakeholders implement these innovations .

according to an fhwa report , this open innovation initiative has had significant and measurable effects in participating states .

in one case , fhwa reported that deploying accelerated bridge construction as an every day counts innovation has allowed states to reduce the time it takes to plan and construct bridges by years , significantly reducing traffic delays , road closures , and often project costs .

in our october 2016 report , we also identified seven practices and 18 related key actions , described in figure 2 , that federal agencies can use to help effectively design , implement , and assess their open innovation initiatives .

to do so , we analyzed and synthesized suggested practices from relevant federal guidance and literature , including public and business administration journals , and publications from research organizations , as well as interviews with experts and agency officials with experience implementing open innovation initiatives .

in that report , we also illustrated how the application of these practices helped agencies effectively implement open innovation initiatives in a way that achieved intended results .

three agencies with government - wide responsibilities — omb , ostp , and gsa — have taken steps to support and encourage the use of open innovation strategies by federal agencies .

omb is the largest component of the executive office of the president , and is responsible for helping agencies across the federal government implement the commitments and priorities of the president .

among other things , it develops policies and provides direction on the use of internet - based technologies that make it easier for citizens to interact with the federal government .

ostp is also a component of the executive office of the president .

among other things , it has responsibility for leading interagency efforts to develop and implement sound science and technology policies .

gsa is responsible for helping federal agencies obtain the facilities , products , and services that they need to serve the public .

among other things , gsa builds , provides , and shares technology , platforms , and processes that support initiatives to invite participation from stakeholders and make data available to the public .

omb , ostp , and gsa developed government - wide policies and implementation guidance to encourage agencies to use open innovation strategies to , among other things , advance their missions , improve programs and services , and inspire new ideas to address specific challenges .

according to staff from these agencies , policies and guidance do this by: clarifying the legal authorities available to use specific strategies ; highlighting the benefits and results that agencies can produce using suggesting actions for agency staff to take when designing and implementing an open innovation initiative .

table 1 lists the government - wide policies and guidance that we , in consultation with omb , ostp , and gsa staff , identified as key for implementing each open innovation strategy .

as table 1 shows , the focus of the guidance varies across the different types of open innovation strategies .

for crowdsourcing and citizen science , as well as prize competitions and challenges , toolkits provide step - by - step guidance for agency staff to follow when implementing those types of initiatives .

omb , ostp , and gsa staff identified the u.s. public participation playbook as key guidance for a variety of approaches to engage the public , including two open innovation strategies: ideation and open dialogues .

they also identified the open data policy and project open data as the key policy and source of guidance , respectively , that cover open data collaboration initiatives .

they told us that these and other resources have a broader focus , generally to help agencies inventory , manage , and release their open data .

these sources do suggest some actions for agencies to take when implementing open data collaboration initiatives .

however , omb , ostp , and gsa staff have not yet developed more detailed , step - by - step guidance for implementing specific initiatives where agencies use events or websites to engage and collaborate with outside stakeholders using open data .

as one example of policy and guidance for an open innovation strategy , in september 2015 , the director of ostp issued a memorandum encouraging agencies to use crowdsourcing and citizen science to enhance scientific research and address problems by drawing on the voluntary participation of the public .

the memorandum also directs agencies to improve their ability to use crowdsourcing and citizen science by identifying internal agency coordinators responsible for seeking opportunities to use this strategy to meet agency goals .

crowdsourcing and citizen science coordinators from four of the selected agencies — doe , hhs , epa , and nasa — told us that having this clear statement of support has helped enhance awareness of and interest in the strategy .

the memorandum was supplemented by the crowdsourcing and citizen science toolkit , which provides a step - by - step list of actions agencies can take to carry out a specific initiative , as shown in figure 3 .

these officials from doe , epa , hhs , and nasa told us that that the toolkit is an important educational resource for agency staff , helping those interested in using crowdsourcing and citizen science understand what the strategy can be used to achieve and the full range of actions they can take to develop and implement an initiative .

an official from gsa's technology transformation service ( tts ) told us gsa is planning updates to the toolkit to reflect the authorities provided by , and requirements of , the american innovation and competitiveness act ( aica ) for crowdsourcing and citizen science initiatives .

in addition to developing relevant policies and implementation guidance , staff from omb , ostp , and gsa told us they also provide ongoing support for the use of open innovation strategies across the federal government by: answering questions from agency staff , sharing lessons learned , and providing advice and assistance ; hosting training sessions and other events for agency staff to highlight how agencies can use open innovation strategies and how agencies have had success using them ; and matching agency staff in need of assistance with mentors , advisors , or partners in other agencies .

table 2 provides additional information on the types of support they provide .

one example of staff supporting the use of open innovation strategies is gsa's challenge.gov program management team .

according to an official from gsa's tts , this team is the primary point - of - contact for staff from other federal agencies , fielding their questions about the site or other aspects of managing a prize competition or challenge .

to help familiarize agency staff with how to manage a prize competition or challenge and use the challenge.gov platform , the team conducts training sessions and has developed on - demand videos and webinars that are available through the website .

in addition , to provide tailored assistance , the team created the challenge and prize mentorship program , which matches staff seeking support with experienced practitioners from other agencies who can assist with all aspects of planning and implementing an initiative using this strategy .

omb , ostp , and gsa staff also support communities of practice for various open innovation strategies .

according to staff from these agencies , these communities provide venues to bring experienced and knowledgeable agency staff together so that they can learn from one another .

through regularly - scheduled meetings and e - mail lists , the communities allow members to share experiences and lessons learned , seek and provide advice and assistance , and ensure members are up to date on relevant issues , such as the development and release of new resources or upcoming initiatives .

omb , ostp , and gsa staff also have leveraged the expertise of community members to develop toolkits and case studies , which capture leading practices and lessons learned to ensure they are readily available to inform future initiatives .

collectively , these communities , which are described in table 3 , help staff across the federal government communicate and collaborate with one another .

the federal challenges and prizes community of practice further illustrates the support such communities can provide .

according to a gsa official , this community was created in 2010 to bring together staff from across the federal government interested in that open innovation strategy .

according to information on the challenge.gov website , the community is comprised of federal employees representing a wide range of agency perspectives and experience levels .

staff from ostp's technology and innovation division and gsa's tts told us that they have worked with volunteers from the community to develop resources , including the federal challenges and prizes toolkit , to try to meet the diverse needs of its members .

the official from gsa's tts also told us that they plan to work with members of the community to revise the toolkit to reflect the updated authorities and requirements for prize competitions and challenges in aica .

this official further stated that the challenge.gov program team provides ongoing support to this community by maintaining its e - mail list , which allows for information sharing between members , and working with members to identify topics and arrange speakers for the community's quarterly meetings .

for instance , the challenge.gov team helped organize the march 2017 meeting , which focused on how aica updates agency authorities and requirements for prize competitions and challenges .

according to staff from omb , ostp , and gsa , their agencies developed several websites , described in table 4 , that support the use of open innovation strategies by: making relevant guidance and information easy to access in a single , providing agency staff access to additional resources , such as applications , templates , and documents from other agencies , that they can replicate to save time and effort ; and providing online platforms for agencies to reach the public with information on their open innovation initiatives .

as examples of such websites , omb , ostp , and gsa developed project open data and data.gov to help agencies manage and release their open data and meet requirements under omb's open data policy .

according to officials from doe , epa , hhs , hud , and nasa , project open data has made it easier for them to find the guidance they need by making it accessible in one place .

project open data also encourages agencies to hold open data events to engage with data users , and to use these opportunities to expand awareness about their open data , and collect feedback and ideas .

the website provides an overview of the main types of open data community events that federal agencies hold , as well as instructions and templates to help agencies organize , publicize , and carry out various types of events .

to increase awareness about the open data events that agencies are holding , and to help agencies increase participation in these events , gsa has also added an “events” page to data.gov .

as shown in figure 4 , this provides agencies with an additional online platform to inform potential participants about open data events they will be hosting .

we determined that key government - wide guidance developed by omb , ostp , and gsa to support the implementation of various open innovation strategies reflects practices for effective implementation to differing extents .

several factors led to these variances , including differing scopes and methodologies used in their development and the dates when they were issued .

we identified key guidance for the implementation of initiatives using each open innovation strategy in consultation with omb , ostp , and gsa staff .

as noted above , staff from these agencies identified the u.s. public participation playbook as key guidance for both ideation initiatives and open dialogues .

therefore , we present our assessment of guidance for those two strategies together below .

omb , ostp , and gsa staff also identified the open data policy and project open data as key sources of guidance for the implementation of open data collaboration initiatives .

these sources suggest some actions for agencies to take when implementing such initiatives .

however , as noted earlier , they have a broader focus to help agencies inventory , manage , and release their open data .

as such , our assessment found that this guidance generally did not reflect practices for effective implementation of individual open data collaboration initiatives .

our october 2016 report identified two key actions agencies should take when attempting to select the most appropriate strategy or strategies for their initiatives: 1 .

clearly articulate the purpose ( s ) they hope to achieve by engaging the public .

as we found in that report , agencies have used open innovation strategies for a number of purposes , including to develop solutions to specific problems and to leverage the expertise of external stakeholders to enhance an agency's ability to achieve a goal .

2 .

consider the agency's capability to implement a strategy .

considerations should include whether agency leadership supports the use of a strategy , and whether they have legal authorities , financial and technological resources , and staff available to support an initiative .

as shown in figure 5 , we determined that guidance for all but one of the strategies — open data collaboration — fully reflects these key actions .

for example , the challenges and prize toolkit encourages agencies to clarify their purpose in engaging the public , including by developing a detailed understanding of the specific problem they want to address through the competition .

in addition , when considering a prize competition or challenge , the toolkit recommends that agency staff: secure the approval of agency leadership to move forward ; work with legal counsel to identify the most appropriate authority under which to conduct the competition , and to address any other potential legal issues ; estimate the budget and resource needs of a competition ; and ensure the availability of staff to monitor or run the challenge throughout its life cycle .

guidance for open data collaboration initiatives does not address the first key action — defining the purpose — and partially reflects the second key action .

although it encourages agencies to consider agency capacity and applicable laws , regulations , and policies , and the availability of resources , it does not address leadership support .

to guide agencies in designing and implementing initiatives , and to provide those involved with a clear understanding of what they are working to achieve , we identified three key actions that agencies should take: 1 .

define specific and measurable goals for their initiatives .

2 .

identify performance measures to assess progress toward those goals .

3 .

align the goals of their open innovation initiatives with the agency's broader mission .

this final action helps to demonstrate the relevance and value of an initiative to others in the agency , and reinforces the connection between the agency's goals and the day - to - day actions of those carrying out the initiative .

as shown in figure 6 , our assessment shows that the guidance for all but one of the strategies fully reflects these three key actions .

for example , the federal crowdsourcing and citizen science toolkit encourages agencies to define their goals for an initiative while considering how they will measure and evaluate these outcomes .

in addition , the toolkit encourages agencies to identify the specific measures that they will use to track the initiative's outputs and activities , such as the number of samples collected or training sessions held , and to determine whether the initiative is achieving its goals .

lastly , it states that those managing initiatives should ensure that the initiative is aligned with their agency's mission , and be able to specify how it will help the agency meet its goals .

as also shown in figure 6 , we determined that guidance for the implementation of open data collaboration initiatives partially addresses the first key action .

specifically , the guidance identifies illustrative goals for various types of open data events .

for example , it states that the goal of a “hackathon” is to build relationships with a community of developers and designers , and to see immediate tools and prototypes built using open data .

however , as written , these illustrative goals are not measurable , nor does the guidance directly encourage agencies to develop such goals for their own specific initiatives .

furthermore , the guidance does not reflect the other two key actions .

to leverage the experience , insights , and expertise of those interested or engaged in the area to be addressed by an initiative , agencies should: 1 .

identify and engage with external stakeholders .

these are individuals or organizations that share an interest in the issue being addressed and may already be active in related efforts .

for a federal agency , external stakeholders can include representatives of relevant nonprofit organizations and foundations , community or citizens' groups , universities and academic institutions , the private sector , members of congress and their staffs , other federal agencies , and state and local governments .

2 .

look for opportunities to partner with other groups and organizations .

partners are organizations and individuals that play a direct role in designing and implementing an initiative .

they provide staff capacity , resources , administrative and logistical support , assistance with communications and community building , or ongoing advice and expertise .

in considering potential partners , agencies should look for groups and organizations that would be interested in , or could benefit from , the results of an open innovation initiative .

as shown in figure 7 , we determined that guidance for all of the open innovation strategies fully reflects both key actions .

for example , guidance for implementing open data collaboration efforts in omb's open data policy encourages agencies to engage with various stakeholders , including entrepreneurs and innovators in the private and nonprofit sectors .

this engagement could then lead to these stakeholders participating in specific initiatives to use agency data to build products , applications , and services .

guidance on the project open data website also suggests agencies use open data collaboration events to bring together various stakeholders , such as entrepreneurs , technology leaders , and policy experts , to explore available data and discuss new ideas for tools that the private sector could create using agency data .

project open data also encourages agencies to invite and partner with other government entities and private companies when developing and carrying out these events .

to ensure that tasks and time frames are clear for all involved in implementing and managing an open innovation initiative , agencies should document roles and responsibilities and develop implementation plans for those initiatives .

the plans should clearly identify specific tasks , the parties responsible for completing them , and the time frames for doing so .

they should also outline when and how the agency will reach out to various participant groups , and how data will be collected and evaluated to determine results .

as shown in figure 8 , we determined that guidance for crowdsourcing and citizen science initiatives , and prize competitions and challenges , fully reflects these key actions .

for example , the challenges and prizes toolkit encourages agencies to create an implementation plan that clearly outlines the roles and responsibilities for those involved in an initiative .

as the toolkit emphasizes , managing a competition can involve a wide range of government and contract staff with varying responsibilities , from project management to providing subject matter expertise and technical , communications , and legal support .

because of this , it is important to be clear about the role of each team member , the duties each is assigned , and how his or her work fits into the timeline for the competition .

as part of this , the toolkit also states that the plan should specify the procedures that will be used to collect relevant data during the course of the challenge .

lastly , the toolkit encourages agencies to develop a communications plan that defines the audiences they want to reach with information on the initiative ; the websites , news outlets , social media , and other outlets it will use to reach them ; and the messages they will use to reach potential participants and encourage them to participate .

in contrast , as also shown in figure 8 , we found that guidance for open data collaboration initiatives does not reflect either of these key actions , and guidance for open dialogue and ideation initiatives partially reflects these actions .

specifically , the guidance for open dialogue and ideation initiatives encourages agencies to document roles and responsibilities , and to develop a plan that specifies the tasks and time frames for recruiting participants .

however , the guidance does not discuss planning for data collection .

to reach the right potential participants , motivate them to participate , and keep partners and participants engaged throughout the implementation of an initiative , we previously identified four key actions agencies should take .

they should: 1 .

use multiple outlets and venues to reach potential participants .

in their outreach , agencies should use the initiative's website , social media , press releases , journals , newsletters , and professional conferences and networks to reach potential participants .

2 .

craft announcements in a way that motivates people to participate .

in doing so , agencies should address the interests of potential participants and explain why it is important for them to participate .

3 .

engage with participants to provide answers to questions and any necessary assistance .

while an initiative is ongoing , agencies can use websites , question - and - answer sessions , e - mails , and other forms of communication to engage with participants and provide ongoing support and assistance .

4 .

hold regular check - ins with those involved in implementation .

agencies should hold regular meetings to help ensure that those working to implement an initiative are aware of the status of efforts and have an opportunity to raise and discuss any concerns .

as shown in figure 9 , our assessment found that guidance for crowdsourcing and citizen science fully reflects all of these key actions , while guidance for the other strategies reflects some , but not all , of these key actions .

for example , the crowdsourcing and citizen science toolkit encourages agencies to find the best platforms for reaching communities with information about their initiatives , and to reach out and communicate with potential participants using media and messages that respond to their interests .

the toolkit also encourages agencies to let those participating in an initiative know how they can engage with the agency to provide information and feedback , and states that the agency should pay attention to shifts in participant needs and interests over time .

lastly , the toolkit instructs agency staff to hold regular meetings with an initiative's implementation team so that everyone can understand how the project is progressing , and discuss new developments and concerns .

as also shown in figure 9 , for the other strategies , we determined that guidance fully reflected actively engaging participants , but none of them reflected holding regular check - ins with those involved in implementation .

guidance for ideation and open dialogue initiatives , and prize competitions and challenges reflects two additional key actions — using multiple outlets and crafting announcements to participants' interests — while the guidance for open data collaboration initiatives does not .

after an initiative has concluded , or at regular intervals if it is a long - standing or continuous effort , agencies should take three steps to assess and report results and identify potential improvements: 1 .

assess the data collected during implementation to determine whether the initiative met its goals .

2 .

analyze feedback from partners and participants .

agencies should identify lessons learned about what went well and what would need to be adjusted or improved for similar initiatives in the future .

3 .

report publicly on results achieved and lessons learned .

doing so can demonstrate the value of an initiative and sustain a dialogue within the community of interested organizations and individuals .

as shown in figure 10 , we determined that guidance for crowdsourcing and citizen science initiatives and prize competitions and challenges fully reflected these key actions .

for example , the challenges and prizes toolkit states that agencies should assess the data they have collected to determine how well the competition achieved its goals , and to identify other results and outcomes it produced , such as quantifiable improvements to existing solutions and technologies .

the toolkit also states that agencies should conduct an after - action assessment to capture feedback , lessons learned , and other institutional knowledge so the agency can improve its challenges in the future .

specifically , it suggests that agencies consider the following questions when conducting this assessment: what worked well ? .

what would you have done differently in challenge design looking back ? .

how might agency clearance and coordination go more smoothly next time ? .

what could have been improved in judging , communications , and operations ? .

did the evaluation process result in the selection of the best submissions ? .

what were any unintended consequences , both positive and negative ? .

lastly , the toolkit reminds agencies to complete required public reporting , sharing results , lessons learned , and success stories , which can be critical to improving how challenges are designed and implemented .

as figure 10 also illustrates , our assessment found that guidance for open data collaboration initiatives did not reflect any of these key actions , and guidance for ideation and open dialogue initiatives reflected most of them .

specifically , that guidance encourages agencies to collect and analyze data to assess goal achievement and results , conduct an after - action review , and report publicly on results .

it does not , however , encourage agencies to report publicly on lessons learned through their experience .

given the time and resources that agencies may invest to build or enhance communities of partners and participants for open innovation initiatives , our october 2016 report identified two key actions agencies should take to sustain these connections over time: 1 .

acknowledge and , as appropriate , reward the efforts and achievements of partners and participants .

2 .

seek ways to maintain communication with members of the community .

doing so can keep them informed of future initiatives and other opportunities , and facilitate communication within the community .

as figure 11 shows , we determined that none of the guidance fully reflects the first action .

the guidance for crowdsourcing and citizen science , and prize competitions and challenges , both encourage agencies to acknowledge the contributions of participants , and reward participants with monetary and nonmonetary incentives ( as appropriate ) .

however , guidance for these strategies does not also encourage agencies to acknowledge the contributions of partner organizations , which can provide critical resources , expertise , and capacity for open innovation initiatives .

the guidance for ideation , open dialogues , and open data collaboration initiatives did not reflect this key action .

as also shown in figure 11 , our assessment found that guidance for most open innovation strategies fully reflect the second action ; however , guidance for open data collaboration does not .

for example , the federal crowdsourcing and citizen science toolkit encourages agencies to continue actively engaging partners and participants , and help direct participants to other initiatives that might interest them .

the toolkit also encourages agencies to create opportunities for participants to socialize and communicate with each other by supporting discussion forums , and identifying group leaders that can help carry forward a discussion among members of the community .

in instances where guidance does not fully reflect the practices and key actions identified in our october 2016 report , agency staff may not be aware of certain steps they should take to better ensure the success of their open innovation initiatives .

in part , the various guidance resources developed by omb , ostp , and gsa do not fully reflect some practices and key actions because those resources almost all pre - date our report .

we also used different methodologies and sources in pulling together our practices and key actions than they did in developing their guidance .

as noted earlier , we developed our practices by analyzing and synthesizing suggested practices and key actions from a wide range of relevant literature , as well as interviews with nongovernmental experts and agency officials .

our scope also was to identify practices applicable to implementing individual initiatives using any open innovation strategy .

the guidance developed by omb , ostp , and gsa , by contrast , is generally strategy specific and based on the experiences of and lessons learned from individuals involved in the relevant communities of practice .

for example , as was highlighted earlier , ostp and gsa staff worked with volunteers from the challenges and prizes community of practice to develop the challenges and prizes toolkit .

other factors contributed to the guidance for open data collaboration not fully reflecting our practices and key actions for effectively implementing open innovation initiatives .

as was previously described , omb , ostp , and gsa developed various resources , including websites and guidance , to help agencies inventory , manage , and release their open data .

according to staff from these agencies , the focus of their efforts and resources generally has been assisting agencies with the management and release of open data , while providing some support for agencies interested in mobilizing and collaborating with participants to use open data ( i.e. , open data collaboration ) , which we identify as an open innovation strategy .

the existing guidance does suggest some actions for agencies to take when implementing open data collaboration initiatives .

in addition , staff from omb's office of the chief information officer and gsa's tts shared several ways in which they and members of the open data working group provide support for staff interested in carrying out open data collaboration initiatives .

for example , they told us that when agency staff ask for advice on how to conduct such initiatives , they often match the individual making the request with experienced staff in another agency , or rely upon members of the open data working group to answer their questions .

an official from gsa's tts stated that because members of this community meet regularly , there are frequent opportunities to share best practices and lessons learned to inform the planning of any current or future agency open data collaboration initiatives .

in addition , the official told us that agency staff can also use the open data community's e - mail list to seek and receive assistance from fellow members .

although omb , ostp , and gsa staff identified various resources that are available to support agency staff interested in carrying out open data collaboration initiatives , those resources do not include detailed and consistent , step - by - step guidance for the implementation of such initiatives .

the agencies we selected for this review — doe , hhs , hud , dot , epa , and nasa — have put in place various resources to support the use of open innovation strategies .

these resources — policies and implementation guidance , supporting staff and organizations , and websites — complement what exists at the government - wide level .

the selected agencies have generally developed , or are developing , resources for the open innovation strategies they use frequently , to provide staff with tailored guidance and support .

this helps ensure staff carry out initiatives in a manner that is informed by the agency's previous experience and that is consistent with agency procedures .

in some instances , agency officials told us that they have not developed certain agency - level resources , generally for one or more of the following reasons: they found government - wide resources to be sufficient ; they have not used a strategy , or use it infrequently , limiting the need for agency - specific resources ; or they have not yet had sufficient experience using a strategy to be able to craft policies or other resources informed by experiences and lessons learned .

officials from the selected agencies told us that agency - level policies and guidance help to raise awareness among agency staff of the value of using open innovation strategies , and how these strategies can be an effective tool for engaging the public to generate new ideas and solutions .

in instances where agency leaders have approved and issued these policies , this also helps demonstrate to agency staff that leadership supports using these strategies .

the policies and guidance at these agencies ( examples provided in table 5 ) also clarify agency - specific steps that staff should take when implementing an initiative , which , according to agency officials , helps ensure that staff take actions necessary to meet requirements and successfully carry out open innovation initiatives .

as illustrated in table 5 , nasa's policy directive on challenges , prize competitions , and crowdsourcing activities , which was signed by the nasa administrator and issued in february 2014 , encourages agency staff to use these open innovation strategies .

the directive states that it is nasa's policy to encourage the use of challenge activities to obtain solutions and stimulate innovation .

according to an official from nasa's center of excellence for collaborative innovation ( coeci ) , the directive was created to supplement government - wide policies and guidance .

it defines the roles that certain agency officials , such as the associate administrator for the space technology mission directorate , are to play in designing and implementing challenges .

it also directs staff to develop and maintain agency - level best practices and implementation guidance for challenges .

in keeping with this policy , in 2014 , coeci , which provides advice and support for nasa teams to design , implement , and evaluate challenges conducted through the nasa tournament lab , developed a challenge coordinator toolkit .

the toolkit provides a detailed checklist for coeci challenge coordinators to follow when supporting teams that are conducting those challenges .

according to a coeci official , staff with varying levels of experience working on challenges serve as challenge coordinators .

the toolkit is intended to ensure that coordinators consistently follow a set of specific procedures for managing all phases of a challenge being conducted using the nasa tournament lab , from completing necessary documentation before a challenge is launched to holding a close - out interview to capture feedback and lessons learned .

see figure 12 for an overview of the steps highlighted in the toolkit .

according to officials at the selected agencies , their agencies have created staff positions and internal organizations to serve as a source of information and guidance , sharing agency - specific and government - wide resources with staff who are conducting open innovation initiatives .

in some instances , agencies have dedicated staff responsible for leading efforts to use a certain strategy in their agency and providing internal support to other staff interested in using the strategy .

in other instances , agency officials told us they also have staff who , given their interest and experience using a strategy , have taken on responsibilities to support others in their agency , but are not devoted full time to these efforts .

some agencies also have brought together multiple staff to create internal organizations responsible for supporting the use of open innovation strategies .

these staff and organizations — selected examples of which are highlighted in tables 6 and 7 , respectively — can provide tailored advice and support based on their experience with these strategies .

according to agency officials , this can involve helping to ensure staff meet requirements and take other actions to successfully propose , design , and implement an open innovation initiative .

as noted in table 6 , dot's chief data officer ( cdo ) told us that he provides technical support for the department's open data efforts .

as part of his responsibilities , he works with staff to improve how the agency collects , manages , and publishes data .

he also has coordinated and supported several open data events that have allowed dot staff to collaborate with the public , including private sector companies using transportation data , state and local stakeholders , academic researchers , and those working on new technologies .

he told us that these events have served to help the agency better understand how people are using transportation data and what improvements that they would like to see made .

for example , in 2015 , the cdo worked with staff from throughout dot to organize a transportation “datapalooza.” according to the cdo , the event allowed dot to both highlight its own data initiatives and learn about the applications and products that private companies , developers , and technologists are developing with dot data .

it also gave dot staff an opportunity to speak directly with data users , identifying areas where they might work collaboratively to expand the use of the data and improve quality .

according to the epa memorandum that established it in july 2015 , epa's challenge review team ( chart ) , mentioned above in table 7 , is intended to help ensure that agency prize competitions and challenges meet applicable legal requirements , and have an adequate scientific foundation , financial support , and a clear communications plan .

as shown in figure 13 , chart brings together officials from several epa offices who , according to an official from epa's office of research and development , need to be involved in prize competitions and challenges , creating an efficient “one - stop” approach to reviewing and approving proposals .

according to the official from epa's office of research and development , the officials who serve on chart provide legal , financial , and scientific expertise and support to epa teams developing proposals for prize competition or challenge initiatives .

as part of the team's review processes , it requires epa staff to complete a chart review form to ensure that ( 1 ) new initiatives will address agency needs , ( 2 ) leadership is aware of and supports the challenge , and ( 3 ) teams have completed required steps and addressed any concerns before they can proceed .

according to the memorandum that established it , chart members must concur before any prize competition or challenge can be announced .

in addition to dedicating staff and organizations , according to officials from selected agencies , some of the agencies also created communities of practice ( cop ) for those staff interested or involved in open innovation initiatives , particularly citizen science .

these include nasa's citizen science cop ; epa's citizen science cop and recently - created prize and challenge cop ; and within hhs , citizen science working groups at the centers for disease control and prevention and national institutes of health ( nih ) .

like government - wide communities , those cops and working groups at the agency level can provide a venue for sharing experiences and lessons learned from designing and implementing open innovation initiatives , or can be used to develop guidance for agency staff .

for example , according to nih officials , the agency's citizen science working group was established in 2012 to address interest among nih staff about how the agency could use crowdsourcing and citizen science methods .

nih officials told us the working group brings together , as of march 2017 , approximately 60 staff from 14 of the 27 nih institutes and centers to discuss various topics and share their experiences with and knowledge of implementing these initiatives .

the group also hosts presentations by outside speakers .

agencies selected for this review have developed websites that , like the government - wide websites described above , can provide a central , publicly available location for stakeholders and participants to access information , including agency data that can be used in open data collaboration initiatives and details of specific projects in which they can volunteer to participate .

however , officials from doe , epa , hhs , and nasa told us that websites developed by agencies , unlike the government - wide websites , are often designed to reach specific audiences and stakeholders that may be likely to participate in the agencies' open innovation initiatives and in some cases provide an online forum for these audiences to collaborate .

table 8 provides examples of websites that agencies have developed .

for example , doe's open energy data website , as shown in figure 14 , provides energy data users and developers with access to doe's catalog of open data sets that they can use to conduct research and develop applications .

in addition , it provides visitors with information on how to engage with doe to provide feedback on the department's open data .

open energy data also highlights and links to open energy information ( openei ) , a website sponsored by doe and other partner organizations that allows users to share datasets and collaborate on energy data initiatives .

hud's switchboard , an online ideation platform that was also highlighted in our october 2016 report , allows employees , stakeholders , and other members of the public to offer ideas for improving hud's processes , programs , and administration .

according to a hud official , the platform was originally created in 2009 to collect input for an update to the agency's strategic plan .

over time , however , hud expanded the purpose of the platform to make it a more general forum for the public and external stakeholders , as well as hud employees .

now switchboard is intended to ensure there is an avenue for anyone to reach hud with any ideas or feedback that could improve the department .

beyond that , the website allows visitors to offer comments on other submissions , and vote in favor of those ideas that they support .

it also helps designated hud staff collect , review , and respond to the ideas submitted , after which they will work to determine whether they can be implemented .

figure 15 illustrates different aspects of switchboard's capabilities .

omb , ostp , gsa , and selected agencies have developed various resources to support the use of open innovation strategies .

of these , government - wide guidance is particularly important , as it helps agency staff understand the full range of actions they should take when designing , implementing , and evaluating an open innovation initiative .

however , key government - wide implementation guidance , in particular for open data collaboration , does not always fully reflect demonstrated good practices and key actions .

better incorporating those practices and key actions would help ensure agency staff are aware of , and can take , the full range of steps to effectively design , implement , and assess their open innovation initiatives .

consistently applying these practices can help agencies ensure that their initiatives successfully achieve intended results .

to help ensure federal agencies effectively design , implement , and assess open innovation initiatives in line with the practices and key actions identified in our past report , we make 22 recommendations for gsa , omb , and ostp to enhance relevant implementation guidance .

we recommend that the director of the office of management and budget , the director of the office of science and technology policy , and the administrator of the general services administration enhance key guidance for open data collaboration initiatives to fully reflect the following 15 key actions: clearly define the purpose of engaging the public ; consider the agency's capability to implement a strategy , including leadership support , legal authority , the availability of resources , and capacity ; define specific and measurable goals for the initiative ; identify performance measures to assess progress ; align goals of the initiative with the agency's broader mission and document the roles and responsibilities for all involved in the initiative ; develop a plan that identifies specific implementation tasks and time frames , including those for participant outreach and data collection ; use multiple outlets and venues to announce the initiative ; craft announcements to respond to the interests and motivations of hold regular check - ins for those involved in the implementation of the collect and analyze data to assess goal achievement and results of conduct an after - action review to identify lessons learned and report on results and lessons learned publicly ; acknowledge and , where appropriate , reward the efforts and achievements of partners and participants ; and seek to maintain communication with , and promote communication among , members of the community .

we recommend that the administrator of the general services administration enhance key guidance for ideation and open dialogue initiatives to fully reflect the following four key actions: develop a plan that identifies specific implementation tasks and time frames , including those for participant outreach and data collection ; hold regular check - ins for those involved in the implementation of the report on results and lessons learned publicly ; and acknowledge the efforts and achievements of partners that have contributed to the implementation of an initiative .

we recommend that the director of the office of science and technology policy and administrator of the general services administration enhance key guidance for crowdsourcing and citizen science initiatives to fully reflect the key action to acknowledge the efforts and achievements of partners that have contributed to implementing an initiative .

we recommend that the director of the office of management and budget and the administrator of the general services administration enhance key guidance for prize competitions and challenges to fully reflect the following two key actions: hold regular check - ins with those involved in the implementation of an initiative ; and acknowledge the efforts and achievements of partners who contributed to the implementation of an initiative .

we provided a draft of the report to the director of the office of management and budget , the acting director of the office of science and technology policy , the acting administrator of the general services administration ( gsa ) , the secretary of energy , the secretary of health and human services , the secretary of housing and urban development , the secretary of transportation , the administrator of the environmental protection agency , and the acting administrator of the national aeronautics and space administration for comment .

in its comments , reproduced in appendix ii , gsa agreed with the recommendations in this report .

staff from omb's office of the federal chief information officer and office of general counsel provided oral comments on may 15 , 2017 , stating that omb generally agreed with the recommendations in this report .

in comments provided by email , ostp's general counsel stated that ostp neither agreed nor disagreed with the recommendations in this report .

she stated that , given their past and ongoing responsibilities related to open innovation in the federal government , omb and gsa are best positioned to address these recommendations .

she further stated that ostp may support omb and gsa in these efforts to the extent that ostp has appropriate staff in the future .

epa , nasa , and omb provided technical comments , which we incorporated as appropriate .

doe , dot , hhs , and hud informed us that they had no comments .

we are sending copies of this report to interested congressional committees , the heads of the agencies identified above and other interested parties .

this report will also be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of our report .

key contributors to this report are listed in appendix iii .

as part of the federal performance management framework originally put into place by the government performance and results act of 1993 ( gpra ) , and updated and expanded by the gpra modernization act of 2010 ( gprama ) , agencies are to identify the strategies and resources they will use to achieve their goals .

gprama also includes a provision for us to periodically review how implementing the act's requirements is affecting agency performance .

this report is part of our response to that mandate , and builds on our october 2016 report that described how agencies are using open innovation strategies , and identified practices to ensure they are implemented effectively .

specifically , this report ( 1 ) identifies key government - wide resources the office of management and budget ( omb ) , the office of science and technology policy ( ostp ) , and the general services administration ( gsa ) put in place to support the use of open innovation strategies ; ( 2 ) examines the extent to which key government - wide guidance reflects practices for effectively implementing these strategies ; and ( 3 ) identifies resources that selected agencies have developed to support the use of open innovation strategies .

to identify the various government - wide resources that omb , ostp , and gsa have developed , we reviewed relevant policy and guidance documents related to each open innovation strategy ( see table 9 below ) .

we also reviewed information available on relevant websites , including citizenscience.gov , challenge.gov , digitalgov.gov , data.gov , and project open data , and corroborated information collected from these websites through interviews with relevant agency staff .

lastly , we interviewed staff from these agencies involved in efforts to support and encourage the use of various open innovation strategies across the federal government .

in these interviews , we asked staff to identify key government - wide policies and guidance for each type of strategy , and to describe their roles and responsibilities in supporting and encouraging agency use of those strategies .

we also asked about their collaboration with staff in other agencies , among other things .

to determine the extent to which government - wide policies and guidance reflect practices and key actions for effectively implementing open innovation strategies , we first , in consultation with omb , ostp , and gsa staff , identified those that were considered key for each type of strategy .

these key policies and guidance are listed in table 9 .

for each strategy type , we then compared key guidance contents to the seven practices and 18 related key actions , listed in figure 16 , that we identified in october 2016 .

we identified those practices and key actions by analyzing and synthesizing suggested practices from relevant federal guidance and literature , including public and business administration journals , and publications from research organizations , as well as interviews with experts and agency officials with experience implementing open innovation initiatives .

we then analyzed the content of the implementation guidance for each strategy to determine the extent to which it reflects the practices and key actions we previously identified .

first , two analysts reviewed each source of guidance to identify all excerpts suggesting actions in line with those we identified .

next , a third analyst separately reviewed the guidance documents to verify the accuracy of the initial determinations , or identify those areas in need of additional discussion .

the analysts involved in the first and second stages of this analysis then made final determinations about whether the actions suggested in the guidance reflected those we identified .

we considered guidance to fully reflect a key action when it suggested steps in line with those outlined in our report .

if guidance suggested some but not all steps , we considered it to partially reflect the key action .

finally , when guidance did not suggest any steps in line with a key action , we considered guidance to not reflect it .

lastly , to identify the resources that selected agencies have put in place , we reviewed agency policies and guidance , and websites related to the use and implementation of open innovation strategies from six agencies: the departments of energy ( doe ) , health and human services ( hhs ) , housing and urban development ( hud ) , and transportation ( dot ) ; the environmental protection agency ( epa ) ; and the national aeronautics and space administration ( nasa ) .

we selected these agencies based on various criteria , including the number and variety of open innovation strategies outlined in their individual agency open government plans .

these selections were also in line with suggestions from knowledgeable staff at omb , ostp , and gsa familiar with agencies that have actively used such strategies .

in interviews with officials from these agencies , we asked them to identify agency - specific policies , procedures , or other guidance that has been developed to aid staff carrying out open innovation initiatives .

through this consultation , we were able to identify relevant policy documents and guidance developed by these agencies .

for instance , we identified epa's memorandum establishing the agency's challenge review team , which is responsible for approving proposals for the agency's prize competitions and challenges .

similarly , we were able to use this consultation to identify the federal highway administration's public involvement techniques for transportation decision - making guide , which provides staff with guidance on how to use various open dialogue approaches .

we were also able to use these interviews to identify relevant websites the agencies have developed that support open innovation efforts .

for instance , we identified the opennasa website , which provides the public with access to nasa's datasets , code , and other tools , as well as information on opportunities to collaborate with others on the use of nasa's data .

similarly , we also used this consultation to identify the national institutes of health's ( nih ) biomedical citizen science hub website , which is an online space for researchers and stakeholders interested in the use of citizen science in biomedicine to collaborate .

in our interviews with officials supporting the use of various open innovation strategies at these six agencies , we asked them to describe , among other things: the steps staff would go through to design , implement , and evaluate a specific type of open innovation strategy ; the support available to staff throughout that process ; and the ways resources at the government - wide level have helped support the use of open innovation strategies in their agency .

we conducted this performance audit from february 2016 to june 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , benjamin t. licht ( assistant director ) and adam miles supervised the development of this report .

steven putansu , lauren shaman , erik shive , wesley sholtes , and andrew j. stephens made significant contributions to this report .

john hussey and donna miller also made key contributions .

james cook , eric gorman , danielle novak , and jason vassilicos verified the information in the report .

