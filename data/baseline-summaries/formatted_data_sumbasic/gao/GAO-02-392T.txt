i am pleased to be here today to discuss the department of defense's ( dod ) management of its investment in the standard procurement system or sps program .

the department launched this program a little more than 7 years ago with the laudable goal of replacing 76 existing procurement systems with a single departmentwide system to more effectively support divergent contracting processes and procedures across its component organizations .

through sps , the department expected to improve efficiency and effectiveness in how it awarded and managed contracts , and , at that time , estimated life - cycle costs to be approximately $3 billion over a 10-year period .

the department's goals for sps are reinforced by the president's recent management agenda , which emphasizes investing in information technology to achieve results .

the agenda also noted that the federal government has not produced measurable gains in productivity commensurate with its investment in information technology , which is now estimated to be more than $50 billion for fiscal year 2003 .

the agenda reiterates that program performance and results are what matters most , and that actual program accomplishments , as well as needs , should be the prerequisite to continued funding .

this emphasis is consistent with information - technology investment management provisions of federal law and guidance and information - technology management practices of leading public - and private - sector companies .

for the sps program , we reported in july 2001 that the department had not met these investment management criteria .

specifically: the department had not economically justified its investment in the program because its latest ( january 2000 ) analysis of costs and benefits was not credible .

further , this flawed analysis showed that the system , as defined , was not a cost - beneficial investment .

it had not effectively addressed the inherent risks associated with investing in a program as large and lengthy as sps because it had not divided the program into incremental investment decisions that coincided with incremental releases of system capabilities .

the department had not met key program commitments that were used to justify the program .

for example , the department committed to implementing a commercially available contract management system ; however , because it had modified so much of the foundational commercial product , sps evolved into a customized dod system .

also , although the department committed to fully implementing the system by march 31 , 2000 , this target date had slipped by 3 ½ years to september 30 , 2003 , and program officials have recently stated that this date will also not be met .

it did not know if it was meeting other key program commitments .

for example , the department had not measured whether promised system benefits were being realized , and the information that was available about system performance showed that users were not satisfied with the system .

also , because dod was not accumulating actual program costs , it did not know the total amount spent on the program to date , yet life - cycle cost projections had grown from about $3 billion to $3.7 billion .

collectively , this meant that the question of whether further investment in sps was justified could not be answered with any certainty .

accordingly , we recommended that investment in future releases or major enhancements to the system be made conditional on the department first demonstrating that the system was producing benefits that exceed costs , and that future investment decisions be based on complete and reliable economic justifications .

we also recommended that program officials clarify organizational accountability and responsibility for the program , determine the program's current status , and identify lessons learned from the sps investment management experience .

in commenting on a draft of our report , the deputy chief information officer ( cio ) generally disagreed with our recommendations , noting that they would delay development and deployment of sps .

since that time , however , the department has either initiated or stated its intention to initiate steps that are consistent with our recommendations .

it has also taken steps to address the findings of several department - sponsored studies initiated at the time of our report .

for example , it has ( 1 ) clarified organizational accountability and responsibility for the program , ( 2 ) established missing controls over key acquisition processes such as requirements management and testing , and ( 3 ) begun addressing users' concerns .

in addition , department officials have stated that the department will prepare an economic analysis before investing beyond already executed contractual commitments and that it will conduct a productivity study to assess the extent to which the department is deriving benefits from sps .

these are positive steps that have advanced the program beyond where it was at the time of our report .

nevertheless , much remains to be done before the department will be in a position to make an informed , data - driven decision about whether further investment in the system is justified .

namely , although program officials have stated their intentions to address our recommendations , they have not yet committed to specific tasks for doing so nor have they established milestone dates for completing these tasks .

further , the department may expand the functionality of the current software release to include requirements previously slated for later releases , which could compound existing problems and increase costs ; and , although intended to be a standard system for the entire department , not all defense components have agreed to adopt sps .

in november 1994 , the office of the director of defense procurement initiated the sps program to acquire and deploy a single automated system to perform all contract - management - related functions for all dod organizations .

at that time , life - cycle costs were estimated to be about $3 billion over a 10-year period .

from 1994 to 1996 , the department defined sps requirements and solicited commercially available vendor products for satisfying these requirements .

subsequently , in april 1997 , the department awarded a contract to american management systems ( ams ) , incorporated , to ( 1 ) use ams's commercially available contract management system as the foundation for sps , ( 2 ) modify this commercial product as necessary to meet dod requirements , and ( 3 ) perform related services .

the department also directed the contractor to deliver functionality for the system in four incremental releases .

the department later increased the number of releases across which this functionality would be delivered to seven , reduced the size of the increments , and allowed certain more critical functionality to be delivered sooner ( see table 1 for proposed sps functionality by increment ) .

since our report of july 2001 , dod has revised its plans .

according to the sps program manager , current plans no longer include increments 6 and 7 or releases 5.0 and 5.1 .

instead , release 4.2 ( increment 5 ) will include at least three , but not more than seven , subreleases .

at this time , only the first of the potentially seven 4.2 subreleases is under contract .

this subrelease is scheduled for delivery in april 2002 , with deployment to the army and the defense logistics agency scheduled for june 2002 .

based on the original delivery date , release 4.2 is about one year overdue .

the department reports that it has yet to define the requirements to be included within the remaining 4.2 subreleases , and has not executed any contract task orders for these subreleases .

according to sps officials , they will decide later this year whether to invest in these additional releases .

as of december 2001 , the department reported that it had deployed four sps releases to over 777 locations .

the director of defense procurement ( ddp ) has responsibility for the sps program , and the cio is the milestone decision authority for sps because the program is classified as a major defense acquisition .

our july 2001 report detailed program problems and investment management weaknesses .

to address these weaknesses , we recommended , among other things , that the department report on the lessons to be learned from its sps experience for the benefit of future system acquisitions .

similarly , other reviews of the program commissioned by the department in the wake of our review raised similar concerns and identified other problems and management weaknesses .

the findings from our report are summarized below in two major categories: lack of economic justification for the program and inability to meet program commitments .

we also summarize the findings of the other studies .

the clinger - cohen act of 1996 , omb guidance , dod policy , and practices of leading organizations provide an effective framework for managing information technology investments , not just when a program is initiated , but continuously throughout the life of the program .

together , they provide for ( 1 ) economically justifying proposed projects on the basis of reliable analyses of expected life - cycle costs , benefits , and risks ; and ( 2 ) using these analyses throughout a project's life - cycle as the basis for investment selection , control , and evaluation decisionmaking , and doing so for large projects ( to the maximum extent practical ) by dividing them into a series of smaller , incremental subprojects or releases and individually justifying investment in each separate increment on the basis of costs , benefits , and risks .

the department had not met these investment management tenets for sps .

first , the latest economic analysis for the program — dated january 2000 — was not based on reliable estimates because most of the cost estimates in the 2000 economic analysis were estimates carried forward from the april 1997 analysis ( adjusted for inflation ) .

only the cost estimates being funded and managed by the sps program office , which were 13 percent of the total estimated life - cycle cost in the analysis , were updated in 2000 to reflect more current contract estimates and actual expenditures / obligations for fiscal years 1995 through 1999 .

moreover , the military services , which share funding responsibility with the sps program office for implementing the program , questioned the reliability of these cost estimates .

however , this uncertainty was not reflected in the economic analysis using any type of sensitivity analysis .

a sensitivity analysis would have disclosed for decisionmakers the investment risk being assumed by relying on the estimates presented in the economic analysis .

moreover , the latest economic analysis ( january 2000 ) was outdated because it did not reflect the program's current status and known problems and risks .

for instance , this analysis was based on a program scope and associated costs and benefits that anticipated four software releases .

however , as mentioned previously , the program now consists of five releases , and subreleases within releases , in order to accommodate changes in sps requirements .

estimates of the full costs , benefits , and risks relating to this additional release and its subreleases were not part of the 2000 economic analysis .

also , this analysis did not fully recognize actual and expected delays in meeting sps's full operational capability milestone , which had been slipped by 3½ years and dod officials say that further delays are currently expected .

such delays not only increase the system acquisition costs but also postpone , and thus reduce , accrual of system benefits .

further , several dod components are now questioning whether they will even deploy the software , which would further reduce sps's cost effectiveness calculations in the 2000 economic analysis .

second , the department had not used these analyses as the basis for deciding whether to continue to invest in the program .

the latest economic analysis showed that sps was not a cost - beneficial investment because the estimated benefits to be realized did not exceed estimated program costs .

in fact , the 2000 analysis showed estimated costs of $3.7 billion and estimated benefits of $1.4 billion , which was a recovery of only 37 percent of costs .

according to the former sps program manager , this analysis was not used to manage the program and there was no dod requirement for updating an economic analysis when changes to the program occurred .

third , dod had not made its investment decisions incrementally as required by the clinger - cohen act and omb guidance .

that is , although the department is planning to acquire and implement sps as a series of five increments , it has not made decisions about whether to invest in each release on the basis of the release's expected return on investment , as well as whether prior releases were actually achieving return - on - investment expectations .

in fact , for the four increments that have been deployed , the department had not validated whether the increments were providing promised benefits and was not accounting for the costs associated with each increment so that it could even determine actual return on investment .

instead , the department had treated investment in this program as one , monolithic investment decision , justified by a single , “all - or - nothing” economic analysis .

our work has shown that it is difficult to estimate , with any degree of accuracy , cost and schedule estimates for many increments to be delivered over many years because later increments are not well understood or defined .

also , these estimates are subject to change based on actual program experiences and changing requirements .

this “all - or - nothing” approach to investing in large system acquisitions , like sps , has repeatedly proven to be ineffective across the federal government , resulting in huge sums being invested in systems that do not provide commensurate benefits .

measuring progress against program commitments is closely aligned with economically justifying information - technology investments , and is equally important to ensuring effective investment management .

the clinger - cohen act , omb guidance , dod policy , and practices of leading organizations provide for making and using such measurements as part of informed investment decisionmaking .

dod had not met key commitments and was uncertain whether it was meeting other commitments because it was not measuring them .

 ( see table 2 for a summary of the department's progress against commitments. ) .

two analyses , such as the number and dollar value of estimated benefits , and the information gathered did not map to the 22 benefit types listed in the 1997 economic analysis .

instead , the study collected subjective judgments ( perceptions ) that were not based on predefined performance metrics for sps capabilities and impacts .

thus , the department was not measuring sps against its promised benefits .

the former program manager told us that knowing whether sps was producing value and meeting commitments was not the program office's objective because there was no departmental requirement to do so .

rather , the objective was simply to acquire and deploy the system .

similarly , cio officials told us that the department was not validating whether deployed releases of sps were producing benefits because there was no dod requirement to do so and no metrics had been defined for such validation .

however , the clinger - cohen act of 1996 and omb guidance emphasize the need to have investment management processes and information to help ensure that information - technology projects are being implemented at acceptable costs and within reasonable and expected time frames and that they are contributing to tangible , observable improvements in mission performance ( i.e. , that projects are meeting the cost , schedule , and performance commitments upon which their approval was justified ) .

for programs such as sps , dod required this cost , schedule , and performance information to be reported quarterly to ensure that programs did not deviate significantly from expectations .

in effect , these requirements and guidance recognize that one cannot manage what one cannot measure .

shortly after receiving our draft report for comment , the department initiated several studies to determine the program's current status , assess program risks , and identify actions to improve the program .

these studies focused on such areas as program costs and benefits , planned commitments , requirements management , program office structure , and systems acceptance testing .

consistent with our findings and recommendations , these studies identified the need to establish performance metrics that will enable the department to measure the program's performance and tie these metrics to benefits and customer satisfaction ; clearly define organizational accountability for the program ; provide training for all new software releases ; standardize the underlying business processes and rules that the system is to support ; acquire the software source code ; and address open customer concerns to ensure user satisfaction .

in addition , the department found other program management concerns not directly within the scope of our review , such as the need to appropriately staff the program management office with sufficient resources and address the current lack of technical expertise in areas such as contracting , software engineering , testing , and configuration management ; modify the existing contract to recognize that the system does not employ a commercial - off - the - shelf software product , but rather is based on customized software product ; establish dod - controlled requirements management and acceptance testing processes and practices that are rigorous and disciplined ; and assess the continued viability of the existing contractor .

to address the many weaknesses in the sps program , we made several recommendations in our july 2001 report .

specifically , we recommended that ( 1 ) investment in future releases or major enhancements to the system be made conditional on the department first demonstrating that the system is producing benefits that exceed costs ; ( 2 ) future investment decisions , including those regarding operations and maintenance , be based on complete and reliable economic justifications ; ( 3 ) any analysis produced to justify further investment in the program be validated by the director , program analysis and evaluation ; ( 4 ) the assistant secretary of defense for command , control , communications , and intelligence ( c3i ) clarify organizational accountability and responsibility for measuring sps program against commitments and to ensure that these responsibilities are met ; ( 5 ) program officials take the necessary actions to determine the current state of progress against program commitments ; and ( 6 ) the assistant secretary of defense for c3i report by october 31 , 2001 , to the secretary of defense and to dod's relevant congressional committees on lessons learned from the sps investment management experience , including what actions will be taken to prevent a recurrence of this experience on other system acquisition programs .

dod's reaction to our report was mixed .

in official comments on a draft of our report , the deputy cio generally disagreed with our recommendations , noting that they would delay development and deployment of sps .

since that time , however , the department has acknowledged its sps problems and begun taking steps to address some of them .

in particular , it has done the following .

the department has established and communicated to applicable dod organizations the program's chain - of - command and defined each participating organization's responsibilities .

for example , the joint requirements board was delegated the responsibility for working with the program users to define and reach agreement on the needed functionality for each software release .

the department has restructured the program office and assigned additional staff , including individuals with expertise in the areas of contracting , software engineering , configuration management , and testing .

however , according to the current program manager , additional critical resources are needed , such as two computer information technology specialists and three contracting experts .

it has renegotiated certain contract provisions to assume greater responsibility and accountability for the requirements management and testing activities .

for example , dod , rather than the contractor , is now responsible for writing the test plans .

however , additional contract changes remain to be addressed , such as training , help - desk structure , facilities support , and system operations and maintenance .

the department has designated a user - satisfaction manager for the program and defined forums and approaches intended to better engage users .

it has established a new testing process , whereby program officials now develop the test plans and maintain control over all software testing performed .

in addition , sps officials have stated their intention to prepare analyses for future program activities beyond those already under contract , such as the acquisition of additional system releases , and use these analyses in deciding whether to continue to deploy sps or pursue another alternative ; define system performance metrics and use these metrics to assess the extent to which benefits have been realized from already deployed system releases ; and report on lessons learned from its sps experience to the secretary of defense and relevant congressional committees .

the department's actions and intentions are positive steps and consistent with our recommendations .

however , much remains to be accomplished .

in particular , the department has yet to implement our recommendations aimed at ensuring that ( 1 ) future releases or major enhancements to the system be made conditional on first demonstrating that the system is producing benefits that exceed costs and ( 2 ) future investment decisions , including those regarding operations and maintenance , be based on a complete and reliable economic justification .

we also remain concerned about the future of sps for several additional reasons .

first , definitive plans for how and when to justify future system releases or major enhancements to existing releases do not yet exist .

second , sps officials told us that release 4.2 , which is currently under contract , may be expanded to include functionality that was envisioned for releases 5.0 and 5.1 .

including such additional functionality could compound existing problems and increase program costs .

third , not all defense components have agreed to adopt sps .

for example , the air force has not committed to deploying the software ; the national imagery and mapping agency , the defense advanced research projects agency , and the defense intelligence agency have not yet decided to use sps ; and the dod education agency has already adopted another system because it deemed sps too expensive .

