the department of defense ( dod ) has proposed that the practices and policies of the office of the director of operational test and evaluation ( dot&e ) be modified to reduce the time and cost of developing and fielding new weapon systems .

to help focus deliberations on dod's proposal , you asked us to review dot&e's operations and organizational structure for overseeing operational testing .

specifically , you asked us to assess ( 1 ) dot&e's efforts and their impact on the quality of operational testing and evaluation in dod and ( 2 ) the strengths and weaknesses of the current organizational framework in dod for operational testing .

as part of our review , we conducted 13 case studies of the testing of individual weapon systems .

 ( our scope and methodology are described in app .

i , and brief descriptions of the 13 weapon systems are provided in app .

ii. ) .

in 1983 , congress established dot&e to coordinate , monitor , and evaluate operational testing of major weapon systems .

as part of the office of the secretary of defense ( osd ) , dot&e is separate from the acquisition community that conducts developmental and operational testing and therefore is in a position to provide the secretary and congress with an independent view .

congress created dot&e in response to reports of conflicts of interest in the acquisition community's oversight of operational testing leading to inadequate testing of operational suitabilityand effectiveness and the fielding of new systems that performed poorly .

 ( dod's system acquisition process is described in app .

iii. ) .

by law , dot&e serves as the principal adviser on operational test and evaluation in dod and bears several key responsibilities , including monitoring and reviewing all operational test and evaluation in dod , reporting to the secretary of defense and congressional committees whether the tests and evaluations of weapon systems were adequate and whether the results confirmed that the system is operationally suitable and effective for combat before a decision is made to proceed to full - rate production , and submitting to the secretary of defense and congressional decisionmakers an annual report summarizing operational test and evaluation activities during the preceding fiscal year .

in 1993 , dod's advisory panel on streamlining and codifying acquisition laws concluded that dot&e was impeding the goals of acquisition reform by ( 1 ) promoting unnecessary oversight , ( 2 ) requiring excessive reporting detail , ( 3 ) inhibiting the services' discretion in testing , and ( 4 ) limiting participation of system contractors in operational tests where such involvement is deemed necessary by the services .

the following year , dod proposed legislative changes that would have reduced the scope and authority of dot&e .

in testimony , we opposed these changes because they were directed at perceived rather than documented problems and would undermine a key management control over the acquisition process — independent oversight of operational test and evaluation .

although the legislative proposals were not adopted , in 1995 the secretary of defense implemented several operational test and evaluation initiatives in the department to ( 1 ) involve operational testers earlier in the acquisition process , ( 2 ) use models and simulations effectively , ( 3 ) combine tests where possible , and ( 4 ) combine tests and training .

the goals of these initiatives included saving time and money by identifying and addressing testing issues earlier in the acquisition process ; merging or closely coordinating historically distinct phases , such as developmental and operational testing to avoid duplication ; and using existing technologies and training exercises to create realistic and affordable test conditions .

a frequent complaint among representatives of the services' operational testing agencies was that dot&e frequently demanded more tests than were proposed by the operational test agencies in draft master plans or test plans .

operational test agency representatives contended that the additional testing was either unnecessary for determining the operational effectiveness or suitability of a program or unrealistic in light of the limitations in the services' testing resources .

however , our review indicated that dot&e urged more testing to reduce the level of risk and number of unknowns prior to the decision to begin full production , while program and service officials typically sought less testing and were willing to accept greater risk when making production decisions .

the additional testing dot&e advocated , often over the objections of service testers , served to meet the underlying objectives of operational testing — to reduce the uncertainty and risk that systems entering full - rate production would not fulfill their requirements .

the impact of dot&e oversight varies with the system under development .

table 1 summarizes the types of impacts that dot&e advocated or facilitated in operational testing among the 13 cases we studied .

while the impacts vary , one consistent pattern in our case studies was a reduction in uncertainty regarding the weapon systems' suitability or effectiveness prior to the full - rate production decision .

each of the impacts are discussed in more detail in tables 2-6 and in subsequent sections .

two of dot&e's typical concerns in reviewing service test plans are that the proposed test methodologies enable ( 1 ) comparisons of a system's effectiveness through side - by - side testing between the existing and modified systems and ( 2 ) assessments of a system's reliability through a sufficient number of test repetitions .

table 2 illustrates examples of cases where additional testing was conducted at dot&e's insistence or with dot&e's support to alleviate these and other types of effectiveness and suitability concerns .

table 3 illustrates examples where the design or conduct of operational testing was modified at dot&e's insistence or with dot&e's support to increase the realism of test conditions and reduce the uncertainty of system suitability or effectiveness .

dot&e can insist on or support changes in data analysis plans that provide more meaningful analyses for decisionmakers .

table 4 illustrates instances in which dot&e altered the proposed data collection or analysis plans to enhance the reliability or utility of the test data .

dot&e's independent analysis of service test data may confirm or dispute the results and conclusions reported by the service .

in the cases described in table 5 , dot&e's analysis of service operational test and evaluation data resulted in divergent , often less favorable conclusions than those reached by the service .

when dot&e concludes that a weapon system has not fully demonstrated operational suitability or effectiveness , or if new testing issues arise during initial operational test and evaluation , it may recommend that follow - on operational test and evaluation be done after the full - rate production decision .

table 6 identifies follow - on operational test and evaluation that dot&e supported .

the existence of a healthy difference of opinion between dot&e and the acquisition community is a viable sign of robust oversight .

in nearly all of the cases we reviewed , the services and dot&e cited at least one testing controversy .

for example , services differ on how they view the relationship between operational testing and their development of tactics , techniques , and procedures .

in addition , dot&e's ability to independently view the development and testing of new systems across the services brings value to the context of testing .

however , several current trends have the potential to adversely affect dot&e's independence and its ability to affect operational test and evaluation , including ( 1 ) service challenges to dot&e's authority to require and oversee follow - on operational testing and evaluation , ( 2 ) declining resources available for oversight , ( 3 ) the management of limited resources to address competing priorities , ( 4 ) dot&e's participation in the acquisition process as a member of the program manager's working - level integrated product teams , and ( 5 ) greater integration of developmental and operational testing .

dot&e's impact on operational testing is dependent upon its ability to manage these divergent forces while maintaining its independence .

although the acquisition community has three central objectives — performance , cost , and schedule — dot&e has but one: operational testing of performance .

these distinct priorities lead to testing disputes .

characteristically , the disputes for each system we reviewed revolved around questions of how , how much , and when to conduct operational testing , not whether to conduct operational testing .

conflicts encompassed issues such as ( 1 ) how many and what types of tests to conduct ; ( 2 ) when testing should occur ; ( 3 ) what data to collect , how to collect it , and how best to analyze it ; and ( 4 ) what conclusions were supportable , given the analysis and limitations of the test program .

the foundation of most disputes lay in different notions of the costs and benefits of testing and the levels of risk that were acceptable when making full - rate production decisions .

dot&e consistently urged more testing ( and consequently more time , resources , and cost ) to reduce the level of risk and number of unknowns before the decision to proceed to full - rate production , while the services consistently sought less testing and accepted more risk when making production decisions .

among our case studies , these divergent dispositions frequently led to healthy debates about the optimal test program , and in a small number of cases , the differences led to contentious working relations .

in reviews of individual weapon systems , we have consistently found that testing and evaluation is generally viewed by the acquisition community as a requirement imposed by outsiders rather than a management tool to identify , evaluate , and reduce risks , and therefore a means to more successful programs .

developers are frustrated by the delays and expense imposed on their programs by what they perceive as overzealous testers .

the program office strives to get the program into production despite uncertainties that the system will work as promised or intended .

therefore , reducing troublesome parts of the acquisition process — such as operational testing — is viewed as a means to reduce the time required to enter production .

nonetheless , the commanders and action officers within the service operational test agencies were nearly unanimous in their support for an independent test and evaluation office within osd .

for example , the commander of the army's operational test and evaluation command commended the style and orientation of the current dot&e director and affirmed the long - term importance of the office and its independent reporting responsibilities to congress .

the commander of the navy's operational test and evaluation force stated that the independence of both dot&e and the operational test agency was an essential element in achieving their common goal of ensuring that new programs pass sufficiently rigorous and realistic operational testing prior to fielding .

the commander of the air force's operational test and evaluation , while critical of dot&e oversight of several major weapon systems , said that the services were well served by dot&e's potential to independently report to congress .

moreover , nearly all the operational test agency action officers we interviewed participate in the integrated product teams with the dot&e action officers and recognized the value of the office's independent oversight role .

the action officers within the service testing organizations also have a degree of independence that enables them to represent the future users of systems developed in the acquisition community .

these action officers stated that their ability to voice positions unpopular with the acquisition community was strengthened when dot&e separately supported their views .

in discussions with over three dozen action officers and analysts responsible for the 13 cases we reviewed , the independence of dot&e emerged as the fundamental condition to enable effective and efficient oversight .

the foundation of interagency ( i.e. , dot&e and service operational test agencies ) relations is based on the independence of dot&e , its legislative mandate , and its independent reporting to congress .

dot&e is outside the chain of command of those responsible for developing and testing new systems .

the services need to cooperate with dot&e primarily because the office must approve all test and evaluation master plans and operational test plans .

moreover , dot&e independently reports on the operational suitability and effectiveness at a system's full - rate production milestone , a report that is sent separately to congress .

dot&e's report on a system's operational suitability and effectiveness is only one of several inputs considered before the full - rate production decision is made .

an unfavorable dot&e report does not necessarily prevent full - rate production .

in each of the cases cited below , an affirmative full - rate production decision was made despite a dot&e report concluding that the system had not demonstrated during operational test and evaluation that it was both operationally suitable and operationally effective: full - rate production of the m1a2 tank was approved despite dot&e's report that found the system unsuitable .

full - rate production of joint stars was approved , though the system demonstrated only limited effectiveness for “operations other than war” and found “as tested is unsuitable.” only 18 of the 71 performance criteria were met ; 53 others required more testing .

full - rate production of the c - 17 airlifter was approved despite a number of operational test and evaluation deficiencies , including immature software and failure to meet combination and brigade airdrop objectives .

the services contend that dot&e does not have authority to insist on , or independently approve the conduct of , follow - on operational test and evaluation .

however , in several of the systems we reviewed , dot&e overcame service opposition and monitored follow - on operational test and evaluation .

it used several means to achieve success , such as ( 1 ) incorporating follow - on operational test and evaluation in test and evaluation master plans developed and approved prior to the full - rate production decision milestone ; ( 2 ) persuading the secretary of defense to specify follow - on operational test and evaluation , and dot&e's oversight role , in the full - rate production acquisition decision memorandum ; and ( 3 ) citing policy , based on title 10 , that entitles dot&e to oversee operational test and evaluation whenever it occurs in the acquisition process .

nonetheless , dot&e action officers stated that the service's acceptance of dot&e's role in follow - on operational test and evaluation varies over time , by service and acquisition system , and is largely dependent upon the convictions of executives in both the services and dot&e .

among the cases reviewed in this report , the services offered a variety of arguments against dot&e's having a role in follow - on operational test and evaluation .

they specifically asserted the following: dot&e need not be involved because the scope of follow - on operational test and evaluation is frequently less encompassing than initial operational test and evaluation .

follow - on operational test and evaluation has been characterized as testing by the user to determine the strengths and weaknesses of the system and to determine ways to compensate for , or fix , shortcomings observed in initial operational test and evaluation .

title 10 provides dot&e with the authority to monitor and review — but not necessarily approve — service follow - on operational test and evaluation plans .

follow - on operational test and evaluation is unnecessary when a system is found to be operationally effective and suitable during initial operational test and evaluation — even though dot&e does not concur .

a clear distinction between dot&e oversight in follow - on operational test and evaluation versus initial operational test and evaluation is that dot&e is not required to report follow - on operational test and evaluation results to congress in the detailed manner of the beyond - lrip report .

therefore , even if follow - on operational test and evaluation is conducted to assess modifications to correct effectiveness or suitability shortcomings reported to congress in the beyond - lrip report , there is no requirement that congress receive a detailed accounting of the impact of these modifications .

dot&e's primary asset to conduct oversight — its cadre of action officers — has decreased in size throughout the decade .

this creates a management challenge for the office because at the same time staff has decreased , the number of programs overseen by dot&e has increased .

as illustrated in table 7 , authorized staffing declined from 48 in fiscal year 1990 to 41 in fiscal year 1997 , as did funding ( in constant dollars ) from $12,725,000 in fiscal year 1990 to $11,437,000 in fiscal year 1997 .

the decline in dot&e funding is consistent with the general decline in dod appropriations during this period .

however , since fiscal year 1990 , while the authorized staffing to oversee operational test and evaluation has declined by 14.6 percent , the number of systems on the oversight list has increased by 17.7 percent .

with declining resources , dot&e must manage competing priorities related to its oversight , advisory , and coordination responsibilities .

dot&e must balance the continuing need to allocate resources to these different priorities while not being perceived as having lost any independence .

dot&e management has flexibility in defining some portion of the scope of its oversight and has continued to electively oversee a substantial number of nonmajor defense acquisition programs and assumed a leading role in advocating an examination of the modernization needs of the test and evaluation infrastructure .

between fiscal year 1990 and 1996 , the number of nonmajor acquisition programs overseen annually by dot&e ranged between 19 and 43 .

in fiscal year 1996 , when the oversight list reached a peak of 219 , 1 of every 8 programs was listed at the discretion of dot&e .

thus , during this period when the resources to oversee operational testing declined and acquisition reforms have placed additional burdens on oversight staff , the directors of dot&e continued to place extra responsibility on their staff by augmenting the required oversight of major acquisition programs with a substantial number of optional systems .

despite a relative decline in resources for oversight , dot&e management has also elected to assume “a larger role in test resource management planning and leadership in an attempt to achieve much - needed resource modernization.” although the director is designated as the principal adviser to the secretary of defense and the under secretary of defense for acquisition and technology on operational test and evaluation , including operational test facilities and equipment , assuming the larger role defined by dot&e may be at the expense of its testing oversight mission and perception of independence .

the dot&e director is now an adviser to the central test and evaluation investment program and previously served as chairman of the test and evaluation committee .

the committee is responsible for the investment program and presides over the planning , programming , and budgeting for development and operational test resources .

when the director served as chairman , we questioned whether these ties created the perception that the director was not independent from developmental testing .

this issue may resurface as dot&e seeks a larger role in test resource management planning .

also , as the emphasis , cost , and time for operational test and evaluation are increasingly questioned in the drive to streamline acquisition , and as oversight assets are stretched , new dot&e initiatives may stress the office's capacity to manage oversight effectively .

in may 1995 , the secretary of defense directed dod to apply the integrated product and process development concept — using integrated product teams — throughout the acquisition process .

the revised dod acquisition regulations ( dod 5000.2-r march 1996 ) also addressed the use of empowered integrated product teams at the program office level .

dot&e action officers participate as members of the working - level integrated product teams , and the dot&e director is a member of the overarching team .

one objective of integrated product teams , and dot&e participation in particular , is to expedite the approval process of test documents by reaching agreement on the strategy and plan through the identification and resolution of issues early , understanding the issues , and documenting a quality test and evaluation master plan that is acceptable to all organizational levels the first time .

integrated product teams are designed to replace a previously sequential test and evaluation master plan development and approval process and therefore enhance timeliness .

while this management tool could increase communication between testers and the program managers , it also poses a challenge to dot&e independence .

the challenge was recognized by the department of defense inspector general ( dod ig ) when after reviewing the conduct of operational testing it subsequently recommended that “to meet the intent of 10 u.s.c .

139 , dot&e should be a nonvoting member [of the working - level integrated product team] so as to maintain his independence.” {emphasis added} though integrated product teams were not used throughout the entire time period covered by this report , several action officers noted that this management tool created threats to their effectiveness other than having their positions out - voted .

one dot&e action officer reported having the lone dissenting opinion in a meeting of 30 participants seeking to reach consensus and resolve issues early .

the pressure of maintaining independent , contrary positions in large working groups can be a test .

several dot&e representatives also noted that the frequency of integrated product team meetings to cover the multiple systems for which they were responsible made it impossible for them to attend all , thereby lessening the possibility that testing issues can be identified and resolved as early as possible .

moreover , program managers and dot&e pursue different objectives through integrated product teams .

the services and program managers view the teams as a way to facilitate their program objectives for cost , schedule , and performance ; dot&e's objective is oversight of performance through operational testing .

the program managers and dot&e share a desire to identify testing issues as early as possible .

however , the priority of the program manager to resolve these issues as early as possible through the teams may conflict with dot&e's mission .

dot&e must remain flexible and react to unknowns as they are disclosed during developmental testing , operational assessments , and initial operational test and evaluation .

thus , dot&e's participation on the teams is a natural source of tension and a potential impediment to the team's decision - making .

the challenge for dot&e action officers is to maintain an independent and potentially contrary position in an ongoing working group during the life of a program , which may extend over several years .

the objectives of developmental and operational testing are distinct .

developmental testing determines whether a system meets its functional requirements and contractual technical performance criteria sufficiently to proceed with operational testing .

operational testing determines whether the system meets the operational requirements and will contribute to mission effectiveness in relevant operational environments sufficiently to justify proceeding with production .

the integration of these two disparate test activities is proposed to save the time and resources required for testing and evaluation .

the sentiment to more closely link developmental and operational testing dates from at least the 1986 blue ribbon commission on defense management ( packard commission ) , which found that “developmental and operational testing have been too divorced , the latter has been undertaken too late in the cycle , and prototypes have been used and tested far too little.” however , both we and the dod ig have found that systems were regularly tested before they were ready for testing .

in its 1996 report , the dod ig reported that “4 of 15 systems we examined for operational testing were not ready for testing .

this situation occurred because a calendar schedule rather than system readiness often drove the start of testing.” similarly , we have observed numerous systems that have been pushed into low - rate initial production without sufficient testing to demonstrate that the system will work as promised or intended .

our reviews of major system development in recent years have found that because insufficient time was dedicated to initial testing , systems were produced that later experienced problems during operational testing and systems entered initial production despite experiencing problems during early operational testing .

in 1996 the secretary of defense also urged the closer integration of developmental and operational testing , and combined tests where possible , in part to enhance the objectives of acquisition reform .

combined developmental and operational testing is only one of many sources of test data that dot&e has used to foster more timely and thorough operational test and evaluation .

other sources of information include contractor developmental testing , builder's trials , component testing , production lot testing , stockpile reliability testing , and operational deployments .

while dot&e has some influence over the quality of operational testing , by independently reviewing the design , execution , analysis , and reporting of such tests , it has no direct involvement or oversight of these other sources of testing information .

the use of alternative sources of test data as substitutes for operational test and evaluation will limit dot&e's oversight mission , which was created to improve the conduct and quality of testing .

dot&e's challenge is to manage an expansion in independent oversight while satisfying the efficiency goals of acquisition reform and undergoing the economic pressures of downsizing .

dot&e oversight is clearly affecting the operational testing of new defense systems .

dot&e actions ( such as the insistence on additional testing , more realistic testing , more rigorous data analysis , and independent assessments ) are resulting in more assurance that new systems fielded to our armed forces are safe , suitable , and effective .

however , dot&e is not , by design or practice , the guarantor of effective and suitable acquisitions .

dot&e oversight reduces , but does not eliminate , the risk that new systems will not be operationally effective and suitable .

affirmative full - rate production decisions are made for systems that have yet to demonstrate their operational effectiveness or suitability .

moreover , the services question dot&e's authority regarding follow - on test and evaluation of subsequent corrective actions by the program office .

we recommend that the secretary of defense revise dod's operational test and evaluation policies in the following ways: require the under secretary of defense for acquisition and technology , in those cases where affirmative full - rate production decisions are made for major systems that have yet to demonstrate their operational effectiveness or suitability , to ( 1 ) take corrective actions to eliminate deficiencies in effectiveness or suitability and ( 2 ) conduct follow - on test and evaluation of corrective actions until the systems are determined to be operationally effective and suitable by the director , operational test and evaluation .

require the director , operational test and evaluation , to ( 1 ) review and approve follow - on test and evaluation master plans and specific operational test plans for major systems before operational testing related to suitability and effectiveness issues left unresolved at the full - rate production decision and ( 2 ) upon the completion of follow - on operational test and evaluation , report to congress , the secretary of defense , and the under secretary of defense for acquisition and technology whether the testing was adequate and whether the results confirmed the system is operationally suitable and effective .

further , in light of increasing operational testing oversight commitments and to accommodate oversight of follow - on operational testing and evaluation , we recommend that the director , operational test and evaluation , prioritize his office's workload to ensure sufficient attention is given to major defense acquisition programs .

in commenting on a draft of this report , dod concurred with our first and third recommendations and partially concurred with our second recommendation .

concerning the recommendation with which it partially concurred , dod stated that system specific reports to the secretary of defense and congress are not warranted for every system that requires follow - on operational test and evaluation .

dod pointed out that for specific programs designated for follow - on oversight , test plans are prepared to correct previously identified deficiencies by milestone iii , and dot&e includes the results of follow - on testing in its next annual report .

we continue to believe our recommendation has merit .

we recommended that the secretary require dot&e approval of follow - on test and evaluation of corrective actions because during our review we found no consensus within the defense acquisition community concerning dot&e's role in follow - on operational test and evaluation .

in its comments dod did not indicate whether it intended to give dot&e a role in follow - on operational test and evaluation that is comparable to its role in initial operational test and evaluation .

moreover , we continue to believe that if a major system goes into full - rate production ( even though it was deemed by dot&e not to be operationally suitable and effective ) based on the premise that corrections will be made and some follow - on operational test and evaluation will be performed , dot&e should report , as promptly as possible , whether or not the follow - on operational test and evaluation results show that the system in question had improved sufficiently to be characterized as both operationally suitable and effective .

dod's comments are reprinted in their entirety in appendix iv , along with our specific evaluation .

as agreed with your offices , unless you publicly announce its contents earlier , we plan no further distribution of this report until 15 days after its date of issue .

we will then send copies to other congressional committees and the secretary of defense .

we will also make copies available to others upon request .

if you have any questions or would like additional information , please do not hesitate to call me at ( 202 ) 512-3092 or the evaluator - in - charge , jeff harris , at ( 202 ) 512-3583 .

to develop information for this report , we selected a case study methodology — evaluating the conduct and practices of dot&e through an analysis of 13 weapon systems .

recognizing that many test and evaluation issues are unique to individual systems , we determined that a case study methodology would offer the greatest probability of illuminating the variety of factors that impact the value or effectiveness of oversight at the level of the office of the secretary of defense ( osd ) .

moreover , with nearly 200 systems subject to review of the director , operational test and evaluation ( dot&e ) at any one time , we sought a sample that would enable us to determine if the office had any impact as well as the ability to examine the variety of programs overseen .

therefore , we selected a judgmental sample of cases reflecting the breadth of program types .

as illustrated in table i.1 , we selected systems ( 1 ) from each of the primary services , ( 2 ) categorized as major defense systems , and ( 3 ) representing a wide array of acquisition and testing phases — from early operational assessments through and beyond the full - rate production decision .

we studied both new and modified systems .

table i.1: characteristics of weapon systems used for case studies service ( s ) ms iii ( 1995 ) ; iot&e ( 1995 ) fot&e ( 1995-96 ) ; bosnia ( 1995 ) fot&e ( 1996-98 ) ; ms iiib ( 1995 ) afsarc iii ( 1997 ) ; iot&e ( 1995-96 ) ms iii ( 2003 ) ; iot&e ( 2002 ) ; lrip ( 1999 ) ms iii ( 1997 ) ; lut ( 1996 ) ; ue ( 1996 ) ( continued ) service ( s ) fot&e ( 1997 ) ; ms iii ( 1996 ) ; bosnia ( 1996 ) ms iii ( 1998 ) ; iot&e ( 1997-98 ) ; bosnia ( 1995 ) ms ii ( 1996 ) ; eoa - 2 ( 1996 ) ; eoa - 1 ( 1994-95 ) fot&e ( 1995-96 ) ; ms iii ( 1994 ) fot&e - 1 ( 1997-98 ) ; ms iii ( 1996 ) ; iot&e - 2 ( 1995-96 ) fot&e ( 1997 ) ; ms iii ( 1996 ) ; opeval ( 1996 ) ms iii ( 1997 ) ; dt / iot&e ( 1994 ) ms iii ( 1998 ) ; opeval ( 1998 ) ; iot&e ( 1997 ) ms iii ( 2000 ) ; opeval ( 1999-00 ) ; iot&e ( 1999 ) opeval ( 1999 ) ; ot - iic ( 1996 ) ( table notes on next page ) dot&e , the service operational test agencies , and the institute for defense analyses ( ida ) personnel agreed that dot&e was influential in the testing done on these 13 systems .

in several cases , the participating agencies vehemently differed on the value of dot&e's actions ; however , whether dot&e had an impact on testing ( be it perceived as positive or negative ) was not in dispute .

in conducting our 13 case studies , we assessed the strengths and weaknesses of the organizational framework in dod for operational testing via test agency representatives , an assessment on the origins and implementation ( exemplified by the 13 cases ) of the title 10 amendments creating and empowering dot&e , and a review of the literature .

to compile case study data , we interviewed current action officers in both dot&e and the appropriate operational test agency and reviewed documentation provided by the operational test agencies , dot&e , and ida .

using structured questionnaires , we interviewed 12 dot&e and 27 operational test agency action officers responsible for the 13 selected systems as well as managers and technical support personnel in each organization .

in addition , we interviewed the commanders of each of the service testing agencies and dot&e .

when possible , we corroborated information obtained from interviews with documentation , including test and evaluation master plans , beyond low - rate initial production reports , defense acquisition executive summary status reports , defense acquisition memoranda , and interagency correspondence .

in washington , d.c. , we obtained data from or performed work at the office of the director of operational test and evaluation , osd ; deputy under secretary of defense for acquisition reform ; directorate of navy test and evaluation and technology requirements , office of the chief of naval operations ; test and evaluation management agency , director of army staff ; air force test and evaluation directorate ; and the dod office of the inspector general .

we also reviewed data and interviewed officials from the army operational test and evaluation command and the institute for defense analyses , alexandria , virginia ; the navy commander , operational test and evaluation force , norfolk , virginia ; and the air force operational test and evaluation command , kirtland air force base , new mexico .

the use of a systematic case study framework enabled us to identify and categorize the types of impacts attributable to dot&e among the systems studied .

in addition , this framework enabled us to identify trends among factors that correlate with dot&e effectiveness .

however , we were unable to generalize to all systems subject to osd - level oversight .

in light of this limitation , we included only major ( high - cost ) systems and systems identified by dot&e and the lead operational test agency as having been affected by dot&e initiatives .

moreover , while our methodology and data collection enabled us to qualitatively assess the impact of dot&e , it was not sufficiently rigorous either to evaluate the cost - effectiveness of dot&e actions or to determine the deterrent effects , if any , the office exerts over the acquisition and testing process .

finally , our methodology did not enable an assessment of whether the additional testing requested by dot&e was necessary to provide full - rate production decisionmakers the essential information on a system's operational effectiveness and suitability or whether the additional data was worth the time , expense , and resources necessary to obtain it .

our review was performed from june 1996 through march 1997 in accordance with generally accepted government auditing standards .

the ah - 64d longbow apache is a remanufactured and upgraded version of the ah - 64a apache helicopter .

this army system is equipped with a mast - mounted fire control radar , fire - and - forget radio frequency hellfire missile , and airframe improvements ( i.e. , integrated cockpit , improved engines , and global positioning system navigation ) .

the airborne self - protection jammer is a defensive electronic countermeasures system using reprogrammable deceptive jamming techniques to protect tactical aircraft from radar - guided weapons .

this navy system is intended to protect navy and marine corps f - 18 and f - 14 aircraft .

the c - 17a airlifter provides strategic / tactical transport of all cargo , including outsized cargo , mostly to main operational bases or to small , austere airfields , if needed .

its four - engine turbofan design enables the transport of large payloads over intercontinental ranges without refueling .

this air force aircraft will replace the retiring c - 141 aircraft and augment the c - 130 and c - 5 transport fleets .

the air force's e - 3 awacs consists of a boeing 707 airframe modified to carry a radome housing a pulse - doppler radar capable of detecting aircraft and cruise missiles , particularly at low altitudes .

the radar system improvement program replaces several components of the radar to improve detection capability and electronic countermeasures as well as reliability , availability , and maintainability .

the f - 22 is an air superiority aircraft with a capability to deliver air - to - ground weapons .

the most significant features include supercruise , the ability to fly efficiently at supersonic speeds without using fuel - consuming afterburners , low observability to adversary systems with the goal to locate and shoot down the f - 22 , and integrated avionics to significantly improve the pilot's battlefield awareness .

the javelin is a man - portable , antiarmor weapon developed for the army and the marine corp to replace the aging dragon system .

it is designed as a fire - and - forget system comprised of a missile and reusable command launch unit .

the joint surveillance target attack radar system is designed to provide intelligence on moving and stationary targets to air force and army command nodes in near real time .

the system comprises a modified boeing 707 aircraft frame equipped with radar , communications equipment , and the air component of the data link , computer workstations , and self - defense suite as well as ground station modules mounted on army vehicles .

the lpd - 17 will be an amphibious assault ship capable of launching ( 1 ) amphibious assault craft from a well deck and ( 2 ) helicopters or vertical takeoff and landing aircraft from an aft flight deck .

it is intended to transport and deploy combat and support elements of marine expeditionary brigades as a key component of amphibious task forces .

the m1a2 abrams main battle tank is an upgrade of the m1a1 and is intended to improve target acquisition and engagement rates and survivability while sustaining equivalent operational suitability .

specifically , the modified tank incorporates a commander's independent thermal viewer , a position navigation system , and an intervehicle command and control system .

the sensor fuzed weapon is an antiarmor cluster munition to be employed by fighter , attack , or bomber aircraft to achieve multiple kills per pass against armored and support combat formations .

each munition contains a tactical munitions dispenser comprising 10 submunitions containing a total of 40 infrared sensing projectiles .

high - altitude accuracy is to be improved through the incorporation of a wind - compensated munition dispenser upgrade .

the standard missile - 2 is a solid propellant - fueled , tail - controlled , surface - to - air missile fired by surface ships .

it was originally designed to counter high - speed , high - altitude antiship missiles in an advanced electronic countermeasures environment .

the block iiia version provides improved capacity against low - altitude targets with an improved warhead .

the block iiib adds an infrared seeker to the block iiia to enhance the missile's capabilities against specific threats .

these improvements are being made to provide capability against theater ballistic missiles while retaining its capabilities against antiair warfare threats .

the tomahawk weapon system is a long - range subsonic cruise missile for land and sea targets .

the baseline iv upgrade is fitted with a terminal seeker , video data link , and two - way digital data link .

the primary baseline iv configuration is the tomahawk multimission missile ; a second variant is the tomahawk hard target penetrator .

the v - 22 is a tilt rotor vertical / short takeoff and landing , multimission aircraft developed to fulfill operational combat requirements in the marine corps and special operations forces .

dot&e's role in the system acquisition process does not become prominent until the latter stages .

as weapon system programs progress through successive phases of the acquisition process , they are subject to major decision points called milestones .

the milestone review process is predicated on the principle that systems advance to higher acquisition phases by demonstrating that they meet prescribed technical and performance thresholds .

figure iii.1 illustrates dod's weapon system acquisition process .

figure iii.1: dod's weapon system acquisition process ( emd ) per dod directive , test and evaluation planning begins in phase 0 , concept exploration .

operational testers are to be involved early to ensure that the test program for the most promising alternative can support the acquisition strategy and to ensure the harmonization of objectives , thresholds , and measures of effectiveness in the operational readiness document and the test and evaluation master plan .

early testing of prototypes in phase i , program definition and risk reduction , and early operational assessments are to be emphasized to assist in identifying risks .

a combined developmental and operational test approach is encouraged to save time and costs .

initial operational test and evaluation is to occur during phase ii to evaluate operational effectiveness and suitability before the full - rate production decision , milestone iii , on all acquisition category i and ii programs .

for all acquisition category i programs and other programs designated for osd test and evaluation oversight , a test and evaluation master plan is prepared and submitted for approval prior to first milestone review ( excluding milestone 0 ) .

the master plan is to be updated at milestones when the program has changed significantly .

dot&e must approve the test and evaluation master plan and the more specific operational test plans prior to their execution .

this process and the required plan approvals provide dot&e opportunities to affect the design and execution of operational testing throughout the acquisition process .

the following are gao's comments on the september 19 , 1997 , letter from the department of defense .

1 .

in prior reviews of individual weapon systems , we have found that operational testing and evaluation is generally viewed by the acquisition community as a costly and time - consuming requirement imposed by outsiders rather than a management tool for more successful programs .

efforts to enhance the efficiency of acquisition , in general — and in operational testing , in particular — need to be well balanced with the requirement to realistically and thoroughly test operational suitability and effectiveness prior to the full - rate production decision .

we attempted to take a broader view of acquisition reform efficiency initiatives to anticipate how these departures from past ways of doing business could impact both the quality of operational testing and the independence of dot&e .

2 .

we were asked to assess the impact of dot&e on the quality and impact of testing and reported on the secretary of defense initiatives only to the extent they may pose a potential impact on dot&e's independence or effectiveness .

moreover , we did not recommend or suggest that testers wait until milestone iii to discover problems that could have been learned and corrected earlier .

since its inception , dot&e has been active in test integration and planning working groups and test and evaluation master plan development during the earliest phases of the acquisition process .

in fact , we have long advocated more early testing to demonstrate positive system performance prior to the low - rate initial production decision .

dot&e's early involvement in test planning is appropriate , necessary , and required by dod regulations .

in this report we do not advocate the elimination of dot&e participation during the early stages of the acquisition process ; rather , we merely observe that dot&e participation through the vehicle of working - level program manager integrated product teams has the potential to complicate independence and may be increasingly difficult to implement with declining resources and increasing oversight responsibilities following milestone iii .

3 .

we did not recommend or suggest that dot&e ignore its statutory responsibility to review and make recommendations to the secretary of defense on budgetary and financial matters related to operational test facilities and equipment .

we only observed that in an era of declining resources , earlier participation , and extended oversight responsibilities , a decision to assume a larger role in test resource management planning and leadership is likely to result in tradeoffs in other responsibilities — the largest being oversight .

4 .

we made this recommendation because dot&e , the services , and the program offices did not necessarily agree on the degree to which system performance requirements have been met in initial operational test and evaluation .

furthermore , there was no consensus within the acquisition community concerning dot&e's authority to oversee follow - on operational test and evaluation conducted to ensure that proposed corrections to previously identified deficiencies were thoroughly tested and evaluated .

5 .

under 10 u.s.c .

2399 , dot&e is required to independently report to congress whether a major acquisition system has proven to be operationally suitable and effective prior to the full - rate production decision .

when follow - on operational test and evaluation is necessary to test measures intended to correct deficiencies identified in initial operational test and evaluation , congress does not receive an equivalent independent report from dot&e that concludes , based on required follow - on operational test and evaluation , whether or not a major system has improved sufficiently to be considered both operationally suitable and effective .

tactical intelligence: joint stars full - rate production decision was premature and risky ( gao / nsiad - 97-68 , apr .

25 , 1997 ) .

weapons acquisition: better use of limited dod acquisition funding would reduce costs ( gao / nsiad - 97-23 , feb. 13 , 1997 ) .

airborne self - protection jammer ( gao / nsiad - 97-46r , jan. 29 , 1997 ) .

army acquisition: javelin is not ready for multiyear procurement ( gao / nsiad - 96-199 , sept. 26 , 1996 ) .

tactical intelligence: accelerated joint stars ground station acquisition strategy is risky ( gao / nsiad - 96-71 , may 23 , 1996 ) .

electronic warfare ( gao / nsiad - 96-109r , mar .

1 , 1996 ) .

longbow apache helicopter: system procurement issues need to be resolved ( gao / nsiad - 95-159 , aug. 24 , 1995 ) .

electronic warfare: most air force alq - 135 jammers procured without operational testing ( gao / nsiad - 95-47 , nov. 22 , 1994 ) .

weapons acquisition: low - rate initial production used to buy weapon systems prematurely ( gao / nsiad - 95-18 , nov. 21 , 1994 ) .

acquisition reform: role of test and evaluation in system acquisition should not be weakened ( gao / t - nsiad - 94-124 , mar .

22 , 1994 ) .

test and evaluation: the director , operational test and evaluation's role in test resources ( gao / nsiad - 90-128 , aug. 27 , 1990 ) .

adequacy of department of defense operational test and evaluation ( gao / t - nsiad - 89-39 , june 16 , 1989 ) .

weapons testing: quality of dod operational testing and reporting ( gao / pemd - 88-32br , july 26 , 1988 ) .

the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

visa and mastercard credit cards are accepted , also .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 37050 washington , dc 20013 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 202 ) 512-6061 , or tdd ( 202 ) 512-2537 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 202 ) 512-6000 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

