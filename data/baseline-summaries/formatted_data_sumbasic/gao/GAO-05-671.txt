in the past , we have reported on declines in the internal revenue service's ( irs ) enforcement programs , including declining exam and collection efforts .

one factor we have cited as contributing to these declines is decreased enforcement productivity as measured by cases closed per staff time .

increasing enforcement productivity through a variety of enforcement improvement projects is one strategy being pursued by irs that could help reverse the declines .

however , evaluating the benefits of these different projects requires good measures of productivity .

irs's ability to correctly measure its productivity has important budget implications .

productivity declines may indicate that irs is not using its resources as efficiently as possible .

increasing the productivity of existing resources might lessen , to some extent , the need for budget increases .

productivity is measured as a ratio of outputs to inputs .

in a january 2004 report on irs's enforcement improvement projects , we recommended that irs invest in enforcement productivity data that better adjust for complexity and quality , taking into consideration the costs and benefits of doing so .

more complete productivity data — data that adjust for complexity and quality — would give managers a clearer picture of how effectively resources are being used .

in addition , congress would have better information about irs's performance and budget needs .

to better understand productivity measurement at irs , you asked us to illustrate methods available to better measure it .

specifically , our objectives were to ( 1 ) describe challenges that irs faces when measuring productivity , ( 2 ) describe alternative methods that irs can use to improve its productivity measures , and ( 3 ) assess the feasibility of using these alternative methods by illustrating their use with existing irs data .

in the context of the productivity literature , output is a general concept representing what is produced .

however , in the performance measurement literature , the term “output,” as defined in the government performance and results act of 1993 ( gpra ) is limited to an activity or effort , while an outcome is the result of a program activity .

activities are typically easily measured , such as transactions completed .

results such as the difference an activity makes in the economy or people's lives are usually less tangible .

in this report , we use the general concept of “output” to define productivity but then distinguish between outputs that are results and those that are activities .

to describe the challenges irs faces when measuring productivity and alternative methods irs can use to improve its productivity measures , we reviewed the literature on the methods used to measure productivity in the public and private sectors .

we also consulted irs officials and reviewed irs documentation on irs's methods for measuring productivity .

to assess the feasibility of using these alternative methods by illustrating their use with existing irs data , we used currently available irs data to calculate alternative exam , or audit , productivity measures .

these methods included calculating unweighted productivity indexes and weighted productivity indexes .

we compared these indexes to show how implementing different methods can provide irs with better measures of productivity and better ways to identify the causes of productivity change .

for this report existing irs examination data were used to illustrate the feasibility of using alternative methods of productivity .

the data are from irs's tax compliance report and automated inventory management system .

in prior reports we recognized that irs's existing examination data have limitations .

for example , direct measures of complexity were not available .

we use type of exam as a proxy for complexity .

we have also recommended that irs improve its input data by implementing a cost accounting system .

while there are reliability issues related to the data , we are using the available irs data for illustrative purposes , and we will not be representing these illustrations as complete measures of irs productivity .

therefore , we determined that the information contained in irs's tax compliance report and automated inventory management system databases were sufficiently reliable for illustrative purposes .

we initiated our review in september 2003 but conducted most of our review from august 2004 through april 2005 in accordance with generally accepted government auditing standards .

productivity is defined as the efficiency with which inputs are used to produce outputs .

it is measured as the ratio of outputs to inputs .

productivity and cost are inversely related — as productivity increases , average costs decrease .

consequently , information about productivity can inform budget debates as a factor that explains the level or changes in the cost of carrying out different types of activities .

improvements in productivity either allow more of an activity to be carried out at the same cost or the same level of activity to be carried out at a lower cost .

irs currently relies on output - to - input ratios such as cases closed per fte to measure productivity and productivity indexes .

a productivity change is measured as an index which compares productivity in a given year to productivity in a base year .

measuring productivity trends requires choosing both output and input measures , and the methods for calculating productivity indexes .

in the past we have reported on declining enforcement trends , finding in 2002 that there were large and pervasive declines in six of eight major compliance and collection programs we reviewed .

in addition to reporting these declines , we reported on the large and growing gap between collection workload and collection work completed and the resultant increase in the number of cases where irs has deferred collection action on delinquent accounts .

in 2003 , we reported on the declining percentage of individual income tax returns that irs was able to examine or audit each year , with this rate falling from 0.92 percent to 0.57 percent between 1993 and 2002 .

since 2000 , the audit rate has increased slightly but not returned to previous levels .

irs conducts two types of audits: field exams that involve complex tax issues and usually face - to - face contact with the taxpayer , and , correspondence exams that cover simpler issues and are done through the mail .

we also reported on enforcement productivity measured by cases closed per fte employee , finding that irs's telephone and field collection productivity declined by about 25 percent from 1996 through 2001 and productivity in irs's three exam programs — individual , corporate , and other audit — declined by 31 to 48 percent .

in january 2004 we reported on the extent to which irs's small business and self - employed ( sb / se ) division followed steps consistent with both gao guidance and the experience of private sector and government organizations when planning its enforcement process improvement projects .

we reported on how the use of a framework would increase the likelihood that projects target the right processes for improvement and lead to the most fruitful improvements .

in that report , we also reported that more complete productivity data — input and output measures adjusted for the complexity and quality of cases worked — would give sb / se managers a more informed basis for decisions on how to identify processes that need improvement , improve processes , and assess the success of process improvement efforts .

this report elaborates on that recommendation , providing more information about the challenges of obtaining complete productivity data .

improving productivity by changing processes is a strategy sb / se is using to address these declining trends .

however , the data available to sb / se managers to assess the productivity of their enforcement activities , identify processes that need improvement , and assess the success of their process improvement efforts are only partially adjusted for complexity and quality of cases worked .

this problem of adjusting for quality and complexity is not unique to sb / se process improvement projects — the data available to process improvement project managers are the same data used throughout sb / se to measure productivity and otherwise manage enforcement operations .

because irs provides services , such as providing information to taxpayers and enforcing the tax laws , that are intangible and complex , measuring output — and therefore productivity — is challenging .

like other providers of intangible and complex services , irs has a choice of measuring activities or the results of its services .

generally , information about results is preferred , but measuring results is often difficult .

in the absence of direct measures of results , activities that are closely related to the results of the service can be used as proxies .

measuring productivity in services is difficult .

unlike manufacturing , which lends itself to objective measurement because output can be measured in terms of units produced , services , which involve changes in the condition of people receiving the service , often have intangible characteristics .

thus , the output of an assembly line is easier to measure than the output of a teacher , doctor , or lawyer .

services may also be complex bundles of individual services , making it difficult to specify the different elements of the service .

for example , financial services provide a range of individual services , such as financial advice , accounts management and processing , and facilitating financial transactions .

irs provides a service .

irs's mission , to help taxpayers understand and meet their tax responsibilities and to apply the tax law with integrity and fairness , requires irs to provide a variety of services ranging from collecting taxes to taxpayer education .

irs , like other service providers , could measure output in terms of its results — its impact on taxpayers — or in terms of activities .

the results of irs's service are the impacts on the condition or behavior of taxpayers .

these taxpayer conditions or behaviors include their compliance with the tax laws , their compliance burden ( the time and money cost of complying with tax laws ) , and their perception of how fairly taxpayers are treated .

irs's activities are what irs does to achieve those results .

these activities include phone calls answered , notices sent to taxpayers , and exams conducted .

generally , information about results is preferred , but measuring such results is often difficult .

in the case of the public sector , this preference is reflected in gpra , which requires that federal agencies measure performance , whenever possible , in terms of results or outcomes for people receiving the agencies' services .

however , results such as compliance and fairness have intangible characteristics that are difficult to measure .

in addition , results are produced in complicated and interrelated ways .

for example , a transaction or activity may affect a number of results: irs's exams may affect taxpayers' compliance , compliance burden , and perceptions of the fairness of the tax system .

in addition , a result may be influenced by a number of transactions or activities: a taxpayer's compliance may be influenced by all irs exams ( through their effect on the probability of an exam ) as well as by other irs activities such as taxpayer assistance services .

irs's activities are easier to measure than results but still present challenges .

activities are easier to measure because they are often service transactions such as exams , levies issued , or calls answered that can be easily counted .

however , unlike measures of results , more informative measurement of activities requires that they be adjusted for quality and complexity , as we noted in our report on irs's enforcement and improvement projects .

a productivity measure based on activities such as cases closed per fte may be misleading if such adjustments are not made .

for example , an increase in exam cases closed per fte would not indicate an increase in true productivity if the increase occurred because ftes were shifted to less complex cases or the examiner allowed the quality of the case review to decline to close cases more quickly .

activities - based productivity measures can provide irs with useful information on the efficiency of irs operations .

measuring output , and therefore productivity , in terms of activities provides irs with measures of how efficiently it is using resources to perform specific functions or transactions .

however , activities do not constitute — and should not be mistaken for — measures of irs's productivity in terms of ultimate results .

while the productivity measures we have examined are based on activities , irs has efforts under way to measure results such as compliance and compliance burden .

recently , we reported on irs's national research program ( nrp ) to measure voluntary compliance and efforts to measure compliance burden .

as we mentioned previously , measuring these results is difficult .

for some results , such as compliance , measurement is also costly and intrusive because taxpayers must be contacted and questioned in detail .

despite these difficulties , irs can improve its productivity measurement by continuing its efforts to get measures of results .

these efforts would give congress and the general public a better idea of what is being achieved by the resources invested in irs .

in the absence of direct measures of results , activities that are closely related to the results of the service are used as proxies .

the value of these proxies depends on the extent to which they are correlated with results .

by carefully choosing these measures , irs may gain some information about the effect of its activities on ultimate results .

because activities may affect a number of results and a single result may be affected by a number of activities , a single activity likely will not be a sufficient proxy for the results of the service .

therefore , a variety of activities would likely be necessary as proxies for the results of the service .

both types of output measures , those that reflect the results of irs's service and those that use activities to measure internal efficiency , should be accurate and consistent over time .

in addition , both output measures should be reliably linked to inputs .

linking the results of irs's service to inputs may be difficult because of outside factors that may also affect measured results .

for example , an increase in compliance could result both from irs actions such as exams and from changes in tax laws .

another challenge is that irs currently has difficulties linking inputs to activities , as we note in a previous report , where we reported irs's lack of a cost accounting system .

in particular , irs only recently implemented a cost accounting system , and has not yet determined the full range of its cost information needs .

table 1 summarizes some of the key differences between activities and results measures .

table 1 also indicates some general criteria that apply to both types of measures .

because inputs are more easily measured and identifiable than outputs , measuring them is more straightforward .

irs , as a government agency , may be able more often to use labor costs or hours as a single input in its productivity measures because it relies heavily on labor .

however , it may be particularly important for irs to use a multifactor measure that includes capital along with labor during periods of modernization that involve increased or high levels of capital investment .

as with outputs , inputs should be measured accurately and consistently over time .

measuring inputs consistently over time may require adjusting for changes in the quality of the labor , which has been done using proxies such as education level or years of experience .

also , as mentioned previously , inputs should be reliably linked to outputs .

the appropriate method for calculating productivity depends on the purpose for which the productivity measure is being calculated .

the alternative methods that can be used for calculating productivity range from computing single ratios — exams closed per fte — to using complex statistical methods to form composite indexes that combine multiple indicators of outputs and inputs .

while single ratios may be adequate for certain purposes , the composite indexes based on statistical methods may be more useful because they provide information about the sources of productivity change .

comparing the ratios of outputs to inputs at different points in time defines a productivity index that measures the percentage increase or decrease in productivity over time .

the ratios that form the index may be single , comparing a single output to a single input or composite , where multiple outputs and inputs are compared .

the single ratios may be useful for evaluating the efficiency of a single noncomplex activity .

composite indexes can measure the productivity of more complicated activities , controlling for complexity and quality .

composite indexes can also be used to measure productivity of resources across an entire organization , where many different activities are being performed .

one method of producing composite indexes is to use weights to combine such disparate activities as telephone calls answered and exams closed .

one common weighting method , used by the bureau of labor statistics ( bls ) , is a labor weight .

weighting outputs by their share of labor in a baseline period controls for how resources are allocated between different types of outputs .

if the productivity of two activities is unchanged but resources are reallocated between the activities , the composite measure of productivity would change unless these weights are employed .

for example , if irs reallocates exam resources so that it does more simple exams and fewer complex exams , the number of total exams might increase .

consequently , a single productivity ratio comparing total exams to inputs would show an increase .

labor weighting deals with this issue .

the weights allow any gains from resource reallocation to be distinguished from gains in the productivity of the underlying activities .

when types of activities can be distinguished by their quality of complexity , labor weighting can also be used to control for quality and complexity differences when resources are shifted between types of outputs .

more complicated statistical methods can be used for calculating composite indexes that allow for greater flexibility in how weights are chosen to combine different outputs and for a wider range of output measures that include both qualitative and quantitative outputs .

data envelopment analysis ( dea ) , which has been widely used to measure the productivity of private industries and public sector services , is an example of such methods dea estimates an efficiency score for each producing unit , such as the firms in an industry or the schools in a school district , or for irs , the separately managed areas and territories composing its business units .

dea estimates the relative efficiency of each producing unit by identifying those units with the best practice — those making the most efficient use of inputs , under current technology , to produce outputs — and measuring how far other units are from this best practice combination of inputs used to produce outputs .

dea estimates provide managers with information on how efficient they are relative to other units and the costs of making individual units more efficient .

these efficiency scores are used to form a composite productivity index called a malmquist index .

an advantage of the malmquist index is that irs managers can restrict the weights to adjust for managerial or congressional preferences to investigate the effect on productivity of a shift , for example , from an organization that emphasizes enforcement to one that emphasizes service .

irs can also include many different types of outputs and inputs , control for complexity and quality , and isolate the effects of certain historical changes , such as the irs restructuring and reform act of 1998 ( rra98 ) .

another advantage of the malmquist index is that productivity changes can be separated into their components , such as efficiency and technology changes .

in this context , efficiency can be measured holding technology constant , and technology can be measured holding efficiency constant .

holding technology constant , irs might improve productivity by improving the management of its existing resources .

on the other hand , technology changes might improve productivity even if the management of resources has not changed .

thus , the productivity change of a given irs unit is determined by both changes in its efficiency relative to the current best - practice irs units and changes in the best practices or technology .

currently available irs data can be used to produce productivity indexes that control for complexity and quality .

the examples that follow focus on productivity indexes that use exams closed as outputs and ftes as inputs .

the data on examinations cover individual returns across irs and irs's lmsb division .

for both individuals and lmsb , the complexity and quality of exams can vary over time .

for example , the proportion of exams that are correspondence versus field , business versus nonbusiness , and eic versus non - eic can vary over time .

as already discussed , failing to take account of such variation can give a misleading picture of productivity change .

while these examples do not encompass all the methods , data , and adjustments that may be used , they illustrate the benefits of the additional analysis that irs can perform using current data .

in addition , as we pointed out in our 2004 report , irs can improve its productivity measurement by investing in better data , taking into account the costs and benefits of doing so .

these better data include measures of complexity , improved measures of quality , and additional measures of output .

figures 1 through 4 illustrate , using currently available data between fiscal years 1997 and 2004 , the difference between weighted indexes that make an adjustment for complexity and unweighted indexes that make no adjustments .

in the illustrations , a labor - weighted composite index , which can control for complexity , is contrasted with a single unweighted index to show how the simpler method may be misleading .

 ( see app .

i for a fuller description of the labor - weighted index. ) .

in each case , complexity is proxied by type of exam .

although the data were limited ( for example , the measure of complexity was crude ) , the illustrations show that making the adjustments that are possible provides a different picture of productivity than would otherwise be available .

the advantage of weighted indexes is that they allow changes in the mix of exams to be separated from changes in the productivity of performing those exams .

in the examples that follow , an unweighted measure could be picking up two effects .

one effect is the change in the number of exams that an auditor can complete if the complexity or quality of the exam changes .

the second effect is the change in the number of exams an auditor can complete if the time an auditor requires to complete an exam changes , holding the quality and complexity of exams constant .

by isolating the latter effect , the weighted index more closely measures productivity , or the efficiency with which the auditor is working the exams .

for individual exams , the comparison of productivity indexes shows that the unweighted index understates the decline in productivity .

as figure 1 shows , between fiscal years 1997 and 2001 , the unweighted productivity index declined by 32 percent while the weighted index declined by 53 percent .

the difference is due largely to the increase in eic exams during the period .

over the period between fiscal years 1997 and 2001 , exams were declining .

however , the mix of exams was changing , with increases in the number of eic exams .

eic exams are disproportionately correspondence exams , and irs can do these exams faster than field exams .

irs shifted to “easier” exams , and that shift caused the unweighted index to give an incomplete picture of productivity .

the shift masked the larger productivity decline shown by the weighted index .

figure 2 provides additional evidence to support the conclusion that the shift to more eic exams is the reason for the difference in productivity shown in figure 1 .

between fiscal years 1997 and 2001 , the weighted and unweighted indexes track each other very closely when the eic exams are removed .

both show a decline in productivity of about 50 percent over this period .

the available data were not sufficient to control for other factors that may have influenced exam productivity .

for example , rra98 imposed additional requirements on irs's auditors , such as certifications that they had verified that past taxes were due .

figure 3 compares unweighted and weighted productivity indexes for exams done in lmsb division .

as figure 3 shows , between fiscal years 2002 and 2004 , the unweighted productivity index increased by 4 percent , while the weighted index increased by 16 percent .

this difference appears largely due to the individual exams and small corporate exams done in lmsb .

over the period , total exams were declining but the mix of exams was changing .

lmsb was shifting away from less labor - intensive individual returns and small corporation returns to more complex business industry and coordinated industry return exams .

this shift caused the unweighted index to give an incomplete picture of productivity .

here , the shift masked the larger productivity increase as shown by the weighted index .

figure 4 provides additional evidence to support the conclusion that the shift away from individual and small corporate exams is the reason for the difference in productivity shown in figure 3 .

between fiscal years 2002 and 2004 , when individual and corporate exams are excluded , the two indexes track more closely , with the unweighted index increasing by 15 percent and the weighted index by 17 percent .

there is evidence that adjusting for quality would show that lmsb's productivity increased more than is apparent in figures 3 and 4 for the years 2002 to 2004 .

average quality scores available for selected types of lmsb exams show quality increasing over the 2-year period .

adjusting for this increase in quality , in addition to adjusting for complexity , would show a productivity increase for these types of exams of 28 percent over the period .

while labor - weighted and other more sophisticated productivity indexes can provide a more complete picture of productivity changes , they do not identify the causes of the changes .

these productivity indexes would be the starting point for any analysis to determine the causes of productivity changes .

another example of the advantages of weighted productivity indexes is provided by irs .

as noted earlier , irs has developed a weighted submission processing productivity measure .

the measure adjusts for differences in the complexity of processing various types of tax returns .

in an internal analysis , irs showed how productivity comparisons over time and across the 10 processing centers depended on whether or not the measure was adjusted for complexity .

for example , the ranking of the processing centers in terms of productivity changed when the measure was adjusted for the complexity of the returns being processed .

the more sophisticated methods for measuring productivity can provide irs and congress with better information about irs's performance .

by controlling for complexity and quality , irs managers would have more complete information about the true productivity of activities , such as exams , that can differ in these dimensions .

in addition , the weighted measures can be used to measure productivity for the organization , where many different activities are being performed .

more complete information about the productivity of irs resources should be useful to both irs managers and congress .

more complete productivity measures would provide better information about the effectiveness of irs resources , irs's budget needs , and irs's efforts to improve efficiency .

although there are examples , such as the submission processing productivity measures , of irs using weighted measures of productivity , irs officials said they generally use single ratios as measures of productivity .

that is consistent with our 2004 report on irs's enforcement improvement projects , where we reported on sb / se's lack of productivity measures that adjust for complexity and quality .

while there would be start - up costs associated with any new methodology , the long - term costs to irs for developing more sophisticated measures of productivity may be modest .

the examples so far in this section demonstrate the feasibility of developing weighted productivity indexes using existing data .

relying on existing data avoids the cost of having to collect new data .

the fact that irs already has some experience implementing weighted productivity measures could reduce the cost of introducing more such measures .

as we stated previously , irs could also improve its productivity measurement by getting better data on quality and complexity .

these improved data could be integrated with the methods for calculating productivity illustrated in this report to further improve irs's productivity measurement .

however , as we acknowledged in our prior report , collecting additional data on quality and complexity may require long - term planning and an investment of additional resources .

any such investment , we noted , must take account of the costs and benefits of acquiring the data .

using more sophisticated methods , such as those summarized in this report , for tracking productivity could produce a much richer picture of how irs manages its resources .

this is important not only because of the size of irs — it will spend about $11 billion in 2005 and employ about 100,000 ftes — but also because we are entering an era of tight budgets .

a more sophisticated understanding of the level of productivity at irs and the reasons for productivity change would better position irs managers to make decisions about how to effectively manage their resources .

such information would also better enable congress and the public to assess the performance of irs .

as we illustrate , more can be done to measure irs's productivity using current data .

however , another advantage of using more sophisticated methods to track productivity is that the methods will highlight the value of better data .

better information about the quality and complexity of irs's activities would enable the methods illustrated in this report to provide even richer information about irs's overall productivity .

we recommend that the commissioner of internal revenue put in place a plan for introducing wider use of alternative methods of measuring productivity , such as those illustrated in this report , taking account of the costs of implementing the new methods .

the commissioner of internal revenue provided written comments on a draft for this report in a june 23 , 2005 , letter .

the commissioner agreed with our recommendation to work on introducing wider use of alternative measure of productivity .

although expressing some caution , he has asked his deputy commissioner for services and enforcement to work with irs's research , analysis , and statistics office to assess the possible use of alternative methods of measuring productivity .

the commissioner recognized that a richer understanding of organizational performance is crucial for effective program delivery .

as agreed with your office , unless you publicly release its contents earlier we plan no further distribution of this report until 30 days from the date of this letter .

at that time , we will send copies to interested congressional committees , the secetary of the treasury , the commissioner of internal revenue , and other interested parties .

we will also make copies available to others on request .

if you or your staff have any questions , please contact me at ( 202 ) 512-9110 .

i can also be reached by e - mail at whitej@gao.gov .

key contributors to this assignment were kevin daly , assistant director , and jennifer gravelle .

methods for calculating productivity range from computing single ratios to using statistical methods .

in its simplest form , a productivity index is the change in the productivity ratio over time relative to a chosen year .

however , this type of productivity index allows for only a single output and a single input .

to account for more than one output , the outputs must be combined to produce a productivity index .

one method is to weight the outputs by their share of inputs used in the chosen base year .

in a case where only labor input is used , following this method provides a labor - weighted output index , which , when divided by the input index , produces the labor - weighted productivity index .

the use of the share of labor used in each output effectively controls for the allocation of labor across the outputs over time .

for example , if productivity in producing two outputs remained fixed over time , a single productivity index may show changes in productivity if resources are reallocated to produce more of one of the outputs .

the bureau of labor statistics ( bls ) has also used labor - weighted indexes .

bls published , under the federal productivity measurement program , data on labor productivity in the federal government for more than two decades ( 1967-94 ) .

due to budgetary constraints , the program is now terminated .

bls's measures used the “final outputs” of a federal program , which correspond generally to what we have called intermediate outputs in this report , as opposed to the outcomes or results of the program .

bls used labor weights because of their availability and their close link to cost weights .

in particular , as with the labor weights in our illustrations , bls used base year labor weights and updated the weights every 5 years .

it relied only on labor and labor compensation , and acknowledges that the indexes did not reflect changes in the quality of labor .

bls measured productivity for a number of federal programs , ranging from social and information services to corrections .

however , bls did not produce productivity measures for irs .

in addition to weighted productivity indexes , there are a number of composite productivity indexes designed to include all the inputs and outputs involved in production .

this group of indexes is called total factor productivity ( tfp ) indexes .

they are called total because they include all the inputs and outputs , as opposed to partial factor productivity indexes , which relate only one input to one output .

many of the main tfp indexes , including tornqvist , fisher , divisia , and paache , require reliable estimates of input and output prices , data not available for industries in the public sector .

therefore we use the malmquist index , which does not require that data .

malmquist indexes are tfp indexes based on changes in the distance from the production frontier , or distance functions .

these distance functions are estimated using data envelopment analysis ( dea ) .

productivity change is represented by the ratio of two different period distance functions .

the malmquist index is the geometric average of these productivity changes ( evaluated at the two different periods ) .

this index can be further decomposed into efficiency and technology changes .

from the decomposition of the malmquist index , productivity change can be shown to equal the efficiency change times the technology change .

 ( xt+1,yt+1 ) / d ( x,y ) ]*[dt+1 ( xt+1,yt+1 ) / dt+1 ( x,y ) ]}^1 / 2 , where x , xt+1 denote the vector of inputs at time t and t+1 , and y , and yt+1 denote the vector of outputs in time t and t+1 and d and dt+1 are distance functions relative to the technology in time t and t+1 .

 ( x,y ) ]*[dt+1 ( xt+1,yt+1 ) / dt+1 ( x,y ) ]}^1 / 2 = [dt+1 ( xt+1 , yt+1 ) / d ( x,y ) ]*{[ d ( xt+1,yt+1 ) / dt+1 ( xt+1,yt+1 ) ]*[d ( x,y ) / dt+1 ( x,y ) ]}^1 / 2=e*t , the efficiency change , e , times the technology change , t. fraction in the first , indicates a movement away from one over time and thus declining productivity .

thus , a productivity change less than one indicates declining productivity and therefore an efficiency change less than one also indicates declining efficiency .

alternatively , if the efficiency change was one , then the productivity change equals the technology change .

following previous analysis , a productivity change less than one indicates declining productivity .

therefore , a technology change less than one indicates an inward shift of the production frontier .

if the technology change is less than one , it must be that the distance function in the first period is less than the distance function in the next period .

thus , the distance in the first period is farther away from one than is the distance in the next period , and the distance from the frontier decreased from the first period to the second period .

since the output and input bundles did not change , the frontier must shift in to produce the decrease in distance .

the internal revenue service ( irs ) can follow this method to generate indexes for the areas and territories and then focus on the average for an estimate of overall irs productivity .

,y ) = [max { φ | ( x , φy ) ∈t}] - 1 and φ* = ( d ( x,y ) ) - 1 , with φ* > 1 and d ( x,y ) < 1 , where φ denotes the value to scale output .

therefore , inefficient relative to firms with a scalar value of one .

thus , output distance functions are less than one .

irs can use this method , treating territories and areas as firms .

the weights used in the linear program are designed to make each firm look its best ; they represent best case scenarios .

while dea is a nonparametric method , there is also a parametric method available called stochastic frontier analysis .

stochastic frontier analysis ( regression ) uses a regression model to estimate cost or production efficiency .

after running the regression of performance and input data , the frontier is found by decomposing the residuals into a stochastic ( statistical noise ) part and a systematic portion attributed to some form of inefficiency .

stochastic frontier analysis thus requires specifying the distributional form of the errors and the functional form of the cost ( or production ) function .

its merits include a specific treatment of noise .

while dea's use of nonparametric methods eliminates the need to specify functional forms , one drawback is a susceptibility to outliers .

