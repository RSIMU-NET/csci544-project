department of defense ( dod ) weapon system acquisition represents one of the largest areas of the federal government's discretionary spending .

in fiscal year 2014 , dod requested $168 billion to develop , test , and acquire weapon systems and other products and equipment .

about 40 percent of that total was for major defense acquisition programs ( mdap ) or acquisition category ( acat ) i programs .

the remaining approximately 60 percent of the budget request included , among other investments , funding for dod's non - major acat ii and iii programs .

these programs , which include everything from a multibillion dollar aircraft radar modernization program to soldier clothing and protective equipment programs in the tens of millions of dollars , are generally less costly than mdaps at the individual program level .

due to the lower level of investment involved on a program - by - program basis , acat ii and iii programs typically have fewer reporting and documentation requirements and are overseen at lower organizational levels than mdap and major automated information system ( mais ) programs .

accordingly , congress's and dod's insight into the performance of these programs is more limited .

you asked us to examine the number , acquisition cost , and performance of dod's acat ii and iii programs .

this report assesses ( 1 ) the extent to which information is available on the number of current acat ii and iii programs , their total estimated acquisition cost , and their cost and schedule performance ; ( 2 ) the factors affecting the cost and schedule performance of selected acat ii and iii programs ; and ( 3 ) the number of dod's current acat ii and iii programs that are likely to become mdaps .

to determine the extent to which information was available on the number of current acat ii and iii programs and their estimated acquisition costs , we used a data collection instrument to collect data on the number and cost of current acat ii and iii programs from five dod components: army , air force , navy , u.s. special operations command ( socom ) , and the dod chemical and biological defense program ( cbdp ) .

our observations on dod's acat ii and iii program data are based on the original data submitted by the components .

to assess information available on cost and schedule performance , we collected and analyzed acquisition program baseline ( apb ) documents for a non - generalizable random sample of 170 non - automated information system acat ii and iii programs .

to assess the reliability of the data , we reviewed the data for missing values and obvious errors , and compared the cost data for our sample of 170 programs to source documents when available .

we found the data were unreliable as further discussed in the report .

to identify the factors that affected the cost and schedule performance of selected acat ii and iii programs , we analyzed factors cited in program documentation and by dod program officials .

we reviewed a non - generalizable sample of 15 programs including each component's largest acat ii and iii program based on data reported by dod components and one additional program per component based on factors such as cost growth or being part of a family of related systems .

for each program , we analyzed apbs , reviewed program documents , and interviewed program officials to assess and identify factors that affected cost or schedule performance .

to determine the number of current acat ii and iii programs that were likely to become mdaps , we analyzed data provided by dod components through our data collection instrument to identify programs that appeared to be within 10 percent of or to have exceeded the acat i threshold for research , development , test , and evaluation ( rdt&e ) or procurement .

we then collected information about these programs from dod components using a set of structured questions .

we determined the data were sufficiently reliable to serve as a starting point to identify the minimum number of programs likely to become mdaps because we were able to confirm data with relevant program offices .

appendix i provides additional details on our scope and methodology .

we conducted this performance audit from october 2013 to march 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

dod acquisition policy defines an acquisition program as a directed , funded effort that provides a new , improved , or continuing materiel , weapon , or information system , or a service capability in response to an as shown in table 1 , defense acquisition programs are approved need.classified into acquisition categories that depend on the value and type of acquisition .

the army , navy , air force , and socom also have supplemental acquisition policies that address certain aspects of acquisition program categorization and management .

acat ii and iii programs encompass a wide range of efforts and program sizes .

programs may range from an acat ii program with a total acquisition cost of more than $3 billion to an acat iii program with an acquisition cost in the millions of dollars or lower .

dod's acquisition policy does not establish a minimum cost for acat iii programs .

the level of oversight for acquisition programs varies based on the assigned acat level .

dod and component acquisition policies specify the organizational level of the milestone decision authority — the designated individual with overall responsibility for a program — for each acat level .

the organizational level at which program requirements and requirements changes are approved may vary by acat level as well .

the organizational level of the milestone decision authority for air force acat i - iii programs is shown in figure 1 as an example .

all acquisition programs are required by statute or dod guidance to provide program information at milestones and other decision points , although these requirements differ by acat level .

mdap and mais programs , also known as acat i and ia programs , require more documentation and analysis to support program decisions and have to regularly report to congress on their cost , schedule , and technical performance .

these programs are required to enter and maintain program cost , schedule , and performance data and create apbs within dod's defense acquisition management information retrieval ( damir ) system , a web - based data system intended to provide data transparency of acquisition management information across dod .

components may use damir for other programs , but it is not required .

appendix ii provides additional detail on acquisition documentation requirements and congressional reporting requirements by acat level .

dod components could not provide sufficiently reliable data for us to accurately determine the number , total cost , or performance of dod's current acat ii and iii programs .

we found that the accuracy , completeness , and consistency of dod's data was undermined by ( 1 ) widespread data entry issues and missing data , and ( 2 ) inconsistent identification of current acat ii and iii programs across and within components .

dod components have taken some steps to improve acat ii and iii program data , but their efforts do not fully address the causes of the problems we identified .

in addition to data reliability problems , dod lacks consistent cost and schedule metrics across components to assess acat ii and iii program performance .

further , the lack of baseline cost and schedule data and comparable schedule milestones prevents dod from consistently measuring the performance of acat ii and iii programs .

taken together , these issues limit the utility of dod's data on acat ii and iii programs for oversight , decision - making , and reporting purposes .

we identified data reliability issues related to accuracy , completeness , or consistency with data for about 60 percent of acat ii and iii programs reported to us by dod components .

these issues prevented us from accurately determining the number , total cost , or performance of dod's current acat ii and iii programs .

according to dod acquisition policy , complete and current program information is essential to the acquisition process .

internal control standards for federal executive branch agencies also emphasize that agencies should have relevant , reliable , and timely information for decision - making and external reporting purposes .

we found obvious accuracy and completeness issues in program cost data for acat ii and iii programs reported to us by dod components .

we also observed consistency issues in program data across components and within some components that affected the comparability of the data .

inaccurate data were evident across all of the components ; issues we observed included reported dollar values outside the range of acat ii and iii programs and basic math errors .

inaccuracies like these suggest overall data quality problems .

further , when we reviewed a sample of programs and compared reported cost estimates to source documents , we found that cost estimate data was often misreported .

components incorrectly reported data or data was missing for 64 out of 95 programs for which we had complete source documents in our non - generalizable sample .

we also observed missing data elements to varying degrees at all of the components except cbdp .

for example , 333 out of 836 programs reported by the components were missing one or more cost estimate elements or basic information such as the acat level .

lastly , in numerous instances components did not follow the instructions of the data collection instrument , which also affected our ability to use the data .

see table 2 for examples of the reliability issues we identified .

in some instances , the accuracy and completeness issues we identified were consistent with known limitations of information in component systems .

for example , army acquisition officials told us that in the past there has been conflicting guidance about whether completed programs should be deleted from the army's acquisition information system , and some completed programs were never removed from the system .

officials at all components , except cbdp , further told us that accuracy of data in their systems relies primarily on the quality of information submitted by the program executive officer ( peo ) or program offices.army , navy , and air force acquisition officials told us that they work with peos to address data quality problems , such as by conducting ad hoc checks to flag obvious errors or missing data elements and following up with peos as necessary .

socom acquisition officials told us the acquisition executive emphasizes the importance of maintaining up - to - date data to program managers .

based on the data provided to us by dod components , these existing data quality practices are insufficient to ensure the accuracy and completeness of data on dod's acat ii and iii programs as required by dod policy and federal internal control standards .

data provided in response to our request for information on current acat ii and iii programs ( 1 ) included acquisitions that were not “current” acat ii and iii programs in accordance with our definition , ( 2 ) likely reflected inconsistent reporting of acquisitions at lower dollar levels across components , and ( 3 ) likely excluded certain acquisitions considered current acat ii and iii programs in accordance with certain component acquisition policies .

for the purposes of our report , we define a current program as one that has been formally initiated in the acquisition process , but has not yet delivered 90 percent of its items or made 90 percent of planned expenditures .

this definition is consistent with statutory reporting thresholds used by congress in its reporting requirements for current mdap programs .

inconsistent interpretations of what constitutes a current program and the inability of some components to reliably identify these programs contributed to the inclusion of programs that were not current in the data that components reported to us .

dod acquisition policy and component guidance generally do not define which acat ii or iii programs are considered to be current for management and reporting purposes .

for example , dod acquisition policy defines when a program is formally initiated and its operations and support phase begins , but it does not identify when a program should be considered current .

further , some components told us peos may have different interpretations of what constitutes a current program .

components also told us that they could not consistently use the information in their data systems to readily identify current programs .

of the 836 programs initially reported by the five components , we identified 199 programs across the army , navy , air force , and socom that should not have been included because they did not meet our criteria for current programs .

for example , according to an army peo , 90 of 140 programs originally reported to us as current acat ii or iii programs by the army were not current based on our definition because these programs had delivered more than 90 percent of planned items or expended more than 90 percent of planned funds .

without the consistent identification of current acat ii and iii programs , dod and component officials do not know the accurate number of these programs and may miss opportunities to identify programs that may need more or less oversight depending on whether or not most of the anticipated acquisition funding has been spent .

additionally , component guidance for defining acquisition programs varies and likely resulted in inconsistent reporting across components of acquisitions at lower dollar values .

dod acquisition policy establishes a cost ceiling and cost floor for acat ii programs and a cost ceiling , but no cost floor , for acat iii programs .

however , some components have supplemental guidance that establishes additional acquisition categories or exclusions .

for example , socom and navy have guidance that provides for certain acquisitions with less than $10 million in total rdt&e contracts , less than $25 million per year in annual procurement funding , and less than $50 million total in procurement contracts to be categorized as non - acat programs .

specifically , socom designates these low cost , schedule , and technical risk efforts to field special operations - peculiar capabilities as abbreviated acquisition projects .

similarly , the navy designates lower dollar value programs that do not require operational testing and evaluation as abbreviated acquisition programs.air force policies do not provide for lower dollar threshold categories army and beneath the acat iii level .

according to officials from the office of the under secretary of defense for acquisition , technology , and logistics , acquisition programs at these dollar levels should be reported as acat iii programs .

however , navy and socom did not include their non - acat programs in the acat ii and iii program data reported to us .

there was also variation within components as to how acquisition programs were categorized .

army and air force acquisition officials told us that some programs that should have been considered acat ii or iii programs in accordance with component acquisition policy were not .

these officials also told us that peos may have counted and handled programs differently in the absence of a clear definition of what should be considered a program of record .

for example , army officials told us that categorizing information technology programs was sometimes challenging , and they have worked with peos to review the categorization of certain information technology programs .

army and air force officials told us that as a result of confusion among peos about whether or not certain programs should be considered acat ii or iii programs , they have needed to add and remove numerous acat ii and iii programs from component information systems over the past year .

the types of issues we identified may have also contributed to components reporting varying numbers of acat ii and iii programs in response to different requests for information during the same time frame .

specifically , concurrent with reporting 755 current acat ii and iii programs to us , the army , navy , and air force reported 1,360 acat ii and iii programs in a presentation to the dod business senior integration group , which is chaired by the under secretary of defense for acquisition , technology , and logistics and oversees dod's better buying power initiatives .

acquisition officials from these components told us they were unable to fully explain the reasons for the difference between the numbers of programs reported .

the army , navy , air force , and socom have established information systems to track cost and schedule data for acat ii and iii programs and taken some steps to address issues related to the completeness and accuracy of information tracked in these systems .

cbdp officials told us they recognize the value of establishing a system to track data on acat ii and iii programs and are determining the capabilities that would be needed in such a system .

specifics of component efforts follow: according to navy officials , data in the navy's research , development & acquisition information system has potentially been incomplete because acat ii and iii program data has not been consistently entered into the system .

the navy issued an updated policy in august 2014 that requires input of programmatic information into the system for all acat programs .

all air force acat ii and iii programs have been required to enter cost and schedule data into the system metric and reporting tool since 2012 , but air force officials told us that not all programs had complied with the requirement .

they told us in june 2014 that they had an ongoing effort to review acat ii and iii programs in the system , including assessing whether cost and schedule data has been populated .

further , the air force has established an investment master list that will capture all programs receiving rdt&e and procurement funding .

socom requires that all acat ii and iii programs enter program and cost data into its centralized acquisition portal data system , however socom officials told us some program managers have been more diligent than others in populating the system .

these officials told us that data from the portal is now used to conduct monthly program reviews , rather than having the program prepare briefing slides , as a way to encourage program managers to populate and regularly update the system .

components have also taken steps to improve the consistency with which they identify current acat ii and iii programs .

for example , officials at four of the five components told us they are exploring ways to identify the acquisition phase — technology maturation and risk reduction , engineering and manufacturing development , production and deployment , operations and support — for programs in component information systems , which could improve their ability to reliably identify current programs .

the air force and army have also made efforts to address concerns they had previously identified related to the consistent identification of acat ii and iii programs within their components .

the air force issued guidance in january 2014 , detailing which programs are and are not considered to be acquisition programs and acquisition policy officials told us they have been meeting with individual peos to clarify any misunderstandings about how acquisition programs should be categorized .

officials from the office of the assistant secretary of the army for acquisition , logistics , and technology also told us they are in the process of revising the army's acquisition guidance , with input from peos , to more precisely define the types of programs that should and should not be considered to be acquisition programs .

however , the components' efforts do not fully address the accuracy , completeness , and consistency issues we identified with acat ii and iii program data .

for example , the components have not established systematic processes to perform data quality tests on peo - submitted data and assess the results to help identify problems , such as basic math errors or missing data , for further review .

these types of tests and assessments can be an important step in determining whether data can be used for its intended purposes .

additionally , the components have not developed plans that detail how they will implement or sustain data improvement efforts .

for example , the components have not developed implementation steps for assessing data reliability on an ongoing basis or metrics to assess the success of data cleanup efforts .

developing such without establishing this plans is a key project management practice.planning foundation , the components will not be in a sound position to effectively monitor and evaluate the implementation of efforts to improve component data .

finally , efforts to consistently identify current acat ii and iii programs have been focused within individual components .

without a consistent understanding about which programs should be considered to be current acat ii or iii programs across components , similar programs will continue to be reported on differently , thereby limiting the consistency and comparability of acat ii and iii program data across dod .

dod components lack consistent cost and schedule performance metrics to assess performance trends across acat ii and iii programs .

as part of the department's better buying power initiatives , the under secretary of defense for acquisition , technology , and logistics instructed dod components to determine how best to measure performance trends for non - acat i programs .

federal internal control standards also emphasize the importance of comparing actual performance to planned or expected results throughout the organization to help ensure effective results are achieved and actions are taken to address risks .

the army , navy , and air force briefed dod's business senior integration group in november 2013 on their current efforts and plans regarding assessing acat ii and iii cost and schedule performance , but no specific follow - on actions or action plans have been developed .

unlike mdaps and mais programs , acat ii and iii programs are not required to report cost and schedule data in a consistent fashion , despite the potential benefits of such reporting .

mdap and mais programs are required to report key cost and schedule metrics to congress in a standardized format through selected acquisition reports and mais annual reports , respectively .

cost and schedule data for these reports are pulled from dod's web - based damir system .

mdap cost and schedule data are used by dod for its annual assessment of the performance of the defense acquisition system , which the department uses to improve acquisition program performance and inform policy and programmatic decisions .

acat ii and iii programs are not required to produce similar cost and schedule reporting as larger programs and do not have to provide program data in damir .

according to officials from the office of the under secretary of defense for acquisition , technology , and logistics , although minor adjustments may be needed for reporting purposes , there is nothing that prevents components from using damir to capture data on acat ii and iii programs , and acquisition officials from the army have considered using it .

additionally , cbdp officials told us they would consider damir when exploring potential systems to track acat ii and iii program data .

dod component officials told us they are not yet sure how best to measure cost and schedule performance across acat ii and iii programs .

for example , army officials told us that analysis of component - wide acat ii and iii performance trends may not make sense given the differences across programs .

navy , socom , air force , and cbdp officials told us they are interested in tracking cost and schedule performance trends across acat ii or iii programs , but are still working to define performance metrics and address limitations in existing data or reporting capabilities .

for example , the air force has attempted to assess cost and schedule performance for a subset of acat ii and iii programs , but acquisition officials noted that the process was very resource - intensive , and they had concerns about the reliability of the cost and schedule information used in their analysis given the lack of variability in program performance over time .

while the components have developed oversight mechanisms to review individual acat ii and iii program performance , such as through periodic program status reviews at the peo level and program or portfolio reviews by senior component acquisition officials , without assessing performance trends across acat ii and iii programs , dod and its components may be missing opportunities to identify and analyze differences between actual and expected performance and develop strategies to address related risks throughout the department .

when we analyzed information available on cost and schedule performance , we determined that we could not assess cost performance for 139 programs out of a non - generalizable sample of 170 programs and schedule performance for 105 of the 170 programs .

in addition to missing or misreported cost data , we identified two challenges to measuring cost and schedule performance trends for acat ii and iii programs: ( 1 ) programs without available apbs and ( 2 ) a lack of consistent and comparable key schedule milestones across programs .

see figure 2 for a summary of our assessment of the data available to measure cost and schedule performance and appendix iii for additional details .

for 75 of the 170 programs that we examined in detail , we could not assess cost or schedule performance because dod components had not developed , or did not provide , an original apb , a current one , or both .

the components were unable to provide apbs for various reasons , such as because they could not locate the original or an apb was not developed at program start or to this point in the life of the program .

apbs are critical management tools that establish how systems will perform , when they will be delivered , and what they will cost .

according to dod acquisition policy , apbs are required of all acquisition programs , and office of the under secretary of defense for acquisition , technology , and logistics officials told us they generally expect all acquisition programs to have one.prior to entry into system development ( milestone b ) , or at program initiation , whichever occurs later .

apbs may be revised at the time of significant program decisions , such as milestones , or as a result of major program changes or breaches to cost , schedule , or performance parameters .

table 3 shows the number of missing apbs by component in our sample .

thirteen of the 15 acat ii or iii programs we reviewed in - depth had exceeded the cost or schedule targets in their original apbs .

these programs cited changing requirements , testing issues , quantity changes , and flaws in original cost estimates , among other factors , as the reasons for cost and schedule growth .

the programs we reviewed cited other factors , such as a reliance on mature technology — including commercial or government off - the - shelf or other non - developmental items — and early involvement of stakeholders or users as contributing to reduced risk of cost or schedule growth .

we have previously reported that similar factors affect the performance of dod's mdaps .

appendix iv provides additional details about the programs we reviewed .

thirteen of the 15 acat ii or iii programs we reviewed in - depth had exceeded the cost or schedule targets in their original apbs.attempt to quantify the extent to which these programs had exceeded cost or schedule targets due to overall concerns about the reliability of acat ii and iii cost and schedule data and because not all of these programs had developed apbs at program start .

these programs most frequently attributed cost growth or schedule delays to changing requirements .

testing issues , quantity changes , and flaws in original cost estimates were also cited by at least 5 of the 13 programs as contributing we did not to cost growth or schedule delays .

all but 1 of the 13 programs cited multiple causes for cost growth or schedule delays , including factors beyond those listed in table 4 .

requirements changes were associated with cost growth or schedule delays by at least one program at each of the five components in our review .

according to program officials , programs added or increased requirements due to situations such as: adding capability to a new platform that had not been planned for when the original requirements were approved ; creating additional variants to meet requirements that emerged after the original requirements were approved ; or making improvements or refinements to a system in development or production as a result of changes in the operational environment , including new threats .

for example , officials from the army's synthetic environment core program , which is providing the army a common virtual environment that links virtual simulators and simulations into an integrated and interoperable training environment , told us that increasing terrain database requirements to meet additional training needs have contributed to program cost increases significant enough to require the program to be recategorized from an acat iii to an acat ii program .

program officials stated that in some cases , the additional requirements have been unrealistic from either a cost or technological perspective , but that historically there had not been an effective process to prioritize requirements or enforce capability tradeoffs .

table 5 provides additional examples from our case studies of factors cited by program offices as contributing to cost growth or schedule delays .

we have previously reported that similar factors have negatively affected the cost and schedule performance of mdaps .

dod's weapons system programs often enter the acquisition process without a full understanding of requirements , and we have reported numerous times that requirements changes or changes to designs to meet requirements are factors in poor cost and schedule outcomes .

additionally , in part due to high levels of uncertainty about requirements , program cost estimates and their related funding needs are often flawed .

for example , in 2008 we assessed cost estimates for 20 mdaps and found that the estimates were too low in most cases and that in some programs , cost estimates were off by billions of dollars.knowledge and detail to develop sound cost estimates , which effectively set programs up for cost growth and schedule delays .

program officials for the acat ii and iii programs we reviewed most frequently cited the reliance on mature technology — including commercial or government off - the - shelf or other non - developmental items — and early involvement of stakeholders or users as factors that helped to reduce the risk of cost or schedule growth .

both of these factors were cited by 5 or more of the 15 acat ii or iii program offices we reviewed .

in some cases , these factors were cited by programs that experienced cost growth or schedule delays , for example , because one of these factors may have helped a program partially recover from a cost or schedule breach or keep initial program costs lower or schedules shorter than otherwise would be expected .

reliance on existing mature technologies was a relevant factor for the two programs we reviewed that did not report cost growth or schedule delays , and the most frequently cited factor contributing to reduced risk of cost or schedule growth among all of the programs we reviewed .

the two programs we reviewed that appeared to be on track to meet original cost and schedule targets — the army's 5.56 millimeter enhanced performance round program and socom's nonstandard aviation program — relied on modified commercial off - the - shelf equipment or modified existing military service equipment or assets .

the army's 5.56 millimeter enhanced performance round was an incremental engineering change to replace the army's general purpose 5.56 millimeter bullet with a new bullet design , which features a copper slug and exposed hardened steel penetrator .

socom's nonstandard aviation program acquires , modifies , fields , and sustains commercial aircraft to transport special operations forces .

the use of mature technologies was also cited as contributing to reduced risk of cost or schedule growth by 6 of the 13 other programs we reviewed .

for example , according to program documentation for the air force's f - 15e radar modernization program , the program planned to leverage existing commercial and government off - the - shelf technology from other fighter aircraft radar systems and the maturity of these technologies significantly lowered program development risk and costs .

early stakeholder or user involvement was cited by 5 of the 15 programs we reviewed as contributing to reduced risk of cost or schedule growth , including 1 of the 2 programs that did not experience cost growth or schedule delays .

for example , officials with the army's 5.56 millimeter enhanced performance round program noted that constant communication with all stakeholders , engineers , testers , and contractors was essential and a key success factor for the program .

similarly , program officials for cbdp's dismounted reconnaissance sets , kits , and outfits program — which provides protective equipment for chemical , biological , radiological , or nuclear hazards — told us that the participation of all of the military services at the beginning of the program helped to the keep program cost and schedule on track .

according to program officials , they integrated user input from the outset , including in developing the concept of operations , which reduced the number of later requirements changes .

at the time of our review , the program was on track to meet its original schedule targets .

the program's unit cost also decreased between the start of development and production .

we have previously reported that similar factors appear to positively affect the cost and schedule performance of mdaps .

for example , in 2010 , we reported on mdaps that appeared to be stable and on track to meet their original cost and schedule targets .

we found that the stable programs we reviewed leveraged mature technologies that had been demonstrated to work in relevant or realistic environments , and either did not consider immature technologies or deferred immature technologies to later program increments .

we also reported in 2012 that early stakeholder involvement in pre - system development reviews helped facilitate trade - offs among cost , schedule , and technical performance requirements .

for example , by involving both the requirements and acquisition communities in these reviews , the army was able to identify trade - offs that reduced the projected unit costs for the joint light tactical vehicle without impairing operational needs .

data provided by dod components indicated that at least five current acat ii programs were approaching or had exceeded acat i cost thresholds as of november 2013 , though dod component officials told us that most were not expected to become mdaps.identify with certainty the number of programs likely to become mdaps because of data reliability issues related to identifying the population of acat ii and iii programs and their estimated cost .

using the 836 programs initially reported by dod components as our starting point , we identified two current acat ii programs that exceeded the acat i threshold for rdt&e — $480 million in fiscal year 2014 constant dollars — and three current acat ii programs that were within 10 percent of the acat i rdt&e or procurement threshold — $2.79 billion in fiscal year 2014 constant dollars — as of november 2013 .

of these five programs , dod component officials told us that four would not become mdaps because , for example , they did not expect further program cost growth or were considering restructuring the program , and that component - level discussions were underway with regard to the status of the remaining program ( see table 6 ) .

dod weapon system acquisition represents one of the largest areas of the government's discretionary spending , but much of this spending is still not well understood .

dod's primary focus has been on overseeing and assessing the performance of its large acat i major defense acquisition programs , but the annual funding spent on acat ii and iii acquisition programs may be just as significant .

yet data provided by dod components were so unreliable that we were unable to accurately identify even a minimum number or total cost of dod's acat ii and iii programs .

while tailoring documentation and reporting requirements for “smaller” programs can be a reasonable approach to help prioritize limited oversight resources , if dod and its components are to effectively manage their investment dollars , they must be able to account for how they are spending their money and how well they are spending it on the full range of acquisition programs .

having timely and reliable data on smaller acquisition programs is also critical for providing effective oversight and bringing the right oversight resources to bear , when needed , to make sure troubled smaller programs do not grow into major ones .

the under secretary of defense for acquisition , technology , and logistics has recognized the value of having good data on dod's acquisition programs — including its acat ii and iii programs — to assess the performance of the defense acquisition system and identify the factors that affect program performance .

but work remains to make sure information on the complete range of dod acquisition programs is consistently available .

dod components have taken and continue to take steps to improve the reliability of acat ii and iii program data , but they do not fully address the limitations we identified — missing data , widespread data entry issues and inconsistent reporting — or the causes of these issues , including: the lack of a common definition of a current acquisition program ; insufficient data reliability testing ; and inconsistent compliance with requirements for acquisition program baselines and reporting on acat ii and iii programs that may become major programs due to cost growth .

components also lacked plans to ensure their intended actions are implemented and improvements to data collection and analysis are sustained over the long term .

until these limitations are addressed , dod components will be unable to generate reliable information to effectively manage and oversee their acat ii and iii programs .

we are making four recommendations to improve dod's ability to collect and maintain reliable data on its acquisitions .

specifically , we recommend that the secretary of defense direct the under secretary of defense for acquisition , technology , and logistics , in consultation with dod components , to take the following actions: establish guidelines on what constitutes a “current” acat ii or iii program for reporting purposes ; the types of programs , if any , that do not require acat designations ; and whether the rules for identifying current mdaps would be appropriate for acat ii and iii programs ; and determine what metrics should be used and what data should be collected on acat ii and iii programs to measure cost and schedule performance ; and whether the use of damir and the mdap selected acquisition report format may be appropriate for collecting data on acat ii and iii programs .

we also recommend that the secretary of defense direct the secretaries of the air force , army , and navy and the commander of socom to take the following actions: assess the reliability of data collected on acat ii and iii programs and work with peos to develop a strategy to improve procedures for the entry and maintenance of data ; and develop implementation plans to coordinate and execute component initiatives to improve data on acat ii and iii programs .

we are also making two recommendations to help ensure compliance with relevant provisions of dod acquisition policy with the purpose of improving dod's ability to provide oversight for acat ii and iii programs , including those programs that may become mdaps .

we recommend that the secretary of defense direct the secretary of the air force and commander of socom to establish a mechanism to ensure compliance with apb requirements in dod policy .

we recommend that the secretary of defense direct the secretaries of the air force , army , and navy to improve component procedures for notifying the defense acquisition executive of programs with a cost estimate within 10 percent of acat i cost thresholds .

we provided a draft of this report to dod for review and comment .

in its written comments , which are reprinted in full in appendix v , dod partially concurred with all six of our recommendations .

however , as discussed below , it is unclear whether the actions that dod plans to take will fully address the issues we raised in this report .

dod partially concurred with our first recommendation to establish guidelines on what constitutes a “current” acat ii or iii program for reporting purposes ; the types of programs , if any , that do not require acat designations ; and whether the rules for identifying current mdaps would be appropriate for acat ii and iii programs .

dod also partially concurred with our second recommendation related to determining what metrics should be used and what data should be collected on acat ii and iii programs to measure cost and schedule performance .

in its response , dod stated that the under secretary of defense for acquisition , technology , and logistics would review the existing policy direction for acat ii and iii programs to determine whether it needs to be altered or supplemented to facilitate data collection or reporting .

however , as our review found , the question is not whether policy needs to be revised , but how it needs to be revised .

we found that the existing policy direction was not adequate to ensure consistent data collection and reporting on acat ii and iii programs or their cost and schedule performance and our recommendations were designed to address those issues .

we continue to believe that additional guidelines for components regarding which programs should be considered current acat ii and iii programs for reporting purposes and consistent metrics to measure performance trends , among other actions , are needed to correct the issues we found .

dod partially concurred with our third and fourth recommendations to assess the reliability of data collected on acat ii and iii programs and work with peos to develop a strategy to improve procedures for the entry and maintenance of data ; and develop implementation plans to coordinate and execute component initiatives to improve data on acat ii and iii programs , respectively .

in its response , dod stated the under secretary of defense for acquisition , technology , and logistics will direct the dod components to evaluate the data collected on acat ii and iii programs and report back to him on their assessment of the reliability of that data and the status of the plans to improve the availability and quality of the data .

dod's response represents a good first step towards assessing the reliability of its acat ii and iii program data , but the response does not fully address our recommendations .

dod's response does not address whether components would be required to develop strategies with peos to improve the entry and maintenance of data , as we recommended .

we continue to believe that developing these strategies with those responsible for entering and maintaining program data on a day - to - day basis , including peos , is important to make sure the causes of dod's data quality problems are fully understood and addressed in a manner that can be implemented .

further , dod's response does not directly address our recommendation to develop implementation plans for component efforts .

we believe that fully implementing this recommendation is essential for ensuring that dod and its components can effectively monitor and evaluate the implementation of component initiatives to improve acat ii and iii data .

dod partially concurred with our fifth recommendation to direct the secretary of the air force and commander of socom to establish a mechanism to ensure compliance with apb requirements in dod policy .

dod also partially concurred with our sixth recommendation to direct the secretaries of the air force , army , and navy to improve component procedures for notifying the defense acquisition executive of programs with a cost estimate within 10 percent of acat i cost thresholds .

in its response , dod stated that the under secretary of defense for acquisition , technology , and logistics will issue guidance to dod components reiterating the apb requirements for acat ii and iii programs and directing that the defense acquisition executive be notified when an increase or estimated increase in program cost is within 10 percent of the acat i cost thresholds .

reiterating existing departmental policy on these issues may help raise awareness at the component level , but without additional enforcement mechanisms it may not address the causes of the deficiencies we discuss in this report .

for example , the air force has issued component - level guidance directing the development of apbs .

however , we found that programs were not in compliance with the guidance , which demonstrates the need to improve enforcement mechanisms , such as ensuring milestone decision authorities do not approve programs to proceed through acquisition milestones without apbs .

similarly , with regard to our recommendation on notification requirements for programs approaching the acat i threshold , we found that component officials cited reasons other than a lack of awareness of the policy for not notifying the defense acquisition executive of these programs' cost growth .

as a result , we continue to believe that dod should fully implement our recommendation by directing components to improve their notification procedures .

we are sending copies of this report to the appropriate congressional committees ; the secretary of defense ; the under secretary of defense for acquisition , technology , and logistics ; the secretaries of the army , navy , and air force ; the commander of u.s. special operations command ; the assistant secretary of defense for nuclear , chemical , and biological defense programs ; and other interested parties .

this report will also be available at no charge on gao's website at http: / / www.gao.gov .

if you or your staff have any questions concerning this report , please contact me at ( 202 ) 512-4841 or by e - mail at sullivanm@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix vi .

our objectives were to assess ( 1 ) the extent to which information is available on the number of the department of defense's ( dod ) current acquisition category ( acat ) ii and iii programs , their total estimated acquisition cost , and cost and schedule performance ; ( 2 ) the factors affecting the cost and schedule performance of selected acat ii and iii programs ; and ( 3 ) the number of current acat ii and iii programs that are likely to become major defense acquisition programs ( mdap ) .

to address our first objective , we used a data collection instrument ( dci ) to identify and collect data on the number and cost of current acat ii and iii programs from five dod components that accounted for approximately 88 percent of dod's requested research , development , test , and evaluation ( rdt&e ) and procurement funding in the president's fiscal year 2014 budget request: army , air force , navy , u.s. special operations command ( socom ) , and dod's chemical and biological defense program ( cbdp ) .

we used a dci to obtain acat ii and iii program data based on preliminary discussions with dod and component officials that a dci would be the best way to collect the information of interest .

we requested that each component identify all of its current acat ii and iii programs and provide cost data and descriptive information for each program .

for the purposes of this report , we defined a current program as one that has been formally initiated in the acquisition process but has not yet delivered 90 percent of its planned units or expended 90 percent of its planned expenditures .

for cost data , we requested components provide baseline and current program estimates in millions of base year dollars , to include estimates for rdt&e , procurement , acquisition operation and maintenance , and military construction , as well as the program's total acquisition cost estimate and the base year associated with the estimate .

we also collected pertinent information for each program including program name , acat level , type of acquisition ( automated information system or non - automated information system ) , milestone decision authority , lead dod component , and program executive office .

to obtain additional information on schedule performance , we collected and analyzed acquisition program baseline ( apb ) documents , which contain program schedule and cost parameters , for a non - generalizable random sample of 170 non - automated information system acat ii and iii programs .

to select the programs , we used the initial data provided to us by dod components that included 836 reported acat ii or iii programs as a starting point .

we adjusted our selection as appropriate to account for known errors in the data at the time of selection in may 2014 , such as programs that were known to not be current acat ii or iii programs .

our intention was to select a sample that would be generalizable to the population of current acat ii and iii programs .

however , after selecting our sample we determined through our data reliability assessment that the population of current acat ii and iii programs could not be reliably determined and that our sample would therefore be non - generalizable.as such , results of this analysis cannot be used to make inferences about all current acat ii and iii programs .

when apb documents were available for programs in our sample , we reviewed them to determine whether they contained comparable program start and initial operational capability milestones to allow us to measure program schedule performance .

we also used the apbs collected from this sample of programs as part of our reliability assessment of acat ii and iii cost data provided by dod components .

our observations on dod's acat ii and iii program data are based on the original data submitted by the components .

we did not assess the reliability of any underlying data systems that may have been used to generate this information .

we analyzed the original data provided by the components because it reflects the information dod would have had on acat ii and iii programs at the time we collected data .

from january through july 2014 , we worked with components to attempt to correct problems we identified in the data .

however , we continued to identify additional errors .

as a result , we determined that the data provided by dod components in response to our dci were not sufficiently reliable to identify the number of current acat ii and iii programs , their estimated acquisition cost , or the cost performance of dod's acat ii and iii programs .

appendix iii contains a more detailed discussion of our data reliability assessment .

we also determined that we could not assess schedule performance for acat ii and iii programs because more than half of programs we reviewed in our sample of 170 programs were missing source documents or lacked comparable schedule milestones .

to address our second objective , we selected a non - generalizable sample of 15 programs from the data provided by dod in response to our dci .

we selected 3 programs from each component included in our review .

for each component , these programs were selected to include the largest current non - automated information system acat ii and iii program based on total acquisition cost as of the president's fiscal year 2014 budget submission and one additional program based on factors such as significant cost growth or whether the program was part of a family of systems , which we defined as a related group of programs consisting of multiple increments or fielding similar capabilities for multiple to select the programs , we used the initial data provided to us platforms.by dod components that included 836 reported acat ii or iii programs as a starting point .

programs that lacked data for current acquisition cost , commodity type , or acat level were excluded from selection .

we also adjusted our selection as appropriate to account for known errors in the data at the time of selection , such as incorrectly - reported cost estimates , or programs that were known to not be current acat ii or iii programs .

however , after our selection we identified additional concerns with the data reported by dod that would likely have changed the results of our selection of the largest acat ii or iii programs at certain components .

we did not make any subsequent adjustments to our original selection because we determined that the data provided by dod was not sufficiently reliable to enable us to determine the largest acat ii or iii program at each component .

for each program , we analyzed key program documents , such as apbs , program status reports , acquisition strategies , acquisition decision memoranda , and requirements documentation , to assess cost and schedule performance and identify factors affecting that performance .

we also conducted semi - structured interviews with program officials to discuss the information identified through reviews of program documentation and obtain additional insights into factors that affected program cost or schedule performance .

additionally , we analyzed prior gao reports to determine the extent to which the factors we identified as affecting cost and schedule performance for selected acat ii and iii programs were similar to factors that we have identified in prior work as affecting performance of mdaps .

to address our third objective , we reviewed dod acquisition policy related to the reclassification of acat ii or iii programs to acat i programs and analyzed program cost data provided by dod components .

based on the requirement in dod acquisition policy for components to notify the defense acquisition executive of acat ii or iii programs within 10 percent of the next acat level , we analyzed data provided by dod through our dci to identify programs that appeared to be within 10 percent of or have exceeded either the acat i rdt&e or procurement threshold .

we were unable to identify an actual number of programs likely to become mdaps because of reliability issues related to identifying the population of acat ii and iii programs .

however , we determined the initial data provided to us by dod that included 836 reported acat ii or iii programs were sufficiently reliable to serve as a starting point to identify the minimum number of programs likely to become mdaps because we were able to confirm data with relevant program offices for those programs that appear to be within 10 percent of or have exceeded either acat i threshold .

we excluded certain programs from further review that were known at the time that we initially identified programs to have incorrectly - reported cost estimates .

for programs that appeared to meet our criteria for current acat ii or iii programs likely to become mdaps , we collected additional information using a structured set of questions to determine whether the relevant dod component had notified the defense acquisition executive that the program was approaching or had exceeded the acat i threshold and whether the program had been or was expected to be reclassified as an acat i program .

we also requested and reviewed supporting documentation when available , including documentation of notification to the defense acquisition executive that the program was within 10 percent of the acat i threshold .

after we received the information from the components , we identified additional programs that had incorrectly reported cost estimates or were no longer current acat ii or iii programs and we removed these programs from our analysis as appropriate .

we conducted this performance audit from october 2013 to march 2015 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

summarizes program cost , schedule , and performance parameters program cost estimate completed outside of the supervision of the entity responsible for the acquisition program documents capability requirements to which the program responds describes program's overall technical approach and details timing and criteria for technical reviews technology readiness assessment assessment of the maturity of critical primary planning and management tool for integrated test program 10 u.s.c .

§ 2366a and b .

provides notification of unit cost breaches above a certain threshold provides notification of cost or schedule changes above a certain threshold 10 u.s.c § 2433 and 2433a .

notification must be provided to congress when the program acquisition unit cost or average procurement unit cost increases by at least 15 percent over the current baseline estimate or 30 percent over the original baseline estimate .

10 u.s.c § 2445c .

notification must be provided to congress when there is a schedule change that will cause a delay of more than 6 months ; an increase in the expected development cost or full life - cycle cost for the program by at least 15 percent ; or a significant , adverse change in the expected performance of the major automated information system to be acquired .

we conducted an analysis to determine whether data provided by department of defense ( dod ) components were sufficiently reliable for the purpose of determining the number , total acquisition cost , and cost performance of dod's current acquisition category ( acat ) ii and iii programs .

for our analysis , we conducted electronic and manual testing on data for all programs reported by components in response to our request for completion of a data collection instrument ( dci ) and compared cost data for a non - generalizable sample of programs to source documents when available .

we also reviewed relevant dod and component acquisition policy , and interviewed knowledgeable officials .

we identified reliability issues with the data for about 60 percent of the programs components initially reported to us .

as a result , we determined that the data provided by dod components were not sufficiently reliable to identify the number of current acat ii and iii programs , their estimated total acquisition cost , or the cost performance of dod's acat ii and iii programs .

to assess the accuracy and completeness of the acat ii and iii program data reported by dod components , we electronically tested the data for: values outside the designated range of values for acat ii and iii programs , defined per dod acquisition policy ; obvious calculation or data entry errors ( for example , individual cost elements do not sum to total reported ) ; missing data in baseline or current cost estimate data elements , including estimates for research , development , test , and evaluation ; procurement ; military construction ; and acquisition operation and maintenance , as well as the total acquisition cost estimate , and base year ; and missing data in program descriptive data elements , such as acat level , milestone decision authority , or commodity type .

additionally , we compared cost data for our sample of 170 programs to source documents when available .

specifically , for each program in our sample , we first requested and reviewed original and current acquisition program baselines ( apb ) to determine whether or not they reflected the actual baseline from program start and the current apb based on the approval date of the apb and relevant schedule milestones that trigger the development of an apb or an apb revision in accordance with dod acquisition policy .

when apbs were not provided or did not appear to reflect the actual baseline and / or current apb , we followed up with dod components to obtain the correct documents when possible .

when we were able to obtain both original and current apbs , we took the following steps to assess the accuracy of the information reported in the dci: compared baseline cost data from the program's original apb to the baseline cost data reported in the dci .

compared cost data in the current apb to cost data reported in the dci to identify obvious errors in the cost data reported in the dci , such as current cost data in the dci that was significantly less than the amount reported in the apb without explanation .

to assess the consistency of acat ii and iii program data , we manually reviewed the data provided in response to the dci and subsequent requests to identify programs that did not appear to meet our criteria for current acat ii and iii programs .

for the purposes of this report , we defined a current program as one that has been formally initiated in the acquisition process but has not yet delivered 90 percent of its planned units or expended 90 percent of its planned expenditures .

for each program that did not appear to be a current acat ii or iii program , we analyzed whether the program was pre - program start , in sustainment , completed , or not a separate acat ii or iii program ( for example , was a subprogram of another acat ii or iii program reported to us ) .

the results of our data reliability analysis capture accuracy , completeness , and consistency issues with the data provided by dod components .

accuracy , completeness , and consistency are key characteristics of reliable data and refer to ( 1 ) the extent that recorded data reflect the actual underlying information ; ( 2 ) data elements for each program are populated appropriately ; and ( 3 ) the need to obtain and use data that are clear and well defined enough to yield similar results in similar analyses , respectively .

we identified numerous types of accuracy and completeness issues with the data provided by dod components , including cost estimate values outside of the acat ii and iii program range , basic math errors , and missing data .

for example , 333 out of 836 acat ii and iii programs reported by the components were missing a baseline or current cost estimate element or descriptive program data .

table 9 provides detail on accuracy and completeness issues by component .

we identified additional issues with the accuracy of the acat ii and iii program cost information when we compared reported cost estimates to available source documents for a non - generalizable sample of acat ii and iii programs .

specifically , of the 81 programs in our sample that reported complete cost estimates and provided source documents , 50 reported incorrect cost data .

for example , for 37 of these 50 programs , we determined that baseline cost data was inaccurate because either the baseline cost estimate or the base year reported for this estimate did not match the source documents .

details of the accuracy and completeness issues we identified when assessing the cost data reported for our sample are provided by component in table 10 .

we identified 226 of the 836 programs reported by dod components that did not meet our criteria for current acat ii or iii programs because , for example , they were not current or they were not stand - alone acquisition programs .

additionally , because information on program phase was not available for all programs reported by dod components , the number of programs we identified as not a current acat ii or iii program reflects a minimum number of such programs .

table 11 provides details on consistency issues we identified by component .

in addition to the contact named above , ron schwenn , assistant director ; leslie ashton , jenny chanley , teakoe coleman , dani greene , john krump , jesse lamarre - vincent , anne mcdonough - hughes , carol petersen , and oziel trevino made key contributions to this report .

