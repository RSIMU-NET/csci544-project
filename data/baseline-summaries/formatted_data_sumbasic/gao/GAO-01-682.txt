the amount of money that the department of defense ( dod ) spends each year contracting for goods and services dwarfs the amounts spent by other federal agencies .

for example , during fiscal year 1999 , dod reported that it spent about $130 billion on contracts for goods and services .

by comparison , the second largest contractor of goods and services in the federal government was the department of energy , which reported that it spent about $15.5 billion during the same period .

since 1992 , we have designated dod's management of its contracts a high - risk area .

it remains on our list of high - risk areas today because , among other things , significant challenges still face dod in overseeing contracts and preventing erroneous and improper payments to contractors .

with the goal of improving its ability to contract for goods and services , dod is about 5 years into what is now at least an 8-year effort to acquire and implement a standard procurement system ( sps ) .

according to dod , sps is to support about 43,000 users at 1,100 sites worldwide in preparing , awarding , and administering contracts .

sps also is to interface with key financial management functions , such as payment processing .

among other things , dod expects sps to replace legacy systems that support divergent contracting processes and procedures across component organizations with a single , standard system that provides electronic commerce capabilities and uses a common data repository .

as agreed with your office , we assessed dod's effectiveness in managing its sps investment .

our objectives were to determine ( 1 ) the progress dod has made against sps program commitments ( i.e. , cost , schedule , and performance expectations , including delivery of promised system capabilities and expected benefits ) and ( 2 ) whether dod has economically justified further investment in sps .

our objectives , scope , and methodology are discussed in detail in appendix i .

dod's procurement process spans numerous defense agencies and military services .

this process provides for acquiring supplies and services from nonfederal sources and , when necessary , administering the related contractual instruments .

it also provides for administering grants , cooperative agreements , and other transactions executed by contracting offices .

the procurement process begins with the receipt of a requirement and ends at the contract closeout .

 ( see fig .

1 for a simplified diagram of the procurement process , the interaction of this process with the logistics and financial management processes , and those functions within the procurement process that sps is to support. ) .

in november 1994 , dod's director of defense procurement ( ddp ) initiated the sps program to acquire and deploy a single automated system to perform all contract management - related functions within dod's procurement process for all dod organizations and activities .

from 1994 to 1996 , dod defined sps requirements and solicited commercially available vendor products for satisfying these requirements .

dod subsequently awarded a contract to american management systems ( ams ) , incorporated , in april 1997 , to ( 1 ) use its commercially available contract management system as the foundation for sps , ( 2 ) modify this commercial product as necessary to meet the requirements , and ( 3 ) perform related services .

dod also directed the contractor to deliver sps functionality in four incremental releases .

the department later increased the number of releases across which this functionality would be delivered to seven ; reduced the size of the increments ; and allowed certain , more critical functionality to be delivered sooner .

over the last 4 years , dod and ams have deployed four releases to 773 locations in support of 21,900 users .

the fifth release was delivered in february 2001 for acceptance testing ; however , due to software deficiencies , this release was sent back to the vendor for rework and has not been deployed .

ams is expected to provide a second version of this release to dod in july 2001 for additional testing .

if accepted , the fifth release is to be deployed to about 4,500 users beginning in fiscal year 2002 .

dod has not yet contracted for the sixth and seventh releases .

 ( see table 2 for the status of the various software releases , and table 3 for the summary of sps functionality by increment. ) .

as planned , sps is to be used to prepare contracts and contract - related documents and to support contracting staff in monitoring and administering them .

sps also is intended to standardize procurement business practices and data elements throughout dod and to provide timely , accurate , and integrated contract information .

using sps , the goal is that required contract and contract payment data will be entered once — at the source of the data's creation — and be stored in a single database .

as depicted in figure 1 , sps is to electronically interface with dod's logistics community , which is the source of goods and services requests , and with the defense finance and accounting service ( dfas ) , which is responsible for contract payments .

ddp is organizationally within the office of the under secretary of defense for acquisition , technology and logistics .

however , as shown in table 3 , the management responsibility for sps is shared among several organizations .

since 1996 , dod's office of the inspector general ( oig ) has issued three reports critical of sps .

in september 1996 , the oig reported that the needs of sps users might not be met and that actual costs could exceed proposed costs because , among other things , the functional requirements were very broad , existing commercial software required substantial modification , and adequate development and operational test strategies had not been developed .

the oig later reported in may 1999 that sps lacked critical functionality and concluded that the system may not meet mission needs with regard to standardizing procurement policy , processes , and procedures .

the report also noted that users were receiving inadequate system training , guidance , and support , thereby forcing users to develop inefficient system workarounds .

finally , the report raised concerns about the cost - effectiveness of dod's contractual reliance on a single vendor to provide system support over the life of sps , adding that an expanded license was needed to give dod the ability to competitively compete support services .

in march 2001 , the oig reported that lack of system functionality was still a serious program concern , productivity had not increased with the implementation of version 4.1 , and users were generally dissatisfied with sps .

sps program officials generally concurred with the oig's findings and agreed to issue guidance on the acquisition of commercial software for major automated information systems , support development of accurate life - cycle cost estimates for sps , clarify responsibilities for the program office , the contractor , and the evaluate the cost and benefits of obtaining additional license rights and renegotiating the contract , require the program office to be aware of additional support contracts , and suggest that the component organizations provide funds to the program office to better integrate user needs , better coordinate training needs among the dod component organizations , and require that before any future deployments of sps , the dod component organizations determine that the version meets their functional requirements and to identify the number of licenses required .

also , in response to the oig's march 2001 report , the sps program office initiated its own study in june 2000 to assess the extent to which benefits will be realized as a result of its implementation of version 4.1 of sps .

the program office plans to publish the study results by october 2001 .

federal information technology ( it ) investment management requirements and guidance recognize the need to measure investment programs' progress against commitments .

in the case of sps , dod is not meeting key commitments and is not measuring whether it is meeting other commitments .

according to the program manager , the program office is not responsible for ensuring that all program commitments are being met .

rather , the program office's sole task is to acquire and deploy an sps system solution that meets defined functional requirements .

given that sps is a major defense acquisition , the dod cio is the decisionmaking authority for sps .

however , according to officials in the cio's office , sps has continued to be approved and funded regardless of progress against expectations on the basis of decisions made by individuals organizationally above the cio's office .

without measuring and reporting progress against program commitments and taking the appropriate actions to address significant deviations , dod runs the serious risk of investing billions of dollars in a system that will not produce commensurate value .

the clinger - cohen act of 1996 and office of management and budget ( omb ) guidance emphasize the need to have investment management processes and information to help ensure that it projects are being implemented at acceptable costs and within reasonable and expected time frames and that they are contributing to tangible , observable improvements in mission performance ( i.e. , that projects are meeting the cost , schedule , and performance commitments upon which their approval was justified ) .

for programs such as sps , dod requires this cost , schedule , and performance information to be reported quarterly to ensure that programs do not deviate significantly from expectations .

in effect , these requirements and guidance recognize that one cannot manage what one cannot measure .

dod has not met key sps commitments concerning the timing of product delivery , user satisfaction with system performance , and the use of a commercial system solution , as discussed below: dod committed to sps' being fully operational at all sites by march 31 , 2000 ; however , this date has slipped by 3-½ years and is likely to slip further .

currently , dod has established a september 30 , 2003 , milestone for making sps fully operational , and the program manager attributed this delay to ( 1 ) problems encountered in modifying and testing the contractor's commercial product to meet dod's requirements and ( 2 ) an increase in requirements .

however , the sps joint requirements board chairperson stated that no additional requirements have been approved .

instead , the original requirements were clarified for the contractor to better ensure that the needs of the user would be met .

however , satisfying even this revised commitment will be problematic for several reasons .

first , the 2003 milestone does not recognize dod components' testing activities that need to occur before the system could be fully operational .

for example , department of the air force officials told us that they are typically 6 to 12 months behind the program office's deployment milestones because of additional testing that the air force performs before it implements the software releases .

second , the 2003 milestone has not been updated to reflect the impact of events .

for example , version 4.1 , the latest deployed release , was recently changed from a single release to five subreleases to correct software problems discovered during operation of version 4.1 ; and version 4.2 recently failed acceptance testing , and the vendor is still attempting to correct identified defects .

third , the official responsible for sps independent operational test and evaluation , as well as the official in dod's office of program analysis and evaluation who is responsible for reviewing the sps economic analyses , told us that this milestone is likely to slip further .

the reasons that these officials cited included incomplete system functionality , increased system complexity , and inadequate training .

dod committed to sps' satisfying the needs of its contracting community and meeting specified system requirements , ultimately increasing contracting efficiency and effectiveness .

however , according to a recent dod oig report , approximately 60 percent of the user population surveyed was not satisfied with the system's functionality and performance , resulting in the continued use of legacy systems and / or manual processes in lieu of sps .

similarly , another dod report describes sps as unstable because the system frequently goes down , meaning that it is unexpectedly unavailable to users who are logged on and using the system , which , in turn , causes users to lose information .

the report also notes that users complained that previously identified problems were not being resolved in later software releases , and that requested changes or enhancements were not being made .

according to the program manager , at any one time , there was a backlog of 100 to 200 problems that needed to be addressed in order for sps to meet specified requirements .

in light of these challenges in meeting requirements and satisfying user needs , the official responsible for independent operational test and evaluation of sps said that dod should not invest in additional releases beyond version 4.2 .

in delivering sps , dod was to use a commercially available software product .

however , the contractor has modified the commercial product extensively in an attempt to satisfy dod's needs ; thus , sps is now a dod - unique system solution .

according to the program manager , dod knew when it selected the commercial product that the product provided only 45 percent of the functionality that dod needed , and that extensive new software development and existing software modification were necessary .

nevertheless , the product was chosen because no commercial product was available that met dod's requirements , and , of the products available , dod believed that ams' product and company would provide the best value .

in accordance with industry best practices , software modifications to a commercial product should not exceed 10 to 15 percent .

beyond this degree of software change , experts generally consider development or acquisition of a custom system solution more cost - effective .

further , dod guidance states that custom modifications to a commercial item , even if made and implemented by the commercial item's vendor , result in custom system solutions .

this guidance emphasizes the use of commercial items to reduce life - cycle costs and increase product reliability and availability .

since sps is not a commercial product , dod will not be able to take advantage of the reduced cost and risk associated with using proven technology that is used by a wide customer base .

when it began the program , dod promised that sps would produce such benefits as ( 1 ) replacing 76 legacy systems and manual processes with a single system and thereby reducing procurement system operations and maintenance costs by an unspecified amount , ( 2 ) standardizing policies , processes , and procedures across the department , and ( 3 ) reducing problem disbursements .

however , dod does not know the extent to which sps is meeting each of these expectations , even though versions have been deployed to about 773 user locations .

first , although dod reports that it has retired two major legacy systems , neither the program office nor the dod cio office could provide us with information on what , if any , savings have been realized by doing so .

additionally , program officials told us that the number of legacy systems and manual processes that sps is to replace is now significantly less than the 76 originally used to justify the program .

in response to our inquiry , the sps program manager recently surveyed the dod component organizations to determine the number of legacy systems .

according to the results of the survey , there were 55 legacy procurement systems .

see table 4 for the status of these systems as of june 2001 .

according to the sps program manager , 45 of the 55 systems remain , and 10 to 12 of these systems are to be replaced by sps .

however , another program official noted that sps was always intended to replace only 14 major legacy systems .

in either case , the latest economic analysis has not been updated to reflect this change in the number of systems to be replaced , and the associated cost savings are not known .

second , the standardization of policies , processes , and procedures benefit is not materializing because each military service is either in the process of developing , or has plans to develop , its own unique policies , processes , and procedures .

third , program officials were unable to provide evidence that implementing sps has reduced problem disbursements or achieved the benefits outlined in the economic analysis .

in fact , the latest economic analysis no longer even cites reducing problem disbursements as a benefit because the dod components' position was that sps would not completely address this problem .

according to the program manager and cio officials , there is no dod policy that requires them to assess whether the expected benefits are in fact being realized .

when the sps program began , dod also committed to a system life - cycle cost of about $3 billion over a 10-year period .

however , total actual program costs are not being accumulated and monitored against estimates , which in 2000 were revised to about $3.7 billion ( a 28-percent increase ) .

thus , dod does not know what has been spent on the program by all dod component organizations .

to date , the only actual program costs being collected and reported are those incurred by the sps program office , which dod reports to be about $322 million through september 30 , 2000 .

to determine the total cost of the sps program through september 30 , 2000 , we requested cost information from 18 defense agencies and the four military services .

these dod components reported that they have collectively spent approximately $125 million through september 30 , 2000 .

however , these reported costs are not complete because ( 1 ) 4 of the 22 dod components did not respond , ( 2 ) components reported that sps costs were being captured with other programs and could not be allocated accurately , and ( 3 ) all sps costs , such as employee salaries and system infrastructure costs , were not included .

according to program officials , no single dod organization is responsible for accumulating the full dod cost of sps .

without knowing the extent to which sps is meeting cost - and - benefit expectations , dod is not in a position to make informed , and thus justified , decisions on whether and how to proceed further on the program .

such a situation introduces a serious risk of investing in a system that will not produce a positive net present value ( i.e. , estimated benefits to be realized would exceed estimated program costs ) .

federal it investment management requirements and guidance , as well as dod policy , recognize the need to economically justify it projects before investing in them and to justify them in an incremental manner in an effort to spread the risk of doing many things over many years on large projects across smaller , more manageable subprojects .

however , the department has not economically justified investing in sps because its own analysis shows that expected life - cycle benefits are less than estimated life - cycle costs .

moreover , dod is not approaching its investment in sps on an incremental basis .

nevertheless , dod continues to invest hundreds of millions of dollars in sps each year , running the serious risk of spending large sums of money on a system that does not produce commensurate value .

according to program and cio officials , dod continues to invest these funds because individuals above the cio's office decided that sps was a departmental priority .

the clinger - cohen act of 1996 and omb guidance provide an effective framework for it investment management .

together , they set requirements for ( 1 ) economically justifying proposed projects on the basis of reliable analyses of expected life - cycle costs , benefits , and risks , ( 2 ) using these analyses throughout a project's life cycle as the basis for investment selection , control , and evaluation decisionmaking , and ( 3 ) doing so for large projects ( to the maximum extent practical ) by dividing them into a series of smaller , incremental subprojects or releases .

by doing so , the tremendous risk associated with investing large sums of money over many years in anticipation of delivering capabilities and expected business value far into the future can be spread across project parts that are smaller , of a shorter duration , and capable of being more reliably justified and more effectively measured against cost , schedule , capability , and benefit expectations .

dod policy also reflects these investment principles by requiring that investments be justified by an economic analysis and , more recently , that investment decisions for major programs , like sps , be made incrementally by ensuring that each incremental part of the program delivers measurable benefit , independent of future increments .

according to the policy , the economic analysis is to reflect both life - cycle cost and benefits estimates , including a return - on - investment calculation , to demonstrate that a proposal to invest in a new system is economically justified before that investment is made .

dod has developed three economic analyses for sps — one in 1995 and two updates ( one in 1997 and another in 2000 ) .

while the initial analysis reflected a positive net present value , the two updates did not .

specifically , the 1997 analysis estimated life - cycle costs and benefits to be $2.9 billion and $1.8 billion , respectively , which is a recovery of only 62 percent of costs ; the 2000 analysis showed even greater costs ( $3.7 billion ) and fewer benefits ( $1.4 billion ) , which is a recovery of only 37 percent of costs ( see fig .

2 ) .

nevertheless , these data were not reflected in the return - on - investment calculation in the analyses that were used as the basis for approving sps .

instead , this return - on - investment calculation ( 1 ) included only those costs estimated to be incurred by the program office and ( 2 ) excluded the sps implementation and operation and maintenance costs of dod agencies and military services .

according to program officials , the latter costs were excluded because either they would have been incurred anyway or the program office did not require them .

for example , the officials stated that the dod agencies and military services routinely upgrade their it infrastructures to support existing systems ; therefore , they assumed that the agencies and services would have purchased new infrastructures even if sps had not been acquired .

also , program officials did not believe that training paid for by dod agencies and military services should be included as a cost element because this is an elective expense ( i.e. , the program management office does not require this additional training ) .

however , some dod component officials told us that some of their infrastructure and other costs were being incurred solely to support implementation of sps .

using dod's estimates , we calculated sps' net present value for fiscal years 1997 and 2000 to be about negative $174 million and negative $655 million , respectively .

dod's office of program analysis and evaluation is responsible for , among other things , verifying and validating the reliability of economic analyses for major programs , such as sps , and providing its results to the program approval authority , which in this case is the dod cio .

according to office of program analysis and evaluation officials , although the economic analyses were reviewed , there are no written results of these reviews .

these officials stated , however , that they orally communicated concerns about the analyses to program officials and to dod cio officials responsible for program oversight and control .

they also stated that while they could not recall specific issues discussed , they concluded that the economic analyses provided a reasonable basis for decisionmaking .

to be useful for informed investment decisionmaking , analyses of project costs , benefits , and risks must be based on reliable estimates .

however , most of the cost estimates in the latest economic analysis are estimates carried forward from the 1997 economic analysis ( adjusted for inflation ) .

only the costs being funded and managed by the sps program office , which are 13 percent of the total life - cycle cost in the analysis , were updated in 2000 to reflect more current contract estimates and actual expenditures / obligations for fiscal years 1995 through 1999 .

the costs to be funded and incurred by dod agencies and the military services were not updated to account for all program changes or to incorporate better information .

in its review of the 2000 economic analysis , the naval center for cost analysis also noted that the dod agencies and the military services' cost information , which accounted for the majority of the program's overall costs , had not been updated .

in fact , only two cost elements were updated for the dod component organizations in the 2000 economic analysis , and the estimates for these cost elements were based on estimates derived for just one service ( the air force ) , and then extrapolated to all other dod components .

according to departments of the army , navy , and air force component representatives , these original estimates of costs , as well as benefits , were highly questionable at best .

however , this uncertainty was not reflected in the economic analysis by any type of sensitivity analysis ( i.e. , an analysis to explicitly present the return - on - investment implications associated with using estimates whose inherent imprecision could produce a range of outcomes ) .

such sensitivity analysis would disclose for decisionmakers the investment risk being assumed by relying on the calculations presented in the economic analysis .

according to the sps program manager , costs in the 2000 economic analysis were not updated because information for the dod components was not readily available for inclusion .

additionally , updating dod component costs was not viewed as relevant because the return - on - investment calculation cited in the latest economic analysis did not include these costs , and the updated analysis was done after dod leadership had decided to increase funding and continue the program .

however , by not using economic analyses that are based on reliable cost estimates , dod is making uninformed , and thus potentially unwise , multimillion - dollar investment decisions .

according to omb guidance , analyses of investment costs , benefits , and risks should be ( 1 ) updated throughout a project's life cycle to reflect material changes in project scopes and estimates and ( 2 ) used as a basis for ongoing investment selection and control decisions .

to do less , risks continued investment in projects on the basis of outdated and invalid economic justification .

the latest economic analysis ( january 2000 ) is outdated because it does not reflect sps' current status and known risks associated with program changes .

for instance , this analysis is based on a program scope and associated costs and benefits that anticipated four software releases , each providing more advanced features and functions .

however , according to the program manager , sps now consists of seven releases over which additional requirements are to be delivered .

estimates of the full costs , benefits , and risks relating to these additional three releases are not part of this latest economic analysis .

also , the 2000 economic analysis does not fully recognize actual and expected delays in meeting sps' full operational capability milestone .

that is , the 2000 economic analysis assumed that this milestone would be september 30 , 2003 .

however , as previously mentioned , this milestone date is unlikely to be met for a variety of reasons , such as user dissatisfaction with current system capabilities .

according to the sps program manager , the latest economic analysis has not been updated to reflect changes because the analysis is not used for managing the program and because there is no dod requirement for updating an economic analysis when changes to the program occur .

by not ensuring that the program is being proactively managed on the basis of current information about costs , benefits , and risks , dod is unnecessarily assuming an excessive amount of investment risk .

as we have previously reported , incremental investment management involves three fundamental components: ( 1 ) developing / acquiring a large system in a series of smaller projects or system increments , ( 2 ) individually justifying investment in each separate increment on the basis of costs , benefits , and risks , and ( 3 ) monitoring actual benefits achieved and costs incurred on completed increments and modifying subsequent increments or investments to reflect lessons learned .

while dod is acquiring and implementing sps in a series of incremental releases ( originally four and now seven ) , it is not making decisions about whether to invest in each release on the basis of the release's costs , benefits , and risks , and it is not measuring whether it is meeting cost - and - benefit expectations for each release that is implemented .

instead , dod is treating investment in sps as one , monolithic investment decision , justified by a single , all - or - nothing economic analysis .

moreover , dod has not measured whether the incremental software releases have produced expected business value , even though its economic analysis aligns expected benefits with the then four incremental releases .

in june 2000 , the sps program office initiated a study in an attempt to validate the extent to which benefits would be realized as a result of dod's implementation of version 4.1 of the software .

however , our review of the methodology and preliminary results revealed that the study was poorly planned and executed and that , while useful information may be compiled , dod would be unable to use the study's results to validate the accrual of benefits .

as a result , dod will have spent hundreds of millions of dollars on the entire system before knowing whether it is producing value commensurate with cost .

the program manager told us that knowing whether sps is producing such value is not the program office's objective .

rather , its objective is to simply acquire and deploy the system .

similarly , dod cio officials told us that although the economic analysis promised a business value that would exceed costs , dod is not validating that implemented releases are producing that value because there is no dod requirement and no metrics defined for doing so .

by not investing incrementally in sps , dod runs the serious risk of discovering too late ( i.e. , after it has invested hundreds of millions of dollars ) that sps is not cost - beneficial .

dod's management of sps is a lesson in how not to justify , make , and monitor the implementation of it investment decisions .

specifically , dod has not ( 1 ) ensured that accountability and responsibility for measuring progress against commitments are clearly understood , performed , and reported , ( 2 ) demonstrated , on the basis of reliable data and credible analysis , that the proposed system solution will produce economic benefits commensurate with costs before investing in it , ( 3 ) used data on progress against project cost , schedule , and performance commitments throughout a project's life cycle to make investment decisions , and ( 4 ) divided this large project into a series of incremental investment decisions to spread the risks over smaller , more manageable components .

currently , dod is not effectively performing any of these basic tenets of effective investment management on sps , and , as a result , dod lacks the basic information needed to make informed decisions about how to proceed with the project .

nevertheless , dod continues to push forward in acquiring and deploying additional versions of sps .

continuing with this approach to investment management introduces considerable risk .

as a result , beyond possibly operating and maintaining already implemented releases for the remainder of fiscal year 2001 and meeting already executed contractual commitments , further investment in sps has not been justified .

we recommend that the secretary of defense direct the assistant secretary of defense for command , control , communications , and intelligence , as the designated approval authority for sps , to clarify organizational accountability and responsibility for measuring sps progress against commitments and to ensure that these responsibilities are met .

we further recommend that the secretary direct the assistant secretary to make investment in each new release , or each enhancement to an existing release , conditional upon ( 1 ) validating that already implemented releases of the system are producing benefits that exceed costs and ( 2 ) demonstrating on the basis of credible analysis and data that ( a ) proposed new releases or enhancements to existing releases will produce benefits that exceed costs and ( b ) operation and maintenance of already deployed releases of sps will produce benefits that exceed costs .

also , we recommend that the secretary direct the director , program analysis and evaluation , to validate any analysis produced to justify further investment in sps and to report any validation results to the assistant secretary of defense for c3i .

we also recommend that no further decisions to invest in sps be made without these validation results .

additionally , we recommend that the secretary direct the assistant secretary of defense for c3i to take the necessary actions , in collaboration with the sps program manager , to immediately determine the current state of progress against program commitments addressed in this report and to ensure that such information is used in all future investment decisions concerning sps .

last , we recommend that the secretary direct the assistant secretary of defense for c3i to report by october 31 , 2001 , to the secretary and to dod's relevant congressional committees on lessons learned from the sps investment management experience , including what actions will be taken to prevent a recurrence of this experience on other system acquisition programs .

in written comments on a draft of this report ( reprinted in appendix ii ) , the acting deputy assistant secretary of defense for command , control , communications , and intelligence , who is also the dod deputy chief information officer ( cio ) , agreed and partially agreed with our recommendations .

in particular , the deputy cio agreed with our recommendation regarding the need to clarify organizational accountability and responsibility for measuring the program's progress and ensuring that these responsibilities are met .

the deputy cio also agreed to document lessons learned and have the director of program analysis and evaluation validate the results of any ongoing and future analyses of sps' return on investment .

however , the deputy cio disagreed with our report's overall finding that continued investment in sps has not been justified , and disagreed with those elements of our recommendations that could delay development and deployment of sps , specifically , acquiring and using the information we believe is needed to make informed investment decisions .

to support its position , however , the deputy cio offered no new facts or analyses .

instead , the comments either cite information already in our report or claims that the demands of incremental investment management are “inefficient , costly , and overly intrusive” and will cause “unwarranted delays and disruption to the program” for no other reason than “to satisfy economists and accountants.” according to dod's comments , the latest sps economic analysis and the existing efforts to measure progress against selected program commitments provide sufficient bases for continuing to invest hundreds of millions of dollars in sps .

in particular , dod stated that it is making progress in improving its ability to standardize contracting for goods and services , adding that this standardization progress is not only saving operating costs by retiring legacy procurement systems , but is also providing a standard environment within dod for the exchange of information and a consistent look and feel of contract information to companies doing business with the department .

in light of these outcomes , dod commented that one of its main goals under the program is the timely fielding of sps capability .

we disagree with these comments .

as we describe in the report , incremental investment management practices are not only a best practice , but are also required by the clinger - cohen act of 1996 and specified in omb guidance and recently revised dod acquisition policy .

therefore , dod's comments regarding incremental investment in sps are at odds with contemporary practices and operative federal requirements and guidance .

additionally , the economic analysis that dod's comments refer to is not reliable for a number of reasons that are discussed in our report .

specifically , this analysis treats sps as a single , monolithic system investment .

experience has shown that such an all - or - nothing economic justification is too imprecise to use in making informed decisions on large investments that span many years .

this kind of approach for justifying investment decisions has historically resulted in agencies' investing huge sums of money in systems that do not provide commensurate benefits , and thus has been abandoned by successful organizations .

further , the need to avoid this pitfall was a major impetus for the clinger - cohen act investment management reforms .

also , as discussed in our report , the analysis highlights a return - on - investment calculation in its summary that does not include all relevant costs , such as the costs to be incurred by dod components .

instead , the summary uses only sps program office costs in this return - on - investment calculation .

further , this return - on - investment calculation does not reflect known changes in the program's scope and schedule that would increase costs and reduce benefits .

as our report points out , it does not , for example , reflect sps' change from four software releases to seven releases nor does it reflect the improbability of meeting a september 30 , 2003 , full operational capability date .

dod's comments also promote continued spending on sps without sufficient awareness of progress against meaningful commitments , such as reliable data measuring and validating whether return - on - investment projections are being met .

in fact , dod's comments emphasize standardization and fast deployment as core commitments .

however , neither factor is an end in and of itself .

unless sps provides the capability to perform procurement and contracting functions better and / or cheaper , and does so to a degree that makes sps a more attractive investment relative to the department's other investment options , dod will not have adequate justification for investing further in sps .

as our report demonstrates , the department presently does not have the information it needs to know whether this investment is justified , and the information that is available raises serious questions about sps' acceptance by its user community and its business value .

nevertheless , dod's comments indicate its intention to implement sps as planned .

our recommendations are aimed at ensuring that the department obtains the information it needs to make informed sps investment decisions before proceeding with additional acquisitions .

dod provided other clarifying comments that have been incorporated as appropriate throughout this report .

the written comments , along with our responses , are reproduced in appendix ii .

as agreed with your office , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the date of this letter .

at that time , we will send copies to the chairmen and ranking minority members of the senate committee on armed services ; senate appropriations subcommittee on defense ; house armed services committee ; house appropriations subcommittee on defense ; subcommittee on government efficiency , financial management , and intergovernmental relations , house committee on government reform ; and subcommittee on national security , veterans affairs , and international relations , house committee on government reform .

we are also sending copies of this report to the director , office of management and budget ; the secretary of defense ; the acting secretary of the army ; the acting secretary of the navy ; the acting secretary of the air force ; the acting assistant secretary of defense command , control , communications , and intelligence / chief information officer ; the under secretary of defense for acquisition , technology and logistics ; the principal deputy and deputy under secretary for management reform ; the acting director of operational testing and evaluation ; the director of program analysis and evaluation ; the director of defense procurement ; the director of the defense contract management agency ; and the director of the defense logistics agency .

if you have any questions on matters discussed in this report , please call me at ( 202 ) 512-3439 or cynthia jackson , assistant director , at ( 202 ) 512-5086 .

we can also be reached by e - mail at hiter@gao.gov and jacksonc@gao.gov , respectively .

key contributors to this assignment are listed in appendix iii .

our objectives were to determine the progress that the department of defense ( dod ) has made against the standard procurement system ( sps ) program commitments and whether dod has economically justified further investment in sps .

to determine the progress made , we first analyzed relevant legislative and office of management and budget ( omb ) requirements , associated federal guidance , and applicable dod policy and guidance on investment management .

we then analyzed relevant program management documents and interviewed program officials to identify estimates and expectations for sps' cost , schedule , and performance , including the system capabilities to be provided and benefits to be produced by these capabilities .

source documents for this information included , but were not limited to , the acquisition strategy and program baseline , acquisition decision memorandums , and the quarterly defense acquisition executive summary report .

we then reviewed program management reports and briefings , interviewed program officials , and solicited information from the various dod component organizations participating in sps' implementation to determine reported cost , schedule , and performance status .

we compared this information against estimates and expectations to identify any variances .

we did not independently validate the status information that we obtained .

in cases where variances were found or status information was not available , we questioned program management and dod's office of the chief information officer ( cio ) oversight officials .

the dod organizations that were part of our scope of contacts included the sps program office within the defense contract management agency ; the office of the director of investments and acquisition within the office of the assistant secretary of defense for command , control , communications , and intelligence ( c3i ) / chief information officer ; the office of the deputy director ( strategic and space programs ) within the office of program analysis and evaluation under the office of the undersecretary of defense ( comptroller / chief financial officer ) ; the office of strategic and c3i systems within the office of the director of operational test and evaluation ; and various offices within the defense agencies and military services responsible for implementing sps .

to determine whether dod had economically justified sps , we reviewed relevant legislative requirements and associated omb guidance , as well as dod policy and guidance on preparing and using economic analyses ( cost , benefit , and risk ) , to measure progress against information technology ( it ) investment decisions and to do so using an incremental or modular approach .

we then obtained the original economic analyses prepared for the program and the two subsequent updates and evaluated them in light of relevant requirements , policies , and guidance to identify strengths and weaknesses .

we also reviewed program management documents and interviewed program and oversight officials to understand how these analyses were reviewed and used , and we compared the results to relevant requirements and guidance .

we also calculated the program's net present value using the 1997 and 2000 economic analyses .

in addition , we interviewed officials from the sps program office , dod cio's office , and dod's program analysis and evaluation office to discuss our results and seek clarifying information .

we reviewed the methodology and preliminary results for the productivity study being conducted by dod to substantiate the benefits to be realized by implementing sps .

we also interviewed officials from the sps program office , vector research incorporated , and logistics management institute to discuss the methodology ( eg , survey execution , sampling , and analysis plans ) and our conclusions on the study .

we conducted our work at dod headquarters offices in washington , d.c. , and alexandria , virginia , and at american management systems , incorporated , headquarters in fairfax , virginia , from october 2000 through june 2001 in accordance with generally accepted government auditing standards .

1 .

see comments 2 through 9 .

2 .

we did not independently validate dod - reported data on the number of sites and procurement personnel who have received sps training , the number of personnel who are located at sites where some version of sps has been deployed , or the number and dollar value of contract actions completed in fiscal year 2000 using sps ; thus we have no basis to comment on the accuracy of these data .

however , we do not agree with this comment's thrust that these data points , combined with statements about dod's “improving its ability to standardize,” “providing a standard environment,” and providing “a consistent look and feel,” are sufficient measures of progress against commitments .

as the clinger - cohen act and omb guidance emphasize , and as we state in our report , investments in information technology need to contribute tangible , observable improvements in mission performance .

thus , standardization should not be viewed as an end in and of itself , but rather the means to an end , such as increased productivity and reduced costs .

dod's comment on progress does not address such tangible , observable benefits .

instead , dod states that sps is saving operating costs by retiring legacy procurement systems , which , when sps was initiated and justified , were to total 76 systems .

however , as we also state in the report , only two legacy systems have been retired thus far as a result of the system's being deployed to 773 sites , and dod could not provide what , if any , savings were being realized by doing so .

moreover , the number of legacy systems that dod eventually expects to be replaced by sps has decreased to between 12 and 14 .

further , while dod states that sps is providing standardization of contracting for goods and services for a segment of its procurement community , our report points out that each service is either in the process of developing , or has plans to develop , its own unique procurement policies , processes , and procedures .

3 .

for the reasons discussed in our report , we do not agree that dod has justified further sps investment in its 2000 economic analysis .

for example , only the sps costs funded and managed by the sps program office , which are 13 percent of the total life - cycle cost in the analysis , were updated in 2000 .

the costs to be funded and incurred by dod agencies and the military services were not updated to account for all program changes or to incorporate better information .

exacerbating this is the fact that only two cost elements were updated for the dod component organizations in the 2000 economic analysis , and the estimates for these cost elements were based on estimates derived for just one service and extrapolated to all other dod components .

as another example , the analysis does not reflect the reduced number of legacy systems to be retired as well as recent evidence of user non - acceptance and non - use of the system , both of which drive benefit accrual .

we also do not agree that the analysis documented that a $163 million additional investment by the sps program office would result in additional benefits of $389.5 million ( in net present value terms ) .

rather , the analysis shows that acquiring , operating , and maintaining sps over its life cycle will cost about $17 million more , but will produce about $390 million more in benefits ( in net present value terms ) than operating and maintaining legacy procurement systems .

however , the analysis also shows that sps as planned is not a cost - beneficial investment , because estimated costs exceed expected program benefits .

4 .

we do not disagree with dod's comments regarding the major program designation of sps and the many organizations involved in the program .

also , while we agree that sps program officials prepared the acquisition program baseline and have reported quarterly against the commitments that are contained in this baseline , the baseline commitments and the associated reporting do not extend to all the relevant program goals and objectives that we cite in the report as needing to be measured in order to effectively manage a program like sps , such as what the system is actually costing dod and whether promised business value is actually being realized .

additionally , the acquisition program baseline is dated may 4 , 1998 , and thus the commitments in this baseline are out of date .

we do not agree with dod's comments characterizing the timing of the 2000 economic analysis update .

as we state in our report , this update was prepared after the increase in sps funding had been approved .

in fact , the program analysis and evaluation official responsible for reviewing the analysis stated that it was for this reason that the review was perfunctory at best .

5 .

we do not agree with dod's comment that delaying investment in new sps releases or enhancements until dod validates that already implemented releases of the system are producing benefits in excess of costs is contrary to best practice and would delay and disrupt sps in a way that is not warranted .

as we state in our report , available evidence raises serious questions about the cost and benefit implications of users' limited acceptance of already deployed versions as well as the cost implications of dod's limiting its maintenance options to a single vendor .

our point is that answers to these questions are needed in order to make informed investment decisions , and to proceed as planned with new investments without this information risks continuing to invest in the wrong system solution faster .

we agree with the comment that the program office initiated a productivity study in the summer of 2000 .

as we state in our report , this study was undertaken in response to dod inspector general findings that raised questions about user acceptance of the system .

however , we do not agree that this study will substantiate the sps benefit estimates and quantitatively document the benefits of sps implementation through 2000 because the study's scope and methodology are limited .

for example: according to the program official responsible for the study , the purpose of the study is to estimate expected benefits to be realized in fiscal year 2003 , from implementation of version 4.1 .

the sample selected was not statistically valid , meaning that the results are not projectable to the population as a whole .

relative to the other services , the air force was not proportionally represented in the study , meaning that any results would not necessarily be reflective of air force sites .

the study was based on the 1997 economic analysis instead of the more current 2000 economic analysis despite key differences between the two analyses .

for example , the 1997 analysis shows 22 benefits valued at approximately $1.8 billion over the program's 10-year life cycle , while the 2000 analysis contains only 19 benefits valued at approximately $1.4 billion .

according to sps program officials , the survey instrument was not rigorously pre - tested .

such pre - testing is important because it ensures that the survey ( 1 ) actually communicates what it was intended to communicate , ( 2 ) is standardized and will be uniformly interpreted by the target population , and ( 3 ) will be free of design flaws that could lead to inaccurate answers .

the information being gathered does not map to the 22 benefit types listed in the 1997 sps economic analysis .

instead , the study is collecting subjective judgments that are not based on predefined performance metrics for sps capabilities and impacts .

thus , dod is not measuring sps against the benefits that it promised sps would provide .

in addition , the senior official responsible for sps implementation in the air force stated that the air force plans to conduct its own , separate survey to determine whether the system is delivering business value , indicating component uneasiness about the reliability of the sps program office's study .

6 .

we disagree .

as we state in the report , incremental investment management practices are not only a best practice , but are also required by the clinger - cohen act of 1996 and specified in omb guidance and recently revised dod acquisition policy .

therefore , dod's comments regarding incremental investment in sps are at odds with contemporary practices and operative federal requirements and guidance .

additionally , the economic analysis that dod's comments refer to is not reliable for a number of reasons that are discussed in our report .

specifically , this analysis treats sps as a single , monolithic system investment .

experience has shown that such an all - or - nothing economic justification is too imprecise to use in making informed decisions on large investments that span many years .

this kind of approach to justifying investment decisions has historically resulted in agencies investing huge sums of money in systems that do not provide commensurate benefits , and thus has been abandoned by successful organizations .

further , the need to avoid this pitfall was a major impetus for the clinger - cohen act investment management reforms .

dod's comments also promote continued spending on sps without sufficient awareness of progress against meaningful commitments , such as reliable data measuring and validating that return - on - investment projections are being met .

in lieu of such measures , dod's comments emphasize standardization and fast deployment as core commitments .

however , neither of these is an end in and of itself .

unless sps provides dod with the capability to perform procurement and contracting functions better and / or cheaper , and does so to a degree that makes sps a more attractive investment relative to the department's other investment options , dod is not justified in investing further in sps .

as our report demonstrates , and as discussed in comments 2 and 3 above , dod presently does not have the kind of reliable information it needs to know whether this investment is justified , and the information that is available raises serious questions about sps' acceptance by its user community and its business value .

with regard to the timely fielding of sps , we note in our report that the program has already been delayed 3-1 / 2 years .

in fact , delivery of version 4.1 of the software was 22 months overdue , and version 4.2 is already 5 months behind .

while the impact of schedule delays and cost increases is a valid concern on any project , these factors are not the sole criteria .

introducing the wrong system solution faster and cheaper is still introducing the wrong solution no matter how it is presented .

it is thus critically important that investment decisions be based on an integrated understanding of cost , benefit , and risk .

7 .

we do not dispute that the cited events have occurred , although we would add for additional context that we met with assistant secretary of defense for command , control , communications , and intelligence ( c3i ) officials on march 15 , 2001 , the day before the memorandum requesting the first program review , to share our concerns and seek clarification , and that we provided our draft report to dod for comment on may 25 , 2001 .

we do not agree with dod's comment that it is not necessary to have the secretary of defense direct the assistant secretary of defense for c3i ( dod cio ) to determine the current state of sps progress against commitments and to ensure that this information is used in future investiment decisions for several reasons .

first , the recent reviews cited in the dod comments were for the defense contract management agency , which is the component acquisition executive , and the office of the director of defense procurement , which is the sps functional sponsor .

neither of these entities is the dod cio , who is the designated decision authority for sps milestones and thus under sps' management structure has ultimate accountability for sps .

second , the recent reviews cited in dod's comments did not satisfy our recommendation for determining the current state of progress against the sps commitments described in our report .

in fact , we attended the april 27 , 2001 , review meeting , during which the senior attending official from the defense contract management agency stated that information being provided at this meeting was insufficient from a program management standpoint , lacking key information needed for informed sps decision - making .

third , the march 16 , 2001 , memorandum cited in dod's comments acknowledges the need to update sps' economic justification in light of the program's cost and schedule changes and to ensure compliance with clinger - cohen act requirements .

fourth , the sps program manager's planned actions to respond to recent reviews are not sufficient to address the uncertainties surrounding sps .

according to the program manager , the acquisition program baseline would be updated to reflect the most recent program costs and expected schedule for full operational capability , but the program office had not planned any other actions .

last , dod's comment stating that the office of the dod cio and the office of the director of defense procurement plan to conduct an independent review of sps within the next 180 days does not satisfy our recommendation because ( 1 ) dod's schedule for sps calls for issuing contract task orders for subsequent sps releases during this 6-month period and ( 2 ) this commitment is only a vague statement to “plan to conduct” a review at some undetermined , potentially distant , future point in time rather than having a review scheduled to occur in time to effect meaningful investment management improvements .

in light of dod's comments regarding this recommendation and for the reasons discussed above , we have modified our recommendation to specify that the recommended determination of the state of progress should occur immediately and should address each of the program commitments discussed in this report .

8 .

we acknowledge dod's agreement with the recommendation , but note that neither our recommendation nor dod's comment specifies when this report would be prepared .

accordingly , we have modified our recommendation to include a timeframe for reporting to the secretary of defense and relevant congressional committees on lessons learned and actions to prevent recurrence of those sps experiences on other system acquisition programs .

additionally , we disagree with dod's comments about the findings and conclusions in our report .

in our view , the totality of evidence presented in our report , along with the results of prior defense inspector general reviews , supports our conclusion that sps is a lesson in how not to justify , make , and measure implementation of investment decisions .

also , as addressed in comments 2 and 3 , we do not agree with dod's point that sps has been justified by the 1997 and 2000 economic analyses .

last , we do not agree with dod's comments that we incorrectly calculated a negative return on investment for sps and that our methodology for calculating net present value is incorrect .

to calculate net present value , we used current omb guidance , which requires that relevant life - cycle cost estimates be used .

additionally , we used dod's own life - cycle cost estimates from its economic analyses .

while we acknowledge that sps officials told us that these life - cycle cost estimates included the costs of operating legacy procurement systems , we also requested that these officials identify what these legacy system costs are so that we could back them out .

however , sps officials told us that they did not know the amount of these costs .

as a result , our calculation is based on the best information that the sps program office had available and could provide .

9 .

see comments 3 and 8 .

also , we agree that applying our net - present - value calculation methodology to the sps and status quo cost - and - benefit data provided in the january 2000 economic analysis show that sps is cheaper than the status quo option .

however , this calculation also shows that sps as planned is not cost beneficial .

also , dod's comments compare only a small portion of sps life - cycle costs ( program office investment costs ) against the difference between expected benefits under the sps scenario and the status quo scenario .

this comparison is illogical because it assumes that an arbitrary part of relevant investment costs can be associated with the total benefit difference between alternatives .

accordingly , we do not agree with dod's comment .

while appendix e of the january 2000 economic analysis contained some of the information provided in the tables contained in dod's comments , it did not provide a net present value calculation .

further , the appendix e tables were not included in the economic analysis' executive summary .

instead , the summary provided a benefits - to - costs ratio that excluded certain relevant costs .

in addition to the person named above , nabajyoti barkakati , harold j. brumm , jr. , sharon o. byrd , james m. fields , sophia harrison , james c. houtz , richard b .

hung , barbarol j. james , and catherine h. schweitzer made key contributions to this report .

standard procurement system use and user satisfaction , office of the inspector general , department of defense ( report no .

d - 2001-075 , march 13 , 2001 ) .

defense management: actions needed to sustain reform initiatives and achieve greater results ( gao / nsiad - 00-72 , july 25 , 2000 ) .

department of defense: implications of financial management issues ( gao / t - aimd / nsiad - 00-264 , july 20 , 2000 ) .

defense management: electronic commerce implementation strategy can be improved ( gao / nsiad - 00-108 , july 18 , 2000 ) .

initial implementation of the standard procurement system , office of the inspector general , department of defense ( report no .

99-166 , may 26 , 1999 ) .

financial management: seven dod initiatives that affect the contract payment process ( gao / aimd - 98-40 , january 30 , 1998 ) .

allegations to the defense hotline concerning the standard procurement system , office of the inspector general , department of defense ( report no .

96-219 , september 5 , 1996 ) .

