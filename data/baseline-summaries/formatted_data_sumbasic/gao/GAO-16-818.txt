grants are an important tool the federal government uses to achieve national objectives .

the federal government spends more than $600 billion a year on federal grants to state and local governments , nonprofits , and educational institutions to fund a wide range of programs and services , including those related to social services , education , and health care .

to better integrate evidence and rigorous evaluation in budget , management , operational , and policy decisions , the office of management and budget ( omb ) has encouraged federal agencies to expand or improve the use of grant program designs that focus federal dollars on effective practices while encouraging innovation in service delivery .

as part of this effort , several federal agencies have implemented tiered evidence grant programs .

under this approach , agencies establish tiers of grant funding based on the level of evidence of effectiveness provided for a grantee's service model .

agencies award smaller amounts to promising service models with a smaller evidence base , while providing larger amounts to those with more supporting evidence .

tiered evidence grantees are also generally required to evaluate their service models as a condition for the receipt of grant funds .

proponents of tiered evidence grants contend that they create incentives for grantees to use approaches backed by strong evidence of effectiveness , encourage learning and feedback loops to inform future investment decisions , and provide some funding to test innovative approaches .

the evidence and evaluation requirements for tiered evidence grants represent a deliberate approach to using evidence that may require different policies , practices for outcome and performance measurement , and capacities for agencies and grantee organizations .

we are required to review the implementation of the gpra modernization act of 2010 ( gprama ) at several critical junctures .

this report is part of our response to that mandate .

the objectives of this report are to describe ( 1 ) key features that tiered evidence grants add to federal grant processes , ( 2 ) benefits and challenges of using tiered evidence grants , and ( 3 ) key factors to facilitate the use of tiered evidence grants , and ( 4 ) to assess the extent to which omb and federal agencies are collaborating on evidence and evaluation requirements in tiered evidence grants .

to meet our objectives , we identified the universe of domestic - focused tiered evidence grant programs that were established prior to 2013: the department of education's ( education ) investing in innovation fund , the department of health and human services ( hhs ) teen pregnancy prevention program and maternal infant , and early childhood home visiting ( federal home visiting ) program , the department of labor's ( dol ) workforce innovation fund , and the corporation for national and community services ( cncs ) social innovation fund .

we identified these programs by conducting a literature review and by reviewing annual agency budget documents .

omb verified that the five programs we identified represented the universe of tiered evidence grant programs meeting our selection criteria .

to address the first three objectives , we reviewed and analyzed relevant documents from the five grant programs — such as notices of funding availability , evaluation reports , and agency guidance — and interviewed agency officials .

to obtain insights on grantee perspectives , we selected three of the five grant programs: education's investing in innovation fund , hhs's teen pregnancy prevention program , and dol's workforce innovation fund .

we interviewed two to three grantees from each funding level for each program .

we conducted these interviews in three metropolitan areas: washington , d.c. / baltimore , new orleans , and the san francisco bay area .

to address the fourth objective , we interviewed officials from education , dol , hhs , and cncs , as well as omb staff on current collaboration mechanisms and any future plans for collaboration .

for criteria , we used our prior work on key practices for enhancing and sustaining collaboration and implementing interagency collaborative mechanisms .

we also held a facilitated discussion with officials from the five grant programs we reviewed .

the discussion focused on the challenges in using tiered evidence grants , the key factors for agencies to consider in addressing these challenges , and interagency collaboration on tiered evidence grants .

we conducted this performance audit from september 2015 to september 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

according to omb's 2013 memorandum on the next steps in the evidence and innovation agenda …evidence includes evaluation results , performance measures , and other relevant data analytics and research studies , with a preference for high - quality experimental and quasi - experimental studies .

since 2010 , omb has used its annual budget guidance to encourage federal agencies to use evidence to consider the effectiveness of their programs and to institutionalize the use of evidence to foster innovation rooted in research and rigorous evaluation .

in a 2013 memorandum , omb identified areas in which agencies could improve their use of evidence ( see sidebar ) , including ( 1 ) strengthening evaluation capacity and proposing new evaluations ; ( 2 ) developing high - quality , low - cost evaluations and rapid , iterative experimentation ; ( 3 ) using innovative outcome - focused grant design ; and ( 4 ) increasing agency capacity to use evidence .

in 2015 , omb directed agencies to provide an overview of evidence - building strategies and identify related priorities with their fiscal year 2017 budget submissions .

to incorporate evidence into grantmaking , the white house and omb have encouraged agencies to use tiered evidence grants , which link funds to an evidence framework .

when applying for grants , grantees demonstrate the strength of evidence on their service models and agencies award funding based on the level of evidence ( see figure 1 ) .

smaller awards are used to test new and innovative service models ; larger awards are used to scale service models with strong evidence .

grantees generally evaluate their service model during the grant period .

as a result , tiered - evidence grant programs have a goal of identifying evidence - based service models that can be replicated .

according to an omb memorandum on evidence and innovation , the goal is that , over time , service models move up tiers as evidence becomes stronger .

omb staff said that other goals of tiered evidence grants include devoting more resources to service models with stronger evidence , allowing for testing of innovations , and encouraging rigorous evaluation .

from fiscal year 2010 to 2015 , agencies have awarded approximately $4.1 billion to the tiered evidence grants we reviewed .

as shown in figure 2 , each agency uses its own names and descriptions for its evidence tiers .

while education , dol , and cncs use a three - tier model , hhs uses a two - tiered model , where the agency makes a larger overall investment in replicating evidence - based practices by awarding more grants in its high tier .

for example , hhs awards 75 percent of funds to replicating service models with moderate or strong evidence for the teen pregnancy prevention program , and awards 25 percent of funds to rigorously evaluate promising approaches .

while there is substantial variation among grant types , competitively awarded federal grants generally follow a life cycle of pre - award ( announcement and application ) , award , implementation , and closeout .

to implement tiered evidence grants , the four agencies in our review add evidence and evaluation requirements throughout the federal grant life cycle .

these added requirements include: ( 1 ) assessing the evidence base and identifying evidence - based approaches , ( 2 ) implementing evidence - based approaches with fidelity , ( 3 ) conducting independent evaluations , and ( 4 ) disseminating evaluation results , as shown in figure 3 .

the four federal agencies in our review had varied approaches to identify evidence - based approaches for tiered evidence grants .

education , dol , and cncs first developed definitions of each evidence tier that grantees must meet to qualify for a grant award as well as the program priorities for the grant competition .

evidence definitions included requirements , such as the type and number of studies on the service models , as well as the scale of service models studied .

the rigor and scale of the evidence requirements increased from tier to tier .

for example , preliminary evidence tier .

cncs's social innovation fund's preliminary evidence tier was designed for service models that have preliminary evidence based on a reasonable hypothesis supported by credible research findings .

moderate evidence tier .

education's investing in innovation fund's moderate evidence tier required at least one experimental study of effectiveness of the grantees' chosen service model , or one quasi - experimental study that was conducted on a large and multi - site sample .

grantees could validate their service model by testing the service model in a new location , testing it on a new or bigger population , or tweaking an aspect of their service to see if it still works on their target population .

strong evidence tier .

dol's workforce innovation fund's strong evidence tier was designed to support significant expansion of service models that had previously demonstrated strong evidence of positive results and will be evaluated using a randomized controlled trial evaluation .

for hhs , the evidence tiers for the teen pregnancy prevention program are defined in statute .

for example , in the appropriation providing funding for the teen pregnancy prevention program for fiscal year 2016 , 75 percent of amounts provided for the grant were to be used for a strong evidence tier to replicate programs that have been proven effective through rigorous evaluation to reduce teenage pregnancy , and behavioral risk factors underlying teenage pregnancy .

the other 25 percent were to be used for a preliminary evidence tier to conduct research and demonstration projects to develop and test additional models and strategies .

after the four agencies in our review defined the evidence tiers , they used the standards to assess the available evidence on the service models .

the agencies varied in terms of when they assessed the evidence: before announcement of grant opportunity .

for the teen pregnancy prevention program and the federal home visiting program , hhs reviewed the available evidence on teen pregnancy prevention and home visiting models before it announced the grant opportunities and identified a list of approved evidence - based models .

tier one ( strong evidence tier ) grantees that replicated evidence - based models selected an evidence - based model from the approved list that met the needs of their target populations when applying for the grant .

as of july 2016 , the teen pregnancy prevention program included 44 service models and the federal home visiting program included 17 service models .

hhs periodically updates the approved models .

during review of grant applications .

grantees of dol's workforce innovation fund and education's investing in innovation fund identified in their grant applications the evidence - based service models they planned to use .

dol and education officials assessed the evidence that supported the service models when they reviewed the grant applications and make award decisions .

for example , education's institute of educational sciences' what works clearinghouse — the entity within education that is focused on research and evaluation — reviewed the citations of the studies that supported investing in innovation fund grant applications to determine whether they met the evidence requirements .

after grant award is made .

for the social innovation fund , cncs made awards to intermediary organizations , which then made awards to subgrantees to implement service models .

cncs provided the intermediary organizations with guidance on the levels of evidence , which they used to assess the evidence for their subawards .

cncs reviewed and confirmed the intermediaries' decisions before they made the final awards .

grantees were responsible for delivering their evidence - based service models with fidelity — in a manner consistent with how the program was designed .

for example , in its guidance to teen pregnancy prevention program grantees , hhs defined fidelity as the degree to which a program is implemented with adherence to its core components – the key ingredients related to achieving the outcomes associated with the program model .

the core components included a model's content and how it is delivered .

fidelity is important because delivering a service model as it was designed increases the likelihood that the participants served will experience similar positive outcomes to those found in the original evaluation study .

the five grant programs we reviewed had evaluation requirements , but they generally had two different evaluation approaches .

for dol , education , and cncs tiered evidence grants , every grantee was required to contract with a third party evaluator to conduct an independent evaluation of its service model and submit it to the respective federal grantmaking agency at the end of the grant period .

dol also commissioned a crosscutting analysis of the findings from , and experiences with , the evaluations of the projects under the workforce innovation fund .

for hhs , the federal home visiting and teen and pregnancy prevention programs used different evaluation approaches for each tier that included both grantee and federal government - led evaluations .

for the preliminary evidence tier to test promising approaches of both grant programs , grantees were required to conduct an independent evaluation .

for the strong evidence tier of the federal home visiting program , grantees receiving competitive awards or tribal grants were also required to conduct grantee - led evaluations.also , state formula grantees are encouraged to conduct grantee - led evaluations .

for the strong evidence tier to replicate evidence - based service models of the teen pregnancy prevention program , all grantees receiving a grant award of greater than $1 million were required to conduct an independent evaluation .

hhs also is conducting programwide evaluations of both grant programs ( see sidebar ) .

when conducting independent evaluations , grantees we reviewed hired an independent evaluator to conduct evaluations .

this work included developing an evaluation plan , collecting and analyzing data , and completing an evaluation report to communicate the service model results .

the four federal agencies we reviewed contracted with a separate external national evaluation coordinator to provide technical assistance to the independent evaluators and grantees on their evaluations .

the national evaluation coordinators developed guidance on evaluations , reviewed evaluation plans , provided ongoing assistance on conducting the evaluations , and provided feedback on the final reports .

the four agencies we reviewed encouraged grantees to conduct rigorous evaluations , such as randomized controlled trials or studies using quasi - experimental designs .

we have previously reported that when developing an evaluation design , an evaluator should select appropriate measures and comparisons that will permit valid conclusions .

the evaluator should explore a variety of options available to collecting and analyzing information , and choose alternatives that will best address the evaluation objectives within available resources .

during and after the grant closeout stage , the four agencies we reviewed disseminated the evaluation results to various audiences .

for example , education and cncs posted links to completed evaluations from the investing in innovation fund and the social innovation fund , respectively , online .

grantees in our review told us that they also took steps to communicate their evaluation results by , for example , posting links to them on their websites and presenting at conferences .

grantees can use their evaluation results to support applications for future tiered evidence grant awards .

the teen pregnancy prevention program and the federal home visiting program included grantee evaluations in their evidence reviews .

officials that we interviewed from the four agencies in our review identified the following potential benefits of tiered evidence grants: provide incentives to implement service models that are supported by evidence .

according to omb staff , tiered evidence grants provide more flexibility than some traditional grants — such as those for which program models are defined in statute or regulation .

tiered evidence grants target funds to the most effective interventions based on the evidence of the service model's effectiveness without being limited to a particular intervention or operating model .

agencies generally define outcomes for the target population in the notice of funding availability .

grantees use service models with the most robust evidence base to meet a program's desired outcomes .

further , tiered evidence grants create incentives for grantees to use evidence - based service models because they award larger funding amounts to approaches with more evidence and grantees may be motivated to use evidence - based models because of the evidence supporting their outcomes .

build the evidence base .

tiered evidence grants generally require grantees to conduct rigorous , independent evaluations , which will contribute to the evidence base for addressing a given social challenge .

tiered evidence grant awards play a role in expanding the research on service models to include new geographic regions or variations in the target population and in the services themselves .

these evaluations will be added to agencies' databases of research studies ( see text box ) .

they are available to the public , policymakers , and service organizations to inform future efforts and contribute to the increased use of evidence - based models over time .

build grantee capacity for evaluation .

experience with tiered evidence grants and third - party evaluations can help grantees build capacity for evaluation .

according to a national assessment of the social innovation fund , there has been substantial improvement in grantees' internal evaluation capacity over time .

apply evidence and evaluation requirements to other grant programs .

three of the four the agencies in our review applied evidence and evaluation requirements from the tiered evidence grant model to other grant programs .

for example , cncs has incorporated evidence requirements from the social innovation fund into its existing americorps state and national grant program .

americorps increased the total number of points assigned to evaluation and evidence effectiveness in the review and scoring process for the service model proposed by the grant applicants .

education amended its education department general administrative regulations , which outline requirements for its grant programs , to include definitions of four levels of evidence , priorities , and selection criteria related to evidence and evaluation .

education officials said that any program office can use the definitions to add evidence or evaluation requirements to their competitive grants in a manner consistent with the purpose and goals of the program .

education has also developed a process to consider the use of evidence and evaluation requirements in competitive grants when developing its annual spending plan ( see textbox ) .

education's use of evidence in grant competitions according to education officials , the agency is building on lessons from the investing in innovation fund and increasing its use of evidence and evaluation requirements in other competitive grants , as appropriate .

education has an agency priority goal to increase the use and generation of credible evidence on what works and what does not work in education .

by september 30 , 2017 , education aims for at least 20 percent of new competitive grant dollars to support evidence - based strategies .

also , by september 30 , 2017 , education will increase by 20 the number of education - funded project evaluations that provide credible evidence about what works in education .

in fiscal year 2016 , education encouraged program offices in building their spending plans to use evidence and evaluation definitions in agency regulations to the fullest extent practicable .

in spending plans , program offices should identify competitions where it would be appropriate to ask applicants to ( 1 ) submit evidence of effectiveness in support of their proposed projects , or ( 2 ) demonstrate a plan for rigorously evaluating their proposed project once it is implemented .

evidence and evaluation requirements add a layer of activities to tiered evidence grants compared to some other federal grants .

officials from the five federal grant programs and 18 grantee organizations in our review reported that they faced challenges in implementing aspects of the evidence and evaluation requirements .

at our facilitated discussion on tiered evidence grants , agency officials agreed that the primary challenges are: identifying evidence - based service models , implementing them with fidelity , communicating evaluation results .

as discussed below and in the following section , the four agencies we reviewed took steps to address challenges in implementing evidence and evaluation requirements .

agency officials agreed at our facilitated discussion that grantees faced challenges in their capacity to understand the evidence base and agencies faced challenges in their capacity to review grant applications .

grantee capacity to understand the evidence base .

at our facilitated discussion , agency officials agreed that some grantees did not have the technical skills and infrastructure to understand the evidence base and select the evidence - based model that would best fit their target populations .

for example , an official at our facilitated discussion said grantees faced challenges in selecting an evidence - based model that was a good fit for their communities .

when a model did not fit a community , grantees had to make many adaptations to the evidence - based model .

as a result , grantees were less likely to achieve the greatest possible impact for the population because the service model was not implemented as it was designed .

agency capacity to review grant applications .

the agencies that managed the five grant programs in our review also faced challenges in reviewing the evidence and evaluation portions of grant applications .

the agencies generally used panels of expert reviewers and program staff to review grant applications .

reviewing evidence and evaluation requires specific technical knowledge and skills , such as reviewers that have experience with rigorous evaluation .

education officials told us that the investing in innovation fund has used reviewers with evaluation expertise to review and score the evaluation criteria .

they said that given the success of using these reviewers , the investing in innovation fund officials work with other education programs that have rigorous evaluation requirements to help structure their peer reviews so they may also use these evaluation experts .

also , three of the five grant programs in our review ran multiple grant competitions simultaneously — one for each tier — which required additional capacity from their program and evaluation offices .

further , during our facilitated discussion , agency officials said that it is challenging to provide technical assistance sufficient to meet the varied needs of applicants during the application phase .

for example , the officials told us that technical assistance must be broad so it is applicable to all organizations that are applying for grants .

they said that while one - on - one technical assistance would benefit applicants , their agencies lack the capacity to help every single applicant , and it is challenging for those agencies to ensure that similar technical assistance is available to all applications that need it .

in many cases , intensive pre - application technical assistance is not permissible under grants policy , as it would be impossible to ensure that no one applicant has a competitive advantage .

agency officials at our facilitated discussion agreed there are two potential challenges with maintaining fidelity for evidence - based service models: grantees delivering service models as intended .

at our facilitated discussion , agency officials said that grantees faced challenges in collecting quality data and using the data to monitor whether they are implementing their service model with fidelity .

some of the grantees we interviewed said that to monitor fidelity , they had to build new systems to collect data — such as attendance data — on how their service models were implemented .

in addition to using data to monitor fidelity , agency officials that participated in our facilitated discussion said that grantees faced challenges in using data during the grant period to improve the implementation of evidence - based service models .

for example , officials said some grantees still see performance data simply as a reporting requirement , and do not use the data to make decisions about their programs .

managing adaptations to service models .

within tiered evidence grants , grantees generally deliver existing service models to new groups of individuals , at times in a new location with varying degrees of similarity to the population studied in the original evaluation .

therefore , grantees may need to make adaptations to the evidence - based model to best serve their population .

for example , a teen pregnancy prevention program grantee we interviewed said that its organization made four adaptations to the evidence - based model selected because the model was studied in the 1990s and the curriculum was out of date .

the adaptations included updating information on hiv / aids for medical accuracy , modifying curriculum scenarios to be more inclusive and culturally relevant , adding basic sexual health information to reach a younger population , and changing when the program would be delivered from a weekend program to during the school day .

at our facilitated discussion , agency officials said there was a tradeoff between implementing the evidence - based model with fidelity and adjusting the model to respond to new information or specific circumstances .

if adaptations compromise the core components of a model , the grantees may be unable to attribute their results to the service model and may not have comparable outcomes .

at our facilitated discussion , one agency official explained that this is challenging because grantees are accustomed to serving people as their highest priority , and their instincts are to make changes to meet the needs of their populations without considering how the changes would affect implementation fidelity or the evaluation of their service models .

model owners can also play a role in managing fidelity of a service model ( see text box ) .

service model owners add a layer of complexity to tiered evidence grants department of health and human services ( hhs ) officials said the implementation and scale - up of evidence - based models is complex because it involves model developers that own the evidence - based models .

grantees pay a fee to implement the model .

the model developers generally provide grantees with the content of the service model , training , and technical assistance .

hhs officials said that working with model developers has created some challenges for tiered evidence grants .

because they have a role in approving adaptations and have an incentive to approve adaptations , grantees would be more likely to use their models again in the future .

there is a need for the federal agencies to have a role in approving adaptations and enhancements to provide another level of review and oversight .

for example , the federal home visiting program changed its oversight process so hhs approves any adaptations in addition to the model developers' approval .

teen pregnancy prevention program grantees we interviewed also described challenges working with model developers including additional reporting requirements .

one grantee also said that it would be helpful for hhs to negotiate fees in advance of the grant announcement so each grantee does not need to individually negotiate fees with the model developer .

all of the grantees included in our review reported that they faced challenges in fulfilling requirements for rigorous evaluation in tiered evidence grants , for example , when planning for their independent evaluations .

identifying and hiring an independent evaluator .

some grantees had not previously worked with an independent evaluator and were not familiar with the qualifications they should look for in an evaluation .

for example , they faced challenges in developing a description of the requirements and hiring an evaluator with the appropriate experiences and skill set .

state and local governments also found it difficult to procure an evaluator within the grant's timeframe .

developing an evaluation plan , including choosing the appropriate evaluation type .

in some cases , grantees are required to submit an evaluation plan as part of their grant application .

at our facilitated discussion , agency officials said grantee capacity to develop an evaluation plan varies widely and some grantees lack understanding of what evaluations are and the related planning involved .

grantees may need to work with independent evaluators on their evaluation plans in advance of submitting their grant applications .

however , grantees may not have the financial resources to obtain evaluator assistance with grant applications .

grantees we interviewed that had an existing relationship with an evaluator said that evaluators' assistance in developing their grant applications facilitated their ability to apply for a tiered evidence grant award .

both grantees and evaluators faced challenges in choosing the evaluation type best suited to the service model .

at our facilitated discussion , agency officials said that some evaluators used the evaluation type with which they had the most experience instead of the type that would be most appropriate .

budgeting for evaluations .

at our facilitated discussion , agency officials agreed that it can be challenging for some grantees to budget for evaluations because they want to dedicate as much funding as possible to serving their target populations .

one agency official also noted that grantees may be unfamiliar with the resources needed to conduct rigorous evaluations , such as randomized controlled trials and quasi - experimental designs .

we have previously reported that randomized controlled trial evaluations are often difficult to carry out because evaluators must be able to maintain a treatment and control group .

similarly , officials from agencies we reviewed , and some grantees , also described challenges that were specific to conducting a randomized controlled trial evaluation .

for example , some grantees from the programs in our review face the following challenges: recruiting sufficient numbers of participants for a treatment and control group .

some grantees faced recruitment challenges because they needed to recruit twice the number of people they planned to serve to maintain a control group for a randomized controlled trial .

one grantee said it is particularly challenging to recruit participants for a randomized controlled trial because participants must agree to participate in the programing knowing there is a chance they will not receive the new service and instead receive the traditional services offered .

retaining participants for data collection .

some grantees experienced challenges in retaining participants to collect data for the evaluation after the service delivery ended , particularly for the control group .

for example , for a teen pregnancy prevention program grantee , many of the participants graduated from high school and moved away during the evaluation period .

this created challenges for the grantee to locate participants and administer follow - up surveys .

navigating potential ethical issues with using a control group .

some grantees faced challenges in managing potential ethical issues in using a control group .

we have previously reported that in randomized controlled trials there can be concerns that the control group should not be harmed by withholding needed services .

a workforce innovation fund grantee expressed concerns about using a control group and possible backlash from the community for withholding the service from individuals in the control group who need the services .

the grantee said it planned to advertise the program as a lottery to help address this challenge .

grantees also faced systemic challenges to conducting evaluations: accessing administrative data .

some grantees that we interviewed reported encountering difficulties obtaining administrative data collected by federal , state , or local agencies to measure outcomes for the evaluation because of legal or privacy reasons .

for example , a workforce innovation fund grantee said that it was challenging or not possible to get access to administrative data , including unemployment data and wage data , from the states where it was implementing its service model .

challenges in accessing administrative data have led to grantees obtaining information for some states and not others , leading to uneven reporting .

we have previously reported on challenges agencies and stakeholders face in using administrative data to conduct evaluations of programs .

external policy changes .

some grantees faced challenges resulting from external policy changes because they potentially prevented a tiered evidence grant award from operating as initially intended .

for example , education officials and investing in innovation fund grantees said that changes in state - level student assessment policies created challenges to conducting rigorous evaluations .

an investing in innovation fund grantee told us that its analyses for its evaluation were complicated by changes in the state testing regime in the state of california .

conducting evaluations within a 3- to 5-year grant period .

tiered evidence grants generally have an initial planning year , up to 3 years to implement the service model , and an additional year for data analysis and reporting .

agency officials agreed during our facilitated discussion that it can be challenging to conduct an evaluation within this 3-5 year grant period because some data may be unavailable or not measured until some timeframe after the service model is delivered .

for example , one dol workforce innovation fund grantee used unemployment insurance wage record data to measure the employment and earnings outcomes of participants .

according to dol officials , unemployment wage record data typically are unavailable for two to three quarters after they are reported .

so , for later cohorts of participants , there was not enough time after program completion to obtain available data before the end of the 5 year grant period .

as a result , the grantee's evaluation will not include unemployment data for the participants in the later cohorts .

education officials also described challenges in conducting evaluations during the grant period for the investing in innovation fund and said the department has taken steps to address this challenge .

in 2013 , education updated its education department general administrative regulations to allow grant periods for competitive grants to be extended beyond 5 years with funding for data collection and analysis .

education approves extensions on a case - by - case basis when extending the project will result in better data and higher quality evaluation .

for example , according to education officials , a 2010 investing in innovation fund grantee is implementing a service model in middle and high schools to increase high school graduation rates .

education awarded an evaluation grant award to continue data collection and analysis for an additional four years beyond the original grant period to track students' high school completion and graduation rates .

at our facilitated discussion , agency officials generally agreed that their agencies also faced challenges in overseeing and providing technical assistance to tiered evidence grant programs .

the agencies provided extensive coaching and technical assistance to walk grantees through the evidence and evaluation requirements in tiered evidence grants , which requires additional capacity to implement evaluation requirements compared to other federal grant programs .

at the facilitated discussion , agency officials emphasized that agency grant officers for tiered evidence grants need both knowledge of evidence and evaluation as well as the ability to provide ongoing coaching to grantees to help them meet the evidence and evaluation requirements .

the officials said that in some cases agencies faced challenges in shifting the culture among their grant officers from a compliance orientation to the addition of a coaching model of oversight to help build grantees' capacity .

officials from the four agencies in our review expressed concern about how to accurately summarize and communicate complex and nuanced evaluation findings to different audiences .

confusion around evaluation results .

at our facilitated discussion , officials agreed that there is confusion among policymakers around evaluation results of randomized controlled trials or quasi - experimental studies .

these evaluation types generally measure several specific outcomes and can show a positive , negative , or null finding .

a positive finding occurs when being in the treatment group is associated with a positive outcome , relative to control group .

a negative finding occurs when being in the treatment group is associated with a negative outcome , relative to control group .

a null finding occurs when there is no statistical difference in outcome between treatment and control groups , which may occur when a sample size is too small to be statistically significant .

mixed results , where some findings are positive , some are negative and some are null , are also common .

at our facilitated discussion , agency officials expressed concerns that policymakers may use negative , null , or mixed evaluation results from one grant award as a justification for discontinuing an entire tiered evidence grant program .

they told us that there is a risk that policymakers will interpret evaluation results based on one outcome to mean that a service model is or is not working .

however , the officials said that evaluation results can be nuanced , and negative , null , and mixed findings can be used to consider what led to the findings and to adjust the service model to improve future results .

similarly , at our facilitated discussion , agency officials expressed concerns about the risk of a single study being interpreted to represent outcomes of an entire tiered evidence grant program .

further , the officials said it is a challenge to communicate that null or negative findings have a valuable role in building the evidence base because they can identify areas where a model can be altered to improve outcomes for its target population .

one official told us that before they did not know whether the service models were working and at least now they will know what does not work .

expectations about grantees moving up evaluation tiers .

another challenge is the expectation from policymakers that grantees will move up from one evidence tier to the next evidence tier .

for example , according to agency officials that attended our facilitated discussion , there is an expectation that a grantee that receives a preliminary evidence tier award will qualify for a moderate evidence award at the end of the grant period .

in practice , evaluation results have led to few of the hundreds of tiered evidence grantees moving up tiers , in part , because a single evaluation may not be enough to qualify for highest level of evidence tiers .

moreover , in some cases , grant recipients moved down tiers .

such was the case with education's investing in innovation fund , where some grantees applied for a lower tier in subsequent rounds to test a variation or adaptation of their existing service model .

for example , a teacher professional development service model focused content on reading moved from the validation ( moderate evidence ) tier to the development ( preliminary evidence ) tier to test an online version of the professional development model .

omb staff said that moving up tiers to scale an evidence - based service model and moving down tiers to test an adaptation of a service model both highlight successful outcomes of the tiered evidence grant model .

officials from the four agencies in our review and grantees identified five key factors that facilitate the use of tiered evidence grants by addressing challenges in administering the evidence and evaluation requirements in tiered evidence grants ( see text box ) .

we have also previously reported on strategies agencies can use to facilitate the use of evaluation in program management and policy making , including demonstrated leadership support for evaluation and engaging stakeholders throughout the evaluation process .

officials from the five grant programs in our review said that comprehensive oversight throughout the grant life cycle is essential to address grantee capacity challenges and facilitate the successful use of tiered evidence grants .

oversight of grantees is also important to reasonably assure that grants are used for their intended purposes and that risks of fraud , waste , and abuse are minimized .

although federal oversight of grantees is not unique to tiered evidence grants , at our facilitated discussion , agency officials said that their agencies provided closer oversight on the evidence and evaluation requirements .

for example , they approved evaluation plans and adaptations to service models .

agencies took several steps to create oversight mechanisms to address challenges grantees face with evidence and evaluation requirements: cooperative agreements provided for closer oversight .

both cooperative agreements and grants provide funding to carry out approved federal activities , but a cooperative agreement allows for substantial programmatic involvement from the federal agency .

officials from three of the four agencies in our review — education , hhs , and cncs — said they structured their programs as cooperative agreements because they allowed the agencies to more closely oversee grantees compared to other grant programs , especially in how grantees adhered to the fidelity of service models .

cooperative agreements are used instead of grants when substantial involvement is expected between the agency and the recipient .

clear and upfront requirements and expectations for evaluations .

the four agencies in our review included specific requirements for independent evaluations in the notice of funding opportunities for tiered evidence grants to help ensure that grantees would be prepared to meet the evidence standards for their evaluations .

for example , education required grantees in the investing in innovation fund's scale - up ( strong evidence ) and validation ( moderate evidence ) tiers to identify the program's elements that can be replicated by other entities , and in a variety of contexts for a variety of students .

agency approval of evaluation plans .

an evaluation plan spells out the research questions and methodology of an evaluation .

officials from the four agencies we reviewed told us that completing an evaluation plan is essential because it provides the groundwork for a robust evaluation , including quality data collection , analysis , and reporting .

for example , for the workforce innovation fund , dol required grantees to submit an initial evaluation design report prepared by an independent evaluator within 9 months after the grant award .

dol and the program's national evaluation coordinator assessed the content of initial evaluation reports and provided comments .

grantees then worked with dol and the national evaluation coordinator to have their final evaluation design reports approved within 11 months of the grant award .

using a pilot period .

similarly , at our facilitated discussion , agency officials said that a planning and piloting year at the beginning of the grant period facilitates grantee implementation of tiered evidence grants because it allows time for grantees to plan and set up the infrastructure for their service models and evaluations .

some grantees said that it was easier for grantees in moderate or strong evidence tiers to develop an evaluation plan because their service models were more fully defined from the beginning of the grant's implementation .

some grantees said they found it particularly helpful to use part of the grant period as a pilot program .

for example , a teen pregnancy prevention grantee said they used the first year of the grant to test its service model in three different environments .

reporting requirements to ensure fidelity to the service delivery model .

the four agencies we reviewed used mechanisms to address challenges with implementing service models with fidelity , including requiring grantees to report specific data elements and conducting in - person observations .

for example , to help ensure grantees implemented their service models with fidelity , hhs's teen pregnancy prevention program required grantees to collect data on fidelity measures reflecting the extent to which community - based organizations , schools , and other organizations adhered to the service models' core elements when administering the models in classrooms and other settings ( see text box ) .

agency officials at our facilitated discussion said these measures are very important to ensure quality implementation .

some grantees also said they found the measures to be useful .

for example , a teen pregnancy prevention program grantee said that the independent observations were particularly helpful during the pilot year of program implementation because they helped to identify inconsistencies in how the service providers delivered the messages in the curriculum at different locations .

the grantee organization addressed the inconsistencies across different locations by having service providers from different organizations teach together for the later years of the project to ensure lessons were taught in a consistent manner .

fidelity monitoring for the hhs teen pregnancy prevention program teen pregnancy prevention program grantee data on outcomes to monitor whether they implemented their evidence - based service model with fidelity , include: attendance data from participants for all sessions completed ; a facilitator self - assessment or fidelity monitoring log for each session implemented ; information on planned and unplanned adaptations ; and observation data on service model administrators from independent observers for 5 to 10 percent of all sessions implemented .

hhs reported that for the fiscal year 2010-2014 cohort 95 percent of all sessions were implemented with high fidelity and 92 percent of all sessions that were independently observed were rated as either very high or high quality .

agency approval and dissemination of adaptations to service models .

to address the challenge of grantees making adaptations to approved service models , the four agencies we reviewed required grantees to obtain approval for making adaptations .

for example , hhs assigned some project officers for the teen pregnancy prevention program to serve as model leads for individual evidence - based programs .

this process allowed the project officers to be aware of all approved adaptations that had been made to the model , and to share this information with all other grantees that are implementing that model .

in addition , hhs compiled and analyzed all approved adaptations from the program's first round of grantees from 2010 to 2015 .

it also published a summary of adaptations that had been made by the grantees for each model so future grantees and others could learn from former grantees .

however , some grantees said that it was time consuming to obtain approval for adaptations .

for example , a workforce innovation fund grantee stated that it felt bound to achieve the outcome goals in its evaluation plan and that it was difficult and time consuming to obtain modifications that deviated from the grant's application .

providing funding for evaluations .

all five tiered evidence grants we reviewed allowed for grantees to spend grant funds on evaluations ; however , agencies structured the funding differently .

officials from the four agencies in our review said the very fact their authorizing statutes allowed for federal funds to be used for various evaluation activities was a facilitating factor .

further , the officials stated that some programs are not a good fit for tiered evidence grants because they are prohibited from using federal funds for evaluation purposes and require all funds to provide services .

since independent evaluations were new to some grantees , agency officials at our facilitated discussion said they had to work closely with grantees to ensure they provided sufficient funding for evaluations .

the officials said that grantees are accustomed to quickly providing services after receiving grant funding , and that grantees faced challenges in budgeting for independent evaluations .

the four agencies in our review provided evaluation technical assistance from the design stage to the analysis and reporting stages of evaluations to help ensure that grantees conducted rigorous evaluations .

some grantees stated that technical assistance and guidance helped them understand and apply the evidence and evaluation requirements .

one teen pregnancy prevention program grantee said that technical assistance and guidance was the key facilitating factor in the success with evidence gathering and evaluation .

according to hhs officials , without technical assistance , at least some of the evaluations would not have met standards for rigor , yielding very little information about the impacts of the program model .

further , agency officials at our facilitated discussion said they have observed an increase in grantees' capacity to apply for and implement tiered evidence grants since they were first used in 2010 .

a cncs - commissioned independent evaluation of the social innovation fund found evidence of improved organizational capacity among grantees as a result of their participation in the social innovation fund .

according to this evaluation , between 2009 and 2014 , social innovation fund grantees expanded organizational capacities and behaviors related to selection of grantees , support for grantees , evaluation , scaling up , and collaboration .

likewise , an hhs report on the federal home visiting program found that technical assistance enhanced grantee capacity in various areas , such as designing and modifying data systems and disseminating evaluation findings .

below are specific ways the four agencies provided technical assistance .

agency and evaluation contractor guidance .

the four agencies we reviewed developed a range of tools and guidance on evidence and evaluation to address challenges grantees faced implementing evidence and evaluation requirements and to build grantee capacity .

for example , cncs developed a number of resources to assist grantees as they moved through each stage of the evaluation process , from planning the evaluation to using evaluation results for action and improvement .

the guidance consisted of written materials and seminars that the agency makes available on its website , and provides information on some of the more common challenges that grantees cited in planning and managing an evaluation .

for example , one of the guidance documents addressed how to budget for an evaluation and identify approaches for creating an evaluation budget .

the four agencies also provided guidance on identifying and measuring the core components of specific service models to help grantees monitor implementation fidelity .

for example , the evaluation technical assistance provider for education's investing in innovation fund developed a fidelity tracking tool to help grantees identify their core components and the data needed to determine if they have been implemented .

in another example , for dol's workforce innovation fund , the program's national evaluation coordinator developed a 74-page toolkit to help grantees plan for and manage an evaluation .

the publication covers a broad range of topics , including identifying and hiring an independent evaluator and protecting the rights of the service model's participants .

technical assistance at each stage of the grant lifecycle .

the four agencies we reviewed provided comprehensive technical assistance to grantees throughout the grant lifecycle .

technical assistance can help grantees deliver services to their intended populations more efficiently to improve outcomes .

for example , the four agencies we reviewed provided assistance to grantees before awarding grants .

this assistance included webinars and guidance available for organizations interested in applying for their tiered evidence grant programs .

for example , before releasing the notice of funding announcement for its teen pregnancy prevention program , hhs provided webinars and guidance to help potential grantees understand the evidence and evaluation requirements to complete the application .

some grantees said that their prior experience with federal grants and evaluations facilitated participating in tiered evidence grant programs , technical assistance was still beneficial in helping them understand the evidence and evaluation requirements specific to their programs .

for example , a workforce innovation fund grantee with prior randomized controlled trial experience said it still found dol's evaluation assistance helpful .

the four agencies also provided assistance to grantees on disseminating evaluation results .

for example , officials from hhs's federal home visiting program stated that grantees lacked the capacity to disseminate their findings to a broader audience because historically they have been more focused on providing services .

as a result , hhs developed a dissemination tool kit for grantees on communicating evaluation results .

assistance to address unexpected external events .

officials from the four agencies and some grantees emphasized that tiered evidence grants can face challenges conducting rigorous evaluations based on unexpected external events .

agency officials at our facilitated discussion agreed it was important to provide comprehensive technical assistance for this type of challenge .

similarly , one investing in innovation fund grantee reported that california did not give its standardized tests during one of the grant period years , which affected the outcome data for the evaluation .

education worked with the grantee to ensure that the evaluation's methodology would not be compromised because of the change .

networks for grantees .

at our facilitated discussion , agency officials said that their agencies provided networks for grantees to communicate with each other and share lessons learned .

for example , the four agencies organized in - person conferences to bring grantees together .

some of the grantees we interviewed said it was particularly helpful to meet at the conferences with grant recipients who had participated in prior award years .

for example , a recipient of a workforce innovation fund grant in 2015 stated that it was helpful to have a roundtable discussion with recipients from prior funding rounds because they shared lessons learned such as developing an evaluation plan .

we also identified factors that helped the four agencies we reviewed to address the challenges in administering the evidence and evaluation requirements in tiered evidence grants .

agency officials at our facilitated discussion said the following factors were essential to the implementation of tiered evidence grants .

build capacity to assess the evidence and review applications .

at our facilitated discussion , officials from the four agencies stated that they had to increase staff capacity before and during the grant application period and that the application period required an upsurge to review applications .

for example , the program office for the investing in innovation fund worked with the education's institute of education sciences to review studies submitted in support of applications .

education officials said that it can be challenging to recruit an adequate number of external reviewers to review applications against the selection criteria .

to address this challenge , education's office that administers the investing in innovation fund coordinated with other education offices to ensure that the application review occurred at a different time from those for other education grant programs .

the office also used academic journals and an electronic mailing list of education experts to recruit more qualified reviewers , according to education officials .

contracting with evaluation providers to address federal capacity issues .

evaluation organizations served as national technical assistance providers to work with both grantees and their independent evaluators on evaluation issues .

agency officials at our facilitated discussion stated that contracting with evaluation providers was essential because the agencies often lacked sufficient federal staffing to provide technical assistance .

in addition to providing grantees and their independent evaluators assistance , as previously discussed , evaluation providers boosted the capacity of agencies , according to federal officials .

for example , for the workforce innovation fund , dol officials also reported contracting with an evaluation provider to provide technical assistance to the individual third party evaluators and to review individual grantee evaluations to assess cross - innovation outcomes .

the evaluation provider will review the individual grantees' evaluation reports and synthesize the findings into lessons that substantially add to the body of knowledge about cost - effective practices for workforce development and the workforce system .

agency officials at our facilitated discussion said that tiered evidence grants have encouraged program management and evaluation offices to leverage resources to focus on the shared goal of providing evidence and evaluation assistance to grantees .

education officials stated that the investing in innovation fund would not have been possible without the institute of education sciences because education program management officials do not have the expertise needed to review the evidence and evaluation requirements in grant applications .

in addition , hhs officials said they added an evaluation specialist in the office of adolescent health to manage the federal evaluation contracts , oversee evaluation technical assistance to grantees , and coordinate evaluation activities with the teen pregnancy program .

increased collaboration between program management and evaluation staff can represent a culture shift in agencies , because , according to agency officials at our facilitated discussion , program management and evaluation staff do not always collaborate when administering grants .

the officials said it was important for program management officials to stay abreast of key developments related to particular service models .

for example , an official said that engagement between the program and evaluation offices creates a richer understanding of the evidence base for officials administering the programs , such as being familiar with recent studies related to various service components or models .

at our facilitated discussion , agency officials said three aspects of high - level leadership commitment are essential to supporting tiered evidence grants .

providing additional time and resources .

because tiered evidence grants require more hands - on assistance from agencies , leaders were willing to invest in the people and resources that are necessary to support both federal officials and grantees .

an education official said that , while it is easy to support the concept of evidence in policy making , there can be resistance once agencies realize the extent to which they must invest in the infrastructure needed to successfully administer such programs .

at our facilitated discussion , agency officials said that policymakers may be unaware of the extent to which evidence and evaluation requirements add time and resources to the federal grant - making process .

they added that agency leadership has a role to play in ensuring that policymakers are aware of this .

it is important that policymakers recognize the extent of agency resources needed when establishing tiered evidence grant programs .

understanding that grantees are taking a risk .

both agency officials at our facilitated discussion and grantees that we interviewed said that mixed or null findings could portray a service model or an entire organization negatively , and that this could affect their reputation in the community and with policymakers .

a commitment to evidence - based practices helps to address this reputational risk for grantees because grantees feel comfortable testing approaches and adding to the evidence base regardless of whether they have positive , mixed , or null findings .

agency officials at our facilitated discussion said they understood upfront that some evaluation findings may not have positive results and may portray some service models or grantees negatively .

they agreed that key agency leaders should be committed to standing by the results of evaluations .

in cases where the results were negative or mixed , agency officials want to better understand the reasons why .

convincing policymakers that evidence is a worthwhile investment .

agency officials at our facilitated discussion agreed that tiered evidence grants present the opportunity to demonstrate to agency leaders and policymakers the benefits and potential of evidence in supporting the effective use of resources .

turnover makes it challenging for leadership to commit to evidence - based practices because new leadership may not be familiar with evidence or evaluation requirements , which can take time to embed into an agency's culture .

to address this challenge , education officials said they are currently working to embed the importance of evidence , including tiered evidence grants , into briefing materials for the next presidential administration .

education has further shown leadership support for tiered evidence grants through one of their agency priory goals for fiscal year 2016-2017: to increase the use and generation of credible evidence on what works and what does not work in education .

beyond agency leadership commitment , agency officials at our facilitated discussion agreed that they consider a program's appropriation amount when deciding whether to create a tiered funding approach .

for example , one agency official told us it may be impractical for agencies to create the evidence tiers , funding announcements , and oversight and technical assistance mechanisms for smaller grant programs .

the four agencies in our review collaborated informally on tiered evidence grants and officials said that they found these informal networks useful .

individual agencies organized informal meetings and workshops on tiered evidence grants that occurred as needed .

for example , cncs convened multiple meetings with other agencies administering tiered evidence grants .

cncs officials told us that they saw a benefit to making those meetings more regular and structured .

for example , cncs officials said that collaboration helped address specific topic areas of interest related to tiered evidence grants , such as approaches to conducting evidence reviews , provision of evaluation capacity building services to grantees , and approaches in communicating evaluation results .

the meetings can also be used to discuss issues of importance to the sector , such as the need for using consistent evidence definitions across agencies .

education also hosted workshops with speakers from various agencies who described how tiered evidence grants function and how agencies implement the evidence and evaluation requirements , according to education officials .

in addition , omb has taken steps to convene agencies on issues related to tiered evidence grants .

for example , in november 2013 , omb hosted a workshop on innovative , evidence - focused grant programs that included tiered evidence grants .

omb staff said omb continues to provide coordination and facilitate focused meetings on issues as they arise , and has made materials and communication mechanisms available to the agencies .

for example , omb created a web portal and discussion board for agency officials to share information on evidence and evaluation requirements , amongst other topics .

however , some of the information on the portal is not current and its focus is on the broader use of evidence in policymaking .

omb staff said that omb addresses issues on tiered evidence grants as they arise and under the broader umbrella of evidence - backed policy , and that no formal collaboration mechanism is needed .

however , officials from the four agencies in our review said that they do not actively use the site for issues that arise specific to tiered evidence grants .

an official from one agency stated that it is unclear whether the site is meant to be used to organize events , as a resource repository , or a means to organize a community of practice .

another official from a different agency said the site is designed is to provide guidance to agencies on developing their annual budgets .

officials from three programs in our review stated that they do not use the site for tiered evidence grants .

at our facilitated discussion , agency officials generally agreed that they would benefit from more formal collaboration on tiered evidence grants and lessons learned from implementing them .

the officials stated that they have benefitted from collaborative efforts to date and are looking for additional ways to communicate about specific issues related to tiered evidence grants .

for example , an education official stated there is not an easy way to locate and contact officials from other agencies who are working on tiered evidence grant programs and that it would be helpful to have a mechanism to identify federal officials working on similar tiered evidence grant programs .

according to the official , such a mechanism would provide agencies a forum to answer questions and share lessons learned .

by relying on ad - hoc collaboration , agencies using tiered evidence grants may miss opportunities to capture and share lessons learned that could strengthen and improve tiered evidence grant making government - wide .

further , federal officials who are new to evidence - based grant making may not be able to tap into the informal network .

in our prior work on collaboration , we reported that collaborative mechanisms , such as interagency groups or collaboration technology , can be used to develop policies , implement programs , and share information .

specifically , more formal collaborative mechanisms can be effective to ensure collaboration continues to address staff turnover in leadership positions .

for example , in response to a recommendation in our prior work on pay for success programs , another relatively new tool for incorporating evidence into policymaking , omb developed an interagency learning network with representatives from 10 agencies to share lessons learned , hone policy , and strengthen implementation .

creating a similar mechanism for tiered evidence grants could help ensure current collaboration efforts are continued , and transcend staff turnover and that key lessons learned are captured .

further , formalized collaboration on tiered evidence grants could continue momentum on agency efforts to institutionalize evidence into policy - making .

tiered evidence grants are an important component of congressional and federal agency efforts to increase the use of evidence in policymaking and distribute federal funds .

tiered evidence grants offer a means for agencies to evaluate the effectiveness of their programs and for congress and agencies to fund programs based on proven and emerging service delivery models .

the lessons agencies have learned in implementing tiered evidence grants could be applied to other federal grant programs that use evidence and evaluation requirements as a condition for receipt of federal grant funds .

agencies and grantees reported a number of challenges in administering tiered evidence grants and have also developed strategies for addressing these challenges .

federal agencies that have administered tiered evidence grant programs found that informal inter - agency collaboration helped them address challenges and share lessons learned .

these collaborative efforts to date have relied on informal networks that have emerged during implementation .

a formal mechanism to facilitate communication and sharing of lessons learned across federal agencies could ensure that the beneficial informal collaborative efforts to date continue in a manner that transcends changes in staffing and individual grant programs .

the absence of a formal mechanism to facilitate agency collaboration and to share and document lessons learned presents a missed opportunity to capitalize on what is known about early implementation of tiered evidence grant programs .

a formal mechanism to facilitate tiered evidence grant collaboration and lessons learned could also help maintain momentum for continuous learning about broader government - wide efforts to institutionalize the use of evidence to consider the effectiveness of federal grant programs .

to facilitate collaboration and identify and broadly disseminate information on leading practices and lessons learned , the director of omb should establish a formal means for agencies to collaborate on tiered evidence grants .

this could include creating a formal working group or providing collaboration technologies , such as an electronic mailing list or web portals .

we provided a draft of this product to the office of management and budget ; departments of education , health and human services , and labor ; and the corporation for national and community service for comment .

the office of management and budget had no comments on the draft report .

we received written comments on the draft report from the department of health and human services , which are reproduced in appendix iii .

the departments of education , health and human services , labor , and the corporation for national and community service provided technical comments that were incorporated into the draft as appropriate .

we are sending copies of this report to the director of the office of management and budget ; secretaries of education , health and human services , and dol ; the chief executive officer of the corporation for national and community service ; and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or sagerm@gao.gov .

contact points for our office of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iv .

we conducted this work under the gpra modernization act of 2010 ( gprama ) , which requires us to review the implementation of the act at several critical junctures .

this report is part of our response to that mandate .

the objectives of this report were to describe 1 ) key features that tiered evidence grants add to federal grant processes , ( 2 ) benefits and challenges of using tiered evidence grants , ( 3 ) key factors to facilitate the use of tiered evidence grants , and ( 4 ) to assess the extent to which the office of management and budget ( omb ) and federal agencies are collaborating on evidence and evaluation requirements in tiered evidence grants .

to meet our objectives , we initially identified nine tiered evidence grant programs by conducting a literature review , reviewing annual budget documents , and validating the list with the omb .

we focused on programs that have operated long enough to show implementation results and lessons learned — specifically those established prior to 2013 .

we then narrowed the grant programs to the five that focus on domestic populations: investing in innovation fund at the department of education ( education ) ; teen pregnancy prevention program at the department of health and human services ( hhs ) ; maternal infant and early childhood home visiting ( federal home visiting ) program at hhs ; workforce innovation fund at the department of labor ( dol ) ; and social innovation fund at the corporation for national and community service ( cncs ) .

omb verified that the five programs we identified represented the universe of tiered evidence grant programs that met our selection criteria .

we also interviewed officials from two other programs — education's supporting effective educator development ( seed ) and dol's trade adjustment assistance community college and career training ( taaccct ) .

while these programs include evidence and evaluation features , they did not meet the full definition of tiered evidence grants for purposes of this report .

seed requires grantees to show moderate evidence on their service models but did not link the size of the grant award to the strength of evidence .

taaccct included a “break the cap” award based on the level of evidence in its first funding round , but did not include evidence tiers in the subsequent funding rounds .

to represent the tiered evidence grantee perspective , we selected three programs for grantee interviews: the investing in innovation fund , teen pregnancy prevention program , and workforce innovation fund .

we selected programs based on design features , including the size of the grant award , whether programs had a matching funding requirement , the timing of when the agency reviewed the evidence , and the number of evidence tiers .

for the size of the grant award , we selected one large grant program , given that the majority of tiered evidence grant programs are relatively small compared to other grant programs .

we defined large as greater than $1 billion in cumulative grant awards .

we selected one grant program that has a matching requirement because most tiered evidence grants do not have such a requirement .

further , we decided to select one grant program that reviewed evidence components of grant applications at different times — both before grant solicitation and after grant solicitation .

moreover , we selected programs to include models with three evidence tiers and two evidence tiers .

we selected grantees in three metropolitan areas — the washington , d.c. / baltimore area , new orleans , and the san francisco bay area — using non - generalizable sampling .

first , we grouped cities with grantees into the same metropolitan area if they were within an approximately 1 hour driving distance .

we focused on metropolitan areas that had grantees in the three programs we selected for site visits ( the investing in innovation fund , teen pregnancy prevention program , and workforce innovation fund ) .

we then focused on metropolitan areas that had at least 10 total grantees to ensure we would have a sufficient number of grantees to interview in each location .

we also focused on metropolitan areas that had grantees in at least two evidence tiers .

finally , we selected grantees from three geographic regions ( east coast , west coast , and south ) to provide for geographic dispersion .

we interviewed two to three grant recipients for each evidence tier in the three programs across our three locations .

to select individual grantees , we focused on grant recipients that have had sufficient time to implement programs and identify lessons learned .

for the investing in innovation fund and teen pregnancy prevention program , to the extent possible , we focused on grantees that received funds from the initial round of funding ( 2010 for both programs ) .

if we could not further differentiate recipients beyond this criterion , we randomly selected grantees to interview .

the workforce innovation fund had only six recipients across our three locations .

we selected all of them for interviews .

we interviewed 18 grantees in total .

when summarizing statements from grantees in our report , we defined “some grantees” as at least three grantees .

to identify the key features tiered evidence grants add to the federal grant making life cycle , we reviewed federal notice of funding announcements that described the evidence and evaluation requirements for tiered evidence grant requirements .

we also interviewed officials from education , hhs , dol , and cncs regarding how the tiered evidence grant - making life cycle differs from traditional grant programs .

to describe the benefits and challenges of using tiered evidence grants and factors for addressing the challenges , we reviewed agency documents , including reports on lessons learned , evaluation reports , and agency guidance .

we interviewed officials from education , hhs , dol , and cncs , as well as the grantees from the investing in innovation fund , teen pregnancy prevention program , and workforce innovation fund .

we also interviewed national evaluation organizations that agencies contracted with to provide technical assistance to grantees and conduct evaluations across multiple grantees .

we synthesized the list of challenges and factors for addressing the challenges .

we then provided this list to key federal officials to provide the basis for a facilitated discussion to validate our list .

for the facilitated discussion , we hosted an in - person meeting with officials from the five programs we selected for analysis .

to assess the extent to which omb and federal agencies collaborated on evidence and evaluation requirements in tiered evidence grants , we interviewed officials from education , hhs , dol , cncs , and omb on current collaboration mechanisms and plans for collaboration moving forward .

we also asked about interagency collaboration on tiered evidence grants during our facilitated discussion .

for criteria , we used our key practices for enhancing and sustaining collaboration and implementing interagency collaborative mechanisms .

we conducted this performance audit from september 2015 to september 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the department of education's ( education ) investing in innovation fund is one example of how a federal agency defines the tiers for its tiered evidence grant program ( see table 1 ) .

education included the definitions of its three evidence tiers in the notice of finding availability announcements for the grant program .

michelle sager , 202-512-6806 or sagerm@gao.gov .

in addition to the contact named above , jeff arkin ( assistant director ) , barbara lancaster ( analyst - in - charge ) , amy bowser , jeff demarco , robert gebhart , steven putansu , and robert robinson made key contributions to this work .

