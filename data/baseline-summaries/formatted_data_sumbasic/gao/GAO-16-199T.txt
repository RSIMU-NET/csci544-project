i am pleased to be here today to discuss our past work examining the effectiveness of transportation security administration ( tsa ) programs and technologies .

it has been over 14 years since the attacks of september 11 , 2001 exposed vulnerabilities in the nation's aviation system .

since then , billions of dollars have been spent on a wide range of programs designed to enhance aviation security .

however , securing the nation's aviation operations remains a daunting task — with hundreds of airports , thousands of aircraft , and thousands of flights daily carrying millions of passengers and pieces of carry - on and checked baggage .

according to tsa , the threat to civil aviation has not diminished — underscoring the need for effective aviation security programs .

as the fiscal pressures facing the government continue , so too does the need for tsa to determine how to allocate its finite resources to have the greatest impact on addressing threats and strengthening the effectiveness of its programs and activities .

over the past several years , tsa has taken numerous steps to strengthen aviation security .

for example , tsa has deployed new screening technology intended to enhance passenger screening , developed processes and procedures to help ensure that individuals and their accessible property receive the appropriate level of screening , and established performance measures to assess progress toward achieving some program goals .

however , we have identified opportunities to improve upon these efforts .

as requested , my testimony today identifies key issues that we have found to have adversely affected the effectiveness of tsa's aviation security investments and programs .

specifically , this testimony addresses the extent to which tsa has ( 1 ) evaluated the overall effectiveness of new technologies , programs , and processes using robust methods of testing and evaluation ; ( 2 ) established performance measures that fully reflect program ( 3 ) used program data to identify opportunities for improvement .

this statement is based on selected reports and testimonies issued by gao from january 2013 through june 2015 related to tsa's efforts to oversee its aviation security measures .

in addition , this statement is based on selected updates conducted from april 2015 through october 2015 related to the current status of the secure flight and behavior detection and analysis programs and managed inclusion process , and progress made in implementing previous gao recommendations .

for our past work , we reviewed applicable laws , regulations , and agency and departmental policies and tsa program documents ; decision memorandums ; results from screener performance reviews and testing of advanced imaging technology ( ait ) , also referred to as full - body scanners ; and other documents .

we also visited airports — four for our behavior detection work , six for our managed inclusion work , and nine for our secure flight work — which we selected based on a variety of factors , such as volume of passengers screened and geographic dispersion , and interviewed department of homeland security ( dhs ) , tsa , and federal bureau of investigation officials , among other things .

further details on the scope and methodology for the previously issued reports and testimonies are available within each published product .

for the updates , we reviewed documents and followed up with tsa officials related to the actions taken to address our recommendations .

we conducted the work on which this statement is based in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the aviation and transportation security act ( atsa ) established tsa as the primary federal agency with responsibility for securing the nation's civil aviation system .

this responsibility includes the screening of all passengers and property transported from and within the united states by commercial passenger aircraft .

in accordance with atsa , all passengers , their accessible property , and their checked baggage are to be screened pursuant to tsa - established procedures at the more than 450 airports at which tsa performs , or oversees the performance of , security screening operations .

these procedures generally provide , among other things , that passengers pass through security checkpoints where their person , identification documents , and accessible property are checked by screening personnel .

the following are some of tsa's transportation security technologies , processes , and programs .

ait systems: according to tsa officials , ait systems provide enhanced security benefits compared with those of walk - through metal detectors by identifying nonmetallic objects and liquids .

following the deployment of ait , the public and others raised privacy concerns because ait systems produced images of passengers' bodies that image operators analyzed to identify objects or anomalies that could pose a threat to an aircraft or to the traveling public .

to mitigate those concerns , tsa began installing automated target recognition ( atr ) software on deployed ait systems in july 2011 .

ait systems equipped with atr ( ait - atr ) automatically interpret the image and display anomalies on a generic outline of a passenger instead of displaying images of actual passenger bodies .

screening officers use the generic image of a passenger to identify and resolve anomalies on - site in the presence of the passenger .

expedited screening and tsa's managed inclusion process: tsa pre✓tm — tsa's expedited screening program — is intended to allow tsa to devote more time and resources at the airport to screening the passengers tsa determined to be higher or unknown risk while providing expedited screening to those passengers determined to pose a lower risk to the aviation system .

to assess whether a passenger is eligible for expedited screening , tsa considers , in general , ( 1 ) inclusion on an approved tsa pre✓tm list of known travelers ; ( 2 ) results from the automated tsa pre✓tm risk assessments of all passengers ; and ( 3 ) real - time threat assessments of passengers , known as managed inclusion , conducted at airport checkpoints .

through its managed inclusion process , tsa has utilized a combination of security measures , including behavior detection officers ( bdo ) and passenger screening canine teams at the checkpoint to identify passengers suitable for expedited screening .

behavior detection and analysis program: tsa's behavior detection and analysis program , formerly known as the screening of passengers by observation techniques ( spot ) program , is intended to identify persons who may pose a risk to aviation security .

through these behavior detection activities , tsa's bdos are to identify passenger behaviors indicative of stress , fear , or deception and refer passengers meeting certain criteria for additional screening of their persons and carry - on baggage .

during this referral screening , if passengers exhibit additional such behaviors , or if other events occur , such as the discovery of a suspected fraudulent document , bdos are to refer these passengers to a law enforcement officer for further investigation , which could result in an arrest , among other outcomes .

secure flight: since tsa began implementing secure flight in 2009 , the passenger prescreening program has changed from a program that identifies passengers as high risk solely by matching them against federal government watch lists — for example , the no fly list , comprised of individuals who should be precluded from boarding an aircraft , and the selectee list , comprised of individuals who should receive enhanced screening at the passenger security checkpoint — to one that uses additional lists and risk - based criteria to assign passengers to a risk category: high risk , low risk , or unknown risk .

in 2010 , following the december 2009 attempted attack on a u.s. - bound flight , which exposed gaps in how agencies used watch lists to screen individuals , tsa began using risk - based criteria to create additional lists for secure flight screening .

these lists are composed of high - risk passengers who may not be in the terrorist screening database ( tsdb ) — the u.s. government's consolidated watch list of known or suspected terrorists — but whom tsa has determined should be subject to enhanced screening procedures .

further , in 2011 , tsa began screening passengers against additional identities in the tsdb that are not included on the no fly or selectee lists .

in addition , as part of tsa pre✓™ , tsa began screening against several new lists of preapproved low - risk travelers .

tsa also began conducting tsa pre✓™ risk assessments — an activity distinct from matching against watch lists — that use the secure flight system to assign passengers scores based upon their travel - related data , for the purpose of identifying them as low risk for a specific flight .

national explosives detection canine team program: one of tsa's security layers is its national explosives detection canine team program ( nedctp ) , composed of over 800 explosives detection canine teams — a canine paired with a handler — aimed at deterring and detecting the use of explosive devices in the u.s. transportation system .

through nedctp , tsa trains , deploys , and certifies explosives detection canine teams .

tsa deploys the teams to screen passengers and air cargo at airports and other transportation modes , including mass transit .

in our 2014 reviews of tsa's ait - atr systems and managed inclusion process , we found that tsa had conducted some testing before adopting the new technology and process , but it had not fully demonstrated their effectiveness .

we have also previously reported on challenges tsa has faced in designing studies and protocols to test the effectiveness of security systems and programs in accordance with established methodological practices , such as in the case of our 2013 review of tsa's behavior detection activities .

tsa has since taken steps to more comprehensively test the effectiveness of the next generation of ait , known as ait - 2 , and further test aspects of the managed inclusion process .

with regard to the ait - atr system , in march 2014 , we reported that , according to tsa officials , checkpoint security is a function of technology , people , and the processes that govern them ; however , we found that tsa did not include each of those factors in determining overall ait - atr system performance .

specifically , we found that tsa evaluated the technology's performance in the laboratory to determine system effectiveness .

laboratory test results provide important insights but do not accurately reflect how well the technology will perform in the field with actual human operators .

additionally , we found that tsa did not assess how alarms are resolved by considering how the technology , people , and processes function collectively as an entire system when determining ait - atr system performance .

ait - atr system effectiveness relies on both the technology's capability to identify threat items and its operators to resolve those threat items .

given that tsa was seeking to procure the second generation of ait systems , known as ait - 2 , we reported that dhs and tsa would be hampered in their ability to ensure that future ait systems meet mission needs and perform as intended at airports unless tsa evaluated system effectiveness based on both the performance of the ait - 2 technology and screening officers who operate the technology .

according to best practices related to federal acquisitions , technologies should be demonstrated to work in their intended environment .

we recommended that tsa measure system effectiveness based on the performance of the ait - 2 technology and screening officers who operate the technology while taking into account current processes and deployment strategies .

tsa concurred and has addressed this recommendation .

specifically , in june 2015 , tsa provided documentation showing that , while conducting operational testing of the ait - 2 system , the agency considered screening officer performance and measured ait - 2 system effectiveness based on both the performance of the ait - 2 technology and the screening officers who operate it .

this should help tsa assess whether this screening system will meet mission needs and perform as intended .

with regard to the managed inclusion process , in december 2014 , we reported that tsa had tested the security effectiveness of the individual components of the managed inclusion process , but had not tested the overall effectiveness of the managed inclusion process as it functions as a whole .

according to tsa officials , tsa tested the security effectiveness of the individual components being used in the managed inclusion process at the time — such as bdos , passenger screening canine teams , and explosives trace detection ( etd ) devices — before implementing managed inclusion , and tsa determined that each layer alone provides an effective level of security .

however , in our prior body of work , we identified challenges in several of the layers used in the managed inclusion process , raising questions regarding their effectiveness .

further , as of the time of our report , tsa officials stated that they had not yet tested the security effectiveness of the managed inclusion process as it functions as a whole .

tsa officials explained that they had been planning to test the process as a whole and estimated that such testing would begin in october 2014 and would take 12 to 18 months to complete .

however , tsa could not provide us with specifics or a plan or documentation showing how the testing was to be conducted , the locations where it was to occur , how those locations were to be selected , or the timeframes for conducting testing at each location .

in general , evaluations are most likely to be successful when key steps are addressed during design , including defining research questions appropriate to the scope of the evaluation , and selecting appropriate measures and study approaches that will lead to valid conclusions .

as a result , we recommended that to ensure tsa's planned testing yields reliable results , the tsa administrator take steps to ensure that tsa's testing of the managed inclusion process adheres to established evaluation design practices .

dhs concurred with our recommendation and has taken some initial steps toward addressing it .

specifically , in august 2015 , tsa officials provided us with a testing schedule for additional testing of canine teams and bdos — two of the security layers used in the managed inclusion process — and stated that they had plans to ensure that the tests adhere to recognized test and evaluation protocols .

however , tsa has not provided documents explaining how it plans to evaluate the managed inclusion process as a whole and how this evaluation will adhere to established evaluation practices .

these documents would need to constitute a research plan specifically tailored to evaluating managed inclusion and include specifics such as the types of data to be collected , the methodology for collecting and analyzing the data , and the steps tsa plans to take to help ensure the study will isolate the security effects of the managed inclusion process itself and rule out plausible alternative explanations for study results .

further , in november 2013 , we found that a 2011 dhs study conducted to validate spot's behavioral indicators did not demonstrate their effectiveness because of study limitations , including the use of unreliable data .

we concluded that the usefulness of dhs's april 2011 validation study was limited , in part because the data the study used to examine the extent to which the spot behavioral indicators led to correct screening decisions at security checkpoints were from the spot database that we had previously found in may 2010 to have several weaknesses , and thus were potentially unreliable .

specifically , in may 2010 , we assessed the reliability of the spot database and concluded that the database lacked controls to help ensure the completeness and accuracy of the data , such as computerized edit checks to review the format , existence , and reasonableness of data .

in that report , we also found , among other things , that bdos could not record all behaviors observed in the spot database because the database limited entry to eight behaviors , six signs of deception , and four types of serious prohibited items per passenger referred for additional screening .

at that time , bdos were trained to identify 94 signs of stress , fear , and deception , or other related indicators .

in may 2010 , we recommended that tsa make changes to ensure the quality of spot referral data , and tsa subsequently made changes to the spot database .

however , we found in our 2013 report that dhs's validation study used data that were collected from 2006 through 2010 , prior to tsa's improvements to the spot database .

as a result , we determined that the data used in the spot validation study were not reliable enough for tsa to use in conducting a statistical analysis of the association between the indicators and high - risk passenger outcomes .

because the study used unreliable data , its conclusions regarding the use of the spot behavioral indicators for passenger screening were questionable and did not support the conclusion that the indicators can or cannot be used to identify threats to aviation security .

due to these and other methodological issues we found in dhs's validation study , we recommended in november 2013 that the secretary of homeland security direct tsa to limit future funding support for the agency's behavior detection activities until tsa can provide scientifically validated evidence that demonstrates that behavioral indicators can be used to identify passengers who may pose a threat to aviation security .

dhs did not concur with our recommendation , in part because it disagreed with our analysis of tsa's behavioral indicators .

in january 2015 , tsa provided documentation describing its plans to enhance its behavioral - based screening program , including the development of revised behavioral indicators and new protocols for their use .

in october 2015 , tsa officials told us they were in the process of pilot testing the new protocols in the airport environment and expect to complete the tests by february 2016 .

officials stated that tsa plans to make a determination about whether the new protocols are ready for further testing , including an operational test to determine the protocol's effectiveness , at that time .

further , tsa officials estimated that the operational test may begin in the summer of 2016 , but they did not have an estimated completion date because the behavior detection covert test methodology had not yet been developed and the threat inject methods had not yet been deemed sufficiently mature to test effectiveness .

until tsa completes its planned tests and study on the use of the new protocols and provides the scientifically validated evidence of effectiveness , such as successful operational testing , the agency continues to fund activities that have not been determined to be effective .

in 2014 , we reported on two instances in which tsa's performance measures made it difficult to assess tsa's performance in meeting its goals .

first , we found that tsa did not have adequate performance measures for all secure flight program goals and second , we found that tsa tracked performance information on the expedited screening program that did not link to program goals .

in september 2014 , we found that secure flight had established program goals that reflect new program functions since implementation began in 2009 to identify additional types of high - risk and also low - risk passengers ; however , the program performance measures in place at that time did not allow tsa to fully assess its progress toward achieving all of its goals .

for example , one program goal was to accurately identify passengers on various watch lists .

to assess performance toward this goal , secure flight collected various types of data , including the number of passengers tsa identifies as matches to high - and low - risk lists , but did not have measures to assess the extent of system matching errors — for example , the extent to which secure flight is missing passengers who are actual matches to these lists .

we concluded that additional measures that address key performance aspects related to program goals , and that clearly identify the activities necessary to achieve goals , in accordance with the government performance and results act , would allow tsa to more fully assess progress toward its goals .

therefore , we recommended that tsa develop such measures , and ensure these measures clearly identify the activities necessary to achieve progress toward the goal .

dhs concurred with our recommendation , and , according to tsa officials , as of april 2015 , tsa's office of intelligence and analysis was evaluating its current secure flight performance goals and measures and determining what new performance measures should be established to fully measure progress against program goals .

establishing additional performance measures that adequately indicate progress toward goals would allow secure flight to more fully assess the extent to which it is meeting program goals .

further , in december 2014 , we reported that tsa's performance measure for assessing its expedited screening program did not accurately link to the program's goals — to ensure: ( 1 ) that 25 percent of air passengers were eligible for expedited screening by the end of calendar year 2013 , and ( 2 ) that 50 percent of passengers were eligible for expedited screening by the end of calendar year 2014 .

according to tsa documents , tsa uses one measure — the total number of air passengers screened daily using expedited screening as a percentage of the total number of passengers screened daily — to assess progress toward these goals .

tsa collects data for this measure by reporting , not the number of passengers designated as eligible for expedited screening , but the number of passengers who actually receive such screening .

however , because expedited screening is voluntary , not all passengers who are eligible necessarily use expedited screening .

for example , a passenger may be traveling with a group in which not all passengers in the group are eligible for expedited screening , so the passenger may choose to forgo expedited screening .

as a result , the information that tsa is reporting to show that it is meeting its goal may be understated and inaccurate .

tsa's chief risk officer agreed that the goals and the measure are not linked , but said that tracking actual screening data rather than eligibility data presents a more accurate picture of the expedited screening program performance .

while we agreed that tracking actual screening data may provide insights about expedited screening program performance , we reported that ensuring goals and measures are aligned is important to provide more accurate performance measurement data to guide program performance and to identify potential areas for improvement .

best practices regarding the key attributes of successful performance measurement state that performance measures should link and align with agency - wide goals and the mission should be clearly communicated throughout the organization .

we recommended that the tsa administrator align tsa's expedited screening performance goals and measures to ensure that tsa , as well as lawmakers , has accurate information by which to measure the performance of its expedited screening programs .

tsa has not yet addressed this recommendation .

we have also reported on findings related to program data — such as canine program assessment data and secure flight screening error data — that tsa collected but had not analyzed , missing opportunities to refine and further improve tsa programs .

in january 2013 , we reported that tsa collected and used key canine program data in support of its nedctp program , but could better analyze these data to identify program trends .

for example , we found that in reviewing short notice assessments ( covert tests ) , tsa did not analyze the results beyond the pass and fail rates .

therefore , tsa was missing an opportunity to determine , for example , if there were any search areas or types of explosives in which canine teams were more effective compared with others , and what , if any , training may be needed to mitigate deficiencies .

standards for internal control in the federal government calls for agencies to ensure that ongoing monitoring occurs during the course of normal operations to help evaluate program effectiveness .

we recommended that tsa regularly analyze available data to identify program trends and areas that are working well and those in need of corrective action to guide program resources and activities .

tsa concurred with our recommendation and has taken actions that address our recommendation .

for example , in the event a canine team fails a short notice assessment , tsa now requires that canine team supervisors complete an analysis of the team's training records to identify an explanation for the failure .

further , in september 2014 , we reported that tsa has processes in place to implement secure flight screening determinations at airport checkpoints , but could evaluate available data on screening errors to identify corrective actions .

tsa information from may 2012 through february 2014 that we assessed indicated that screening personnel had made errors in implementing secure flight determinations at the checkpoint .

tsa officials we spoke with at five of the nine airports where we conducted interviews stated that they conduct after - action reviews of screening errors at the checkpoint and have used these reviews to take action to address the root causes of those errors .

however , we found that tsa did not have a systematic process for evaluating the root causes of these screening errors at the checkpoint across airports , which could allow tsa to identify trends across airports and target nationwide efforts to address these issues .

consistent with standards for internal control in the federal government , we recommended in september 2014 that tsa develop a process for evaluating the root causes of screening errors at the checkpoint and then implement corrective measures to address those causes .

dhs concurred with our recommendation and has taken actions to address them .

specifically , tsa provided us with documentation of its analysis of screening errors that occurred over a 3 month period — june 2015 through early september 2015 — and the root causes of those errors .

additionally , in september 2015 , tsa made changes to its screening procedures to address the root causes of errors identified through its analysis .

chairman chaffetz , ranking member cummings , and members of the committee , this completes my prepared statement .

i would be pleased to respond to any questions that you may have at this time .

for questions about this statement , please contact jennifer grover at ( 202 ) 512-7141 or groverj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this statement .

individuals making key contributions to this statement include maria strudwick ( assistant director ) , claudia becker , michele fejfar , susan hsu , and tom lombardi .

key contributors for the previous work that this testimony is based on are listed in each product .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

