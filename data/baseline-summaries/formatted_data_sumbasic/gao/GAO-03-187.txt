almost half of all americans over the age of 65 will spend time in a nursing home at some point in their lives .

a series of congressional hearings since 1998 has focused considerable attention on the unacceptably high number of nursing homes with repeated , serious care problems that harmed residents or placed them at risk of death or serious injury .

given the large number of nursing home residents and the growing public concerns over quality - of - care problems , the centers for medicare & medicaid services ( cms ) has shown a strong commitment to providing assistance to individuals and their families in choosing a nursing home .

cms is the agency within the department of health and human services ( hhs ) that manages medicare and medicaid and oversees compliance with federal nursing home quality standards .

in 1998 , the agency launched a web site — “nursing home compare” — that has progressively expanded the availability of public information on nursing homes and the quality of care provided .

initially , it posted data on deficiencies identified during routine state nursing home inspections , known as surveys .

data were later added on resident characteristics , such as the percentage of residents with pressure sores or physical restraints , nursing staff levels , and deficiencies found during state investigations of complaints .

in november 2001 , cms announced a 12-month timeline for an initiative to ( 1 ) augment existing public data on nursing home quality , and ( 2 ) provide assistance to nursing homes to help improve their quality of care .

in addition to the valuable data already available on its web site , cms proposed including newly developed quality indicators that permit a fairer comparison across homes by adjusting for differences in residents' health characteristics .

quality indicators are essentially numeric warning signs of problems , such as more frequent than expected pressure sores among nursing home residents .

they are based on data from facility - reported assessments — known as the minimum data set ( mds ) — conducted at established intervals during each resident's nursing home stay .

the initiative also envisioned a new role for medicare quality improvement organizations ( qio ) : engaging in partnership building and local promotional activities designed to put quality information into the hands of consumers and working with nursing homes on a voluntary basis to help improve quality of care .

in april 2002 , cms launched a six - state pilot to refine the initiative before planned nationwide implementation in november 2002 .

the six pilot states are colorado , florida , maryland , ohio , rhode island , and washington .

medicare qios are working with from 6 to 11 nursing homes in each pilot state on projects including improving pain management and preventing pressure sores .

in view of the importance of cms's quality indicator initiative and the relatively short pilot time frame prior to national implementation , you asked us to review the ( 1 ) development of the new quality indicators for public reporting , ( 2 ) status of cms's efforts to ensure the accuracy of the underlying data used to calculate the quality indicators , ( 3 ) assistance offered by cms to the public in understanding the new quality indicator data , and ( 4 ) results of cms's evaluation of the pilot .

to do so , we reviewed pertinent documents from cms on the development of the new quality indicators , the approaches identified to adjust for differences in residents' characteristics in each facility , the validation of the new indicators , the operation and evaluation of the pilot , and the role of the qios .

we discussed these areas with cms officials and with researchers from abt associates , inc. , the lead cms contractor responsible for the development and validation of the new quality indicators .

we also interviewed and reviewed materials provided by officials of the national quality forum ( nqf ) , a group cms contracted with to review abt's work and provide recommendations on quality indicators for public reporting .

we examined the consistency of both the quality indicator data available in the six pilot states and the extent of agreement between such data and the results of nursing home surveys .

to determine how cms is assisting consumers in understanding the quality indicator data available in the six pilot states , we posed questions about discrepancies we identified between the indicators and survey deficiencies to staff who field public inquiries received by the medicare and qio toll - free telephone numbers .

we conducted our work from march through september 2002 in accordance with generally accepted government auditing standards .

since 1998 , the results from state surveys of nursing homes have been the principal source of public information on nursing home quality , which is posted and routinely updated on cms's nursing home compare web site .

under contract with cms , states are required to conduct periodic surveys that focus on determining whether care and services meet the assessed needs of the residents and whether homes are in compliance with federal quality requirements , such as preventing avoidable pressure sores , weight loss , or accidents .

during a survey , a team that includes registered nurses spends several days at a home reviewing the quality of care provided to a sample of residents .

states are also required to investigate complaints lodged against nursing homes by residents , families , and others .

in contrast to surveys , complaint investigations generally target a single area in response to a complaint filed against a home .

any deficiencies identified during routine surveys or complaint investigations are classified according to the number of residents potentially or actually affected ( isolated , pattern , or widespread ) and their severity ( potential for minimal harm , potential for more than minimal harm , actual harm , and immediate jeopardy ) .

to improve the rigor of the survey process , hcfa contracted for the development of quality indicators and required their use by state surveyors beginning in 1999 .

quality indicators are derived from data collected during nursing homes' assessments of residents , known as the minimum data set ( mds ) .

the mds contains individual assessment items covering 17 areas , such as mood and behavior , physical functioning , and skin conditions .

mds assessments of each resident are conducted in the first 14 days after admission and periodically thereafter and are used to develop a resident's plan of care .

facility - reported mds data are used by state surveyors to help identify quality problems at nursing homes and by cms to determine the level of nursing home payments for medicare ; some states also use mds data to calculate medicaid nursing home payments .

because it also envisioned using indicators to communicate nursing home quality to consumers , hcfa recognized that any publicly reported indicators must pass a very rigorous standard for validity and reliability .

valid quality indicators that distinguish between good and poor care provided by nursing homes would be a useful adjunct to existing quality data .

such indicators must also be reliable — that is , they must consistently distinguish between good and bad care .

hcfa contracted with abt to review existing quality indicators and determine if they were suitable for public reporting .

abt catalogued and evaluated 143 existing quality indicators , including those used by state surveyors .

it also identified the need for additional indicators both for individuals with chronic conditions who are long - term residents of a facility and for individuals who enter a nursing home for a short period , such as after a hospitalization ( a postacute stay ) .

according to abt , a main concern about publicly reporting quality indicators was that the quality indicator scores might be influenced by other factors , such as residents' health status .

abt concluded that the specification of appropriate risk adjustment models was a key requirement for the validity of any quality indicators .

risk adjustment is important because it provides consumers with an “apples - to - apples” comparison of nursing homes by taking into consideration the characteristics of individual residents and adjusting quality indicator scores accordingly .

for example , a home with a disproportionate number of residents who are bedfast or who present a challenge for maintaining an adequate level of nutrition — factors that contribute to the development of pressures sores — may have a higher pressure sore score .

adjusting a home's quality indicator score to fairly represent to what extent a home does — or does not — admit such residents is important for consumers who may wish to compare one home to another .

after several years of work , abt recommended 39 risk - adjusted quality indicators to cms in october 2001 .

twenty - two were based on existing indicators and the remaining 17 were newly developed by abt , including 9 indicators for nursing home residents with chronic conditions and 8 indicators for individuals who enter a nursing home for a short period .

in september 2001 , cms contracted with the nqf to review abt's work with the objective of ( 1 ) recommending a set of quality indicators for use in its planned six - state pilot and ( 2 ) developing a core set of indicators for national implementation of the initiative scheduled for late 2002 .

nqf established a steering committee to accomplish these two tasks .

the steering committee met in november 2001 and identified 11 indicators for use in the pilot , 9 of which were selected by cms .

the committee made its selection from among abt's list of 39 indicators but it did not recommend use of abt's risk - adjustment approach .

moreover , the steering committee indicated that it would not be limited to the same abt list in developing its recommended core set of indicators for national implementation .

in april 2002 , nqf released a draft consensus report identifying the indicators it had distributed to its members and the public for comment on their potential inclusion in the national implementation .

under its contract , nqf was scheduled to make a final recommendation to cms prior to the national reporting of quality indicators .

cms's initiative to augment existing public data on nursing home quality has considerable merit but more time is needed to assure that the indicators proposed by cms for public reporting are appropriate in terms of their validity and reliability .

based on work by abt to validate the indicators it developed for cms , the agency selected quality indicators for national reporting .

the full abt validation report — which is important for a thorough analysis of the appropriateness of the quality indicators - - was still not available to us as of october 28 , 2002 .

our review of available portions of the abt report , however , raised serious questions about whether testing and validation of the selected indicators has been sufficient to move forward with national reporting at this time .

moreover , cms plans to initiate national reporting before it receives recommendations from nqf , its contractor , on appropriate quality indicators .

on august 9 , 2002 , cms announced the 10 indicators selected for its nationwide reporting of quality indicators , which it plans to launch in mid - november 2002 .

cms selected these indicators from those that abt had validated in its august 2 , 2002 , validation report .

abt classified the indicators it studied as to the degree of validity — top , middle , or not valid .

the indicators that cms selected were in the top category with one exception — residents in physical restraints — which was in the middle category .

the objective of abt's validation study was to confirm that the indicators reflect the actual quality of care that individual nursing facilities provide , after taking into account resident and facility - level characteristics .

for example , a validation analysis could confirm that a low percentage of pressure sores among residents was linked to a facility's use of procedures to prevent their development .

successful validation reduces the chance that publicly reported data could misrepresent a high - quality facility as a low - quality facility — or vice versa .

cms's decision to implement national reporting in november 2002 is troubling , given the issues raised by our review of the available portions of abt's validation report .

although we asked cms for a copy of abt's 11 technical appendixes , as of october 28 , 2002 , they were still undergoing review and were not available to us .

the technical appendixes are essential to adequately understand and evaluate abt's validation approach .

our review of the available portions of the abt report raised serious questions about whether the effort to date has been sufficient to validate the indicators .

the validation study is based on a sample that is drawn from six states ; it is not representative of nursing homes nationwide and may not be representative of facilities in these six states .

selected facilities were allowed to decline participation and about 50 percent did so .

for those facilities in the validation study , abt deemed most of the indicators as valid — that is , better care processes were associated with higher quality indicator scores , taking into account resident and facility - level characteristics .

however , we could not evaluate these findings because abt provided little information on the specific care processes against which the indicators were validated .

unresolved questions also exist about the risk adjustment of the quality indicators .

risk adjustment is a particularly important element in determining certain quality indicators because it may change the ranking of individual facilities — a facility that is among the highest on a particular quality indicator without risk adjustment may fall to the middle or below after risk adjustment — and vice versa .

data released by cms in march 2002 demonstrated that abt's risk adjustment approaches could either lower or raise facility scores by 40 percent or more .

although such changes in ranking may be appropriate , abt did not provide detailed information on how its risk adjustment approaches changed facility rankings or a basis for assessing the appropriateness of the changes .

in addition to the questions raised by our review of the abt validation report , cms is not planning to wait for the expert advice it sought on quality indicators through its contract with the nqf .

under this contract , the nqf steering committee issued a consensus draft in april 2002 with a set of potential indicators for public reporting .

the steering committee had planned to complete its review of these indicators using its consensus process by august 2002 .

in late june , however , cms asked nqf to delay finalizing its recommendations until early 2003 to allow ( 1 ) consideration of abt's august 2002 report on the validity of its indicators and risk - adjustment methods — including the technical appendices , when they become available and ( 2 ) a review of the pilot evaluation results expected in october 2002 .

an nqf official told us that the organization agreed to the delay because the proposed rapid implementation timeline had been a concern since the initiative's inception .

cms's list of quality indicators for the november 2002 national rollout did not include six indicators under consideration by nqf — depression , incontinence , catheterization , bedfast residents , weight loss , and rehospitalization ( see app .

i ) .

instead , cms intends to consider nqf's recommendations and revise the indicators used in the mid - november national rollout sometime next year .

cms is also moving forward without a consensus on risk adjustment of quality indicators .

cms is planning to report one indicator with facility - level adjustment based on a profile of residents' status at admission , and two indicators both with and without this abt - developed risk adjuster .

however , both abt and nqf have concluded that adjusting for the type of residents admitted to the nursing home required further research to determine its validity .

we believe that reporting the same indicator with and without facility - level risk adjustment could serve to confuse rather than help consumers .

two of the three consultants hired by nqf specifically recommended against the use of facility - level adjustments in public reporting at this time .

we also found that , as of october 1 , 2002 , cms had not reached internal consensus on how to describe the risk - adjustment methods used in each of the 10 indicators it plans to begin reporting nationally in november 2002 .

several agency officials agreed with our assessment that the descriptions on its web site were inconsistent with abt's own descriptions of the risk adjustment associated with each indicator .

two different abt studies have presented cms with conflicting messages about the accuracy of mds data .

abt's august 2002 quality indicator validation report suggested that the underlying data used to calculate most indicators were , in the aggregate , very reliable .

however , our analysis of more detailed facility - level data in a february 2001 abt report raised questions about the reliability of some of the same mds data .

because mds data are used by cms and some states to determine the level of nursing home payments for medicare and medicaid and to calculate quality indicators , ensuring its accuracy at the facility level is critical both for determining appropriate payments and for public reporting of the quality indicators .

recognizing the importance of accurate mds data , cms is in the process of implementing a national mds accuracy review program expected to become fully operational in 2003 , after the nationwide reporting of quality indicators begins in november 2002 .

we recently reported that cms's review program is too limited in scope to provide adequate confidence in the accuracy of mds assessments in the vast bulk of nursing homes nationwide .

abt's august 2 , 2002 , validation report concluded that the reliability of the underlying mds data used to calculate 39 quality indicators ranged from acceptable to superior , with the data for only 1 indicator proving unacceptable .

abt's findings were based on a comparison of assessments conducted by its own nurses to assessments performed by the nursing home staff in 209 sample facilities .

for each quality indicator , abt reported the overall reliability for all of the facilities in its sample .

however , because quality indicators will be reported for each nursing home , overall reliability is not a sufficient assurance that the underlying mds data are reliable for each nursing home .

although abt did not provide information on mds reliability for individual facilities , it noted that reliability varied considerably within and across states .

earlier work by abt and others calls into question the reliability of mds data .

abt's february 2001 report on mds data accuracy identified significant variation in the rate of mds errors across the 30 facilities sampled .

differences between assessments conducted by abt's nurses and the nursing home staff were classified as errors by abt .

error rates for all mds items averaged 11.7 percent but varied across facilities by a factor of almost two — from 7.8 percent to 14.5 percent .

as shown in figure 1 , the majority of error rates were higher than 10.5 percent .

furthermore , error rates for some of the individual mds items used to calculate the quality indicators were much higher than the average error rate .

according to abt , the least accurate sections of the mds included physical functioning and skin conditions .

abt also noted that there was a tendency for facilities to underreport residents with pain .

mds items from these portions of the assessment are used to calculate several quality indicators that cms plans to report nationally in november 2002 — activities of daily living , pressure sores , and pain management .

table 1 shows that the error rate across the residents sampled ranged from 18 percent for pressure sores to 42 percent for pain intensity .

abt's february 2001 findings were consistent with areas that states have identified as having a high potential for error , including activities of daily living and skin conditions .

moreover , a study by the hhs office of inspector general ( oig ) , which identified differences between the mds assessment and the medical record , found that activities of daily living was among the areas that provided the greatest source of differences .

in addition , the oig report noted that 40 percent of the nursing home mds coordinators it surveyed identified the physical functioning section , used to calculate the quality indicator on activities of daily living , as the most difficult to complete .

some coordinators explained that facility staff view a resident's capabilities differently and thus the assessments tend to be subjective .

as part of cms's efforts to improve mds accuracy , its contractor is still field - testing the on - site aspect of its approach , which is not expected to be implemented until 2003 .

although abt's february 2001 report found widespread mds errors , cms intends to review roughly 1 percent of the mds assessments prepared over the course of a year , which numbered 14.7 million in 2001 .

moreover , only 10 percent of the reviews will be conducted on - site at nursing homes .

in contrast , our prior work on mds found that 9 of the 10 states with mds - based medicaid payment systems that examine mds data's accuracy conduct periodic on - site reviews in all or a significant portion of their nursing homes , generally examining from 10 to 40 percent of assessments .

on - site reviews heighten facility staff awareness of the importance of mds data and can lead to the correction of practices that contribute to mds errors .

we reported earlier that cms's approach may yield some broad sense of the accuracy of mds assessments on an aggregate level but is insufficient to provide confidence about the accuracy of mds assessments in the vast bulk of nursing homes nationwide .

while cms is strongly committed to making more information available to the public on nursing home quality and such an initiative has considerable merit , the agency had not demonstrated a readiness to assist the public in understanding and using those data .

we found that cms's reporting of quality indicators in the six pilot states was neither consumer friendly nor reported in a format consistent with the data's limitations , implying a greater degree of precision than is currently warranted .

our analysis of the data currently available in the six pilot states demonstrated the potential for public confusion over both the quality indicators themselves and inconsistencies with other available data on deficiencies identified during nursing home surveys — which , to date , are the primary source of public data on nursing home quality .

moreover , our phone calls to the medicare and qio toll - free numbers revealed that cms was not adequately prepared to address consumers' questions raised by discrepancies between conflicting sources of quality data .

our review of the quality indicators on the cms web site found that the presentation of the data was not consumer friendly and that the reporting format implies a greater confidence in the data's precision than may be warranted at this time .

quality indicators are reported as the percentage of residents in a facility having the particular characteristics measured by each indicator .

the web site explains that having a low percentage of residents with pressure sores or pain is better than having a high percentage .

in the six - state pilot , the public can compare a nursing home's score to the statewide and overall average for each quality indicator .

we believe that equating a high score with poor performance is counterintuitive and could prove confusing to consumers .

despite the web site's explanation of how to interpret the scores , the public might well assume that a high score is a positive sign .

in addition , reporting actual quality indicator scores rather than the range of scores a home falls into for an indicator — a low , medium , or high score — can be confusing and implies a confidence in the precision of the results that is currently a goal rather than a reality .

consumers will find it difficult to assess a home with a score that is 5 to 10 percentage points from the state average .

such a home could be an outlier — one of the best or the worst on that indicator ; alternatively , it could be that the home was close to the state average because the outliers involved much larger differences .

concerns about the validity of the indicators and the potential reliability of the data make comparisons of homes with similar scores questionable .

consumers may be misled if a difference of several percentage points between two homes is perceived as demonstrating that one is better or worse than the other .

to partially address these types of concerns , maryland has reported quality indicator data on its own web site since august 2001 in ranges rather than individual values .

thus , it indicates if a facility falls into the bottom 10 percent , the middle 70 percent , or the top 20 percent of facilities in the state .

consumers may also be confused about how to interpret missing information .

although the cms web site explains that quality indicator scores are not reported for nursing homes with too few residents , it does not acknowledge the extent of such missing data .

we found that 6 percent of all nursing homes in the six pilot states have no score for any of the nine quality indicators and that , for individual indicators , from 9 percent to 40 percent of facilities have missing scores ( see table 2 ) .

when data for homes of potential interest to consumers are not reported , consumers may need some assistance in how to incorporate such instances into their decisionmaking .

consumer confusion may also occur when quality indicator scores send conflicting messages about the overall quality of care at a home .

we found that the web site data for a significant number of facilities contained such inconsistencies .

seventeen percent of nursing homes in the six pilot states had an equal number of highly positive and highly negative quality indicator scores .

we defined highly positive scores as those indicating that a facility was among the 25 percent of homes with the lowest percentage of residents exhibiting poor outcomes , such as a decline in their ability to walk or use the toilet .

in contrast , facilities with a highly negative score were among the top 25 percent of homes with poor outcomes .

we also found that 37 percent of nursing homes with four or more highly positive quality indicator scores had two or more highly negative scores .

in addition , our comparison of survey deficiency data available on the web site with quality indicator scores also revealed inconsistencies .

for example , 17 percent of nursing homes with four or more highly positive quality indicator scores and no highly negative scores — seemingly “good” nursing homes — had at least one serious quality - of - care deficiency on a recent state survey .

we have found that serious deficiencies cited by state nursing home surveyors were generally warranted and indeed reflected instances of documented actual harm to nursing home residents .

moreover , 73 percent of nursing homes with four or more highly negative quality indicator scores — seemingly “bad” facilities — had no serious quality - of - care deficiencies on a recent survey ( see table 3 ) .

the latter situation is consistent with our past work that surveyors often miss serious quality - of - care problems .

nevertheless , consumers will generally lack such insights on the reliability of state surveys that would permit them to better assess the available data on quality of care .

with the apparent need for assistance to consumers in interpreting and using this information , the important role of the medicare and qio toll - free numbers is evident .

we requested and reviewed copies of the medicare hotline and qio scripts and found that they did not address the issue of responding to questions about conflicting or confusing quality data .

furthermore , our calls to the medicare hotline and to qio toll - free numbers in the six pilot states demonstrated that the staff were not adequately prepared to handle basic questions about the quality data available under the pilot .

cms officials had told us that medicare hotline callers with complicated questions would be seamlessly transferred to a qio without having to hang up and call another number .

although we asked the medicare hotline staff if another organization might be better able to respond to our questions , no one offered to refer us to qios , even when we specifically asked about them .

in fact , one hotline staff member told us that a qio would not be an appropriate referral .

consequently , we independently attempted to call the qios in the six pilot states .

we found that it was difficult to reach a qio staff member qualified to answer questions .

each qio had a toll - free number but neither the automated recordings at four qios nor operators at the remaining two indicated that the caller had reached a qio .

in addition , the automated recordings did not contain a menu choice for questions about nursing home quality indicators .

we were unable to contact one qio because the hotline had neither an operator nor a voice mail capability .

on other calls , after reaching a qio staff person , it frequently took several referrals to identify an appropriate contact point .

one qio took 5 working days for a staff member to call us back .

four of the five qios we contacted explained that their primary role was to work with nursing homes to improve quality of care .

in general , qio staff were not prepared to respond to consumer questions .

staff at the medicare hotline and the qios varied greatly in their basic understanding of quality indicators and survey deficiencies .

while two of the nine staff we contacted were generally knowledgeable about different types of quality data , others were unable to answer simple questions and the majority provided erroneous or misleading data .

one qio staff member told us that mds data were not representative of all residents of a nursing home but only presented a “little picture” based on a few residents .

however , assessments of all residents are taken into consideration in calculating quality indicators .

when we expressed concern about a home identified on the web site with a “level - 3” deficiency , a medicare hotline staff member incorrectly told us that it was not a serious deficiency because level 3 indicated potential harm .

cms designates actual harm deficiencies as “level - 3” deficiencies .

a qio staff member incorrectly told us that actual harm pressure sore deficiencies had nothing to do with patient care and might be related to paperwork .

our review of survey reports has shown that actual harm deficiencies generally involved serious quality - of - care problems resulting in resident harm .

generally , hotline staff did not express a preference for using either nursing home surveys or quality indicators in choosing a nursing home .

two qio staff , however , stated that the nursing home survey information gave a better picture of nursing home care than the quality indicators , which they judged to be imprecise and subject to variability .

cms's evaluation of the pilot is limited and will not be completed prior to national reporting of quality indicators because of the short period of time between the launch of the pilot and the planned november 2002 national implementation .

according to cms officials , the pilot evaluation was never intended to help decide whether the initiative should be implemented nationally or to measure the impact on nursing home quality .

while cms is interested in whether nursing home quality actually improves as a result of the initiative , it will be some time before such a determination can be made .

thus , cms focused the pilot evaluation on identifying improvements that could be incorporated into the initiative's design prior to the scheduled national implementation in november 2002 .

a cms official told us that initial pilot evaluation results were expected by early october 2002 , allowing just over a month to incorporate any lessons learned .

in commenting on a draft of this report , cms stated that it was using preliminary findings to steer national implementation .

the final results of the pilot evaluation will not be completed until sometime in 2003 .

cms's evaluation of the pilot is focused on identifying how to communicate more effectively with consumers about the initiative and how to improve qio interaction with nursing homes .

specifically , cms will assess whether ( 1 ) the target audiences were reached ; ( 2 ) the initiative increased consumer use of nursing home quality information ; ( 3 ) consumers used the new information to choose a nursing home ; ( 4 ) qio activities influenced nursing home quality improvement activities ; ( 5 ) nursing homes found the assistance provided by qios useful ; and ( 6 ) the initiative influenced those who might assist consumers in selecting a nursing home , such as hospital discharge planners and physicians .

information is being collected by conducting consumer focus groups , tracking web site “hits” and toll - free telephone inquiries , administering a web site satisfaction survey , and surveying nursing homes , hospital discharge planners , and physicians .

as of late august 2002 , cms teams were also in the process of completing site visits to stakeholders in the six pilot states , including qios , nursing homes , ombudsmen , survey agencies , nursing home industry representatives , and consumer advocacy groups .

the teams' objective is to obtain a first - hand perspective of how the initiative is working with the goal of implementing necessary changes and better supporting the program in the future .

although cms's initiative to publicly report nursing home quality indicators is a commendable and worthwhile goal , we believe that it is important for cms to wait for and consider input from nqf and make necessary adjustments to the initiative based on its input .

we believe several factors demonstrate that cms's planned national reporting of quality indicators in november 2002 is premature .

our review of the available portions of abt's validation report raised serious questions about whether the effort to date has been sufficient to validate the quality indicators .

nqf was asked to delay recommending a set of indicators for national reporting until 2003 , in part , to provide sufficient time for it to review abt's report .

although limited in scope , cms's planned mds accuracy review program will not begin on - site accuracy reviews of the data underlying quality indicators until 2003 .

cms's own evaluation of the pilot , designed to help refine the initiative , was limited to fit cms's timetable for the initiative and the preliminary finding were not available until october 2002 , leaving little time to incorporate the results into the planned national rollout .

other aspects of the evaluation will not be available until early 2003 .

we also have serious concerns about the potential for public confusion over quality data , highlighting the need for clear descriptions of the data's limitations and easy access to informed experts at both the medicare and qio hotlines .

cms has not yet demonstrated its readiness to meet these consumer needs either directly or through the qios .

to ensure that publicly reported quality indicator data accurately reflect the status of quality in nursing homes and fairly compare homes to one another , we recommend that the administrator of cms delay the implementation of nationwide reporting of quality indicators until there is greater assurance that the quality indicators are appropriate for public reporting — including the validity of the indicators selected and the use of an appropriate risk - adjustment methodology — based on input from the nqf and other experts and , if necessary , additional analysis and testing ; and a more thorough evaluation of the pilot is completed to help improve the initiative's effectiveness , including an assessment of the presentation of information on the web site and the resources needed to assist consumers' use of the information .

cms and the nqf reviewed and provided comments on a draft of this report .

 ( see app .

ii and app .

iii , respectively ) .

cms reiterated its commitment to continually improve the quality indicators and to work to resolve the issues discussed in our report .

although cms stated it would use our report to help improve the initiative over time , it intends to move forward with national implementation in november 2002 as planned .

it stated that “waiting for more reliability , more validity , more accuracy , and more usefulness will delay needed public accountability , and deprive consumers , clinicians , and providers of important information they can use now.” the nqf commented that it unequivocally supports cms's plans to publicly report quality indicators but indicated that the initiative would benefit from a short - term postponement of 3 to 4 months to achieve a consensus on a set of indicators and to provide additional time to prepare the public on how to use and interpret the data .

we continue to support the concept of reporting quality indicators , but remain concerned that a flawed implementation could seriously undercut support for and the potential effectiveness of this very worthwhile initiative .

cms's comments and our evaluation focused largely on two issues: ( 1 ) the selection and validity of quality indicators , and ( 2 ) lessons learned from cms's evaluation of the pilot initiative .

cms asserts that the quality indicators it plans to report nationally are reliable , valid , accurate , and useful and that it has received input from a number of sources in selecting the indicators for this initiative .

however , cms provided no new evidence addressing our findings regarding the appropriateness of the quality indicators selected for public reporting and the accuracy of the underlying data .

we continue to believe that , prior to nationwide implementation , cms should resolve these open issues .

cms intends to move forward with nationwide implementation without a requested nqf assessment of the full abt validation report and without nqf's final recommendations on quality indicators .

cms would not share the technical appendices to abt's validation report with us because they were undergoing review and revision .

the technical appendices are critical to assessing abt's validation approach .

cms's comments did not address our specific findings on the available portions of abt's validation report , including: ( 1 ) the validation results are not representative of nursing homes nationwide because of limitations in the selection of a sample of nursing homes to participate in the validation study , and ( 2 ) abt provided little information on the specific care processes against which the indicators were validated or how its risk adjustment approaches changed facility rankings and the appropriateness of the changes .

although both abt and the nqf concluded that abt's facility - level risk adjustment approach required further research to determine its validity , cms plans to report two indicators with and without facility - level adjustments .

cms's comments indicated that it has chosen to report these measures both ways in order to evaluate their usefulness and to allow facilities and consumers the additional information .

we continue to believe that reporting data of uncertain validity is inappropriate and , as such , will likely not be useful to either facilities or consumers .

for quality indicators to be reliable , the underlying mds data used to calculate the indicators must be accurate .

cms's comments did not specifically address the conflicting findings on mds accuracy from abt's august 2002 validation report and its february 2001 report to cms .

abt's august 2002 validation report concluded that , in aggregate , the underlying mds data were very reliable but that the reliability varied considerably within and across states .

aggregate reliability , however , is insufficient because quality indicators are reported separately for each facility .

in its february 2001 report to cms , abt identified widespread errors in the accuracy of facility - specific assessments used to calculate some of the quality indicators that cms has selected for reporting in november .

cms indicated that its efforts since 1999 have improved mds accuracy .

but because cms does not plan to begin limited on - site mds accuracy reviews until 2003 , there is little evidence to support this assertion .

cms commented that findings from a number of activities evaluating the six - state pilot were not available prior to the time we asked for comments on our draft report .

while final reports are not yet available for some of these studies , cms stated that the pilot allowed it to work through important issues and incorporate lessons learned before a national launch .

we pointed out that the pilot evaluation was limited and incomplete — an additional reason to delay the initiative .

cms also did not evaluate a key implementation issue — the adequacy of assistance available to consumers through its toll - free telephone hotlines .

moreover , the lack of formal evaluation reports to help guide the development of a consensus about key issues , such as how quality indicators should be reported , is troubling .

in its comments , cms stated that it was committed to working aggressively to help the public understand nursing home quality information using lessons learned from the pilot .

however , cms learned about the flaws in its hotline operations not from its pilot evaluation but from our attempts to use the medicare and qio toll - free phone numbers to obtain information on quality data .

acknowledging the weaknesses we identified , the agency laid out a series of actions intended to strengthen the hotlines' ability to respond to public inquiries , such as providing additional training to customer service representatives prior to the national launch of the initiative .

cms outlined other steps it plans to take such as providing its customer service representatives with new scripts and questions and answers to the most frequently asked questions .

at the outset of the pilot in april 2002 , cms described seamless transfers from the medicare to the qio hotlines for complicated consumer questions but now acknowledges that limitations in qio telephone technology prevent such transfers .

instead of automatic transfers , cms stated that , when referrals to qios are necessary , callers will be provided with a direct toll - free phone number .

cms also commented that consumers should be encouraged to consider multiple types of information on nursing home quality .

while we agree , we believe it is critical that customer service representatives have a clear understanding of the strengths and limitations of different types of data to properly inform consumers when they inquire .

cms commented that we offered no explanation of the analysis that led us to conclude that ( 1 ) consumers could be confused because scores on quality indicators can conflict with each other and the results of routine nursing home surveys , and ( 2 ) the public may confuse a high quality indicator score with a positive result .

our draft clearly states that our findings were based on our analysis of the quality indicator data and survey results available in the six pilot states — a database that cms provided at our request .

in its comments , cms provided limited data to support its assertion that consumers are not confused by the quality indicators and are very satisfied with the current presentation on its web site .

according to cms , over two - thirds of respondents to its august 2002 online satisfaction survey of randomly chosen users of nursing home compare information said they were highly satisfied with the information , for example , it was clearly displayed , easy to understand , and valuable .

it is not clear , however , that these responses were representative of all nursing home consumers accessing the web site , as cms implied .

for example , cms informed us that this survey was part of a larger survey of all medicare web site users , which had a low overall response rate of 29 percent .

moreover , of the 654 respondents to the nursing home compare component of the survey , fewer than half ( 40 percent ) were identified as medicare beneficiaries , family members , or friends .

nqf feedback to cms on its web site presentation was consistent with our findings .

in commenting on our draft report , nqf noted that it had offered informal guidance to cms , such as using positive or neutral wording to describe indicators , exploring alternative ways of presenting information about differences among facilities , and ensuring that the presentation of the data reflects meaningful differences in topics important to consumers .

while justifying its current presentation of quality indicator data , cms commented that it is seriously considering not reporting individual nursing home scores but rather grouping homes into ranges such as the bottom 10 percent , middle 70 percent , and top 20 percent of facilities in a state .

such a change , however , would not come before the national rollout .

we agree with cms that , when grouping homes into ranges , homes on the margin — close to the bottom 10 percent or top 20 percent — may not be significantly different from one another .

however , the same is true of reporting individual facility scores .

moreover , reporting ranges more clearly identifies homes that are outliers for consumers .

cms also commented on our characterization of the scope of the nursing home quality initiative .

cms stated that we had narrowly framed the initiative as one designed solely for consumers , ignoring the qio's quality improvement activities with individual nursing homes requesting assistance .

our report acknowledged and briefly outlined the quality improvement role of the qios .

however , based on our requestors' concerns about the relatively short pilot timeframe prior to national implementation of public reporting of quality indicators , we focused our work on that key aspect of the initiative .

cms cited its interim report on evaluation activities for the nursing home quality initiative to support its conclusion that the initiative was successful in promoting quality improvement activities among nursing homes .

the improvements cited in the interim report were self - reported by facilities and cms offered no insights on the nature of the quality improvement changes .

the interim report was not available when we sent our draft report to cms for comment .

cms provided several technical comments which we incorporated as appropriate .

as agreed with your offices , unless you publicly announce its contents earlier , we plan no further distribution of this report until 30 days after its issue date .

at that time , we will send copies to the administrator of cms , appropriate congressional committees , and other interested parties .

we will also make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions , please call me at ( 202 ) 512-7118 or walter ochinko at ( 202 ) 512-7157 .

gao staff acknowledgments are listed in appendix iv .

the following staff made important contributions to this report: laura sutton elsberg , patricia a. jones , dean mohs , dae park , jonathan ratner , peter schmidt , paul m. thomas , and phyllis thorburn .

the general accounting office , the investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents at no cost is through the internet .

gao's web site ( www.gao.gov ) contains abstracts and full - text files of current reports and testimony and an expanding archive of older products .

the web site features a search engine to help you locate documents using key words and phrases .

you can print these documents in their entirety , including charts and other graphics .

each day , gao issues a list of newly released reports , testimony , and correspondence .

gao posts this list , known as “today's reports,” on its web site daily .

the list contains links to the full - text document files .

to have gao e - mail this list to you every afternoon , go to www.gao.gov and select “subscribe to gao mailing lists” under “order gao products” heading .

