i am pleased to be here today to discuss the importance of key aspects of the federal government's acquisition of information technology ( it ) investments .

as reported to the office of management and budget ( omb ) , federal agencies plan to spend at least $82 billion on it in fiscal year 2014 .

given the size of these investments and the criticality of many of these systems to the health , economy , and security of the nation , it is important that federal agencies successfully acquire these systems — that is , ensure that the systems are acquired on time and within budget , and that they deliver the expected benefits and functionality .

however , as we have previously reported and testified , federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

during the past several years , we have issued multiple reports and testimonies on federal initiatives to acquire and improve the management of it investments .

in those reports , we made numerous recommendations to federal agencies and omb to further enhance the management and oversight of it programs .

as part of its response to our prior work , omb deployed a public website in 2009 , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 700 major federal it investments .

in addition , omb has initiated other significant efforts following the creation of the dashboard .

for example , omb began leading reviews — known as techstat accountability sessions ( techstats ) — of selected it investments to increase accountability and improve performance .

further , in 2011 we reported on the critical factors underlying successful federal major it acquisitions .

in that report , we identified seven successful investment acquisitions and nine common factors critical to their success .

as discussed with committee staff , i am testifying today on it acquisition best practices , with a focus on the results of our report on the critical success factors of major it acquisitions .

accordingly , my testimony specifically focuses on those success factors and their importance to improving it investment oversight and management .

i will also address several initiatives put into place by omb to address the transparency of it investments and to review troubled projects .

all work on which this testimony is based was performed in accordance with all sections of gao's quality assurance framework that were relevant to our objectives .

the framework requires that we plan and perform the engagement to obtain sufficient and appropriate evidence to meet our stated objectives and to discuss any limitations in our work .

we believe that the information and data obtained , and the analysis conducted , provide a reasonable basis for any findings and conclusions in this product .

a more detailed discussion of the objectives , scope , and methodology of this work is included in each of the reports on which this testimony is based .

information technology should enable government to better serve the american people .

however , according to omb , despite spending more than $600 billion on it over the past decade , the federal government has achieved little of the productivity improvements that private industry has realized from it .

too often , federal it projects run over budget , behind schedule , or fail to deliver promised functionality .

in combating this problem , proper oversight is critical .

both omb and federal agencies have key roles and responsibilities for overseeing it investment management and omb is responsible for working with agencies to ensure investments are appropriately planned and justified .

however , as we have described in numerous reports , although a variety of best practice documentation exists to guide their successful acquisition , federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

it acquisition best practices have been developed by both industry and the federal government .

for example , the software engineering institute has developed highly regarded and widely used guidance on best practices , such as requirements development and management , risk management , configuration management , validation and verification , and project monitoring and control .

this guidance also describes disciplined project management practices that call for the development of project details , such as objectives , scope of work , schedules , costs , and requirements against which projects can be managed and executed .

in the federal government , gao's own research in it management best practices led to the development of the information technology investment management framework , which describes essential and complementary it investment management disciplines , such as oversight of system development and acquisition management , and organizes them into a set of critical processes for successful investments .

this guidance further describes five progressive stages of maturity that an agency can achieve in its investment management capabilities , and was developed on the basis of our research into the it investment management practices of leading private - and public - sector organizations .

gao has also identified opportunities to improve the role played by chief information officers ( cio ) in it management .

in noting that federal law provides cios with adequate authority to manage it for their agencies , gao also reported on limitations that impeded their ability to exercise this authority .

specifically , cios have not always had sufficient control over it investments ; more consistent implementation of cios' authority could enhance their effectiveness .

congress has also enacted legislation that reflects it management best practices .

for example , the clinger - cohen act of 1996 , which was informed by gao best practice recommendations , requires federal agencies to focus more on the results they have achieved through it investments , while concurrently improving their it acquisition processes .

specifically , the act requires agency heads to implement a process to maximize the value of the agency's it investments and assess , manage , and evaluate the risks of its it acquisitions .

further , the act establishes cios to advise and assist agency heads in carrying out these responsibilities .

the act also requires omb to encourage agencies to develop and use best practices in it acquisition .

additionally , the e - government act of 2002 established a cio council , which is led by the federal cio , to be the principal interagency forum for improving agency practices related to the development , acquisition , and management of information resources , including sharing best practices .

although these best practices and legislation can have a positive impact on major it programs , we have previously testified that the federal government continues to invest in numerous failed and troubled projects .

we stated that while omb's and agencies' recent efforts had resulted in greater transparency and oversight of federal spending , continued leadership and attention was necessary to build on the progress that had been made .

in an effort to end the recurring cycle of failed it projects , this committee has introduced legislation to improve it acquisition management .

among other things , this legislation would eliminate duplication and waste in it acquisition , and increase the authority of agency cios , strengthen and streamline it acquisition management practices .

we have previously testified in support of this legislation .

omb plays a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

in june 2009 , omb established the it dashboard to improve the transparency into and oversight of agencies' it investments .

according to omb officials , agency cios are required to update each major investment in the it dashboard with a rating based on the cio's evaluation of certain aspects of the investment , such as risk management , requirements management , contractor oversight , and human capital .

according to omb , these data are intended to provide a near real - time perspective of the performance of these investments , as well as a historical perspective .

further , the public display of these data is intended to allow omb , congressional and other oversight bodies , and the general public to hold government agencies accountable for results and progress .

in january 2010 , the federal cio began leading techstat sessions — reviews of selected it investments between omb and agency leadership — to increase accountability and transparency and improve performance .

omb has identified factors that may result in an investment being selected for a techstat session , such as — but not limited to — evidence of ( 1 ) poor performance ; ( 2 ) duplication with other systems or projects ; ( 3 ) unmitigated risks ; and ( 4 ) misalignment with policies and best practices .

omb reported that as of april 2013 , 79 techstat sessions had been held with federal agencies .

according to omb , these sessions enabled the government to improve or terminate it investments that were experiencing performance problems .

for example , in june 2010 the federal cio led a techstat on the national archives and records administration's ( nara ) electronic records archives investment that resulted in six corrective actions , including halting fiscal year 2012 development funding pending the completion of a strategic plan .

similarly , in january 2011 , we reported that nara had not been positioned to identify potential cost and schedule problems early , and had not been able to take timely actions to correct problems , delays , and cost increases on this system acquisition program .

moreover , we estimated that the program would likely overrun costs by between $205 and $405 million if the agency completed the program as originally designed .

we made multiple recommendations to the archivist of the united states , including establishing a comprehensive plan for all remaining work , improving the accuracy of key performance reports , and engaging executive leadership in correcting negative performance trends .

drawing on the visibility into federal it investments provided by the it dashboard and techstat sessions , in december 2010 , omb issued a plan to reform it management throughout the federal government over an 18-month time frame .

among other things , the plan noted the goal of turning around or terminating at least one - third of underperforming projects by june 2012 .

the plan contained two high - level objectives: achieving operational efficiency , and effectively managing large - scale it programs .

to achieve operational efficiencies , the plan outlined actions required to adopt cloud solutions and leverage shared services .

to effectively manage it acquisitions , the plan identified key actions , such as improving accountability and governance and aligning acquisition processes with the technology cycle .

our april 2012 report on the federal government's progress on implementing the plan found that not all action items had been completed .

these findings are discussed in greater detail later in the next section .

we have previously reported that omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , by improving the accuracy of investment ratings , and by creating a plan to reform federal it .

however , we also found issues with the accuracy and data reliability of cost and schedule data , and recommended steps that omb should take to improve these data .

in july 2010 , we reported that the cost and schedule ratings on omb's dashboard were not always accurate for the investments we reviewed , because these ratings did not take into consideration current performance .

as a result , the ratings were based on outdated information .

we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting .

omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and completed activities .

similarly , our report in march 2011 noted that omb had initiated several efforts to increase the dashboard's value as an oversight tool and had used its data to improve federal it management .

however , we also reported that agency practices and the dashboard's calculations contributed to inaccuracies in the reported investment performance data .

for instance , we found missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data .

as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance .

most agencies generally concurred with our recommendations and three have taken steps to address them .

omb agreed with our recommendation for improving ratings for schedule variance .

it disagreed with our recommendation to improve how it reflects current performance in cost and schedule ratings , but more recently made changes to dashboard calculations to address this while also noting challenges in comprehensively evaluating cost and schedule data for these investments .

subsequently , in november 2011 , we noted that the accuracy of investment cost and schedule ratings had improved since our july 2010 report because omb refined the dashboard's cost and schedule calculations .

most of the ratings for the eight investments we reviewed as part of our november 2011 report were accurate , although we noted that more could be done to inform oversight and decision making by emphasizing recent performance in the ratings .

we recommended that the general services administration comply with omb's guidance for updating its ratings when new information becomes available ( including when investments are rebaselined ) .

the agency concurred and has since taken actions to address this recommendation .

since we previously recommended that omb improve how it rates investments , we did not make any further recommendations .

further , in april 2012 , we reported that omb and key federal agencies had made progress on implementing actions items from its plan to reform it management , but found that there were several areas where more remained to be done .

specifically , we reviewed 10 actions and found that 3 were complete , while 7 were incomplete .

for example , we found that omb had reformed and strengthened investment review boards , but had only partially issued guidance on modular development .

accordingly , we recommended , among other things , that omb ensure that the action items called for in the plan be completed by the responsible parties prior to the completion of the plan's 18-month deadline of june 2012 , or if the june 2012 deadline could not be met , by another clearly defined deadline .

omb agreed to complete the key action items .

finally , we reviewed omb's efforts to help agencies address it projects with cost overruns , schedule delays , and performance shortfalls in june 2013 .

in particular , we reported that omb used cio ratings from the dashboard , among other sources , to select at - risk investments for reviews known as techstats .

omb initiated these reviews in january 2010 to further improve investment performance , and subsequently incorporated the techstat model into its plan for reforming it management .

we reported that omb and selected agencies had held multiple techstat sessions but additional omb oversight was needed to ensure that these meetings were having the appropriate impact on underperforming projects and that resulting cost savings were valid .

among other things , we recommended that omb require agencies to address their highest - risk investments and to report on how they validated the outcomes .

omb generally agreed with our recommendations , and stated that it and the agencies were taking appropriate steps to address them .

subsequent to the launch of the dashboard and the techstat reviews , and to help the federal agencies address the well - documented acquisition challenges they face , in 2011 , we reported on nine common factors critical to the success of it investment acquisitions .

specifically , we reported that department officials from seven agencies each identified a successful investment acquisition , in that they best achieved their respective cost , schedule , scope , and performance goals .

to identify these investments , we interviewed officials from the 10 departments with the largest planned it budgets in order for each department to identify one mission - critical , major it investment that best achieved its cost , schedule , scope , and performance goals .

of the 10 departments , 7 identified successful it investments , for a total of 7 investments .

officials from the 7 investments cited a number of success factors that contributed to these investments' success .

according to federal department officials , the following seven investments ( shown in table 1 ) best achieved their respective cost , schedule , scope , and performance goals .

the estimated total life - cycle cost of the seven investments is about $5 billion .

among these seven it investments , officials identified nine factors as critical to the success of three or more of the seven .

the factors most commonly identified include active engagement of stakeholders , program staff with the necessary knowledge and skills , and senior department and agency executive support for the program .

these nine critical success factors are consistent with leading industry practices for it acquisitions .

table 2 shows how many of the investments reported the nine factors and selected examples of how agencies implemented them are discussed below .

a more detailed discussion of the investments' identification of success factors can be found in our 2011 report .

officials from all seven selected investments cited active engagement with program stakeholders — individuals or groups ( including , in some cases , end users ) with an interest in the success of the acquisition — as a critical factor to the success of those investments .

agency officials stated that stakeholders , among other things , reviewed contractor proposals during the procurement process , regularly attended program management office sponsored meetings , were working members of integrated project teams , and were notified of problems and concerns as soon as possible .

in addition , officials from the two investments at national nuclear security administration and u.s. customs and border protection noted that actively engaging with stakeholders created transparency and trust , and increased the support from the stakeholders .

additionally , officials from six of the seven selected investments indicated that the knowledge and skills of the program staff were critical to the success of the program .

this included knowledge of acquisitions and procurement processes , monitoring of contracts , large - scale organizational transformation , agile software development concepts , and areas of program management such as earned value management and technical monitoring .

finally , officials from five of the seven selected investments identified having the end users test and validate the system components prior to formal end user acceptance testing for deployment as critical to the success of their program .

similar to this factor , leading guidance recommends testing selected products and product components throughout the program life cycle .

testing of functionality by end users prior to acceptance demonstrates , earlier rather than later in the program life cycle , that the functionality will fulfill its intended use .

if problems are found during this testing , programs are typically positioned to make changes that are less costly and disruptive than ones made later in the life cycle would be .

in summary , the expanded use of these critical it acquisition success factors , in conjunction with industry and government best practices , should result in the more effective delivery of mission - critical systems .

further , these factors support omb's objective of improving the management of large - scale it acquisitions across the federal government , and wide dissemination of these factors could complement omb's efforts .

while omb's and agencies' recent efforts have resulted in greater transparency and oversight of federal spending , continued leadership and attention are necessary to build on the progress that has been made .

by improving the accuracy of information on the it dashboard , and holding additional techstat reviews , management attention can be better focused on troubled projects and establishing clear action items to turn these projects around or terminate them .

further , legislation such as that proposed by this committee can play an important role in increasing the authority of agency cios and improving federal it acquisition management practices .

overall , the implementation of our numerous recommendations regarding key aspects of it acquisition management can help omb and federal agencies continue to improve the efficiency and transparency with which it investments are managed , in order to ensure that the federal government's substantial investment in it is being wisely spent .

chairman issa , ranking member cummings , and members of the committee , this completes my prepared statement .

i would be pleased to respond to any questions that you may have at this time .

if you or your staffs have any questions about this testimony , please contact me at ( 202 ) 512-9286 or at pownerd@gao.gov .

individuals who made key contributions to this testimony are dave hinchman ( assistant director ) , deborah davis , rebecca eyler , kaelin kuhn , thomas murphy , jamelyn payan , and jessica waselkow .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

