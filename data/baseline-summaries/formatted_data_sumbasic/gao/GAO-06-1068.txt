since president bush signed an executive order establishing the global war on terrorism ( gwot ) on september 14 , 2001 , hundreds of thousands of national guard and reserve servicemembers have been activated , mobilized , and deployed in support of efforts in , among other places , afghanistan and iraq .

as the department of defense ( dod ) continues to rely on about 1.3 million reservists to carry out its military operations domestically and abroad , there continues to be congressional interest in the impact of gwot on reserve employment , income change , medical and health status of reservists , and other issues .

for decades , dod has been collecting and reporting information on active duty and reserve component servicemembers .

however , it was not until october 2001 , less than a month after the terrorist attacks , that dod emphasized the need for the services to specifically report information about mobilized and deployed reservists who support contingencies .

while dod has been collecting this information , several reports have emphasized information about reservists who have been mobilized , not deployed .

further , some of our prior reports have raised concerns about dod's ability to effectively track reservists who are being deployed to the theater of operation .

information about reservists' deployments is needed to assess reserve force availability and to link reservists' locations with exposure to medical hazards .

our past work has also confirmed that it is critical that dod collect , maintain , and report reliable information on deployed reservists .

in our experience , the data that dod has reported in the past about the number of reservists who have been mobilized and deployed have not been consistent because , for example , the data used came from different or varied sources and the analyses performed were based on different analytical assumptions .

this report , initiated under the comptroller general's authority to conduct evaluations on his own initiative , addressed the following issues: ( 1 ) what dod data indicate are the number of reservists mobilized and deployed in support of gwot and the selected demographic and deployment characteristics of those deployed and ( 2 ) whether dod's reserve deployment and mobilization data and analyses are reliable .

for this report , we used data provided by the defense manpower data center ( dmdc ) , which is dod's repository for departmentwide data .

we outline the major assumptions we used to analyze the data in the scope and methodology section of this report ( see app .

i ) .

specifically , to address our objectives , we obtained and analyzed data from dmdc's contingency tracking system ( cts ) .

cts is dmdc's system that brings together data about gwot from many sources and , according to a senior dmdc official , is the only source of these data within dod .

the joint staff's manpower and personnel office is working toward using only cts data to determine reserve force availability for future operations .

we also performed reliability assessments on the data after obtaining an understanding of the data file structure and the sources of the data .

specifically , we ( 1 ) performed electronic testing of the data files for completeness ( that is , missing data ) , out - of - range values , and dates outside of valid time frames ; ( 2 ) assessed the relationships among data elements ; and ( 3 ) worked with agency officials to identify data problems , such as which variables may be unreliable .

we also analyzed the extent to which data provided by dmdc changed during this review as a result of dmdc's data cleaning effort , known as a rebaselining .

using applicable guidance , we interviewed knowledgeable officials at dmdc about internal control procedures and other matters .

in addition to the officials at dmdc , we also interviewed knowledgeable officials from the services , the office of the assistant secretary of defense for reserve affairs , and the joint chiefs of staff manpower and personnel office .

the data we report are sufficiently reliable for our purposes ( that is , providing descriptive information ) with one caveat .

since the army is in the process of updating its mobilization data , we could not assess the reliability of the army's mobilization data to the same extent as those of the other services .

in comparing our analyses of the data with the analyses reported by dmdc , we determined that dmdc's analyses were not sufficiently reliable for this report .

we performed our audit work from december 2005 through august 2006 in accordance with generally accepted government auditing standards .

a more thorough description of our scope and methodology is provided in appendix i .

in 1975 , dod implemented the reserve components common personnel data system ( rccpds ) to collect information on current and past members of the six reserve components — army national guard , air national guard , army reserve , navy reserve , marine corps reserve , and air force reserve .

this information included data on reservists' personal characteristics , such as name , social security number , date of birth , gender , home address , and education , as well as data on their military characteristics , such as service , reserve component , prior service status , and date of initial entry into the reserve forces .

according to the director of dmdc , the services send daily , weekly , and monthly updated data submissions to dmdc in accordance with applicable guidance .

after the first gulf war , in a may 15 , 1991 , memorandum , dod identified 16 recommendations requiring action by many offices within dod regarding desert storm personnel data issues .

for example , the memorandum said that dod should consistently report on who participated in the operations and cites examples of key terms , such as in theater , that were being interpreted differently by dmdc , the office of the secretary of defense , and the services .

in december 1991 , dod reported on how dmdc provided information about operations desert shield and desert storm .

this report cited areas for improvement .

for example , the report indicated that dmdc created makeshift procedures to establish and maintain the new data sources and to accommodate varied data requests .

the report cited that these procedures sometimes resulted in inconsistent or incomplete data being provided in response to a request .

on may 2 , 2001 , dod updated guidance to the military services , among others , to maintain a centralized database of active duty personnel .

in this guidance , dod requires the services to report personnel information about all active duty military servicemembers as well as reservists who are ordered to active duty .

while this instruction called for the services to report information about servicemembers on active duty in support of a contingency , the requirements for reporting contingency data were not specific .

on october 4 , 2001 , the under secretary of defense , personnel and readiness ( usd ( p&r ) ) , issued a memorandum that required the services to report personnel information to dmdc on all active and reserve component personnel mobilized or deployed in support of gwot , in accordance with dod guidance .

the purpose of gwot data was , among other things , to establish eligibility for benefits and entitlements as a result of participation in the named contingencies .

the information is critical because it provides a historical database with which to assess the impact of policies and processes , events , and exposures on the health of deployed reserve component servicemembers .

dmdc was tasked with providing reporting guidance to the services for these data submissions .

dmdc sent this guidance to the services on october 12 , 2001 .

dmdc is a civilian - led agency with a mission to deliver timely and quality support to its customers , and to ensure that data received from different sources are consistent , accurate , and appropriate when used to respond to inquiries .

dmdc reports to the deputy under secretary of defense for program integration , who is in the office of the usd ( p&r ) ( see fig .

1 ) .

in february 2002 , usd ( p&r ) reminded the services in another memorandum of its earlier requirement for reporting personnel data to dmdc and informed the services that they had 2 weeks to provide plans to dmdc on how they were going to correct any personnel data reporting problems .

on august 6 , 2004 , dod updated prior guidance regarding rccpds to include an enclosure that set out specific requirements for the services to report personnel information for all reserve component servicemembers supporting a named contingency , unlike previous guidance .

the purpose of the new enclosure was to ensure more accurate reporting on a named contingency , such as gwot missions , as well as to establish eligibility for benefits and entitlements , and to develop a registry of participants for tracking in support of research and evaluation of dod programs and policies .

according to dod officials , the services , in general , were still reporting data according to previous guidance for a few years after the new guidance was issued .

in august 2004 , dmdc began operation of its cts database to address dod's reporting requirements , including those in the new enclosure ( that is , enclosure 11 ) .

the cts database is dod's repository for collecting activation , mobilization , and deployment data for reservists who have served and continue to serve in support of gwot .

the cts database contains both an activation file , which contains mobilization data , and a deployment file .

both files are updated monthly by service submissions and cover gwot from september 11 , 2001 , to the present .

the purpose of the activation file is to account for and provide medical and educational benefits for all reservists called to active duty in support of gwot contingencies , and it allows dod to provide data on the number of reservists who have been mobilized in support of gwot .

the purpose of the cts deployment file is to account for a deployed servicemember's deployment date and location during each deployment event in support of deployment health surveillance and dod guidance .

the database is also used to track and report the number of reservists who have been deployed in support of gwot since september 11 , 2001 .

our analysis of dod data indicates that more than 531,000 reservists have been mobilized in support of gwot and more than 378,000 reservists , or about 71 percent of the number mobilized , have been deployed in support of gwot through june 30 , 2006 ( see fig .

2 ) .

the army national guard deployed the greatest number of reservists in support of gwot from september 2001 through june 30 , 2006 , and , of those , the majority were deployed once .

the data also indicate that the vast majority of reservists who deployed in support of gwot were u.s. citizens , white , and male .

further , the data indicate that most of the reservists spent 1 year or less deployed .

dod guidance requires the services to report timely , accurate , and complete activation , mobilization , and deployment data .

dod guidance also requires dmdc to collect and maintain mobilization and deployment data from the services about the reservists .

dod is required by policy to report personnel data about reservists , such as service , service component , reserve component category , race , ethnicity , gender , citizenship status , occupation , unit , and volunteer status regarding a current mobilization .

in addition , dod is required by policy to capture deployment information such as the location a reservist is deployed to and the dates the reservist was deployed to that location .

our analysis of dod data indicates that more than 531,000 reservists have been mobilized in support of gwot and more than 378,000 reservists , or 71 percent of the number mobilized , have been deployed in support of gwot through june 30 , 2006 ( see fig .

2 ) .

the number of mobilizations and deployments peaked in fiscal year 2003 with about 206,000 reservists mobilized and about 127,000 reservists deployed ( see figs .

3 and 4 ) .

since fiscal year 2003 , the total number of mobilizations has declined , while the number of deployments remained stable through fiscal year 2005 .

the army national guard has mobilized and deployed the greatest number of reservists — more than 230,000 mobilized and more than 163,000 deployed .

the navy reserve had the least number of reservists mobilized — with about 29,000 — while the marine corps reserve had the fewest number deployed with about 19,000 reservists ( see fig .

2 ) .

the percentage of the total reservists mobilized or deployed varies across the fiscal years ( see figs .

3 and 4 ) .

for example , looking at the percentage of mobilizations by component each year , navy reserve , air force reserve , and air national guard mobilizations occurred early in gwot and have generally declined over time .

conversely , the percentage of army national guard and army reserve mobilizations has generally increased over time .

the greatest number of army national guard deployments — more than 60,000 — occurred in fiscal year 2005 ( see table 5 totals in app .

ii ) , while also in fiscal year 2005 , the army national guard represented the largest deploying component , with 52 percent of deployments belonging to it ( see fig .

4 ) .

although reservists usually deployed only once , some experienced multiple deployments ( see fig .

5 ) .

for example , compared to the other reserve components , the air national guard and the air force reserve had nearly half of their reservists deploying two and three or more times , but they tend to have shorter deployment cycles according to the air expeditionary force cycle .

under this cycle , reservists deploy for about 120 days in a 20-month cycle .

however , servicemembers assigned to stressed specialties deploy for longer periods of time and in greater frequency .

at the unit level , some deployment rules have been modified to increase volunteerism or to add stability to key missions .

the army national guard and the marine corps reserve had the lowest percentage of reservists deploying two and three or more times , but they tend to have longer deployment cycles .

in general , dod policy stipulates that army units spend 1 year “boots on the ground” in theater .

this policy also states that marine corps units below the regimental or group level deploy for 7 months while regimental and group headquarters units and above deploy for 12 months .

this policy also states that the chief of naval operations' goal is for servicemembers to have a 6-month deployment with 12 months in a nondeployed status .

our analysis of dod data indicates that across the services , the majority of reservists have been deployed once , and of those deployed in support of gwot , most — about 307,000 reservists , or 81 percent — have spent a year or less deployed .

alternatively , more than 65,000 reservists , or 17 percent , have spent more than 1 year but less than 2 years deployed , and about 6,000 reservists , or fewer than 2 percent , have spent more than 2 years deployed .

the data also indicate that the marine corps reserve had the highest percentage of reservists serving more than 2 years .

in addition , the data also indicate that very few — less than 1 percent — of air national guard reservists served more than 2 years ( see fig .

6 ) .

our analysis of dod data indicates that most reservists who have deployed in support of gwot through june 30 , 2006 , were members of the selected reserve ( see fig .

7 and table 5 in app .

ii ) .

the majority of units and individuals in each reserve component are part of the selected reserve .

these units and individuals have been designated as so essential to the initial wartime mission that they have priority for training , equipment , and personnel over all categories of reservists .

congress authorizes end strength for selected reserve personnel each year .

the authorized end strength for the army national guard has been about 350,000 for the past several years .

for fiscal year 2005 , data provided by the services to dmdc indicate that the army national guard deployed more than 60,000 selected reserve servicemembers , which represents the highest number of selected reserve servicemembers deployed in a single fiscal year by a single reserve component since gwot began .

although the services are authorized a maximum number of selected reservists , the actual number of reservists will fluctuate when additional reservists are recruited or others leave the reserve component .

in addition , reservists such as those in the individual ready reserve , are also available for deployment .

in general , reservists are trained to have specific skills and specialties and may not be suited to deploy for a specific mission until additional training is provided .

in addition , some reservists may not be available for deployment because they are in training , on medical leave , or awaiting training .

our analysis of dod data indicates that almost 98 percent of reservists who have deployed in support of gwot through june 30 , 2006 , were u.s. citizens at the time of their most current deployment ( see fig .

8 ) .

the data indicate that about 1 percent of reservists were non - u.s. citizens or non - nationals at the time of their most current deployment .

the citizenship status of more than 1,400 reservists was unknown .

dod data also indicate that 168 reservists' citizenship status changed .

table 1 shows the citizenship status of reservists by reserve component by fiscal year .

our analysis of dod data indicates that about 78 percent of those deployed for gwot were white ; about 14 percent were black or african american ; about 2 percent were asian , native hawaiian , or other pacific islander ; and about 1 percent were american indian or alaskan native ( see table 2 ) .

overall , about 5 percent of the deployed reservists declined to indicate their race .

the army national guard , the air national guard , and the air force reserve had the highest percentages of the reservists who identified themselves as white .

further , about 90 percent of those who responded identified themselves as non - hispanic and 8 percent as hispanic ( see table 3 ) .

our analysis of dod data indicates that about 338,000 reservists , or about 89 percent of the number deployed , were male ( see table 4 ) .

about 11 percent of those deployed in support of gwot were female .

of the approximately 163,500 army national guard servicemembers who have been deployed through june 30 , 2006 , more than 92 percent were male .

almost 98 percent of those deployed in support of gwot through june 30 , 2006 , for the marine corps reserve were male , representing the highest percentage of males compared with females for all of the reserve components .

our analysis of dod data indicates that california , texas , pennsylvania , and florida had the highest numbers of reservists who have deployed in support of gwot through june 2006 ( see table 6 in app .

ii for the number of reservists deployed by state of residence by reserve component by fiscal year ) .

the 4 states combined had more than 76,000 reservists in residence at the time of their deployments .

eleven states deployed more than 10,000 reservists each , accounting for more than 160,000 reservist deployments .

of those deployed , about 39 percent came from states in the southern united states , about 23 percent from the midwest , about 18 percent from states in the western united states , and about 15 percent came from states in the northeast part of the country .

more than 20,000 reservists indicated california or texas as their state of residence at the time they were deployed ( see fig .

9 ) .

nineteen states and 5 territories had fewer than 5,000 reservists in residence at the time of their deployment and 20 states and 1 territory had from 5,000 to 9,999 reservists in residence at the time of their deployment .

our analysis of dod data indicates that since gwot began , the occupational areas of enlisted reservists deployed in support of gwot have stayed somewhat consistent across all services .

for example , the army national guard , the air force reserve , and the marine corps reserve have deployed reservists mostly in infantry occupational areas including such groups as infantry , air crew , and combat engineering .

all six reserve components have deployed electrical and mechanical equipment repairers , such as automotive , aircraft , and armament and munitions .

three of the six reserve components — the army national guard , the army reserve , and the marine corps reserve — have deployed reservists who are service and supply handlers , such as law enforcement and motor transport .

since gwot began , the occupational areas most deployed for reserve component officers have varied , but all reserve components primarily deployed tactical operations officers , to include ground and naval arms , helicopter pilots , and operations staff subgroups .

the army national guard , the air national guard , and the navy reserve have deployed engineering and maintenance officers , such as the communications and radar and aviation maintenance occupational subgroups .

the air national guard , the army reserve , and the air force reserve have deployed reservists in the health care officer occupational areas , including physicians and nurses .

the army reserve and the marine corps reserve have deployed supply and procurement occupational areas that include transportation , general logistics , and supply occupational subgroups .

the air force reserve has also deployed intelligence officers in occupational subgroups such as general intelligence and counterintelligence .

we were unable to analyze the volunteer status variable because the data do not exist for all of the reserve components .

similarly , we were unable to analyze the deployment location and deployment unit variables because we determined , in agreement with dmdc officials , that the data in these fields were not reliable .

this issue is discussed further below .

while we found selected deployment and mobilization data to be sufficiently reliable for our purposes ( that is , providing descriptive data ) , some of the data were not reliable enough for us to report , even for descriptive purposes .

dmdc and the services , as required by dod policy , have taken steps to improve the reliability of the mobilization data ; however , more action is needed to improve the reliability of cts data and dmdc's analyses of those data .

for example , ( 1 ) the rebaselining effort resulted in substantial changes being made to the mobilization data , and the army — which has mobilized and deployed the largest number of reservists for gwot — has not completed this rebaselining effort , which the joint staff tasked dmdc and the services to do in november 2005 ; ( 2 ) we identified data issues that dod has not addressed that could further improve the reliability of the data , such as standardizing the use of key terms like deployment ; and ( 3 ) dmdc does not have effective controls for ensuring the accuracy of its data analyses used to produce reports as required by federal government internal control standards .

although dmdc and dod have undertaken a major data cleaning — or rebaselining — effort to improve the reliability of mobilization data , the effort does not address some fundamental data quality issues .

while we recognize that such a large - scale effort , although replete with challenges , is a positive step toward better quality data , if data reporting requirements and definitions are not uniform , and if there are no quality reviews of dmdc's analyses , some data elements and dmdc's analyses of those data may continue to be unreliable .

a senior dmdc official stated that it emphasizes getting data to customers in a timely manner rather than documenting the internal control procedures needed to improve the reliability of the data and the data analyses produced .

however , with proper internal controls , dmdc could potentially achieve both timeliness and accuracy .

without reliable data and analyses , dod cannot make sound data - driven decisions about reserve force availability .

moreover , dod may not be able to link reservists' locations with exposure to medical hazards .

we have found the deployment and mobilization data we used to be sufficiently reliable for our purposes ( that is , providing descriptive data ) , and dmdc and the services have recently taken steps to improve the reliability of mobilization data .

however , additional steps are needed to make mobilization data more reliable .

as previously noted , dod guidance requires the services to report timely , accurate , and complete activation , mobilization , and deployment data .

dmdc officials responsible for overseeing the cts database stated that a rebaseline of the deployment data was not necessary because the deployment data matched the data in the defense finance and accounting service's ( dfas ) systems by more than 98 percent .

although dmdc and the services rebaselining of the mobilization data in cts has resulted in improvements , the army , which has mobilized the greatest number of reservists for gwot , has not completed its rebaselining effort .

a senior - level dmdc official responsible for overseeing the cts database said that the mobilization data in the cts database prior to the rebaselining effort were less than 80 percent accurate for the army , the navy , and the air force , but that the marine corps' data were generally considered to be accurate prior to the rebaselining effort .

the official also stated that dmdc expects that the mobilization data within the cts database will be 90 percent accurate because of this rebaselining effort , which was still ongoing through august 2006 .

while we recognize that this is a considerable undertaking , to date , only the navy and the air force have validated or certified their mobilization data files .

navy officials said that the navy has validated its personnel records and established a common baseline of data with dmdc .

air force reserve officials said that their data within cts are now 99 to 100 percent accurate .

the chief of the personnel data systems division for the air national guard certified that although file discrepancies are still being reconciled , the data that were processed by dmdc on june 11 , 2006 , were the most accurate activation data and that data accuracy will improve with each future file sent to dmdc .

the dmdc official said that the marine corps had only partially completed its rebaselining effort and would not be finished until the marine corps provided its august 2006 data file in september 2006 .

the army national guard and army reserve are still working to rebaseline their mobilization data , and the army has not provided a time frame for completing the effort .

however , we still have concerns regarding the reliability of the mobilization data , because the scope of the rebaselining effort changed and the data changed substantially as a result of the rebaselining .

at the beginning of our review , dmdc and the services referred to the rebaselining effort as a “reconciliation,” which , according to a dmdc official and a reserve affairs official , would have resulted in all data ( current and past ) being reviewed and corrected as needed .

we acknowledge that some degree of change is expected in any data cleaning effort , especially with large - scale , multisource collection methods such as dmdc's data collection process .

however , our experience has shown that cleaning efforts that result in a large degree of change would suggest systematic error .

such error raises concerns about the reliability of both the original data and the “cleaned” data .

if both the source data and the cleaned data are populated with the same assumptions and information , any reconciliation of data points should result in relatively small change that correct simply for random error , such as from keypunch or data source errors .

however , for some variables , the data changed substantially as a result of dmdc and the services' rebaselining or data cleaning effort .

our analysis shows that data from the period of september 2001 through december 2005 have changed by about 4 percent to as much as 20 percent .

for example: the number of reservists mobilized for gwot through december 2005 went from about 478,000 to about 506,000 — an increase of more than 27,000 reservists or a change of more than 5 percent .

the army reserve data sustained the greatest change during this time with a more than 19 percent increase in the number of reservists mobilized .

the number of mobilized army national guard reservists increased more than 7 percent .

according to a senior dmdc official , the army data are expected to continue to change , perhaps substantially enough to require the rebaselining of the data again in the future .

the number of air national guard reservists mobilized decreased by more than 13 percent .

the navy reserve , the marine corps reserve , and the air force reserve data all changed about 5 percent .

dod officials stated that the rebaselining effort occurred because the joint staff tasked dmdc and the services with ensuring that the data the joint staff's manpower and personnel office was using in cts were the same data as the services were using to determine reserve force availability .

according to a senior - level dmdc official responsible for overseeing cts , the rebaselining effort's scope changed because all of the services agreed that starting over and replacing all of the data would make more sense than trying to correct transactions already in cts , because the services found errors in the cts files initially used for the reconciliation .

service officials said that some of the data discrepancies developed because of a dmdc quality check procedure that sometimes resulted in dmdc replacing the service - submitted data with data from other sources .

dmdc officials said that they did this because the services were unable to report some of the required cts data .

according to dmdc officials , service submissions have become more complete over time , resulting in dmdc now using the quality check procedures only to check the data rather than to populate the cts database .

this dmdc official stated that dmdc expected the data to change substantially based on the issues identified with service data during the initial reconciliation effort and the subsequent rebaselining effort .

because the rebaselining effort is not complete and the army — which has mobilized and deployed the largest number of reservists for gwot — has not finished the rebaselining , we do not know how much the data will continue to change as dmdc and the services work to finish this effort .

dod data on reservists' mobilizations and deployments are important because decision makers at dod and in congress need the data to make sound decisions about personnel issues and for planning and budgeting purposes .

prior to the rebaselining effort , some services recognized that there were data issues that needed to be addressed and took steps to do so , as dod guidance requires the services to report accurate and complete mobilization and deployment data .

however , some data issues that would ensure more accurate , complete , and consistent mobilization and deployment data across the services in the future have not been fully addressed by dod .

some examples of data issues being addressed include the following: the air force and the navy were having difficulty tracking mobilizations based on reservists' mobilization orders , which has resulted in both services independently working to develop and implement systems that write reservists' orders .

the army reserve recently began to modify its mobilization systems , which army officials expect will improve the collection of reservists' mobilization data .

the air force identified problems with the way in which the defense eligibility enrollment reporting system ( deers ) processed end dates for reservists' mobilizations , which resulted in some reservists not receiving appropriate benefits ( for example , dental benefits ) .

air force officials worked with officials from the office of the secretary of defense and dmdc to identify and address the data processing logic issues .

despite these positive steps , service process improvements are not all complete , and further , there has been no comprehensive review across dod to identify data issues that if addressed , could result in more complete , accurate , and consistent mobilization and deployment data across and within the services .

reserve affairs officials in the office of reserve systems integration said that a more sustainable fix to the processes of collecting data is needed to ensure that data captured in the future are accurate and more efficiently collected .

we agree and have identified some issues that may continue to affect data reliability , such as the following: the use of terms , such as activated , mobilized , and deployed , has not been standardized across the services .

although the department has defined these terms in the department of defense dictionary of military and associated terms , the terms are used differently by the individual services .

in the air force , “activation” can refer to the time when a reservist either volunteers or is involuntarily mobilized ; however , the term “mobilized” refers only to someone who is not a volunteer .

even within a single service , these words can have different meanings .

for example , an army national guard official who participated in the rebaselining effort said that army national guard servicemembers who backfill active duty servicemembers are not considered deployed since they have not left the united states .

however , according to this official , some staff in the army national guard use “deployed” to include reservists who are mobilized within the united states .

there is no single data entry process that would minimize the potential for contradictory data about reservists in multiple systems .

currently , data about reservists are entered separately into multiple systems .

there is no mechanism for dmdc to ensure that the services are addressing the data inconsistencies dmdc identifies during its ongoing , monthly validation process , such as social security numbers that are duplicated in two reserve components .

dod has taken an ad hoc , episodic approach to identifying data reporting requirements and to addressing data issues .

dod has periodically issued policies regarding its need to collect and report specific data , such as volunteer status and location deployed , about active duty servicemembers and reservists .

as a result of changing requirements , many of these policies have addendums that include these additional data requirements , which are not immediately supported by the services' existing systems that are used to collect the data .

over time , this has led to disjointed policies that overlap and that require the services to modify their existing systems and processes , which can take months to complete .

there are incomplete data submissions across the services .

specifically , data for volunteer status was not available in cts for all service components , and the location deployed and deploying unit data were not reliable enough for the purposes of this report .

only three of the six reserve components — the air national guard , the marine corps reserve , and the air force reserve — provide information on a reservist's volunteer status , which neither we nor dmdc report because it is not available for all six components .

further , dmdc officials said that they consider cts location data incomplete although the data are improving with each fiscal year .

dmdc officials said that most unit information is based on the unit a reservist is assigned to and may not represent the unit the reservist is currently deployed with in theater .

for this reason , we did not consider these data reliable enough to report .

a dmdc official stated that dmdc does not have the authority to direct the services to correct data errors or inconsistencies or to address data issues .

dmdc does , however , work with the services and tries to identify and address data challenges .

according to some service officials , the department plans to implement a new , integrated payroll and personnel system — defense integrated military human resource system ( dimhrs ) — and that the services have been diverting resources needed to modify their existing systems and relevant processes to support dimhrs .

however , our past work has shown that dod has encountered a number of challenges with dimhrs , which is behind schedule , and the current schedule has it available no sooner than april 2008 , when the army is scheduled to begin implementing the system .

in general , service officials said that they are working to collect data on volunteer status , location deployed , and deploying unit ; however , air force officials stated that they do collect data on location deployed and deploying unit and that these data are accurate and are being provided to dmdc .

army reserve officials stated that they currently do not have plans to collect data on volunteer status .

dmdc has not documented ( 1 ) its procedures for verifying that the data analyses it performs are correct and ( 2 ) the procedures for monthly validation of service data or the procedures used to perform analyses of data .

either of these issues could , if documented as part of dmdc's verification process , address some of our concerns about internal controls .

dmdc is required by policy to develop and produce reports about mobilization data and respond to requests for information about deployed personnel .

dod policy requires dmdc and the reserve components to ensure the accuracy of files and the resulting reports .

federal government internal control standards require that data control activities , such as edit checks , verifications , and reconciliations , be conducted and documented to help provide reasonable assurance that agency objectives are being met .

dmdc officials said that they have internal verification procedures that require supervisors to review all data analyses used to generate reports , although these procedures are not documented .

specifically , the supervisors are to review ( 1 ) the statistical programming code used to generate the data analyses to ensure that the code includes the customer's data analyses parameters ( that is , the assumptions used to produce the analyses ) and ( 2 ) the “totals” generated to ensure that these totals match the control totals that show the number of reservists currently or ever mobilized or deployed in support of gwot .

dmdc officials acknowledge the importance of verifying the accuracy of the data analyses prior to providing the reports to customers , and they stated that they had verified the accuracy of the analyses provided to us .

however , we found numerous errors in the initial and subsequent analyses we received of the gwot data through may 2006 , causing us to question whether dmdc verified the data analyses it provided to us and , if it did , whether the current process is adequate .

for example , we found that dmdc had done the following: counted reservists with more than one deployment during gwot also among those who deployed only once during gwot , which resulted in overcounting the number of reservists' deployments .

used ethnicity responses to identify race despite having told us that the internal policy was changed in 2006 and that this was no longer an acceptable practice .

counted reservists whose ethnicity was “unknown” as “non - hispanic” although “unknown” does not necessarily mean someone's ethnicity is “non - hispanic” and there was a category for unknowns .

repeatedly categorized data based on a reservist's first deployment ( when there was more than one ) despite agreeing to modify this analytical assumption so that we could present data by the reservist's most current deployment .

reported thousands of reservists as having changed citizenship status during gwot although , in our analyses , we found that only 168 reservists had changed status .

analyzed data by reserve component categories ( for example , selected reserve and individual ready reserve ) rather than by reserve component as we had asked .

by analyzing the number of days a reservist was deployed by reserve component category , a reservist could be counted multiple times within one component if he or she changed category .

this error affected the way in which the total number of days a reservist was deployed was calculated .

for example , if the same reservist served 350 days as an army national guard selected reserve member and an additional 350 days as an army national guard individual ready reserve member , he or she would be counted as two reservists who were each deployed for less than a year .

however , our intent was to report that the same individual had been deployed for a total of 700 days .

in our analysis , all of a reservist's days deployed were totaled and counted once for each reserve component , regardless of which category he or she belonged to when deployed .

miscoded the end date for the analysis of how many days reservists were deployed for gwot .

this resulted in up to an additional 90 days of deployment being counted for reservists who were still deployed at the time the data were submitted to dmdc .

in our discussions with dmdc officials , they readily acknowledged that errors had been made , although they stated that the analyses had undergone supervisory review prior to our receiving them .

during these discussions , we also discovered that many of these errors occurred because dmdc had not used all of our data analyses parameters , although these officials had stated that this was one of the verification process steps followed .

although we were able to work with dmdc officials and identify the analytical assumptions they were going to use to complete our analyses , without documented analytical procedures , it is unclear to what degree the analyses dmdc provides to other users of the data also contain errors since many may not similarly verify the analyses provided to them by dmdc .

in addition , dmdc officials have not documented additional processes that would further support a verification process , such as ( 1 ) the ongoing , monthly validation process of service - provided data and ( 2 ) the procedures to perform analyses and generate reports , including the assumptions dmdc uses when producing periodic and special reports for customers .

in the past , according to the services , the ongoing , monthly validation process dmdc used resulted in two sets of data — one set of service data and one set of dmdc data — that may not have been the same .

for example , we were told by the air force that , in some cases , service data were replaced with default values because of a business rule that dmdc applied to the data and that this change resulted in errors to the service - provided data .

these inconsistent data caused the joint staff to request that the services and dmdc reconcile the data .

as stated above , there were errors in the analyses performed to generate the reports dmdc provided to us , including dmdc's not using many of the assumptions we agreed to for the analyses .

dmdc also made errors that contradicted its own undocumented policy .

a senior dmdc official said dmdc has not documented these procedures because the organization emphasizes getting data and reports to its customers in a timely manner rather than preparing this documentation .

this official said that documentation is not a top priority because situations change rapidly , and it would be hard to keep these documents up - to - date .

the official also said that the errors made in the analyses provided to us were caused by human error and the need to provide data quickly .

further , the dmdc official said that while there are standard data requests that are generated frequently , gao's request was an ad hoc request , and the procedures for addressing such requests , in practice , are not as well defined .

while we agree that our requests met dmdc's definition of an ad hoc request , we disagree that sufficient time was not allowed for dmdc to prepare the analyses .

for the initial request , we worked with dmdc over the course of about 5 business days to define the analytical assumptions that would be used during the analysis .

dmdc then took about 8 business days to complete the analysis and provide it to us .

dod data analyses are important because decision makers at dod and in congress need the data to make sound decisions about reserve force availability , medical surveillance , and planning and budgeting .

in the absence of documented procedures and the necessary controls to ensure that they are implemented , it is difficult for an organization to ensure that it has established a robust process that is being consistently applied and that accurate results are being achieved .

joint staff and reserve affairs officials are emphasizing the need to use one data source for most analyses to further reduce the inconsistencies in data analyses because service - produced analyses and dmdc - produced analyses could differ if both are not using the same set of data and assumptions .

otherwise , it is possible that the data analyses provided to decision makers at dod or in congress will be incomplete and inconsistent .

if the data analyses are incorrect , users could draw erroneous conclusions based on the data , which could lead to policies that affect reservists in unanticipated ways .

dod recognizes the need for accurate , complete , and consistent data and data analyses , and it has taken some preliminary , ad hoc steps to improve its data , including undertaking a considerable effort to rebaseline its mobilization data .

it has not , however , addressed some of the inconsistencies in data and data analyses departmentwide , such as when terms are used differently from one service to the next .

further , service officials stated that it is anticipated that a lot of these problems will be addressed when dimhrs is implemented .

however , the schedule for dimhrs continues to slip , so it is unclear when this solution will be available .

we recognize that the need for accurate , complete , and consistent data and data analyses about reservist mobilization and deployment is always important , and even more so during higher levels of mobilization and deployment , such as is the case now with gwot .

this is especially true since , in general , there are restrictions on the maximum length of time a reservist can be involuntarily activated .

thus , having accurate and complete data on a reservist's status is critical for determining availability for future deployments .

this is especially true of the cts data since the manpower and personnel office in the joint staff and the office of the assistant secretary of defense for reserve affairs mostly use the data found in cts .

these data also help dod and congress to understand the potential impacts of policy decisions as they relate to reservists who are eligible for tricare reserve select and educational benefits based on the number of days a reservist is deployed .

dod has not provided guidance to the services to better define and standardize the use of key terms .

dod also has not collected and maintained all essential data nor has it established a process for ensuring that data inconsistencies are resolved .

further , dod has not documented key procedures and processes for verifying the data analyses it provides to its customers , thus compromising its ability to ensure the accuracy , completeness , and consistency of these analyses .

until decision makers in dod and congress have accurate , complete , and consistent data and analyses , they will not be in the best position to make informed decisions about the myriad of reserve deployment matters .

we recommend that the secretary of defense take the following four actions: direct the under secretary of defense , personnel and readiness , to provide guidance to the services to better define and standardize the use of key terms , like activation , mobilization , and deployment , to promote the completeness , accuracy , and consistency of the data within cts .

direct the service secretaries to ( 1 ) take the steps necessary to provide all required data to dmdc , such as volunteer status and location deployed , and ( 2 ) have the services address data inconsistencies identified by dmdc .

direct the service secretaries to establish the needed protocols to have the services report data consistent with the guidance above .

direct the under secretary of defense , personnel and readiness , to require dmdc to document its internal procedures and processes , including the assumptions it uses in data analyses .

in doing this , the under secretary of defense , personnel and readiness , should collaborate on the reasonableness of the assumptions established and used by dmdc in its data analyses with the office of the assistant secretary of defense for reserve affairs and the joint staff .

the under secretary of defense , personnel and readiness , provided written comments on a draft of this report and stated that we changed one of our original audit objectives and did not inform the department of this change .

we disagree .

while the scope of our audit did change after our initial notification letter of june 17 , 2005 , was sent to dod , we notified the proper officials of this change in a december 2 , 2005 , email to the agency - designated liaison within the dod inspector general's office .

in this email , we specifically said that we would be contacting dmdc and that we would be focusing on data for reserve component activation , mobilization , and deployment for gwot .

in accordance with generally accepted government auditing standards ( gagas ) , gao analysts are expected , as appropriate , to review an agency's internal controls as they relate to the scope of the performance audit .

specifically , we are required by gagas to review the reliability of the data and the data analyses provided to us .

to assess the reliability of data and data analyses , we often review an agency's internal controls that are put in place to ensure the accuracy of the data and analyses .

as we discuss in our report , we found the data to be sufficiently reliable for our purposes .

however , over the course of the work , the analyses of the data dmdc provided to us continued to have errors .

this raised concerns about the adequacy of dmdc's internal controls for preparing and verifying these analyses , which dmdc stated were not documented .

in accordance with gagas , when reporting on the results of their work , auditors are responsible for disclosing all material or significant facts known to them which , if not disclosed , could mislead knowledgeable users or misrepresent the results .

consistent errors in dmdc's analyses led us to include an audit objective on the reliability of the data and the data analyses .

in its written comments , dod generally concurred with three of our recommendations and did not concur with one of our recommendations .

dod also provided technical comments , which we have incorporated in the report , as appropriate .

regarding our recommendation that dod provide guidance to the services to better define and standardize the use of key terms , dod stated that this requirement has already been addressed because these terms are defined .

we acknowledged in our draft report that these key terms are defined in the department of defense dictionary of military and associated terms .

however , as we state in our report , our audit work indicates that the services are not operationalizing the use of the terms in a consistent manner .

the intent of our recommendation is to have dod standardize the use of the key terms across the services .

dod generally concurred with our recommendation that the services provide all required data to dmdc and address data inconsistencies , and stated that the services have been directed to provide all necessary data and are working to address data inconsistencies .

while we agree that the services are working with dmdc to address data inconsistencies with regard to the rebaselining of mobilization data , we also identified other data inconsistencies that dod has not addressed , such as social security numbers that are duplicated in more than one reserve component .

we agree with dod that some requirements cannot be immediately supported by service data systems and modifications to them can take time to complete .

however , as our report notes , some service officials stated that resources are being diverted from these efforts to the dimhrs program , which we reported is behind schedule .

we continue to observe the need for the services to provide all necessary data , to address these data inconsistencies , and to establish needed protocols to have the services report data consistent with dod guidance , especially since the data are used to determine reserve force availability and for medical surveillance .

dod also generally concurred with our recommendation that dmdc document its internal procedures and processes , including the assumptions it uses in data analyses .

in its written comments , dod stated that dmdc is in the process of developing documentation on its internal procedures and processes and has a draft that addresses the processes used from receipt of the data from the service components to the final quality control of the consolidated file .

dod also stated that dmdc has a draft product regarding many of the data analyses procedures used .

during this engagement , we asked if these procedures and processes were documented .

as we say in the report , dmdc stated that they were undocumented and that documenting them was not a priority .

although dod stated that it is in the process of drafting these procedures and processes , we were never provided a draft of these documents .

dod also stated that while dmdc attempts to document the assumptions made in resulting report titles and footnotes , the disclosure of assumptions used in data analyses remain the responsibility of the requester of the data analyses .

although we agree that the requesters of the data bear responsibility to disclose the analytical assumptions used in the data analyses , our audit work indicates that there are basic assumptions that dmdc establishes and uses that , if documented and discussed with those who request data analyses , would allow the users to understand how the information can be used , as well as the limitations of the data analyses .

for example , during a discussion with a reserve affairs official , who uses the data analyses provided by dmdc to provide information to senior dod officials , we stated that dmdc defaults to using a servicemember's first deployment rather than the most current deployment when preparing data analyses .

this official was unaware that dmdc used this assumption and stated that the expectation was that dmdc was using the most current deployment to generate the analyses .

this official planned to discuss this issue with dmdc in the future .

in its written comments , dod did not concur with what it characterized as our fourth recommendation .

specifically , dod separated a single recommendation into two recommendations .

in the draft report we sent to dod , the recommendation read: “we recommend that the secretary of defense direct the under secretary of defense , personnel and readiness , to require dmdc to document its internal procedures and processes , including the assumptions it uses in data analyses .

in doing this , the under secretary of defense , personnel and readiness , should collaborate on the reasonableness of the assumptions used by dmdc in its data analyses with the office of the assistant secretary of defense for reserve affairs and the joint staff.” dod stated that dmdc is a support organization that generates reports for a multitude of organizations and that each organization that requests reports provides the assumptions that dmdc uses to develop the reports .

however , our audit work showed that dmdc has established and uses some basic assumptions in analyzing data and that dmdc may not always discuss these assumptions with other dod offices , such as reserve affairs .

as a result , we continue to emphasize the need for dmdc to document these assumptions and to collaborate with these offices to ensure a common understanding of these assumptions .

although dod organizations can request data analyses using multiple assumptions , without written documentation other organizations may not be fully aware of the analytical assumptions used by dmdc and this may lead to miscommunication and , ultimately , the data analyses may not be valid in that it does not report what the user intended .

we continue to believe that the assumptions used need to be documented and discussed with other dod offices as we recommended .

based on dod's comments , we modified this recommendation to clarify our intent .

we are sending copies of this report to the secretary of defense ; the secretaries of the army , the navy , and the air force ; the commandant of the marine corps ; the under secretary of defense , personnel and readiness ; and other interested parties .

we will also make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions on this report , please contact me at ( 202 ) 512-5559 or stewartd@gao .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iv .

our objectives were to determine ( 1 ) what department of defense ( dod ) data indicate are the number of reservists mobilized and deployed in support of the global war on terrorism ( gwot ) , and the selected demographic and deployment characteristics of those deployed and ( 2 ) whether dod's reserve deployment and mobilization data and analyses are reliable .

we identified , based on congressional interest and our knowledge of dod issues , selected demographic and deployment variables to review .

we then worked with the defense manpower data center ( dmdc ) to identify the data fields within dmdc's contingency tracking system ( cts ) that best provided information about the selected demographic and deployment variables we wanted to analyze .

although we wanted to analyze the locations to which reservists were deployed and the units with which reservists were deployed , dmdc officials said , and we agreed based on our review of the data , that the data were not reliable enough for those purposes .

our selected variables included the number of deployed reservists who volunteered for at least one deployment ; the number of deployed reservists who have served one , two , or three or more deployments ; the race and ethnicity of the deployed reservists ; the gender of the deployed reservists ; the state of residence of the deployed reservists ; the number of deployed reservists who were selected reserve , individual ready reserve , standby reserve , or retired reserve ; the number of deployed reservists who were citizens at the time of their deployment ; the number of days the reservists were deployed ; and the top occupational areas for reservists deployed in support of gwot .

to address objective 1 , we obtained and analyzed data for september 2001 through june 2006 from dmdc's cts .

cts consists of two files — the activation file , which tracks activations and mobilizations , and the deployment file , which tracks deployments .

using cts data from both files , we analyzed the number of national guard and reserve servicemembers mobilized and deployed in support of gwot , as well as selected demographic and deployment variables , using statistical analysis software .

to address objective 2 , we performed a data reliability assessment on the data provided by dmdc from cts' activation and deployment files .

we requested dmdc reports that replicated our analyses and then compared those report results to our analyses , and we reviewed the programming code dmdc used to generate those reports .

to assess the reliability of cts data , we obtained an understanding of the data , the file structure , the sources of the data , and relevant dod guidance .

specifically , we ( 1 ) performed electronic testing of the data files for completeness ( that is , missing data ) , out - of - range values , and dates outside of valid time frames ; ( 2 ) assessed the relationships among data elements ( for example , determining whether deployment dates were overlapping since each record in the deployment file is intended to represent one deployment ) ; ( 3 ) reviewed existing information about the data and the systems that produced them ; ( 4 ) interviewed department officials to identify known problems or limitations in the data , as well as to understand the relationship between the two files and how data are received from the services , cleaned ( “rebaselined” ) , and processed by dmdc ; and ( 5 ) compared “prerebaselined” mobilization data to “postrebaselined” mobilization data to determine the extent to which the data changed as a result of the cleaning effort .

when we found discrepancies ( for example , overlapping deployment dates ) , we worked with dmdc to understand the discrepancies .

in our interviews with dmdc officials , we discussed the purpose and uses of cts , the service data rebaselining effort and the internal controls for verifying data analyses , monthly validation of data , and performing data analyses .

similarly , we discussed data collection , processing , and reliability issues as well as service - specific data issues and the rebaselining effort with officials from the office of the assistant secretary of defense for reserve affairs and from each of the reserve components , including the u.s. army national guard , the u.s. air national guard , the u.s. army reserve , the u.s. navy reserve , the u.s. marine corps reserve , and the u.s. air force reserve .

we also discussed the reliability of the services' data , the rebaselining effort , and the results of a previous joint staff review of the quality of service data within cts with officials in the joint chiefs of staff manpower and personnel office .

finally , we interviewed officials from the deployment health surveillance directorate and the army medical surveillance activity about the quality of the deployment data and how they use the data .

in the course of our review , we determined that some data fields were highly unreliable .

for example , electronic testing indicated that data on location and reservist unit information were missing in many cases .

based on our conversations with dmdc and our understanding of the data system , we decided not to conduct lower level analyses ( for example , analyses of reservists' assigned units ) because the results would be less reliable than aggregate level analyses .

although we are reasonably confident in the reliability of most cts data fields at the aggregate level , because we could not compare source documentation from each of the services to a sample of dmdc data , we could not estimate precise margins of error .

consequently , we used the data for descriptive purposes , and we did not base any recommendations on the results of our analyses .

in addition , we presented only higher level , aggregate data from fields that we determined were sufficiently reliable for our reporting purposes .

for these purposes , and presented in this way , the cts data we use are sufficiently reliable with the following caveat: the army had not completed its rebaselining effort for mobilization data before the completion of our review , and we could not , therefore , assess the reliability of army mobilization data to the same extent as those of the other services .

however , based on our electronic testing , data comparisons , and interviews with officials , we believe that the data are sufficiently reliable to present as descriptive information .

to assess the reliability of dmdc's reports ( that is , its own analyses ) of cts data , we compared our independent analyses of national guard and reserve servicemembers' mobilization and deployment statistics with results that dmdc provided from its own analyses of the same data .

to pinpoint differences in analytical assumptions , we reviewed the statistical code dmdc used to produce its reports and compared it with our programming code .

through an iterative process , we noted errors in dmdc's programs and requested changes and reruns of the data .

we worked with dmdc to ensure that discrepancies were not caused by differences in our analytical assumptions .

where there were discrepancies , we reached the following consensus on how to address them: removed the coast guard entries from our analyses of the cts database since , as we state in this report , the coast guard reserve is under the day - to - day control of the department of homeland security rather than dod .

combined a reservist's social security number with his or her reserve component to create a unique identifier .

dmdc officials said they do this because they are unsure where the source of the error is when they find that a social security number corresponds with two reserve components for a deployment during approximately the same time period .

dod's policy , when there is a duplicate social security number for more than one reserve component , is to count both transactions .

however , the use of duplicate social security numbers results in overcounting .

specifically , the june 2006 file had 38 reservists with overlapping mobilizations , 20 reservists with overlapping deployments , and more than 800 deployed reservists who appeared to have legitimately changed components .

to compensate for the 58 “errors” where dmdc did not know which mobilization or deployment to count , it double - counted all 58 reservists .

likewise , the 800 deployed reservists who changed reserve components during gwot were also double - counted .

removed reservists from all analyses when their reserve component category is unknown , so that the numeric totals across analyses would be consistent .

dmdc officials said that this is an undocumented standard operating procedure .

utilized the reservists' information for most recent deployment to provide the most current information possible in cases where a reservist deployed more than once .

calculated the length of a reservist's deployment by including both the day the deployment began and the day on which the deployment ended .

thus , the number of days deployed is inclusive of the beginning and end dates .

combined the race categories for asian , native hawaiian , and other pacific islander because , prior to 2003 , the distinction between these two groups was not captured in the data .

after clarifying and agreeing on the analytical assumptions , we again reviewed dmdc's code and compared its results with our own to determine whether and why there were remaining discrepancies .

we also requested written documentation of dmdc's internal control procedures for the cts data and , when no documentation was available , interviewed knowledgeable officials about existing internal control procedures .

using the framework of standards for internal control for the federal government , we compared the information from those documents and interviews with our numerous , iterative reviews of dmdc's statistical programs used to generate comparative reports to assess the reliability of dmdc - generated reports from cts .

we determined that the reports dmdc generated for our review were not sufficiently reliable for our reporting purpose .

thus , we completed our own data analyses .

we performed our work from december 2005 through august 2006 in accordance with generally accepted government auditing standards .

our analysis of dod data indicates that most reservists who deployed in support of gwot through june 30 , 2006 , were part of the selected reserve ( see table 5 ) .

in addition , california , texas , pennsylvania , and florida had the highest numbers of reservists who have deployed in support of gwot through june 30 , 2006 ( see table 6 ) .

in addition to the contact named above , cynthia jackson , assistant director ; crystal bernard ; tina kirschbaum ; marie a. mak ; ricardo marquez ; julie matta ; lynn milan ; rebecca shea ; and cheryl weissman made key contributions to this report .

