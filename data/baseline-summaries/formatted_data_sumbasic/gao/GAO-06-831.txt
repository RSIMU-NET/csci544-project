a well - defined enterprise architecture is an essential tool for leveraging information technology ( it ) in the transformation of business and mission operations .

our experience with federal departments and agencies has shown that attempting to modernize and evolve it environments without an enterprise architecture to guide and constrain investments often results in operations and systems that are duplicative , not well integrated , unnecessarily costly to maintain and interface , and ineffective in supporting mission goals .

moreover , the development , implementation , and maintenance of architectures are widely recognized as hallmarks of successful public and private organizations , and their use is required by the clinger - cohen act and the office of management and budget ( omb ) .

in light of the importance of these architectures , you requested that we determine the current status of major federal department and agency enterprise architecture efforts .

to accomplish our objective , we surveyed 27 major federal departments and agencies using a questionnaire that was based on our maturity framework for assessing and improving enterprise architecture management , and we collected and reviewed documentation to verify agency responses .

we then analyzed the results to determine the extent to which each of the 27 satisfied our maturity framework , and the challenges and benefits that each department and agency sees .

we also collected information about , for example , department and agency architecture costs and architecture framework and tool use and satisfaction , which is summarized in appendixes i and ii .

because our framework defines what needs to be done to effectively manage an enterprise architecture program , and not the details surrounding how it needs to be done , the scope of our review did not include assessing the quality of enterprise architecture products and activities and associated management structures and processes that make up our framework .

as such , scoring high on our maturity scale should be viewed as an indicator of , and not a guarantee that , a department or agency necessarily has a well - defined architecture and that it is being effectively implemented .

we conducted our work in accordance with generally accepted government auditing standards .

details of our objective , scope , and methodology are in appendix iii .

an enterprise architecture is a blueprint that describes the current and desired state of an organization or functional area in both logical and technical terms , as well as a plan for transitioning between the two states .

enterprise architectures are a recognized tenet of organizational transformation and it management in public and private organizations .

without an enterprise architecture , it is unlikely that an organization will be able to transform business processes and modernize supporting systems to minimize overlap and maximize interoperability .

the concept of enterprise architectures originated in the mid - 1980s ; various frameworks for defining the content of these architectures have been published by government agencies and omb .

moreover , legislation and federal guidance requires agencies to develop and use architectures .

for more than a decade , we have conducted work to improve agency architecture efforts .

to this end , we developed an enterprise architecture management maturity framework that provides federal agencies with a common benchmarking tool for assessing the management of their enterprise architecture efforts and developing improvement plans .

an enterprise can be viewed as either a single organization or a functional area that transcends more than one organization ( eg , financial management , homeland security ) .

an architecture can be viewed as the structure ( or structural description ) of any activity .

thus , enterprise architectures are basically systematically derived and captured descriptions — in useful models , diagrams , and narrative .

more specifically , an architecture describes the enterprise in logical terms ( such as interrelated business processes and business rules , information needs and flows , and work locations and users ) as well as in technical terms ( such as hardware , software , data , communications , and security attributes and performance standards ) .

it provides these perspectives both for the enterprise's current or “as - is” environment and for its target or “to - be” environment , as well as a transition plan for moving from the “as - is” to the “to - be” environment .

the importance of enterprise architectures is a basic tenet of both organizational transformation and it management , and their effective use is a recognized hallmark of successful public and private organizations .

for over a decade , we have promoted the use of architectures , recognizing them as a crucial means to a challenging end: optimized agency operations and performance .

the alternative , as our work has shown , is the perpetuation of the kinds of operational environments that burden most agencies today , where a lack of integration among business operations and the it resources supporting them leads to systems that are duplicative , poorly integrated , and unnecessarily costly to maintain and interface .

employed in concert with other important it management controls ( such as portfolio - based capital planning and investment control practices ) , architectures can greatly increase the chances that the organizations' operational and it environments will be configured so as to optimize mission performance .

during the mid - 1980s , john zachman , widely recognized as a leader in the field of enterprise architecture , identified the need to use a logical construction blueprint ( i.e. , an architecture ) for defining and controlling the integration of systems and their components .

accordingly , zachman developed a structure or framework for defining and capturing an architecture , which provides for six perspectives or “windows” from which to view the enterprise .

zachman also proposed six abstractions or models associated with each of these perspectives .

zachman's framework provides a way to identify and describe an entity's existing and planned component parts and the parts' relationships before the entity begins the costly and time - consuming efforts associated with developing or transforming itself .

since zachman introduced his framework , a number of frameworks have emerged within the federal government , beginning with the publication of the national institute of standards and technology ( nist ) framework in 1989 .

since that time , other federal entities have issued frameworks , including the department of defense ( dod ) and the department of the treasury .

in september 1999 , the federal chief information officers ( cio ) council published the federal enterprise architecture framework ( feaf ) , which was intended to provide federal agencies with a common construct for their architectures , thereby facilitating the coordination of common business processes , technology insertion , information flows , and system investments among federal agencies .

the feaf described an approach , including models and definitions , for developing and documenting architecture descriptions for multi - organizational functional segments of the federal government .

more recently , omb established the federal enterprise architecture program management office ( feapmo ) to develop a federal enterprise architecture according to a collection of five reference models ( see table 1 ) .

these models are intended to facilitate governmentwide improvement through cross - agency analysis and the identification of duplicative investments , gaps , and opportunities for collaboration , interoperability , and integration within and across government agencies .

omb has identified multiple purposes for the federal enterprise architecture , such as the following: informing agency enterprise architectures and facilitating their development by providing a common classification structure and vocabulary ; providing a governmentwide framework that can increase agency awareness of it capabilities that other agencies have or plan to acquire , so that they can explore opportunities for reuse ; helping omb decision makers identify opportunities for collaboration among agencies through the implementation of common , reusable , and interoperable solutions ; and providing the congress with information that it can use as it considers the authorization and appropriation of funding for federal programs .

although these post - zachman frameworks differ in their nomenclatures and modeling approaches , each consistently provides for defining an enterprise's operations in both logical and technical terms , provides for defining these perspectives for the enterprise's current and target environments , and calls for a transition plan between the two .

several laws and regulations address enterprise architecture .

for example , the clinger - cohen act of 1996 directs the cios of major departments and agencies to develop , maintain , and facilitate the implementation of information technology architectures as a means of integrating agency goals and business processes with information technology .

also , omb circular a - 130 , which implements the clinger - cohen act , requires that agencies document and submit their initial enterprise architectures to omb and that agencies submit updates when significant changes to their enterprise architectures occur .

the circular also directs omb to use various reviews to evaluate the adequacy and efficiency of each agency's compliance with the circular .

we began reviewing federal agencies' use of enterprise architectures in 1994 , initially focusing on those agencies that were pursuing major systems modernization programs that were high risk .

these included the national weather service systems modernization , the federal aviation administration ( faa ) air traffic control modernization , and the internal revenue service tax systems modernization .

generally , we reported that these agencies' enterprise architectures were incomplete , and we made recommendations that they develop and implement complete enterprise architectures to guide their modernization efforts .

since then , we have reviewed enterprise architecture management at other federal agencies , including the department of education ( education ) , the customs service , the immigration and naturalization service , the centers for medicare and medicaid services , faa , and the federal bureau of investigation ( fbi ) .

we have also reviewed the use of enterprise architectures for critical agency functional areas , such as the integration and sharing of terrorist watch lists across key federal departments and dod financial management , logistics management , combat identification , and business systems modernization .

these reviews continued to identify the absence of complete and enforced enterprise architectures , which in turn has led to agency business operations , systems , and data that are duplicative , incompatible , and not integrated ; these conditions have either prevented agencies from sharing data or forced them to depend on expensive , custom - developed system interfaces to do so .

accordingly , we made recommendations to improve the respective architecture efforts .

in some cases progress has been made , such as at dod and fbi .

as a practical matter , however , considerable time is needed to completely address the kind of substantive issues that we have raised and to make progress in establishing more mature architecture programs .

in 2002 and 2003 , we also published reports on the status of enterprise architectures governmentwide .

the first report ( february 2002 ) showed that about 52 percent of federal agencies self - reported having at least the management foundation that is needed to successfully develop , implement , and maintain an enterprise architecture , and that about 48 percent of agencies had not yet advanced to that basic stage of maturity .

we attributed this state of architecture management to four management challenges: ( 1 ) overcoming limited executive understanding , ( 2 ) inadequate funding , ( 3 ) insufficient number of skilled staff , and ( 4 ) organizational parochialism .

additionally , we recognized omb's efforts to promote and oversee agencies' enterprise architecture efforts .

nevertheless , we determined that omb's leadership and oversight could be improved by , for example , using a more structured means of measuring agencies' progress and by addressing the above management challenges .

the second report ( november 2003 ) showed the percentage of agencies that had established at least a foundation for enterprise architecture management was virtually unchanged .

we attributed this to long - standing enterprise architecture challenges that had yet to be addressed .

in particular , more agencies reported lack of agency executive understanding of enterprise architecture and the scarcity of skilled architecture staff as significant challenges .

omb generally agreed with our findings and the need for additional agency assessments .

further , it stated that fully implementing our recommendations would require sustained management attention , and that it had begun by working with the cio council to establish the chief architect forum and to increase the information omb reports on enterprise architecture to congress .

since then , omb has developed and implemented an enterprise architecture assessment tool .

according to omb , the tool helps better understand the current state of an agency's architecture and assists agencies in integrating architectures into their decision - making processes .

the latest version of the assessment tool ( 2.0 ) was released in december 2005 and includes three capability areas: ( 1 ) completion , ( 2 ) use , and ( 3 ) results .

table 2 describes each of these areas .

the tool also includes criteria for scoring an agency's architecture program on a scale of 0 to 5 .

in early 2006 , the major departments and agencies were required by omb to self assess their architecture programs using the tool .

omb then used the self assessment to develop its own assessment .

these assessment results are to be used in determining the agency's e - government score within the president's management agenda .

in 2002 , we developed version 1.0 of our enterprise architecture management maturity framework ( eammf ) to provide federal agencies with a common benchmarking tool for planning and measuring their efforts to improve enterprise architecture management , as well as to provide omb with a means for doing the same governmentwide .

we issued an update of the framework ( version 1.1 ) in 2003 .

this framework is an extension of a practical guide to federal enterprise architecture , version 1.0 , published by the cio council .

version 1.1 of the framework arranges 31 core elements ( practices or conditions that are needed for effective enterprise architecture management ) into a matrix of five hierarchical maturity stages and four critical success attributes that apply to each stage .

within a given stage , each critical success attribute includes between one and four core elements .

based on the implicit dependencies among the core elements , the eammf associates each element with one of five maturity stages ( see fig .

1 ) .

the core elements can be further categorized by four groups: architecture governance , content , use , and measurement .

stage 1: creating ea awareness .

at stage 1 , either an organization does not have plans to develop and use an architecture , or it has plans that do not demonstrate an awareness of the value of having and using an architecture .

while stage 1 agencies may have initiated some enterprise architecture activity , these agencies' efforts are ad hoc and unstructured , lack institutional leadership and direction , and do not provide the management foundation necessary for successful enterprise architecture development as defined in stage 2 .

stage 2: building the ea management foundation .

an organization at stage 2 recognizes that the enterprise architecture is a corporate asset by vesting accountability for it in an executive body that represents the entire enterprise .

at this stage , an organization assigns enterprise architecture management roles and responsibilities and establishes plans for developing enterprise architecture products and for measuring program progress and product quality ; it also commits the resources necessary for developing an architecture — people , processes , and tools .

specifically , a stage 2 organization has designated a chief architect and established and staffed a program office responsible for enterprise architecture development and maintenance .

further , it has established a committee or group that has responsibility for enterprise architecture governance ( i.e. , directing , overseeing , and approving architecture development and maintenance ) .

this committee or group membership has enterprisewide representation .

at stage 2 , the organization either has plans for developing or has started developing at least some enterprise architecture products , and it has developed an enterprisewide awareness of the value of enterprise architecture and its intended use in managing its it investments .

the organization has also selected a framework and a methodology that will be the basis for developing the enterprise architecture products and has selected a tool for automating these activities .

stage 3: developing the ea .

an organization at stage 3 focuses on developing architecture products according to the selected framework , methodology , tool , and established management plans .

roles and responsibilities assigned in the previous stage are in place , and resources are being applied to develop actual enterprise architecture products .

at this stage , the scope of the architecture has been defined to encompass the entire enterprise , whether organization - based or function - based .

although the products may not be complete , they are intended to describe the organization in terms of business , performance , information / data , service / application , and technology ( including security explicitly in each ) as provided for in the framework , methodology , tool , and management plans .

further , the products are to describe the current ( as - is ) and future ( to - be ) states and the plan for transitioning from the current to the future state ( the sequencing plan ) .

as the products are developed and evolve , they are subject to configuration management .

further , through the established enterprise architecture management foundation , the organization is tracking and measuring its progress against plans , identifying and addressing variances , as appropriate , and then reporting on its progress .

stage 4: completing the ea .

an organization at stage 4 has completed its enterprise architecture products , meaning that the products have been approved by the enterprise architecture steering committee ( established in stage 2 ) or an investment review board , and by the cio .

the completed products collectively describe the enterprise in terms of business , performance , information / data , service / application , and technology for both its current and future operating states , and the products include a plan for transitioning from the current to the future state .

further , an independent agent has assessed the quality ( i.e. , completeness and accuracy ) of the enterprise architecture products .

additionally , evolution of the approved products is governed by a written enterprise architecture maintenance policy approved by the head of the organization .

stage 5: leveraging the ea to manage change .

an organization at stage 5 has secured senior leadership approval of the enterprise architecture products and a written institutional policy stating that it investments must comply with the architecture , unless granted an explicit compliance waiver .

further , decision makers are using the architecture to identify and address ongoing and proposed it investments that are conflicting , overlapping , not strategically linked , or redundant .

as a result , stage 5 entities avoid unwarranted overlap across investments and ensure maximum systems interoperability , which in turn ensures the selection and funding of it investments with manageable risks and returns .

also , at stage 5 , the organization tracks and measures enterprise architecture benefits or return on investment , and adjustments are continuously made to both the enterprise architecture management process and the enterprise architecture products .

attribute 1: demonstrates commitment .

because the enterprise architecture is a corporate asset for systematically managing institutional change , the support and sponsorship of the head of the enterprise are essential to the success of the architecture effort .

an approved enterprise policy statement provides such support and sponsorship , promoting institutional buy - in and encouraging resource commitment from participating components .

equally important in demonstrating commitment is vesting ownership of the architecture with an executive body that collectively owns the enterprise .

attribute 2: provides capability to meet commitment .

the success of the enterprise architecture effort depends largely on the organization's capacity to develop , maintain , and implement the enterprise architecture .

consistent with any large it project , these capabilities include providing adequate resources ( i.e. , people , processes , and technology ) , defining clear roles and responsibilities , and defining and implementing organizational structures and process management controls that promote accountability and effective project execution .

attribute 3: demonstrates satisfaction of commitment .

satisfaction of the organization's commitment to develop , maintain , and implement an enterprise architecture is demonstrated by the production of artifacts ( eg , the plans and products ) .

such artifacts demonstrate follow through — that is , actual enterprise architecture production .

satisfaction of commitment is further demonstrated by senior leadership approval of enterprise architecture documents and artifacts ; such approval communicates institutional endorsement and ownership of the architecture and the change that it is intended to drive .

attribute 4: verifies satisfaction of commitment .

this attribute focuses on measuring and disclosing the extent to which efforts to develop , maintain , and implement the enterprise architecture have fulfilled stated goals or commitments of the enterprise architecture .

measuring such performance allows for tracking progress that has been made toward stated goals , allows appropriate actions to be taken when performance deviates significantly from goals , and creates incentives to influence both institutional and individual behaviors .

the framework's 31 core elements can also be placed in one of four groups of architecture related activities , processes , products , events , and structures .

the groups are architecture governance , content , use , and measurement .

these groups are generally consistent with the capability area descriptions in the previously discussed omb enterprise architecture assessment tool .

for example , omb's completion capability area addresses ensuring that architecture products describe the agency in terms of processes , services , data , technology , and performance and that the agency has developed a transition strategy .

similarly , our content group includes developing and completing these same enterprise architecture products .

in addition , omb's results capability area addresses performance measurement as does our measurement group , and omb's use capability area addresses many of the same elements in our governance and use groups .

table 3 lists the core elements according to eammf group .

most of the 27 major departments and agencies have not fully satisfied all the core elements associated with stage 2 of our maturity framework .

at the same time , however , most have satisfied a number of core elements at stages 3 , 4 , and 5 .

specifically , although only seven have fully satisfied all the stage 2 elements , the 27 have on average fully satisfied 80 , 78 , 61 , and 52 percent of the stage 2 , 3 , 4 , and 5 elements , respectively .

of the core elements that have been fully satisfied , 77 percent of those related to architecture governance have been fully satisfied , while 68 , 52 , and 47 percent of those related to architecture content , use , and measurement , respectively , have been fully satisfied .

most of the 27 have also at least partially satisfied a number of additional core elements across all the stages .

for example , all but 7 have at least partially satisfied all the elements required to achieve stage 3 or higher .

collectively , this means efforts are underway to mature the management of most agency enterprise architecture programs , but overall these efforts are uneven and still a work - in - progress and they face numerous challenges that departments and agencies identified .

it also means that some architecture programs provide examples from which less mature programs could learn and improve .

without mature enterprise architecture programs , some departments and agencies will not realize the many benefits that they attributed to architectures , and they are at risk of investing in it assets that are duplicative , not well - integrated , and do not optimally support mission operations .

to qualify for a given stage of maturity under our architecture management framework , a department or agency had to fully satisfy all of the core elements at that stage .

using this criterion , three departments and agencies are at stage 2 , meaning that they demonstrated to us through verifiable documentation that they have established the foundational commitments and capabilities needed to manage the development of an architecture .

in addition , four are at stage 3 , meaning that they similarly demonstrated that their architecture development efforts reflect employment of the basic control measures in our framework .

table 4 summarizes the maturity stage of each architecture program that we assessed .

appendix iv provides the detailed results of our assessment of each department and agency architecture program against our maturity framework .

while using this criterion provides an important perspective on the state of department and agency architecture programs , it can mask the fact that the programs have met a number of core elements across higher stages of maturity .

when the percentage of core elements that have been fully satisfied at each stage is considered , the state of the architecture efforts generally shows both a larger number of more robust architecture programs as well as more variability across the departments and agencies .

specifically , 16 departments and agencies have fully satisfied more than 70 percent of the core elements .

examples include commerce , which has satisfied 87 percent of the core elements , including 75 percent of the stage 5 elements , even though it is at stage 1 because its enterprise architecture approval board does not have enterprisewide representation ( a stage 2 core element ) .

similarly , ssa , which is also a stage 1 because the agency's enterprise architecture methodology does not describe the steps for developing , maintaining , and validating the agency's enterprise architecture ( a stage 2 core element ) , has at the same time satisfied 87 percent of all the elements , including 63 percent of the stage 5 elements .

in contrast , the army , which is also in stage 1 , has satisfied but 3 percent of all framework elements .

overall , 10 agency architecture programs fully satisfied more than 75 percent of the core elements , 14 between 50 and 75 percent , and 4 fewer than 50 percent .

these four included the three military departments .

table 5 summarizes for each department and agency the percentage of core elements fully satisfied in total and by maturity stage .

notwithstanding the additional perspective that the percentage of core elements fully satisfied across all stages provides , it is important to note that the staged core elements in our framework represent a hierarchical or systematic progression to establishing a well - managed architecture program , meaning that core elements associated with lower framework stages generally support the effective execution of higher maturity stage core elements .

for instance , if a program has developed its full suite of “as - is” and “to - be” architecture products , including a sequencing plan ( stage 4 core elements ) , but the products are not under configuration management ( stage 3 core element ) , then the integrity and consistency of the products will be not be assured .

our analysis showed that this was the case for a number of architecture programs .

for example , state has developed certain “as - is” and “to - be” products for the joint enterprise architecture , which is being developed in collaboration with usaid , but an enterprise architecture configuration management plan has not yet been finalized .

further , not satisfying even a single core element can have a significant impact on the effectiveness of an architecture program .

for example , not having adequate human capital with the requisite knowledge and skills ( stage 2 core element ) , not using a defined framework or methodology ( stage 2 core element ) , or not using an independent verification and validation agent ( stage 4 core element ) , could significantly limit the quality and utility of an architecture .

the dod's experience between 2001 and 2005 in developing its bea is a case in point .

during this time , we identified the need for the department to have an enterprise architecture for its business operations , and we made a series of recommendations grounded in , among other things , our architecture management framework to ensure that it was successful in doing so .

in 2005 , we reported that the department had not implemented most of our recommendations .

we further reported that despite developing multiple versions of a wide range of architecture products , and having invested hundreds of millions of dollars and 4 years in doing so , the department did not have a well - defined architecture and that what it had developed had limited utility .

among other things , we attributed the poor state of its architecture products to ineffective program governance , communications , program planning , human capital , and configuration management , most of which are stage 2 and 3 foundational core elements .

to the department's credit , we recently reported that it has since taken a number of actions to address these fundamental weaknesses and our related recommendations and that it is now producing architecture products that provide a basis upon which to build .

the significance of not satisfying a single core element is also readily apparent for elements associated with the framework's content group .

in particular , the framework emphasizes the importance of planning for , developing , and completing an architecture that includes the “as - is” and the “to - be” environments as well as a plan for transitioning between the two .

it also recognizes that the “as - is” and “to - be” should address the business , performance , information / data , application / service , technology , and security aspects of the enterprise .

to the extent these aspects are not addressed in this way , the quality of the architecture and thus its utility will suffer .

in this regard , we found examples of departments and agencies that were addressing some but not all of these aspects .

for example , hud has yet to adequately incorporate security into its architecture .

this is significant because security is relevant to all the other aspects of its architecture , such as information / data and applications / services .

as another example , nasa's architecture does not include a plan for transitioning from the “as - is” to the “to - be” environments .

according to the administration's chief enterprise architect , a transition plan has not yet been developed because of insufficient time and staff .

looking across all the departments and agencies at core elements that are fully satisfied , not by stage of maturity , but by related groupings of core elements , provides an additional perspective on the state of the federal government's architecture efforts .

as noted earlier , these groupings of core elements are architecture governance , content , use , and measurement .

overall , departments and agencies on average have fully satisfied 77 percent of the governance - related elements .

in particular , 93 and 96 percent of the agencies have established an architecture program office and appointed a chief architect , respectively .

in addition , 93 percent have plans that call for their respective architectures to describe the “as - is” and the “to - be” environments , and for having a plan for transitioning between the two ( see fig .

2 ) .

in contrast , however , the core element associated with having a committee or group with representation from across the enterprise directing , overseeing , and approving the architecture was fully satisfied by only 57 percent of the agencies .

this core element is important because the architecture is a corporate asset that needs to be enterprisewide in scope and accepted by senior leadership if it is to be leveraged for organizational change .

in contrast to governance , the extent of full satisfaction of those core elements that are associated with what an architecture should contain varies widely ( see fig .

3 ) .

for example , the three content elements that address prospectively what the architecture will contain , either in relation to plans or some provision for including needed content , were fully satisfied about 90 percent of the time .

however , the core elements addressing whether the products now contain such content were fully satisfied much less frequently ( between 54 and 68 percent of the time , depending on the core element ) , and the core elements associated with ensuring the quality of included content , such as employing configuration management and undergoing independent verification and validation , were also fully satisfied much less frequently ( 54 and 21 percent of the time , respectively ) .

the state of these core elements raises important questions about the quality and utility of the department and agency architectures .

the degree of full satisfaction of those core elements associated with the remaining two groups — use and measurement — is even lower ( see figs .

4 and 5 , respectively ) .

for example , the architecture use - related core elements were fully satisfied between 39 and 64 percent of the time , while the measurement - related elements were satisfied between 14 and 71 percent .

of particular note is that only 39 percent of the departments and agencies could demonstrate that it investments comply with their enterprise architectures , only 43 percent of the departments and agencies could demonstrate that compliance with the enterprise architecture is measured and reported , and only 14 percent were measuring and reporting on their respective architecture program's return on investment .

as our work and related best practices show , the value in having an architecture is using it to affect change and produce results .

such results , as reported by the departments and agencies include improved information sharing , increased consolidation , enhanced productivity , and lower costs , all of which contribute to improved agency performance .

to realize these benefits , however , it investments need to comply with the architecture and measurement of architecture activities , including accrual of expected benefits , needs to occur .

in those instances where departments and agencies have not fully satisfied certain core elements in our framework , most have at least partially satisfied these elements .

to illustrate , 4 agencies would improve to at least stage 4 if the criterion for being a given stage was relaxed to only partially satisfying a core element .

moreover , 11 of the remaining agencies would advance by two stages under such a less demanding criterion , and only 6 would not improve their stage of maturity under these circumstances .

a case in point is commerce , which could move from stage 1 to stage 5 under these circumstances because it has fully satisfied all but four core elements and these remaining four ( one each at stages 2 and 4 and two at stage 5 ) are partially satisfied .

another case in point is the ssa , which has fully satisfied all but four core elements ( one at stage 2 and three at stage 5 ) and has partially satisfied three of these remaining four .

if the criterion used allowed advancement to the next stage by only partially satisfying core elements , the administration would be stage 4 .

 ( see fig .

6 for a comparison of department and agency program maturity stages under the two criteria. ) .

as mentioned earlier , departments and agencies can require considerable time to completely address issues related to their respective enterprise architecture programs .

it is thus important to note that even though certain core elements are partially satisfied , fully satisfying some of them may not be accomplished quickly and easily .

it is also important to note the importance of fully , rather than partially , satisfying certain elements , such as those that fall within the architecture content group .

in this regard , 18 , 18 , and 21 percent of the departments and agencies partially satisfied the following stage 4 content - related core elements , respectively: “ea products describe ‘as - is' environment , ‘to - be' environment and sequencing plan” ; “both ‘as - is' and ‘to - be' environments are described in terms of business , performance , information / data , application / service , and technology” ; and “these descriptions fully address security.” not fully satisfying these elements can have important implications for the quality of an architecture , and thus its usability and results .

seven departments or agencies would meet our criterion for stage 5 if each was to fully satisfy one to five additional core elements ( see table 6 ) .

for example , interior could achieve stage 5 by satisfying one additional element: “ea products and management processes undergo independent verification and validation.” in this regard , interior officials have drafted a statement of work intended to ensure that independent verification and validation of enterprise architecture products and management processes is performed .

the other six departments and agencies are hud and opm , which could achieve stage 5 by satisfying two additional elements ; commerce , labor , and ssa , which could achieve the same by satisfying four additional elements ; and education which could be at stage 5 by satisfying five additional elements .

of these seven , five have not fully satisfied the independent verification and validation core element .

notwithstanding the fact that five or fewer core elements need to be satisfied by these agencies to be at stage 5 , it is important to note that in some cases the core elements not being satisfied are not only very important , but also neither quickly nor easily satisfied .

for example , one of the two elements that hud needs to satisfy is having its architecture products address security .

this is extremely important as security is an integral aspect of the architecture's performance , business , information / data , application / service , and technical models , and needs to be reflected thoroughly and consistently across each of them .

the challenges facing departments and agencies in developing and using enterprise architectures are formidable .

the challenge that most departments and agencies cited as being experienced to the greatest extent is the one that having and using an architecture is intended to overcome — organizational parochialism and cultural resistance to adopting an enterprisewide mode of operation in which organizational parts are sub - optimized in order to optimize the performance and results of the enterprise as a whole .

specifically , 93 percent of the departments and agencies reported that they encountered this challenge to a significant ( very great or great ) or moderate extent .

other challenges reported to this same extent were ensuring that the architecture program had adequate funding ( 89 percent ) , obtaining staff skilled in the architecture discipline ( 86 percent ) , and having the department or agency senior leaders understand the importance and role of the enterprise architecture ( 82 percent ) .

as we have previously reported , sustained top management leadership is the key to overcoming each of these challenges .

in this regard , our enterprise architecture management maturity framework provides for such leadership and addressing these and other challenges through a number of core elements .

these elements contain mechanisms aimed at , for example , establishing responsibility and accountability for the architecture with senior leaders and ensuring that the necessary institutional commitments are made to the architecture program , such as through issuance of architecture policy and provision of adequate resources ( both funding and people ) .

see table 7 for a listing of the reported challenges and the extent to which they are being experienced .

a large percentage of the departments and agencies reported that they have already accrued numerous benefits from their respective architecture programs ( see table 8 ) .

for example , 70 percent said that have already improved the alignment between their business operations and the it that supports these operations to a significant extent .

such alignment is extremely important .

according to our it investment management maturity framework , alignment between business needs and it investments is a critical process in building the foundation for an effective approach to it investment management .

in addition , 64 percent responded that they have also improved information / knowledge sharing to a significant or moderate extent .

such sharing is also very important .

in 2005 , for example , we added homeland security information sharing to our list of high - risk areas because despite the importance of information to fighting terrorism and maintaining the security of our nation , many aspects of homeland security information sharing remain ineffective and fragmented .

other examples of mission - effectiveness related benefits reported as already being achieved to a significant or moderate extent by roughly one - half of the departments and agencies included improved agency management and change management and improved system and application interoperability .

beyond these benefits , departments and agencies also reported already accruing , to a significant or moderate extent , a number of efficiency and productivity benefits .

for example , 56 percent reported that they have increased the use of enterprise software licenses , which can permit cost savings through economies of scale purchases ; 56 percent report that they have been able to consolidate their it infrastructure environments , which can reduce the costs of operating and maintaining duplicative capabilities ; 41 percent reported that they have been able to reduce the number of applications , which is a key to reducing expensive maintenance costs ; and 37 percent report productivity improvements , which can free resources to focus on other high priority matters .

notwithstanding the number and extent of benefits that department and agency responses show have already been realized , these same responses also show even more benefits that they have yet to realize ( see table 8 ) .

for example , 30 percent reported that they have thus far achieved , to little or no extent , better business and it alignment .

they similarly reported that they have largely untapped many other effectiveness and efficiency benefits , with between 36 and 70 percent saying these benefits have been achieved to little or no extent , depending on benefit .

moreover , for all the cited benefits , a far greater percentage of the departments and agencies ( 74 to 93 percent ) reported that they expect to realize each of the benefits to a significant or moderate extent sometime in the future .

what this suggests is that the real value in the federal government from developing and using enterprise architecture remains largely unrealized potential .

our architecture maturity framework recognizes that a key to realizing this potential is effectively managing department and agency enterprise architecture programs .

however , knowing whether benefits and results are in fact being achieved requires having associated measures and metrics .

in this regard , very few ( 21 percent ) of the departments and agencies fully satisfied our stage 5 core element , “return on ea investment is measured and reported.” without satisfying this element , it is unlikely that the degree to which expected benefits are accrued will be known .

if managed effectively , enterprise architectures can be a useful change management and organizational transformation tool .

the conditions for effectively managing enterprise architecture programs are contained in our architecture management maturity framework .

while a few of the federal government's 27 major departments and agencies have fully satisfied all the conditions needed to be at stage 2 or above in our framework , many have fully satisfied a large percentage of the core elements across most of the stages , particularly those elements related to architecture governance .

nevertheless , most departments and agencies are not yet where they need to be relative to architecture content , use , and measurement and thus the federal government is not as well positioned as it should be to realize the significant benefits that a well - managed architecture program can provide .

moving beyond this status will require most departments and agencies to overcome some significant obstacles and challenges .

the key to doing so continues to be sustained organizational leadership .

without such organizational leadership , the benefits of enterprise architecture will not be fully realized .

to assist the 27 major departments and agencies in addressing enterprise architecture challenges , managing their architecture programs , and realizing architecture benefits , we recommend that the administrators of the environmental protection agency , general services administration , national aeronautics and space administration , small business administration , and u.s. agency for international development ; the attorney general ; the commissioners of the nuclear regulatory commission and social security administration ; the directors of the national science foundation and the office of personnel management ; and the secretaries of the departments of agriculture , commerce , defense , education , energy , health and human services , homeland security , housing and urban development , interior , labor , state , transportation , treasury , and veterans affairs ensure that their respective enterprise architecture programs develop and implement plans for fully satisfying each of the conditions in our enterprise architecture management maturity framework .

we received written or oral comments on a draft of this report from 25 of the departments and agencies in our review .

of the 25 departments and agencies , all but one department fully agreed with our recommendation .

nineteen departments and agencies agreed and six partially agreed with our findings .

areas of disagreement for these six centered on ( 1 ) the adequacy of the documentation that they provided to demonstrate satisfaction of certain core elements and ( 2 ) recognition of steps that they reported taking to satisfy certain core elements after we concluded our review .

for the most part , these isolated areas of disagreement did not result in any changes to our findings for two primary reasons .

first , our findings across the departments and agencies were based on consistently applied evaluation criteria governing the adequacy of documentation , and were not adjusted to accommodate any one particular department or agency .

second , our findings represent the state of each architecture program as of march 2006 , and thus to be consistent do not reflect activities that may have occurred after this time .

beyond these comments , several agencies offered suggestions for improving our framework , which we will consider prior to issuing the next version of the framework .

the departments' and agencies' respective comments and our responses , as warranted , are as follows: agriculture's associate cio provided e - mail comments stating that the department will incorporate our recommendation into its enterprise architecture program plan .

commerce's cio stated in written comments that the department concurred with our findings and will consider actions to address our recommendation .

commerce's written comments are reproduced in appendix v. dod's director , architecture and interoperability , stated in written comments that the department generally concurred with our recommendation to the five dod architecture programs included in our review .

however , the department stated that it did not concur with the one aspect of the recommendation directed at the gig architecture concerning independent verification and validation ( iv&v ) because it believes that its current internal verification and validation activities are sufficient .

we do not agree for two reasons .

first , these internal processes are not independently performed .

as we have previously reported , iv&v is a recognized hallmark of well managed programs , including architecture programs , and to be effective , it must be performed by an entity that is independent of the processes and products that are being reviewed .

second , the scope of the internal verification and validation activities only extends to a subset of the architecture products and management processes .

the department also stated that it did not concur with one aspect of our finding directed at bea addressing security .

according to dod , because gig addresses security and the gig states that it extends to all defense mission areas , including the business mission area , the bea in effect addresses security .

we do not fully agree .

while we acknowledge that gig addresses security and states that it is to extend to all dod mission areas , including the business mission area , it does not describe how this will be accomplished for bea .

moreover , nowhere in the bea is security addressed , either through statement or reference , relative to the architecture's performance , business , information / data , application / service , and technology products .

dod's written comments , along with our responses , are reproduced in appendix vi .

education's assistant secretary for management and acting cio stated in written comments that the department plans to address our findings .

education's written comments are reproduced in appendix vii .

energy's acting associate cio for information technology reform stated in written comments that the department concurs with our report .

energy's written comments are reproduced in appendix viii .

dhs's director , departmental gao / oig liaison office , stated in written comments that the department has taken , and plans to take , steps to address our recommendation .

dhs's written comments , along with our responses to its suggestions for improving our framework , are reproduced in appendix ix .

dhs also provided technical comments via e - mail , which we have incorporated , as appropriate , in the report .

hud's cio stated in written comments that the department generally concurs with our findings and is developing a plan to address our recommendation .

the cio also provided updated information about activities that the department is taking to address security in its architecture .

hud's written comments are reproduced in appendix x. interior's assistant secretary , policy , management and budget , stated in written comments that the department agrees with our findings and recommendation and that it has recently taken action to address them .

interior's written comments are reproduced in appendix xi .

doj's cio stated in written comments that our findings accurately reflect the state of the department's enterprise architecture program and the areas that it needs to address .

the cio added that our report will help guide the department's architecture program and provided suggestions for improving our framework and its application .

doj's written comments , along with our responses to its suggestions , are reproduced in appendix xii .

labor's deputy cio provided e - mail comments stating that the department concurs with our findings .

the deputy cio also provided technical comments that we have incorporated , as appropriate , in the report .

state's assistant secretary for resource management and chief financial officer provided written comments that summarize actions that the department will take to fully satisfy certain core elements and that suggest some degree of disagreement with our findings relative to three other core elements .

first , the department stated that its architecture configuration management plan has been approved by both the state and usaid cios .

however , it provided no evidence to demonstrate that this was the case as of march 2006 when we concluded our review , and thus we did not change our finding relative to architecture products being under configuration management .

second , the department stated that its enterprise architecture has been approved by state and usaid executive offices .

however , it did not provide any documentation showing such approval .

moreover , it did not identify which executive offices it was referring to so as to allow a determination of whether they were collectively representative of the enterprise .

as a result , we did not change our finding relative to whether a committee or group representing the enterprise or an investment review board has approved the current version of the architecture .

third , the department stated that it provided us with it investment score sheets during our review that demonstrate that investment compliance with the architecture is measured and reported .

however , no such score sheets were provided to us .

therefore , we did not change our finding .

the department's written comments , along with more detailed responses , are reproduced in appendix xiii .

treasury's associate cio for e - government stated in written comments that the department concurs with our findings and discussed steps being taken to mature its enterprise architecture program .

the associate cio also stated that our findings confirm the department's need to provide executive leadership in developing its architecture program and to codify the program into department policy .

treasury's written comments are reproduced in appendix xiv .

va's deputy secretary stated in written comments that the department concurred with our recommendation and that it will provide a detailed plan to implement our recommendation .

va's written comments are reproduced in appendix xv .

epa's acting assistant administrator and cio stated in written comments that the agency generally agreed with our findings and that our assessment is a valuable benchmarking exercise that will help improve agency performance .

the agency also provided comments on our findings relative to five core elements .

for one of these core elements , the comments directed us to information previously provided about the agency's architecture committee that corrected our understanding and resulted in us changing our finding about this core element .

with respect to the other four core elements concerning use of an architecture methodology , measurement of progress against program plans , integration of the architecture into investment decision making , and management of architecture change , the comments also directed us to information previously provided but this did not result in any changes to our findings because evidence demonstrating full satisfaction of each core element was not apparent .

epa's written comments , along with more detailed responses to each , are reproduced in appendix xvi .

gsa's administrator stated in written comments that the agency concurs with our recommendation .

the administrator added that our findings will be critical as the agency works towards further implementing our framework's core elements .

gsa's written comments are reproduced in appendix xvii .

nasa's deputy administrator stated in written comments that the agency concurs with our recommendation .

nasa's written comments are reproduced in appendix xviii .

nasa's gao liaison also provided technical comments via e - mail , which we have incorporated , as appropriate , in the report .

nsf's cio provided e - mail comments stating that the agency will use the information in our report , where applicable , for future planning and investment in its architecture program .

the cio also provided technical comments that we have incorporated , as appropriate , in the report .

nrc's gao liaison provided e - mail comments stating that the agency substantially agrees with our findings and describing activities it has recently taken to address them .

opm's cio provided e - mail comments stating that the agency agrees with our findings and describing actions it is taking to address them .

sba's gao liaison provided e - mail comments in which the agency disagreed with our findings on two core elements .

first , and notwithstanding agency officials' statements that its architecture program did not have adequate resources , the liaison did not agree with our “partially satisfied” assessment for this core element because , according to the liaison , the agency has limited discretionary funds and competing , but unfunded , federal mandates to comply with that limit discretionary funding for an agency of its size .

while we acknowledge sba's challenges , we would note that they are not unlike the resource constraints and competing priority decisions that face most agencies , and that while the reasons why an architecture program may not be adequately resourced may be justified , the fact remains that any assessment of the architecture program's maturity , and thus its likelihood of success , needs to recognize whether adequate resources exist .

therefore , we did not change our finding on this core element .

second , the liaison did not agree with our finding that the agency did not have plans for developing metrics for measuring architecture progress , quality , compliance , and return on investment .

however , our review of documentation provided by sba and cited by the liaison showed that while such plans address metric development for architecture progress , quality , and compliance , they do not address architecture return on investment .

therefore , we did not change our finding that this core element was partially satisfied .

ssa's commissioner stated in written comments that the report is both informative and useful , and that the agency agrees with our recommendation and generally agrees with our findings .

nevertheless , the agency disagreed with our findings on two core elements .

first , the agency stated that documentation provided to us showed that it has a methodology for developing , maintaining , and validating its architecture .

we do not agree .

in particular , our review of ssa provided documentation showed that it did not adequately describe the steps to be followed relative to development , maintenance , or validation .

second , the agency stated that having the head of the agency approve the current version of the architecture is satisfied in ssa's case because the clinger - cohen act of 1996 vests its cio with enterprise architecture approval authority and the cio has approved the architecture .

we do not agree .

the core element in our framework concerning enterprise architecture approval by the agency head is derived from federal guidance and best practices upon which our framework is based .

this guidance and related practices , and thus our framework , recognize that an enterprise architecture is a corporate asset that is to be owned and implemented by senior management across the enterprise , and that a key characteristic of a mature architecture program is having the architecture approved by the department or agency head .

because the clinger - cohen act does not address approval of an enterprise architecture , our framework's core element for agency head approval of an enterprise architecture is not inconsistent with , and is not superseded by , that act .

ssa's written comments , along with more detailed responses , are reproduced in appendix xix .

usaid's acting chief financial officer stated in written comments stated that the agency will work with state to implement our recommendation .

usaid's written comments are reproduced in appendix xx .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to the administrators of the environmental protection agency , general services administration , national aeronautics and space administration , small business administration , and u.s. agency for international development ; the attorney general ; the commissioners of the nuclear regulatory commission and social security administration ; the directors of the national science foundation and the office of personnel management ; and the secretaries of the departments of agriculture , commerce , defense , education , energy , health and human services , homeland security , housing and urban development , interior , labor , state , transportation , treasury , and veterans affairs .

we will also make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you have any questions concerning this information , please contact me at ( 202 ) 512-3439 or by e - mail at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix xxi .

department - and agency - reported data show wide variability in their costs to develop and maintain their enterprise architectures .

generally , the costs could be allocated to several categories with the majority of costs attributable to contractor support and agency personnel .

as we have previously reported , the depth and detail of the architecture to be developed and maintained is dictated by the scope and nature of the enterprise and the extent of enterprise transformation and modernization envisioned .

therefore , the architecture should be tailored to the individual enterprise and that enterprise's intended use of the architecture .

accordingly , the level of resources that a given department or agency invests in its architecture is likely to vary .

departments and agencies reported that they have collectively invested a total of $836 million to date on enterprise architecture development .

across the 27 departments and agencies , these development costs ranged from a low of $2 million by the department of the navy to a high of $433 million by the department of defense ( dod ) on its business enterprise architecture ( bea ) .

department and agency estimates of the costs to complete their planned architecture development efforts collectively total about $328 million .

the department and agencies combined estimates of annual architecture maintenance costs is about $146 million .

these development and maintenance estimates , however , do not include the departments of the army and justice because neither provided these cost estimates .

figures 7 through 9 depict the variability of cost data reported by the departments and agencies .

all of the departments and agencies reported developing their architecture in - house using contractor support .

all but two of the departments and agencies allocated their respective architecture development costs to the following cost categories: contractor support , agency personnel , tools , methodologies , training , and other .

these 26 agencies accounted for about $741 million of the $836 million total development costs cited above .

the vast majority ( 84 percent ) of the $741 million were allocated to contractor services ( $621 million ) , followed next by agency personnel ( 13 percent or $94 million ) .

the remaining $26 million were allocated as follows: $12 million ( 2 percent ) to architecture tools ; $9 million ( 1 percent ) to “other” costs ; $4 million ( 1 percent ) to architecture methodologies ; and $2 million ( less than 1 percent ) to training .

 ( see fig .

10. ) .

the departments and agencies allocated the reported $621 million in contractor - related costs to the following five contractor cost categories: architecture development , independent verification and validation , methodology , support services , and other .

of these categories , architecture development activities accounted for the majority of costs — about $594 million ( 87 percent ) .

the remaining $85 million was allocated as follows: $51 million ( 7 percent ) to support services , $13 million ( 2 percent ) to “other” costs , $11 million ( 2 percent ) to independent verification and validation , and $10 million ( 1 percent ) to methodologies .

 ( see fig .

11. ) .

departments and agencies reported additional information related to the implementation of their enterprise architectures .

this information includes architecture tools and frameworks .

as stated in our enterprise architecture management maturity framework , an automated architecture tool serves as the repository of architecture artifacts , which are the work products that are produced and used to capture and convey architectural information .

an agency's choice of tool should be based on a number of considerations , including agency needs and the size and complexity of the architecture .

the departments and agencies reported that they use various automated tools to develop and maintain their enterprise architectures , with 12 reporting that they use more than one tool .

in descending order of frequency , the architecture tools identified were system architect ( 18 instances ) , microsoft visio ( 17 ) , metis ( 12 ) , rational rose ( 8 ) , and enterprise architecture management system ( eams ) ( 4 ) .

in addition , 21 departments and agencies reported using one or more other architecture tools .

figure 12 shows the number of departments and agencies using each architecture tool , including the other tools .

the departments and agencies also reported various levels of satisfaction with the different enterprise architecture tools .

specifically , about 75 percent of those using microsoft visio were either very or somewhat satisfied with the tool , as compared to about 67 percent of those using metis , about 63 percent of those using rational rose , about 59 percent of those using system architect , and 25 percent of those using eams .

this means that the percentage of departments and agencies that were dissatisfied , either somewhat or very , with their respective tools ranged from a high of 75 percent of those using eams , to a low of about 6 percent of those using system architect .

no departments or agencies that used metis , rational rose , or microsoft visio reported any dissatisfaction .

see table 9 for a summary of department and agency reported satisfaction with their respective tools .

as we have previously stated , an enterprise architecture framework provides a formal structure for representing the architecture's content and serves as the basis for the specific architecture products and artifacts that the department or agency develops and maintains .

as such , a framework helps ensure the consistent representation of information from across the organization and supports orderly capture and maintenance of architecture content .

the departments and agencies reported using various frameworks to develop and maintain their enterprise architectures .

the most frequently cited frameworks were the federal enterprise architecture program management office ( feapmo ) reference models ( 25 departments and agencies ) , the federal enterprise architecture framework ( feaf ) ( 19 departments and agencies ) , and the zachman framework ( 17 departments and agencies ) , with 24 reporting using more than one framework .

other , less frequently reported frameworks were the department of defense architecture framework ( dodaf ) , the national institute of standards and technology ( nist ) framework , and the open group architecture framework ( togaf ) .

see figure 13 for a summary of the number of departments and agencies that reported using each framework .

departments and agencies also reported varying levels of satisfaction with their respective architecture .

specifically , about 72 percent of those using the feaf indicated that they were either very or somewhat satisfied , and about 67 and 61 percent of those using the zachman framework and the feapmo reference models , respectively , reported that they were similarly satisfied .

as table 10 shows , few of the agencies that responded to our survey reported being dissatisfied with any of the frameworks .

our objective was to determine the current status of federal department and agency enterprise architecture efforts .

to accomplish this objective , we focused on 28 enterprise architecture programs relating to 27 major departments and agencies .

these 27 included the 24 departments and agencies included in the chief financial officers act .

in addition , we included the three military services ( the departments of the army , air force , and navy ) at the request of department of defense ( dod ) officials .

for the dod , we also included both of its departmentwide enterprise architecture programs — the global information grid and the business enterprise architecture .

the u.s. agency for international development ( usaid ) , which is developing a usaid enterprise architecture and working with the department of state ( state ) to develop a joint enterprise architecture , asked that we evaluate its efforts to develop the usaid enterprise architecture .

state officials asked that we evaluate their agency's enterprise architecture effort based the joint enterprise architecture being developed with usaid .

we honored both of these requests .

table 11 lists the 28 department and agency enterprise architecture programs that formed the scope of our review .

to determine the status of each of these architecture programs , we developed a data collection instrument based on our enterprise architecture management maturity framework ( eammf ) , and related guidance , such as omb circular a - 130 and guidance published by the federal chief information officers ( cio ) council , and our past reports and guidance on the management and content of enterprise architectures .

we pretested this instrument at one department and one agency .

based on the results of the pretest , we modified our instrument as appropriate to ensure that our areas of inquiry were complete and clear .

next , we identified the chief architect or comparable official at each of the 27 departments and agencies , and met with them to discuss our scope and methodology , share our data collection instrument , and discuss the type and nature of supporting documentation needed to verify responses to our instrument questions .

on the basis of department and agency provided documentation to support their respective responses to our data collection instrument , we analyzed the extent to which each satisfied the 31 core elements in our architecture maturity framework .

to guide our analysis , we defined detailed evaluation criteria for determining whether a given core element was fully satisfied , partially satisfied , or not satisfied .

the criteria for the stage 2 , 3 , 4 , and 5 core elements are contained in tables 12 , 13 , 14 , and 15 respectively .

to fully satisfy a core element , sufficient documentation had to be provided to permit us to verify that all aspects of the core element were met .

to partially satisfy a core element , sufficient documentation had to be provided to permit us to verify that at least some aspects of the core element were met .

core elements that were neither fully nor partially satisfied were judged to be not satisfied .

our evaluation included first analyzing the extent to which each department and agency satisfied the core elements in our framework , and then meeting with department and agency representatives to discuss core elements that were not fully satisfied and why .

as part of this interaction , we sought , and in some cases were provided , additional supporting documentation .

we then considered this documentation in arriving at our final determinations about the degree to which each department and agency satisfied each core element in our framework .

in applying our evaluation criteria , we analyzed the results of our analysis across different core elements to determine patterns and issues .

our analysis made use of computer programs that were developed by an experienced staff ; these programs were independently verified .

through our data collection instrument , we also solicited from each department and agency information on enterprise architecture challenges and benefits , including the extent to which they had been or were expected to be experienced .

in addition , we solicited information on architecture costs , including costs to date and estimated costs to complete and maintain each architecture .

we also solicited other information , such as use of and satisfaction with architecture tools and frameworks .

we analyzed these additional data to determine relevant patterns .

we did not independently verify these data .

the results presented in this report reflect the state of department and agency architecture programs as of march 8 , 2006 .

we conducted our work in the washington , d.c. , metropolitan area , from may 2005 to june 2006 , in accordance with generally accepted government auditing standards .

table 16 shows usda's satisfaction of framework elements in version 1.1 of gao's eammf .

version 1.1 of gao's eammf .

table 18 shows army's satisfaction of framework elements in version 1.1 of gao's eammf .

table 19 shows commerce's satisfaction of framework elements in version 1.1 of gao's eammf .

table 20 shows the bea's satisfaction of framework elements in version 1.1 of gao's eammf .

table 21 shows the gig's satisfaction of framework elements in version 1.1 of gao's eammf .

table 22 shows education's satisfaction of framework elements in version 1.1 of gao's eammf .

table 23 shows energy's satisfaction of framework elements in version 1.1 of gao's eammf .

table 24 shows hhs's satisfaction of framework elements in version 1.1 of gao's eammf .

table 25 shows dhs's satisfaction of framework elements in version 1.1 of gao's eammf .

table 26 shows hud's satisfaction of framework elements in version 1.1 of gao's eammf .

table 27 shows doi's satisfaction of framework elements in version 1.1 of gao's eammf .

table 28 shows doj's satisfaction of framework elements in version 1.1 of gao's eammf .

table 29 shows labor's satisfaction of framework elements in version 1.1 of gao's eammf .

table 30 shows navy's satisfaction of framework elements in version 1.1 of gao's eammf .

table 31 shows state's satisfaction of framework elements in version 1.1 of gao's eammf .

table 32 shows transportation's satisfaction of framework elements in version 1.1 of gao's eammf .

table 33 shows the treasury's satisfaction of framework elements in version 1.1 of gao's eammf .

table 34 shows va's satisfaction of framework elements in version 1.1 of gao's eammf .

table 35 shows epa's satisfaction of framework elements in version 1.1 of gao's eammf .

table 36 shows gsa's satisfaction of framework elements in version 1.1 of gao's eammf .

table 37 shows nasa's satisfaction of framework elements in version 1.1 of gao's eammf .

table 38 shows nsf's satisfaction of framework elements in version 1.1 of gao's eammf .

table 39 shows nrc's satisfaction of framework elements in version 1.1 of gao's eammf .

table 40 shows opm's satisfaction of framework elements in version 1.1 of gao's eammf .

table 41 shows sba's satisfaction of framework elements in version 1.1 of gao's eammf .

table 42 shows ssa's satisfaction of framework elements in version 1.1 of gao's eammf .

table 43 shows usaid's satisfaction of framework elements in version 1.1 of gao's eammf .

1 .

we do not agree for two reasons .

first , dod's internal processes for reviewing and validating the global information grid ( gig ) , while important and valuable to ensuring architecture quality , are not independently performed .

as we have previously reported , independent verification and validation is a recognized hallmark of well - managed programs , including architecture programs .

to be effective , it should be performed by an entity that is independent of the processes and products that are being reviewed to help ensure that it is done in an unbiased manner and that is based on objective evidence .

second , the scope of these internal review and validation efforts only extends to a subset of gig products and management processes .

according to our framework , independent verification and validation should address both the architecture products and the processes used to develop them .

2 .

while we acknowledge that gig program plans provide for addressing security , and our findings relative to the gig reflect this , this is not the case for dod's business enterprise architecture ( bea ) .

more specifically , how security will be addressed in the bea performance , business , information / data , application / service , and technology products is not addressed in the bea either by explicit statement or reference .

this finding relative to the bea is consistent with our recent report on dod's business system modernization .

1 .

we acknowledge this recommendation and offer three comments in response .

first , we have taken a number of steps over the last 5 years to coordinate our framework with omb .

for example , in 2002 , we based version 1.0 of our framework on the omb - sponsored cio council practical guide to federal enterprise architecture , and we obtained concurrence on the framework from the practical guide's principal authors .

further , we provided a draft of this version to omb for comment , and in our 2002 report in which we assessed federal departments and agencies against this version , we recommended that omb use the framework to guide and assess agency architecture efforts .

in addition , in developing the second version of our framework in 2003 , we solicited comments from omb as well as federal departments and agencies .

we also reiterated our recommendation to omb to use the framework in our 2003 report in which we assessed federal departments and agencies against the second version of the framework .

second , we have discussed alignment of our framework and omb's architecture assessment tool with omb officials .

for example , after omb developed the first version of its architecture assessment tool in 2004 , we met with omb officials to discuss our respective tools and periodic agency assessments .

we also discussed omb's plans for issuing the next version of its assessment tool and how this next version would align with our framework .

at that time , we advocated the development of comprehensive federal standards governing all aspects of architecture development , maintenance , and use .

in our view , neither our framework nor omb's assessment tool provide such comprehensive standards , and in the case of our framework , it is not intended to provide such standards .

nevertheless , we plan to continue to evolve , refine , and improve our framework , and will be issuing an updated version that incorporates lessons learned from the results of this review .

in doing so , we will continue to solicit comments from federal departments and agencies , including omb .

third , we believe that while our framework and omb's assessment tool are not identical , they nevertheless consist of a common cadre of best practices and characteristics , as well as other relevant criteria that , taken together , are complementary and provide greater direction to , and visibility into , agency architecture programs than either does alone .

1 .

see dhs comment 1 in appendix ix .

also , while we do not have a basis for commenting on the content of the department's omb evaluation submission package because we did not receive it , we would note that the information that we solicit to evaluate a department or agency against our framework includes only information that should be readily available as part of any well - managed architecture program .

2 .

we understand the principles of federated and segmented architectures , but would emphasize that our framework is intentionally neutral with respect to these and other architecture approaches ( eg , service - oriented ) .

that is , the scope of the framework , by design , does not extend to defining how various architecture approaches should specifically be pursued , although we recognize that supplemental guidance on this approach would be useful .

our framework was created to organize fundamental ( core ) architecture management practices and characteristics ( elements ) into a logical progression .

as such , it was intended to fill an architecture management void that existed in 2001 and thereby provide the context for more detailed standards and guidance in a variety of areas .

it was not intended to be the single source of all relevant architecture guidance .

3 .

we agree , and believe that this report , by clearly identifying those departments and agencies that have fully satisfied each core element , serves as the only readily available reference tool of which we are aware for gaining such best practice insights .

1 .

we acknowledge the comment that both cios approved the configuration management plan .

however , the department did not provide us with any documentation to support this statement .

2 .

we acknowledge the comment that the architecture has been approved by state and usaid executive offices .

however , the department did not provide any documentation describing to which executive offices the department is referring to allow a determination of whether they were collectively representative of the enterprise .

moreover , as we state in the report , the chief architect told us that a body representative of the enterprise has not approved the current version of the architecture , and according to documentation provided , the joint management council is to be responsible for approving the architecture .

3 .

we acknowledge that steps have been taken and are planned to treat the enterprise architecture as an integral part of the investment management process , as our report findings reflect .

however , our point with respect to this core element is whether the department's investment portfolio compliance with the architecture is being measured and reported to senior leadership .

in this regard , state did not provide the score sheets referred to in its comments , nor did it provide any other evidence that such reporting is occurring .

1 .

we agree and have modified our report to recognize evidence contained in the documents .

2 .

we do not agree .

the 2002 documents do not contain steps for architecture maintenance .

further , evidence was not provided demonstrating that the recently prepared methodology documents were approved prior to the completion of our evaluation .

3 .

we do not agree .

while we do not question whether epa's ea transition strategy and sequencing plan illustrates how annual progress in achieving the target architectural environment is measured and reported , this is not the focus of this core element .

rather , this core element addresses whether progress against the architecture program management plan is tracked and reported .

while we acknowledge epa's comment that it tracks and reports such progress against plans on a monthly basis , neither a program plan nor reports of progress against this plan were provided as documentary evidence to support this statement .

4 .

we do not agree .

first , while epa's it investment management process provides for consideration of the enterprise architecture in investment selection and control activities , no evidence was provided demonstrating that the process has been implemented .

second , while epa provided a description of its architecture change management process , no evidence was provided that this process has been approved and implemented .

1 .

we do not agree .

neither the governance committee charter nor the configuration management plan explicitly describe a methodology that includes detailed steps to be followed for developing , maintaining , and validating the architecture .

rather , these documents describe , for example , the responsibilities of the architecture governance committee and architecture configuration management procedures .

2 .

we do not agree .

the core element in our framework concerning enterprise architecture approval by the agency head is derived from federal guidance and best practices upon which our framework is based .

this guidance and related practices , and thus our framework , recognize that an enterprise architecture is a corporate asset that is to be owned and implemented by senior management across the enterprise , and that a key characteristic of a mature architecture program is having the architecture approved by the department or agency head .

because the clinger - cohen act does not address approval of an enterprise architecture , our framework's core element for agency head approval of an enterprise architecture is not inconsistent with , and is not superseded by , that act .

in addition to the person named above , edward ballard , naba barkakati , mark bird , jeremy canfield , jamey collins , ed derocher , neil doherty , mary j. dorsey , marianna j. dunn , joshua eisenberg , michael holland , valerie hopkins , james houtz , ashfaq huda , cathy hurley , cynthia jackson , donna wagner jones , ruby jones , stu kaufman , sandra kerr , george kovachick , neela lakhmani , anh le , stephanie lee , jayne litzinger , teresa m. neven , freda paintsil , altony rice , keith rhodes , teresa smith , mark stefan , dr. rona stillman , amos tevelow , and jennifer vitalbo made key contributions to this report .

