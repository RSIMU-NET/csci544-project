for decades , the department of defense ( dod ) has been challenged in modernizing its timeworn business systems .

in 1995 , we designated dod's business systems modernization program as high risk and continue to do so today .

our reasons include the modernization's large size , complexity , and critical role in addressing other long - standing transformation and financial management challenges .

other reasons are that dod has yet to institutionalize key system modernization management controls , and it has not demonstrated the ability to consistently deliver promised system capabilities and benefits on time and within budget .

nevertheless , dod continues to invest billions of dollars in thousands of business systems , including about a hundred that the department has labeled as business transformational programs , 12 of which account for about 50 percent of these programs' costs .

the global combat support system - marine corps ( gcss - mc ) is one such program .

initiated in 2003 , gcss - mc is to modernize the marine corps logistics systems and thereby provide decision makers with timely and complete logistics information to support the warfighter .

as envisioned , the program consists of a series of major increments , the first of which is expected to cost approximately $442 million and be fully deployed in fiscal year 2010 .

as agreed , our objective was to determine whether the department of the navy is effectively implementing information technology ( it ) management controls on gcss - mc .

to accomplish this , we focused on the first increment of gcss - mc by analyzing a range of program documentation and interviewing cognizant officials relative to the following management areas: architectural alignment , economic justification , earned value management , requirements management , risk management , and system quality measurement .

we conducted our performance audit from june 2007 to july 2008 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

additional details on our objective , scope , and methodology are in appendix i .

the department of the navy ( don ) is a major component of dod , consisting of two uniformed services: the navy and the marine corps .

the marine corps' primary mission is to serve as a “total force in readiness” by responding quickly in a wide spectrum of responsibilities , such as attacks from sea to land in support of naval operations , air combat , and security of naval bases .

as the only service that operates in three dimensions — in the air , on land , and at sea , the marine corps must be equipped to provide rapid and precise logistics support to operating forces in any environment .

the marine corps' many and dispersed organization components rely heavily on it to perform their respective mission - critical operations and related business functions , such as logistics and financial management .

for fiscal year 2008 , the marine corps budget for it business systems is about $1.3 billion , of which $746 million ( 57 percent ) is for operations and maintenance of existing systems and $553 million ( 43 percent ) is for systems development and modernization .

of the approximately 904 systems in don's current inventory , the marine corps accounts for 81 , or about 9 percent , of the total .

the gcss - mc is one such system investment .

according to dod , it is intended to address the marine corps' long - standing problem of stove - piped logistics systems that collectively provide limited data visibility and access , are unable to present a common , integrated logistics picture in support of the warfighter , and do not provide important decision support tools .

in september 2003 , the marine corps initiated gcss - mc to ( 1 ) deliver integrated functionality across the logistics areas ( eg , supply and maintenance ) , ( 2 ) provide timely and complete logistics information to authorized users for decision making , and ( 3 ) provide access to logistics information and applications regardless of location .

the system is intended to function in three operational environments — deployed operations ( i.e. , in theater of war or exercise environment on land or at sea ) , in - transit , and in garrison .

when gcss - mc is fully implemented , it is to support about 33,000 users located around the world .

gcss - mc is being developed in a series of large and complex increments using commercially available enterprise resource planning ( erp ) software and hardware components .

the first increment is currently the only funded portion of the program and is to provide a range of asset management capabilities , including planning inventory requirements to support current and future requesting and tracking the status of products ( eg , supplies and personnel ) and services ( eg , maintenance and engineering ) ; allocating resources ( eg , inventory , warehouse capacity , and personnel ) to support unit demands for specific products ; and scheduling maintenance resources ( eg , manpower , equipment , and supplies ) for specific assets , such as vehicles .

additionally , the first increment is to replace four legacy systems scheduled for retirement in 2010 .

table 1 describes these four systems .

future increments are to provide additional functionality ( eg , transportation and wholesale inventory management ) , enhance existing functionality , and potentially replace up to 44 additional legacy systems .

the program office estimates the total life cycle cost for the first increment to be about $442 million , including $169 million for acquisition and $273 million for operations and maintenance .

the total life cycle cost of the entire program has not yet been determined because future increments are currently in the planning stages and have not been defined .

as of april 2008 , the program office reported that approximately $125 million has been spent on the first increment .

to manage the acquisition and deployment of gcss - mc , the marine corps established a program management office within the program executive office for executive information systems .

the program office is led by the program manager who is responsible for managing the program's scope and funding and ensuring that the program meets its objectives .

to accomplish this , the program office is responsible for key acquisition management controls , such as architectural alignment , economic justification , evm , requirements management , risk management , and system quality measurement .

in addition , various dod and don organizations share program oversight and review activities relative to these and other acquisition management controls .

a listing of key entities and their roles and responsibilities is in table 2 .

the program reports that the first increment of gcss - mc is currently in the system development and demonstration phase of the defense acquisition system ( das ) .

the das consists of five key program life cycle phases and three related milestone decision points .

these five phases and related milestones are described along with a summary of key program activities completed during , or planned , for each phase as follows: 1 .

concept refinement: the purpose of this phase is to refine the initial system solution ( concept ) and create a strategy for acquiring the investment solution .

during this phase , the program office defined the acquisition strategy and analyzed alternative solutions .

the first increment completed this phase on july 23 , 2004 , which was 1 month later than planned , and the mda approved a milestone a decision to move to the next phase .

2 .

technology development: the purpose of this phase is to determine the appropriate set of technologies to be integrated into the investment solution by iteratively assessing the viability of various technologies while simultaneously refining user requirements .

during this phase , the program office selected oracle's e - business suite as the commercial off - the - shelf erp software .

in addition , the program office awarded accenture the system integration contract to , among other things , configure the software , establish system interfaces , and implement the new system .

this system integration contract was divided into two phases — part 1 for the planning , analysis , and conceptual design of the solution and part 2 for detailed design , build , test , and deployment of the solution .

the program office did not exercise the option for part 2 of the contract to accenture and shortly thereafter established a new program baseline in june 2006 .

in november 2006 , it awarded a time - and - materials system integration contract valued at $28.4 million for solution design to oracle .

the first increment completed this phase on june 8 , 2007 , which was 25 months later than planned due in part to contractual performance shortfalls , and the mda approved a milestone b decision to move to the next phase .

3 .

system development and demonstration: the purpose of this phase is to develop the system and demonstrate through developer testing that the system can function in its target environment .

during this phase , the program office extended the solution design contract and increased funding to $67.5 million due , in part , to delays in completing the detailed design activities .

as a result , the program office has not yet awarded the next contract ( which includes both firm - fixed - price and time - and - materials task orders ) for build and testing activities , originally planned for july 2007 .

instead , it entered what it termed a “transition period” to complete detailed design activities .

according to the program's baseline , the mda is expected to approve a milestone c decision to move to the next phase in october 2008 .

however , program officials stated that milestone c is now scheduled for april 2009 , which is 35 months later than originally planned .

4 .

production and deployment: the purpose of this phase is to achieve an operational capability that satisfies the mission needs , as verified through independent operational test and evaluation , and implement the system at all applicable locations .

the program office plans to award a separate firm - fixed - price plus award fee contract for these activities with estimated costs yet to be determined .

5 .

operations and support: the purpose of this phase is to operationally sustain the system in the most cost - effective manner over its life cycle .

the details of this phase have not yet been defined .

overall , gcss - mc was originally planned to reach full operational capability ( foc ) in fiscal year 2007 at an estimated cost of about $126 million over a 7-year life cycle .

this cost estimate was later revised in 2005 to about $249 million over a 13-year life cycle .

however , the program now expects to reach foc in fiscal year 2010 at a cost of about $442 million over a 12-year life cycle .

figures 1 and 2 show the program's current status against original milestones and original , revised , and current cost estimates .

acquisition best practices are tried and proven methods , processes , techniques , and activities that organizations define and use to minimize program risks and maximize the chances of a program's success .

using best practices can result in better outcomes , including cost savings , improved service and product quality , and a better return on investment .

for example , two software engineering analyses of nearly 200 systems acquisition projects indicate that teams using systems acquisition best practices produced cost savings of at least 11 percent over similar projects conducted by teams that did not employ the kind of rigor and discipline embedded in these practices .

in addition , our research shows that best practices are a significant factor in successful acquisition outcomes and increase the likelihood that programs and projects will be executed within cost and schedule estimates .

we and others have identified and promoted the use of a number of best practices associated with acquiring it systems .

see table 3 for a description of several of these activities .

we have previously reported that dod has not effectively managed a number of business system investments .

among other things , our reviews of individual system investments have identified weaknesses in such areas as architectural alignment and informed investment decision making , which are also the focus areas of the fiscal year 2005 national defense authorization act business system provisions .

our reviews have also identified weaknesses in other system acquisition and investment management areas — such as evm , economic justification , requirements management , risk management , and test management .

most recently , for example , we reported that the army's approach to investing about $5 billion over the next several years in its general fund enterprise business system , global combat support system - army field / tactical , and logistics modernization program did not include alignment with army enterprise architecture or use a portfolio - based business system investment review process .

moreover , we reported that the army did not have reliable analyses , such as economic analyses , to support its management of these programs .

we concluded that until the army adopts a business system investment management approach that provides for reviewing groups of systems and making enterprise decisions on how these groups will collectively interoperate to provide a desired capability , it runs the risk of investing significant resources in business systems that do not provide the desired functionality and efficiency .

accordingly , we made recommendations aimed at improving the department's efforts to achieve total asset visibility and enhancing its efforts to improve its control and accountability over business system investments .

the department agreed with our recommendations .

we also reported that don had not , among other things , economically justified its ongoing and planned investment in the naval tactical command support system ( ntcss ) and had not invested in ntcss within the context of a well - defined dod or don enterprise architecture .

in addition , we reported that don had not effectively performed key measurement , reporting , budgeting , and oversight activities and had not adequately conducted requirements management and testing activities .

we concluded that , without this information , don could not determine whether ntcss , as defined , and as being developed , is the right solution to meet its strategic business and technological needs .

accordingly , we recommended that the department develop the analytical basis to determine if continued investment in the ntcss represents prudent use of limited resources and to strengthen management of the program , conditional upon a decision to proceed with further investment in the program .

the department largely agreed with these recommendations .

in addition , we reported that the army had not defined and developed its transportation coordinators' automated information for movements system ii ( tc - aims ii ) — a joint services system with the goal of helping to manage the movement of forces and equipment within the united states and abroad — in the context of a dod enterprise architecture .

we also reported that the army had not economically justified the program on the basis of reliable estimates of life cycle costs and benefits and had not effectively implemented risk management .

as a result , we concluded that the army did not know if its investment in tc - aims ii , as planned , is warranted or represents a prudent use of limited dod resources .

accordingly , we recommended that dod , among other things , develop the analytical basis needed to determine if continued investment in tc - aims ii , as planned , represents prudent use of limited defense resources .

in response , the department largely agreed with our recommendations and has since reduced the program's scope by canceling planned investments .

dod it - related acquisition policies and guidance , along with other relevant guidance , provide an acquisition management control framework within which to manage business system programs like gcss - mc .

effective implementation of this framework can minimize program risks and better ensure that system investments are defined in a way to optimally support mission operations and performance , as well as deliver promised system capabilities and benefits on time and within budget .

thus far , gcss - mc has not been managed in accordance with key aspects of this framework , which has already contributed to more than 3 years in program schedule delays and about $193 million in cost increases .

these it acquisition management control weaknesses include compliance with dod's federated bea not being sufficiently expected costs not being reliably estimated ; earned value management not being adequately implemented ; system requirements not always being effectively managed , although this has recently improved ; key program risks not being effectively managed ; and key system quality measures not being used .

the reasons that these key practices have not been sufficiently executed include limitations in the applicable dod guidance and tools , and not collecting relevant data , each of which is described in the applicable sections of this report .

by not effectively implementing these key it acquisition management controls , the program has already experienced sizeable schedule and cost increases , and it is at increased risk of ( 1 ) not being defined in a way that best meets corporate mission needs and enhances performance and ( 2 ) costing more and taking longer than necessary to complete .

dod and federal guidance recognize the importance of investing in business systems within the context of an enterprise architecture .

moreover , the 2005 national defense authorization act requires that defense business systems be compliant with dod's federated bea .

our research and experience in reviewing federal agencies show that not making investments within the context of a well - defined enterprise architecture often results in systems that are duplicative , are not well integrated , are unnecessarily costly to interface and maintain , and do not optimally support mission outcomes .

to its credit , the program office has followed dod's bea compliance guidance .

however , this guidance does not adequately provide for addressing all relevant aspects of bea compliance .

moreover , don's enterprise architecture , which is a major component of dod's federated bea , as well as key aspects of dod's corporate bea , have yet to be sufficiently defined to permit thorough compliance determinations .

in addition , current policies and guidance do not require don investments to comply with its enterprise architecture .

this means that the department does not have a sufficient basis for knowing if gcss - mc has been defined to optimize don and dod business operations .

each of these architecture alignment limitations is discussed as follows: the program's compliance assessments did not include all relevant architecture products .

in particular , the program did not assess compliance with the bea's technical standards profile , which outlines , for example , the standards governing how systems physically communicate with other systems and how they secure data from unauthorized access .

this is particularly important because systems , like gcss - mc , need to employ common standards in order to effectively and efficiently share information with other systems .

a case in point is gcss - mc and the navy enterprise resource planning program .

specifically , gcss - mc has identified 13 technical standards that are not in the bea technical standards profile , and navy enterprise resource planning has identified 25 technical standards that are not in the profile .

of these , some relate to networking protocols , which could limit information sharing between these and other systems .

in addition , the program office did not assess compliance with the bea products that describe system characteristics .

this is important because doing so would create a body of information about programs that could be used to identify common system components and services that could potentially be shared by the programs , thus avoiding wasteful duplication .

for example , our analysis of gcss - mc program documentation shows that they contain such system functions as receiving goods , taking physical inventories , and returning goods , which are also system functions cited by the navy enterprise resource planning program .

however , because compliance with the bea system products was not assessed , the extent to which these functions are potentially duplicative was not considered .

furthermore , the program office did not assess compliance with bea system products that describe data exchanges among systems .

as we previously reported , establishing and using standard system interfaces is a critical enabler to sharing data .

for example , gcss - mc program documentation indicates that it is to exchange order and status data with other systems .

however , the program office has not fully developed its architecture product describing these exchanges and thus does not have the basis for understanding how its approach to exchanging information differs from that of other systems that it is to interface with .

compliance against each of these bea products was not assessed because dod's compliance guidance does not provide for doing so and , according to bta and program officials , some bea and program - level architecture products are not sufficiently defined .

according to these officials , bta plans to continue to define these products as the bea evolves .

the compliance assessment was not used to identify potential areas of duplication across programs , which dod has stated is an explicit goal of its federated bea and associated investment review and decision - making processes .

more specifically , even though the compliance guidance provides for assessing programs' compliance with the bea product that defines dod operational activities , and gcss - mc was assessed for compliance with this product , the results were not used to identify programs that support the same operational activities and related business processes .

given that the federated bea is intended to identify and avoid not only duplications within dod components , but also between dod components , it is important that such commonality be addressed .

for example , program - level architecture products for gcss - mc and navy enterprise resource planning , as well as two air force programs ( defense enterprise accounting and management system - air force and the air force expeditionary combat support system ) show that each supports at least six of the same bea operational activities ( eg , conducting physical inventory , delivering property , and services ) and three of these four programs support at least 18 additional operational activities ( eg , performing budgeting , managing receipt , and acceptance ) .

as a result , these programs may be investing in duplicative functionality .

reasons for not doing so were that compliance guidance does not provide for such analyses to be conducted , and programs have not been granted access rights to use this functionality .

the program's compliance assessment did not address compliance against the don's enterprise architecture , which is one of the biggest members of the federated bea .

this is particularly important given that dod's approach to fully satisfying the architecture requirements of the 2005 national defense authorization act is to develop and use a federated architecture in which component architectures are to provide the additional details needed to supplement the thin layer of corporate policies , rules , and standards included in the corporate bea .

as we recently reported , the don's enterprise architecture is not mature because , among other things , it is missing a sufficient description of its current and future environments in terms of business and information / data .

however , certain aspects of an architecture nevertheless exist and , according to don , these aspects will be leveraged in its efforts to develop a complete enterprise architecture .

for example , the forcenet architecture documents don's technical infrastructure .

therefore , opportunities exist for don to assess its programs in relation to these architecture products and to understand where its programs are exposed to risks because products do not exist , are not mature , or are at odds with other don programs .

according to dod officials , compliance with the don architecture was not assessed because dod compliance policy is limited to compliance with the corporate bea , and the don enterprise architecture has yet to be sufficiently developed .

the program's compliance assessment was not validated by dod or don investment oversight and decision - making authorities .

more specifically , neither the dod irbs nor the dbsmc , nor the bta in supporting both of these investment oversight and decision - making authorities , reviewed the program's assessments .

according to bta officials , under dod's tiered approach to investment accountability , these entities are not responsible for validating programs' compliance assessments .

rather , this is a component responsibility , and thus they rely on the military departments and defense agencies to validate the assessments .

however , the don office of the cio , which is responsible for precertifying investments as compliant before they are reviewed by the irb , did not evaluate any of the programs' compliance assessments .

according to cio officials , they rely on functional area managers to validate a program's compliance assessments .

however , no don policy or guidance exists that describes how the functional area managers should conduct such validations .

validation of program assessments is further complicated by the absence of information captured in the assessment tool about what program documentation or other source materials were used by the program office in making its compliance determinations .

specifically , the tool is only configured , and thus was only used , to capture the results of a program's comparison of program architecture products to bea products .

thus , it was not used to capture the system products used in making these determinations .

in addition , the program office did not develop certain program - level architecture products that are needed to support and validate the program's compliance assessment and assertions .

according to the compliance guidance , program - level architecture products , such as those defining information exchanges and system data requirements are not required to be used until after the system has been deployed .

this is important because waiting until the system is deployed is too late to avoid the costly rework required to address areas of noncompliance .

moreover , it is not consistent with other dod guidance , which states that program - level architecture products that describe , for example , information exchanges , should be developed before a program begins system development .

the limitations in existing bea compliance - related policy and guidance , the supporting compliance assessment tool , and the federated bea , puts programs like gcss - mc at increased risk of being defined and implemented in a way that does not sufficiently ensure interoperability and avoid duplication and overlap .

we currently have a review under way for the senate armed services committee , subcommittee on readiness and management support , which is examining multiple programs' compliance with the federated bea .

the investment in the first increment of gcss - mc has not been economically justified on the basis of reliable analyses of estimated system costs over the life of the program .

according to the program's economic analysis , the first increment will have an estimated life cycle cost of about $442 million and deliver an estimated $1.04 billion in risk - adjusted estimated benefits during this same life cycle .

this equates to a net present value of about $688 million .

while the most recent cost estimate was derived using some effective estimating practices , it did not make use of other practices that are essential to having an accurate and credible estimate .

as a result , the marine corps does not have a sufficient basis for deciding whether gcss - mc , as defined , is the most cost - effective solution to meeting its mission needs , and it does not have a reliable basis against which to measure cost performance .

a reliable cost estimate is critical to the success of any it program , as it provides the basis for informed investment decision making , realistic budget formulation and program resourcing , meaningful progress measurement , proactive course correction , and accountability for results .

according to the office of management and budget ( omb ) , programs must maintain current and well - documented cost estimates , and these estimates must encompass the full life cycle of the program .

omb states that generating reliable cost estimates is a critical function necessary to support omb's capital programming process .

without reliable estimates , programs are at increased risk of experiencing cost overruns , missed deadlines , and performance shortfalls .

our research has identified a number of practices for effective program cost estimating .

we have issued guidance that associates these practices with four characteristics of a reliable cost estimate .

these four characteristic are specifically defined as follows: comprehensive: the cost estimates should include both government and contractor costs over the program's full life cycle , from the inception of the program through design , development , deployment , and operation and maintenance , to retirement .

they should also provide a level of detail appropriate to ensure that cost elements are neither omitted nor double counted and include documentation of all cost - influencing ground rules and assumptions .

well - documented: the cost estimates should have clearly defined purposes and be supported by documented descriptions of key program or system characteristics ( eg , relationships with other systems , performance parameters ) .

additionally , they should capture in writing such things as the source data used and their significance , the calculations performed and their results , and the rationale for choosing a particular estimating method or reference .

moreover , this information should be captured in such a way that the data used to derive the estimate can be traced back to , and verified against , their sources .

the final cost estimate should be reviewed and accepted by management on the basis of confidence in the estimating process and the estimate produced by the process .

accurate: the cost estimates should provide for results that are unbiased and should not be overly conservative or optimistic ( i.e. , should represent the most likely costs ) .

in addition , the estimates should be updated regularly to reflect material changes in the program , and steps should be taken to minimize mathematical mistakes and their significance .

the estimates should also be grounded in a historical record of cost estimating and actual experiences on comparable programs .

credible: the cost estimates should discuss any limitations in the analysis performed that are due to uncertainty or biases surrounding data or assumptions .

further , the estimates' derivation should provide for varying any major assumptions and recalculating outcomes based on sensitivity analyses , and the estimates' associated risks and inherent uncertainty should be disclosed .

also , the estimates should be verified based on cross - checks using other estimating methods and by comparing the results with independent cost estimates .

the $442 million life cycle cost estimate for the first increment reflects many of the practices associated with a reliable cost estimate , including all practices associated with being comprehensive and well - documented , and several related to being accurate and credible .

 ( see table 4. ) .

however , several important accuracy and credibility practices were not satisfied .

the cost estimate is comprehensive because it includes both the government and contractor costs specific to development , acquisition ( nondevelopment ) , implementation , and operations and support over the program's 12-year life cycle .

moreover , the estimate clearly describes how the various subelements are summed to produce the amounts for each cost category , thereby ensuring that all pertinent costs are included , and no costs are double counted .

lastly , cost - influencing ground rules and assumptions , such as the program's schedule , labor rates , and inflation rates are documented .

the cost estimate is also well - documented in that the purpose of the cost estimate was clearly defined , and a technical baseline has been documented that includes , among others things , the relationships with other systems and planned performance parameters .

furthermore , the calculations and results used to derive the estimate are documented , including descriptions of the methodologies used and traceability back to source data ( eg , vendor quotes , salary tables ) .

also , the cost estimate was reviewed both by the naval center for cost analysis and the office of the secretary of defense , director for program analysis and evaluation , which ensures a level of confidence in the estimating process and the estimate produced .

however , the estimate lacks accuracy because not all important practices related to this characteristic were satisfied .

specifically , while the estimate is grounded in documented assumptions ( eg , hardware refreshment every 5 years ) , and periodically updated to reflect changes to the program , it is not grounded in historical experience with comparable programs .

as stated in our guide , estimates should be based on historical records of cost and schedule estimates from comparable programs , and such historical data should be maintained and used for evaluation purposes and future estimates on other comparable programs .

the importance of doing so is evident by the fact that gcss - mc's cost estimate has increased by about $193 million since july 2005 , which program officials attributed to , among other things , schedule delays , software development complexity , and the lack of historical data from similar erp programs .

while the program office did leverage historical cost data from other erp programs , including the navy's enterprise resource planning pilot programs and programs at the bureau of prisons and the department of agriculture , program officials told us that these programs' scopes were not comparable .

for example , none of the programs had to utilize a communication architecture as complex as the marine corps , which officials cited as a significant factor in the cost increases and a challenge in estimating costs .

the absence of analogous cost data for large - scale erp programs is due in part to the fact that dod has not established a standardized cost element structure for erp programs that can be used to capture actual cost data .

according to officials with the defense cost and resource center , such cost element structures are needed , along with a requirement for programs to report on their costs , but approval and resources have yet to be gained for either these structures or the reporting of their costs .

until a standardized data structure exists , programs like gcss - mc will continue to lack a historical database containing cost estimates and actual cost experiences of comparable erp programs .

this means that the current and future gcss - mc cost estimates will lack sufficient accuracy for effective investment decision making and performance measurement .

compounding the estimate's limited accuracy are limitations in its credibility .

specifically , while the estimate satisfies some of the key practices for a credible cost estimate ( eg , confirming key cost drivers , performing sensitivity analyses , having an independent cost estimate prepared by the naval center for cost analysis that was within 4 percent of the program's estimate , and conducting a risk analysis that showed a range of estimated costs of $411 million to $523 million ) , no risk analysis was performed to determine the program schedule's risks and associated impact on the cost estimate .

as described earlier in this report , the program has experienced about 3 years in schedule delays and recently experienced delays in completing the solution design phase .

therefore , the importance of conducting a schedule risk analysis and using the results to assess the variability in the cost estimate is critical for ensuring a credible cost estimate .

program officials agreed that the program's schedule is aggressive and risky and that this risk was not assessed in determining the cost estimate's variability .

without doing so , the program's cost estimate is not credible , and thus the program is at risk of cost overruns as a result of schedule delays .

forecasting expected benefits over the life of a program is also a key aspect of economically justifying an investment .

omb guidance advocates economically justifying investments on the basis of net present value .

if net present value is positive , then the corresponding benefit - to - cost ratio will be greater than 1 ( and vice versa ) .

this guidance also advocates updating the analyses over the life of the program to reflect material changes in expected benefits , costs , and risks .

since estimates of benefits can be uncertain because of the imprecision in both the underlying data and modeling assumptions used , effects of this uncertainty should be analyzed and reported .

by doing this , informed investment decision making can occur through the life of the program , and a baseline can be established against which to compare the accrual of actual benefits from deployed system capabilities .

the original benefit estimate for the first increment was based on questionable assumptions and insufficient data from comparable programs .

the most recent economic analysis , dated january 2007 , includes monetized , yearly benefit estimates for fiscal years 2010 – 2019 in three key areas — inventory reductions , reductions in inventory carrying costs , and improvements in maintenance processes .

collectively , these benefits totaled about $2.89 billion ( not risk - adjusted ) .

however , these calculations were made using questionable assumptions and limited data .

for example , the total value of the marine corps inventory needed to calculate inventory reductions and reductions in carrying costs could not be determined because of limitations with existing logistic systems .

the cost savings resulting from improvements in maintenance processes were calculated based on assumptions from an erp implementation in the commercial sector that , according to program officials , is not comparable in scope to gcss - mc .

to account for the uncertainty inherent in the benefits estimate , the program office performed a monte carlo simulation .

according to the program office , this risk analysis generated a discounted and risk - adjusted benefits estimate of $1.04 billion .

as a result of the $1.85 billion adjustment to estimated benefits , the program office has a more realistic benefit baseline against which to compare the accrual of actual benefits from deployed system capabilities .

the program office has elected to implement evm , which is a proven means for measuring program progress and thereby identifying potential cost overruns and schedule delays early , when they can be minimized .

in doing so , it has adopted a tailored evm approach that focuses on schedule .

however , this schedule - focused approach has not been effectively implemented because it is based on a baseline schedule that was not derived using key schedule estimating practices .

according to program officials , the schedule was driven by an aggressive program completion date established in response to direction from oversight entities to complete the program as soon as possible .

as a result , they said that following these practices would have delayed this completion date .

regardless , this means that the schedule baseline is not reliable , and progress will likely not track to the schedule .

the program office has adopted a tailored approach to performing evm because of the contract type being used .

as noted earlier , the contract types associated with gcss - mc integration and implementation vary , and include , for example , firm - fixed - price contracts and time - and - materials contracts .

under a firm - fixed - price contract , the price is not subject to any adjustment on the basis of the contractor's cost experience in performing the contract .

for a time - and - materials contract , supplies or services are acquired on the basis of ( 1 ) an undefined number of direct labor hours that are paid at specified fixed hourly rates and ( 2 ) actual cost for materials .

according to dod guidance , evm is generally not encouraged for firm - fixed - price , level of effort , and time - and - material contracts .

in these situations , the guidance states that programs can use a tailored evm approach in which an integrated master schedule ( ims ) is exclusively used to provide visibility into program performance .

don has chosen to implement this tailored evm approach on gcss - mc .

in doing so , it is measuring progress against schedule commitments , and not cost commitments , using an ims for each program phase .

according to program officials , the ims describes and guides the execution of program activities .

regardless of the approach used , effective implementation depends on having a reliable ims .

the success of any program depends in part on having a reliable schedule specifying when the program's set of work activities will occur , how long they will take , and how they are related to one another .

as such , the schedule not only provides a road map for the systematic execution of a program , but it also provides the means by which to gauge progress , identify and address potential problems , and promote accountability .

our research has identified nine practices associated with effective schedule estimating .

these practices are ( 1 ) capturing key activities , ( 2 ) sequencing key activities , ( 3 ) assigning resources to key activities , ( 4 ) integrating key activities horizontally and vertically , ( 5 ) establishing the duration of key activities , ( 6 ) establishing the critical path for key activities , ( 7 ) identifying “float time” between key activities , ( 8 ) distributing reserves to high - risk activities , and ( 9 ) performing a schedule risk analysis .

the current ims for the solution design and transition - to - build phase of the first increment was developed using some of these practices .

however , it does not reflect several practices that are fundamental to having a schedule baseline that provides a sufficiently reliable basis for measuring progress and forecasting slippages .

to the program office's credit , its ims captures and sequences key activities required to complete the project , integrates the tasks horizontally , and identifies the program's critical path .

however , the program office is not monitoring the actual durations of scheduled activities so that it can address the impact of any deviations on later scheduled activities .

moreover , the schedule does not adequately identify the resources needed to complete the tasks and is not integrated vertically , meaning that multiple teams executing different aspects of the program cannot effectively work to the same master schedule .

further , the ims does not adequately mitigate schedule risk by identifying float time between key activities , introducing schedule reserve for high - risk activities , or including the results of a schedule risk analysis .

see table 5 for the results of our analyses relative to each of the nine practices .

according to program officials , they intend to begin monitoring actual activity start and completion dates so that they can proactively adjust later scheduled activities that are affected by deviations .

however , they do not plan to perform the three practices related to understanding and managing schedule risk because doing so would likely extend the program's completion date , and they set this date to be responsive to direction from dod and don oversight entities to complete the program as soon as possible .

in our view , not performing these practices does not allow the inherent risks in meeting this imposed completion date to be proactively understood and addressed .

the consequence of omitting these practices is a schedule that does not provide a reliable basis for performing evm .

well - defined and managed requirements are recognized by dod guidance as essential and can be viewed as a cornerstone of effective system acquisition .

one aspect of effective requirements management is requirements traceability .

by tracing requirements both backward from system requirements to higher level business or operational requirements and forward to system design specifications and test plans , the chances of the deployed product satisfying requirements are increased , and the ability to understand the impact of any requirement changes , and thus make informed decision about such changes , is enhanced .

the program office recently strengthened its requirements traceability .

in november 2007 , and again in february 2008 , the program office was unable to demonstrate for us that it could adequately trace its 1,375 system requirements to both design specifications and test documentation .

specifically , the program office was at that time using a tool called doors® , which if implemented properly , allows each requirement to be linked from its most conceptual definition to its most detailed definition , as well as to design specifications and test cases .

in effect , the tool maintains the linkages among requirement documents , design documents , and test cases even if requirements change .

however , the system integration contractor was not using the tool .

instead the contractor was submitting its 244 work products , accompanied by spreadsheets that linked each work product to one or more system requirements and test cases .

the program office then had to verify and validate the spreadsheets and import and link each work product to the corresponding requirement and test case in doors .

because of the sheer number of requirements and work products and its potential to impact cost , schedule , and performance , the program designated this approach as a medium risk .

it later closed the risk because the proposed mitigation strategy failed to mitigate it , and it was realized as a high - priority program issue ( i.e. , problem ) .

according to program officials , this requirements traceability approach resulted in time - consuming delays in approving the design work products and importing and establishing links between these products and the requirements in doors , in part because the work products were not accompanied by complete spreadsheets that established the traceability .

as a result , about 30 percent of the contractor's work products had yet to be validated , approved , and linked to requirements when the design phase was originally scheduled to be complete .

officials stated that the contractor was not required to use doors because it was not experienced with this tool and becoming proficient with it would have required time and resources , thereby increasing both the program's cost and schedule .

ironically , however , not investing the time and resources to address the limitations in the program's traceability approach contributed to recent delays in completing the solution design activities , and additional resources had to be invested to address its requirements traceability problems .

the program office now reports that it can trace requirements backward and forward .

in april 2008 , we verified this by tracing 60 out of 61 randomly sampled requirements backward to system requirements and forward to approved design specifications and test plans .

program officials explained that the reason that we could not trace the one requirement was that the related work products had not yet been approved .

in addition , they stated that there were additional work products that had yet to be finalized and traced .

without adequate traceability , the risk of a system not performing as intended and requiring expensive rework is increased .

to address its requirements traceability weakness , program officials told us that they now intend to require the contractor to use doors during the next phase of the program ( build and test ) .

if implemented effectively , the new process should address previous requirements traceability weaknesses and thereby avoid a repeat of past problems .

proactively managing program risks is a key acquisition management control and , if done properly , can greatly increase the chances of programs delivering promised capabilities and benefits on time and within budget .

to the program office's credit , it has defined a risk management process that meets relevant guidance .

however , it has not effectively implemented the process for all identified risks .

as a result , these risks have become actual program problems that have impacted the program's cost , schedule , and performance commitments .

dod acquisition management guidance , as well as other relevant guidance , advocates identifying facts and circumstances that can increase the probability of an acquisition's failing to meet cost , schedule , and performance commitments and then taking steps to reduce the probability of their occurrence and impact .

in brief , effective risk management consists of: ( 1 ) establishing a written plan for managing risks ; ( 2 ) designating responsibility for risk management activities ; ( 3 ) encouraging project - wide participation in the identification and mitigation of risks ; ( 4 ) defining and implementing a process that provides for the identification , analysis , and mitigation of risks ; and ( 5 ) examining the status of identified risks in program milestone reviews .

the program office has developed a written plan for managing risks , and established a process that together provide for the above cited risk management practices , and it has followed many key aspects of its plan and process .

for example , the program manager has been assigned overall responsibility for managing risks .

also , individuals have been assigned ownership of each risk , to include conducting risk analyses , implementing mitigation strategies , and working with the risk support team .

the plan and process encourage project - wide participation in the identification and mitigation of risks by allowing program staff to submit a risk for inclusion in a risk database and take ownership of the risk and the strategy for mitigating it .

in addition , stakeholders can bring potential risks to the program manager's attention through interviews , where potential risks are considered and evaluated .

the program office has thus far identified and categorized individual risks .

as of december 2007 , the risk database contained 27 active risks — 2 high , 15 medium , and 10 low .

program risks are considered during program milestone reviews .

specifically , our review of documentation for the design readiness review , a key decision point during the system development and demonstration phase leading up to a milestone c decision , showed that key risks were discussed .

furthermore , the program manager reviews program risks' status through a risk watch list and bimonthly risk briefings .

however , the program office has not consistently followed other aspects of its process .

for example , it did not perform key practices for identifying and managing schedule risks , such as conducting a schedule risk assessment and building in reserve time to its schedule .

in addition , mitigation steps for several key risks were either not performed in accordance with the risk management strategy , or risks that were closed as having been mitigated were later found to be actual program issues ( i.e. , problems ) .

for 25 medium risks in the closed risk database , as of february 2008 , 4 were closed because mitigation steps were not performed in accordance with the strategy and the risks ultimately became actual issues .

examples from these medium risks are as follows: in one case , the mitigation strategy was for the contractor to deliver certain design documents that were traced to system requirements and to do so before beginning the solution build phase .

the design documents , however , were not received in accordance with the mitigation strategy .

specifically , program officials told us that the design documents contained inaccuracies or misinterpretations of the requirements and were not completed on time because of the lack of resources to correct these problems .

as a result , the program experienced delays in completing its solution design activities .

in another case , the mitigation strategy included creating the documentation needed to execute the contract for monitoring the build phase activities .

however , the mitigation steps were not performed due to , among other things , delays in approving the contractual approach .

as a result , the risk became a high - priority issue in february 2008 .

according to a program issue report , the lack of a contract to monitor system development progress may result in unnecessary rework and thus additional program cost overruns , schedule delays , and performance shortfalls .

four of the same 25 medium risks were retired because key mitigation steps for each one were implemented , but the strategies proved ineffective , and the risks became actual program issues .

included in these 4 risks were the following: in one case , the program office closed a risk regarding data exchange with another don system because key mitigation steps to establish exchange requirements were fully implemented .

however , in february 2008 , a high - priority issue was identified regarding the exchange of data with this system .

according to program officials , the risk was mitigated to the fullest extent possible and closed based on the understanding that continued evaluation of data exchange requirements would be needed .

however , because the risk was retired , this evaluation did not occur .

in another case , a requirements management risk was closed on the basis of having implemented mitigation steps , which involved establishing a requirements management process , including having complete requirements traceability spreadsheets .

however , although several of the mitigation steps were not fully implemented , the risk was closed on the basis of what program officials described as an understanding reached with the contractor regarding the requirements management process .

several months later , a high - priority issue concerning requirements traceability was identified because the program office discovered that the contractor was not adhering to the understanding .

unless risk mitigation strategies are monitored to ensure that they are fully implemented and that they produce the intended outcomes , and additional mitigation steps are taken when they are not , the program office will continue to be challenged in preventing risks from developing into actual cost , schedule , and performance problems .

effective management of programs like gcss - mc depends in part on the ability to measure the quality of the system being acquired and implemented .

two measures of system quality are trends in ( 1 ) the number of unresolved severe system defects and ( 2 ) the number of unaddressed high - priority system change requests .

gcss - mc documentation recognizes the importance of monitoring such trends .

moreover , the program office has established processes for ( 1 ) collecting and tracking data on the status of program issues , including problems discovered during early test events , and ( 2 ) capturing data on the status of requests for changes to the system .

however , its processes do not provide the full complement of data that are needed to generate a reliable and meaningful picture of trends in these areas .

in particular , data on problems and change request priority levels and closure dates are either not captured or not consistently maintained .

further , program office oversight of contractor - identified issues or defects is limited .

program officials acknowledged these data limitations , but they stated that oversight of contractor - identified issues is not their responsibility .

without tracking trends in key indicators , the program office cannot adequately understand and report to dod decision makers whether gcss - mc's quality and stability are moving in the right direction .

program guidance and related best practices encourage trend analysis and the reporting of system defects and program problems as measures or indicators of system quality and program maturity .

as we have previously reported , these indicators include trends in the number of unresolved problems according to their significance or priority .

to the program office's credit , it collects and tracks what it calls program issues , which are problems identified by program office staff or the system integrator that are process , procedure , or management related .

these issues are contained in the program's issues - risk management information system ( i - rmis ) .

among other things , each issue in i - rmis is to have an opened and closed date and an assigned priority level of high , medium , or low .

in addition , the integration contractor tracks issues that its staff identifies related to such areas as system test defects .

these issues are contained in the contractor's marine corps issue tracking system ( mcits ) .

each issue in mcits is to have a date when it was opened and is to be assigned a priority on a scale of 1-5 .

according to program officials , the priority levels are based on guidance from the institute of electrical and electronics engineers ( ieee ) .

 ( see table 6 for a description of each priority level. ) .

however , neither i - rmis nor mcits contain all the data needed to reliably produce key measures or indicators of system quality and program maturity .

examples of these limitations are as follows: for i - rmis , the program office has not established a standard definition of the priority levels used .

rather , according to program officials , each issue owner is allowed to assign a priority based on the owner's definition of what high , medium , and low mean .

by not using standard priority definitions for categorizing issues , the program office cannot ensure that it has an accurate and useful understanding of the problems it is facing at any given time , and it will not know if it is addressing the highest priority issues first .

for mcits , the integration contractor does not track closure dates for all issues .

for example , as of april 2008 , over 30 percent of the closed issues did not have closure dates .

this is important because it limits the contractor's ability to understand trends in the number of high - priority issues that are unresolved .

program officials acknowledged the need to have closure dates for all closed issues and stated that they intend to correct this .

if it is not corrected , the program office will not be able to create a reliable measure of system quality and program maturity .

compounding the above limitations in mcits data is the program office's decision not to use contractor - generated reports that are based on mcits data .

specifically , reports summarizing mcits issues are posted to a sharepoint site for the program office to review .

however , program officials stated that they do not review these reports because the mcits issues are not their responsibility , but the contractor's .

however , without tracking and monitoring contractor - identified issues , which include such things as having the right skill - sets and having the resources to track and monitor issues captured in separate databases , the program office is missing an opportunity to understand whether proactive action is needed to address emerging quality shortfalls in a timely manner .

program guidance and related best practices encourage trend reporting of change requests as measures or indicators of system stability and quality .

these indicators include trends in the number and priority of approved changes to the system's baseline functional and performance capabilities that have yet to be resolved .

to its credit , the program office collects and tracks changes to the system , which can range from minor or administrative changes to more significant changes that propose or impact important system functionality .

these changes can be identified by either the program office or the contractor , and they are captured in a master change request spreadsheet .

further , the changes are to be prioritized according to the level described in table 7 , and the dates that change requests are opened and closed are to be recorded .

however , the change request master spreadsheet does not contain the data needed to reliably produce key measures or indicators of system stability and quality .

examples of these limitations are as follows: the program office has not prioritized proposed changes or managed these changes according to their priorities .

for example , of the 572 change requests as of april 2008 , 171 were assigned a priority level , and 401 were not .

of these 171 , 132 were categorized as priority 1 .

since then , the program office has temporarily recategorized the 401 change requests to priority 3 until each one's priority can be evaluated .

the program office has yet to establish a time frame for doing so .

the dates that change requests are resolved are not captured in the master spreadsheet .

rather , program officials said that these dates are in the program's ims and are shown there as target implementation dates .

while the ims does include the dates changes will be implemented , these dates are not actual dates , and they are not used to establish trends in unresolved change requests .

without the full complement of data needed to monitor and measure change requests , the program office cannot know and disclose to dod decision makers whether the quality and stability of the system are moving in the right direction .

dod's success in delivering large - scale business systems , such as gcss - mc , is in large part determined by the extent to which it employs the kind of rigorous and disciplined it management controls that are reflected in dod policies and related guidance .

while implementing these controls does not guarantee a successful program , it does minimize a program's exposure to risk and thus the likelihood that it will fall short of expectations .

in the case of gcss - mc , living up to expectations is important because the program is large , complex , and critical to supporting the department's warfighting mission .

the department has not effectively implemented a number of essential it management controls on gcss - mc , which has already contributed to significant cost overruns and schedule delays , and has increased the program's risk going forward of not delivering a cost - effective system solution and not meeting future cost , schedule , capability , and benefit commitments .

moreover , gcss - mc could be duplicating the functionality of related systems and may be challenged in interoperating with these systems because compliance with key aspects of dod's federated bea has not been demonstrated .

also , the program's estimated return on investment , and thus the economic basis for pursing the proposed system solution , is uncertain because of limitations in how the program's cost estimate was derived , raising questions as to whether the nature and level of future investment in the program needs to be adjusted .

in addition , the program's schedule was not derived using several key schedule estimating practices , which impacts the integrity of the cost estimate and precludes effective implementation of evm .

without effective evm , the program cannot reliably gauge progress of the work being performed so that shortfalls can be known and addressed early , when they require less time and fewer resources to overcome .

another related indicator of progress , trends in system problems and change requests , also cannot be gauged because the data needed to do so are not being collected .

collectively , these weaknesses have already helped to push back the completion of the program's first increment by more than 3 years and added about $193 million in costs , and they are introducing a number of risks that , if not effectively managed , could further impact the program .

however , whether these risks will be effectively managed is uncertain because the program has not always followed its defined risk management process and , as a result , has allowed yesterday's potential problems to become today's actual cost , schedule , and performance problems .

while the program office is primarily responsible for ensuring that effective it management controls are implemented on gcss - mc , other oversight and stakeholder organizations share some responsibility .

in particular , even though the program office has not demonstrated its alignment with the federated bea , it nevertheless followed established dod architecture compliance guidance and used the related compliance assessment tool in assessing and asserting its compliance .

the root cause for not demonstrating compliance thus is not traceable to the program office , but rather is due to , among other things , the compliance guidance and tool being limited , and the program's oversight entities not validating the compliance assessment and assertion .

also , even though the program's cost estimate was not informed by the cost experiences of other erp programs of the same scope , the program office is not to blame because the root cause for this is that the defense cost and resource center has not maintained a standardized cost element structure for its erp programs and a historical database of erp program costs for program's like gcss - mc to use .

in contrast , other weaknesses are within the program office's control , as evidenced by its positive actions to address the requirements traceability shortcomings that we brought to its attention during of the course of our work and its well - defined risk management process .

all told , this means that addressing the gcss - mc it management control weaknesses require the combined efforts of the various dod organizations that share responsibility for defining , justifying , managing , and overseeing the program .

by doing so , the department can better assure itself that gcss - mc will optimally support its mission operations and performance goals and will deliver promised capabilities and benefits , on time and within budget .

to ensure that each gcss - mc system increment is economically justified on the basis of a full and reliable understanding of costs , benefits , and risks , we recommend that the secretary of defense direct the secretary of the navy to ensure that investment in the next acquisition phase of the program's first increment is conditional upon fully disclosing to program oversight and approval entities the steps under way or planned to address each of the risks discussed in this report , including the risk of not being architecturally compliant and being duplicative of related programs , not producing expected mission benefits commensurate with reliably estimated costs , not effectively implementing evm , not mitigating known program risks , and not knowing whether the system is becoming more or less mature and stable .

we further recommend that investment in all future gcss - mc increments be limited if the management control weaknesses that are the source of these risks , and which are discussed in this report , have not been fully addressed .

to address each of the it management control weaknesses discussed in this report , we are also making a number of additional recommendations .

however , we are not making recommendations for the architecture compliance weaknesses discussed in this report because we have a broader review of don program compliance to the bea and don enterprise architecture that will be issued shortly and will contain appropriate recommendations .

to improve the accuracy of the gcss - mc cost estimate , as well as other cost estimates for the department's erp programs , we recommend that the secretary of defense direct the appropriate organization within dod to collaborate with relevant organizations to standardize the cost element structure for the department's erp programs and to use this standard structure to maintain cost data for its erp programs , including gcss - mc , and to use this cost data in developing future cost estimates .

to improve the credibility of the gcss - mc cost estimate , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that the program's current economic analysis is adjusted to reflect the risks associated with it not reflecting cost data for comparable erp programs , and otherwise not having been derived according to other key cost estimating practices , and that future updates to the gcss - mc economic analysis similarly do so .

to enhance gcss - mc's use of evm , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that the program office ( 1 ) monitors the actual start and completion dates of work activities performed so that the impact of deviations on downstream scheduled work can be proactively addressed ; ( 2 ) allocates resources , such as labor hours and material , to all key activities on the schedule ; ( 3 ) integrates key activities and supporting tasks and subtasks ; ( 4 ) identifies and allocates the amount of float time needed for key activities to account for potential problems that might occur along or near the schedule's critical path ; ( 5 ) performs a schedule risk analysis to determine the level of confidence in meeting the program's activities and completion date ; ( 6 ) allocates schedule reserve for high - risk activities on the critical path ; and ( 7 ) discloses the inherent risks and limitations associated with any future use of the program's evm reports until the schedule has been risk - adjusted .

to improve gcss - mc management of program risks , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that the program office ( 1 ) adds each of the risks discussed in this report to its active inventory of risks , ( 2 ) tracks and evaluates the implementation of mitigation plans for all risks , ( 3 ) discloses to appropriate program oversight and approval authorities whether mitigation plans have been fully executed and have produced the intended outcome ( s ) , and ( 4 ) only closes a risk if its mitigation plan has been fully executed and produced the intended outcome ( s ) .

to strengthen gcss - mc system quality measurement , we recommend that the secretary of defense direct the secretary of the navy , through the appropriate chain of command , to ensure that the program office ( 1 ) collects the data needed to develop trends in unresolved system defects and change requests according to their priority and severity and ( 2 ) discloses these trends to appropriate program oversight and approval authorities .

in written comments on a draft of this report , signed by the deputy under secretary of defense ( business transformation ) and reprinted in appendix ii , the department stated that it concurred with two of our recommendations and partially concurred with the remaining five .

in general , the department partially concurred because it said that efforts were either under way or planned that will address some of the weaknesses that these recommendations are aimed at correcting .

for example , the department stated that gcss - mc will begin to use a recently developed risk assessment tool that is expected to assist programs in identifying and mitigating internal and external risks .

further , it said that these risks will be reported to appropriate department decision makers .

we support the efforts that dod described in its comments because they are generally consistent with the intent of our recommendations and believe that if they are fully and properly implemented , they will go a long way in addressing the management control weaknesses that our recommendations are aimed at correcting .

in addition , we have made a slight modification to one of these five recommendations to provide the department with greater flexibility in determining which organizations should provide for the recommendation's implementation .

we are sending copies of this report to interested congressional committees ; the director , office of management and budget ; the congressional budget office ; the secretary of defense ; and the department of defense office of the inspector general .

we also will make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-3439 or hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix iii .

our objective was to determine whether the department of the navy is effectively implementing information technology management controls on the global combat support system - marine corps ( gcss - mc ) .

to accomplish this , we focused on the first increment of gcss - mc relative to the following management areas: architectural alignment , economic justification , earned value management , requirements management , risk management , and system quality measurement .

in doing so , we analyzed a range of program documentation , such as the acquisition strategy , program management plan , and acquisition program baseline , and interviewed cognizant program officials .

to determine whether gcss - mc was aligned with the department of defense's ( dod ) federated business enterprise architecture ( bea ) , we reviewed the program's bea compliance assessments and system architecture products , as well as versions 4.0 , 4.1 , and 5.0 of the bea and compared them with the bea compliance requirements described in the fiscal year 2005 national defense authorization act and dod's bea compliance guidance and evaluated the extent to which the compliance assessments addressed all relevant bea products .

we also determined the extent to which the program - level architecture documentation supported the bea compliance assessments .

we obtained documentation , such as the bea compliance assessments from the gcss - mc and navy enterprise resource planning programs , as well as the air force's defense enterprise accounting and management system and air force expeditionary combat support system programs .

we then compared these assessments to identify potential redundancies or opportunities for reuse and determined if the compliance assessments examined duplication across programs and if the tool that supports these assessments is being used to identify such duplication .

in doing so , we interviewed program officials and officials from the department of the navy , office of the chief information officer , and reviewed recent gao reports to determine the extent to which the programs were assessed for compliance against the department of the navy enterprise architecture .

we also interviewed program officials and officials from the business transformation agency and the department of the navy , including the logistics functional area manager , and obtained guidance documentation from these officials to determine the extent to which the compliance assessments were subject to oversight or validation .

to determine whether the program had economically justified its investment in gcss - mc , we reviewed the latest economic analysis to determine the basis for the cost and benefit estimates .

this included evaluating the analysis against office of management and budget guidance and gao's cost assessment guide .

in doing so , we interviewed cognizant program officials , including the program manager and cost analysis team , regarding their respective roles , responsibilities , and actual efforts in developing and / or reviewing the economic analysis .

we also interviewed officials at the office of program analysis and evaluation and naval center for cost analysis as to their respective roles , responsibilities , and actual efforts in developing and / or reviewing the economic analysis .

to determine the extent to which the program had effectively implemented earned value management , we reviewed relevant documentation , such the contractor's monthly status reports , acquisition program baselines , and schedule estimates and compared them with dod policies and guidance .

we also reviewed the program's schedule estimates and compared them with relevant best practices to determine the extent to which they reflect key estimating practices that are fundamental to having a reliable schedule .

in doing so , we interviewed cognizant program officials to discuss their use of best practices in creating the program's current schedule .

to determine the extent to which the program implemented requirements management , we reviewed relevant program documentation , such as the baseline list of requirements and system specifications and evaluated them against relevant best practices to determine the extent to which the program has effectively managed the system's requirements and maintained traceability backward to high - level business operation requirements and system requirements , and forward to system design specifications , and test plans .

to determine the extent to which the requirements were traceable , we randomly selected 61 program requirements and traced them both backward and forward .

this sample was designed with a 5 percent tolerable error rate at the 95 percent level of confidence , so that , if we found 0 problems in our sample , we could conclude statistically that the error rate was less than 5 percent .

based upon the weight of all other factors included in our evaluation , our verification of 60 out of 61 requirements was sufficient to demonstrate traceability .

in addition , we interviewed program officials involved in the requirements management process to discuss their roles and responsibilities for managing requirements .

to determine the extent to which the program implemented risk management , we reviewed relevant risk management documentation , such as risk plans and risk database reports demonstrating the status of the program's major risks and compared the program office's activities with dod acquisition management guidance and related best practices .

we also reviewed the program's mitigation process with respect to key risks , including 25 medium risks in the retired risk database that were actively addressed by the program office , to determine the extent to which these risks were effectively managed .

in doing so , we interviewed cognizant program officials responsible , such as the program manager , risk manager , and subject matter experts to discuss their roles and responsibilities and obtain clarification on the program's approach to managing risks associated with acquiring and implementing gcss - mc .

to determine the extent to which the program is collecting the data and monitoring trends in the number of unresolved system defects and the number of unaddressed change requests , we reviewed program documentation such as the testing strategy , configuration management policy , test defect reports , change request logs , and issue data logs .

we compared the program's data collection and analysis practices relative to these areas to program guidance and best practices to determine the extent to which the program is measuring important aspects of system quality .

we also interviewed program officials such as system developers , relevant program management staff , and change control managers to discuss their roles and responsibilities for system quality measurement .

we conducted our work at dod offices and contractor facilities in the washington , d.c. , metropolitan area , and triangle , va. , from june 2007 to july 2008 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objective .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objective .

in addition to the individual named above , key contributors to this report were neelaxi lakhmani , assistant director ; monica anatalio ; harold brumm ; neil doherty ; cheryl dottermusch ; nancy glover ; mustafa hassan ; michael holland ; ethan iczkovitz ; anh le ; josh leiling ; emily longcore ; lee mccracken ; madhav panwar ; karen richey ; melissa schermerhorn ; karl seifert ; sushmita srikanth ; jonathan ticehurst ; christy tyson ; and adam vodraska .

