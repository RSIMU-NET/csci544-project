federal decision makers need evidence about whether federal programs and activities achieve intended results as they set priorities and consider how to make progress toward national objectives .

the office of management and budget ( omb ) defines evidence as “the available body of facts or information indicating whether a belief or proposition is true or valid.” omb's guidance further states that evidence may come from a variety of sources , including descriptive statistics , performance measurement , policy analysis , program evaluations , and other research .

to ensure that decision makers have the evidence they need , agencies undertake a range of activities .

evidence - building activities involve assessing existing evidence and identifying any need for additional evidence ; determining which new evidence to generate , when , and how ( i.e. , prioritizing new evidence ) ; generating that evidence ; and using evidence in decision - making .

congress and omb have taken actions intended to strengthen federal evidence - building activities .

for example , the government performance and results act of 1993 ( gpra ) , as updated and expanded by the gpra modernization act of 2010 , established a government - wide framework for generating and using performance information .

in march 2016 , congress passed , and the president signed , legislation establishing the commission on evidence - based policymaking to study the availability and use of evidence in government .

in its final report , issued in september 2017 , the commission found that within federal agencies , multiple entities ( i.e. , component agencies or offices ) had responsibilities for generating different sources of evidence .

however , the commission found that federal agencies' capacities to generate a full range of evidence were uneven .

the commission further found that where capacity existed , it was often poorly coordinated .

this included coordination within an agency — across its different evidence - building entities .

in total , the commission made 22 recommendations aimed at strengthening federal evidence - building activities .

subsequently , the foundations for evidence - based policymaking act of 2018 ( evidence act ) , enacted in january 2019 , created a framework intended to take a more comprehensive and integrated approach to federal evidence - building activities .

according to omb , the evidence act addressed about half of the commission's recommendations , advancing data and evidence - building functions in the federal government .

for example , in line with the commission's findings and recommendations , 24 major federal agencies are to designate an evaluation officer , who has responsibilities for coordinating evidence - building activities required by the evidence act with other relevant agency officials .

you asked us to examine the coordination of federal evidence - building activities .

in response to that request , this report ( 1 ) describes activities selected agencies took that aligned with congressional and omb direction to strengthen evidence - building , and ( 2 ) examines the extent to which selected agencies' processes for assessing and prioritizing evidence needs reflect leading practices for collaboration .

to address both objectives , we analyzed agency documents about federal evidence - building activities and interviewed relevant staff at omb and officials at five selected agencies: the departments of education , health and human services , and labor ; the corporation for national and community service ; and the u.s. agency for international development .

we selected these five agencies based on the greater number of experiences they had in comparison to other agencies' incorporation of evidence - building activities into the design and implementation of certain programs .

these experiences included evidence - based approaches , such as pay for success projects , performance partnerships , and tiered evidence grants .

for the first objective , we reviewed information from the five selected agencies and identified examples of evidence - building activities within each agency since 2010 .

we then determined where these examples illustrated actions that aligned with evidence - building statutory requirements and directions from omb , including guidance , memorandums , and activities outlined in the president's management agenda .

for the second objective , we evaluated processes each selected agency established to take a coordinated approach to assessing and prioritizing evidence needs across the agency .

we compared these processes to leading practices for collaboration identified in our prior work .

for this report , we focused on a subset of four collaboration practices: defining a leadership model ; involving all relevant participants ; clarifying roles and responsibilities of those involved ; and ensuring processes are documented and explained through written guidance .

we selected these four collaboration practices because our past work on evidence - building activities , such as analysis of performance information and program evaluations , has similarly identified them as key approaches related to evidence building .

appendix i provides additional details about our objectives , scope , and methodology .

we conducted this performance audit from april 2018 to december 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

according to omb guidance , evidence can consist of quantitative or qualitative information and may be derived from a variety of sources .

those sources include foundational fact - finding ( eg , aggregate indicators , exploratory studies , descriptive statistics , and other research ) , performance measurement , policy analysis , and program evaluation .

omb recommends that agencies build a portfolio of high - quality , credible sources of evidence — rather than a single source — to support decision - making .

further , since different sources of evidence have varying degrees of credibility , the use of evidence in decision - making requires an understanding of what conclusions can — and cannot — be drawn from the information .

evidence - building can be viewed as a cycle of activities that can help decision makers obtain the evidence they need to address policy questions or identify the questions they should address .

as illustrated in figure 1 , the following four activities comprise the evidence - building cycle: assessing existing evidence to determine its sufficiency and if additional evidence is needed to further understand results and inform decision - making ; prioritizing among the identified needs which new evidence to generate , when , and how ; generating new evidence , by collecting , analyzing , and synthesizing sources of data and research results ; and using that evidence to support learning and decision - making processes .

our prior work highlights long - standing challenges agencies continue to face in generating some sources of evidence — developing performance measures for federal programs and conducting evaluations of their programs .

our work also identified variations in the use of evidence for decision - making by agency leaders and managers .

fragmentation refers to those circumstances in which more than one federal agency ( or organization within an agency ) is involved in the same activity and opportunities exist to improve implementation of that activity .

the commission on evidence - based policymaking found that evidence - building activities are fragmented in the federal government .

for example , it found that within agencies , many organizations have evidence - building responsibilities , including statistical agencies and programs , evaluation and policy research offices , performance management offices , policy analysis offices , and program administrators .

in addition , the commission highlighted challenges the federal government faces in fully addressing cross - cutting research and policy questions when evidence - building activities span multiple agencies .

the commission's final report noted that this fragmentation ( see sidebar ) can lead to duplication of effort or missed opportunities for collaboration .

the commission's report stated that when activities are fragmented within an agency or across the federal government , they should be coordinated to improve the capacity to fully address a specific research or policy question .

similarly , our past work highlights the importance of coordination and collaboration to reduce or better manage fragmentation , overlap , and duplication .

we found that uncoordinated or poorly coordinated efforts can waste scarce funds and limit their effectiveness .

even when efforts are coordinated , enhancements to those efforts can lead to improvements in effectiveness .

as noted earlier , our work also identified leading practices that can help agencies enhance and sustain their implementation of collaborative efforts .

congress and omb have taken actions to strengthen federal evidence - building activities and improve coordination of those activities during the last decade .

figure 2 provides a timeline of selected actions .

appendix ii provides additional detail regarding the selected actions .

tiered evidence grants seek to incorporate evidence of effectiveness into grant making .

federal agencies establish tiers of grant funding based on the level of evidence grantees provide on their approaches to deliver social , educational , health , or other services .

grantees generally are required to evaluate their service models as a condition for the receipt of grant funds .

spent on home visiting models with sufficient evidence of their effectiveness .

to support this requirement , the program incorporated activities across each element of the evidence - building cycle .

for example , through its home visiting evidence of effectiveness review , hhs annually assesses existing evidence about the effectiveness of new and existing home visiting models to identify those that meet criteria for inclusion in the program .

the most recent review , in october 2018 , identified 20 models that met hhs's criteria for an evidence - based early childhood home visiting model .

of those , hhs determined that 18 models were eligible for miechv grantees to select for implementation .

in addition , based on statutory requirements , officials prioritized the generation of new evidence to assess the program's results in certain areas , including child health and development , and child maltreatment .

the program generated this evidence through program evaluations assessing both program implementation and results .

for example , an impact evaluation of four home visiting models published in january 2019 found that these models may reduce household aggression .

because child abuse has been shown to be associated with negative long - term outcomes , reducing household aggression could benefit children as they grow older .

in another example of the use of tiered evidence , the department of labor's ( dol ) workforce innovation fund , established in 2011 , intends to generate long - term improvements in the performance of the public workforce system .

the fund established and funded projects in three different tiers: 1. those that proposed new and untested approaches , with little or no evidence of effectiveness ; 2. those with promising approaches that were tested and existing evidence suggested could be effective ; and 3. those that adapted proven approaches , supported by ample and robust evidence .

to further build dol's base of evidence on the effectiveness of evidence - based approaches , it required grantees to plan for third - party evaluations of their programs .

during the first grant round in 2012 , the workforce innovation fund awarded 26 grants , including one for approximately $1.4 million in tier one funding to the pasco - hernando workforce board in florida .

this grant supported making one - stop services , such as employment workshops and workforce program orientations , more accessible to job seekers by providing online access .

in addition , the grant supported offering virtual case management and business services through a call - in employment support center to individuals who found it difficult to access these services in person .

according to a 2016 case study of this project conducted by dol , users of the online one - stop accessed services nearly twice as much during this 3-year grant period when compared to the prior 3-year period .

in addition , the case study found there was a 53 percent increase in job placements during this 3-year grant period .

the selected agencies' evidence - building activities also aligned with implementation actions outlined by omb for selected cross - agency priority ( cap ) goals .

as required by the gpra modernization act of 2010 , at least every 4 years , omb is to coordinate with other agencies to develop and implement cap goals .

two current cap goals , established in march 2018 in the president's management agenda , place a particular focus on evidence - building activities .

leveraging data as a strategic asset .

omb and agency efforts to implement this goal included developing a long - term , enterprise - wide federal data strategy to better govern and leverage the federal government's data .

published in june 2019 , this strategy established 10 principles and 40 practices intended to leverage the value of federal data assets while protecting security , privacy , and confidentiality .

officials at each of the five selected agencies described actions taken by their agencies that aligned with the federal data strategy's principles and practices .

federal evidence clearinghouses according to the office of management and budget ( omb ) , evidence or “what works" clearinghouses are repositories that synthesize evaluation findings in ways that make research more useful to decision makers , researchers , and service organizations .

these repositories provide tools for understanding what service models are ready for replication or expansion and disseminating results .

grade .

officials told us in september 2018 that preliminary evidence suggested the model could help close the literacy gap for the target population .

in addition , officials told us they intended to disseminate the final results to stakeholders to help inform their decision - making about the approach .

to do so , education officials developed a communication plan to share this evidence via the oela website , its facebook account , the national clearinghouse for english language acquisition ( see sidebar ) , and a listserv of more than 10,000 recipients , among other means .

as of september 2019 , this study had not been completed .

therefore education has not implemented its communication plan .

results - oriented accountability for grants .

one of the four strategies for this cap goal focuses on the achievement of grant program goals and objectives .

in october 2019 , omb staff told us that the strategy aims to hold grant recipients accountable for promising performance practices that support the achievement of those goals and objectives while streamlining compliance requirements for those grant programs that demonstrate results .

according to the september 2019 quarterly update for this goal , initial efforts for this strategy involved developing performance management processes to help grant - making entities improve their ability to monitor , and ultimately improve , the performance of grantees .

the update stated that omb and the chief financial officers council completed efforts in fiscal year 2019 that included soliciting information from agencies on their current grants performance management practices and identifying emerging and innovative performance practices .

subsequent efforts for this goal involved hosting monthly grants practitioner sessions ( called innovation exchange sessions ) to share new ideas and approaches to grants management , which began in may 2019 .

the september 2019 session focused on data - driven decision - making for grants .

we identified actions that each of the selected agencies took , aligned with the intent of this cap goal , to better assess the performance of their grant programs .

officials at each agency told us that they took steps to further incorporate evidence - building requirements into their grant programs .

they told us they did this based in part on their experiences in implementing the evidence - based approaches , such as the tiered evidence grants described earlier in this report .

for example , officials at the corporation for national and community service ( cncs ) described their incorporation of evidence - building requirements into the agency's americorps state and national program .

agency officials told us that grantees have been required to evaluate their programs since 2005 .

in recent years , cncs embedded the evidence generated by these evaluations into their grant - making activities .

for instance , its grant announcement for 2019 stated that americorps state and national applications would be scored , in part , based on the reported empirical evidence supporting the applicants' proposed projects .

in addition , the announcement required applicants proposing projects in the education focus area to choose one of 13 models that had previously demonstrated effectiveness .

according to cncs officials , this was based on evidence generated in previous projects supported by americorps state and national grants or cncs's social innovation fund .

although the evidence act's requirements apply to the agency - wide level , omb's guidance strongly encourages lower - level organizations within agencies to develop and implement their own learning agendas ( see side bar ) .

we found instances where officials developed learning agendas at lower organizational levels within several of the selected agencies prior to the issuance of the june 2019 omb guidance .

these learning agendas covered individual component agencies , bureaus , offices , and programs .

learning agendas according to office of management and budget ( omb ) guidance for implementing the evidence act , a learning agenda is to define and prioritize relevant questions and identify strategies for building evidence to answer them .

in developing a learning agenda , an agency should involve key leaders and stakeholders , to help ( 1 ) meet their evidence needs for decision - making and ( 2 ) coordinate evidence - building activities across the agency .

for example , from september 2016 to june 2017 , the u.s. agency for international development ( usaid ) conducted a landscape analysis of learning agendas , in which officials identified 15 documented , office - , bureau - , or initiative - wide learning agenda processes at different stages of development within usaid .

this included an office - wide learning agenda developed by the center of excellence on democracy , human rights , and governance ( drg ) .

according to usaid , drg seeks to elevate and integrate democracy , human rights , and governance issues within usaid's overall development portfolio .

according to drg's 2017 learning agenda , its development was informed by ongoing drg research and evaluation efforts , and consultations with a range of internal stakeholders , including usaid staff from other bureaus and missions .

the learning agenda included a set of 11 questions across five thematic areas , as illustrated in figure 3 .

drg outlined steps it planned to take throughout 2017 to address each question , such as assessing existing evidence , identifying any gaps , and conducting new research and evaluation activities to fill those gaps .

for example , drg commissioned a study to help answer a question about the effects of human rights awareness campaigns .

the study , published in september 2017 , synthesized the results of a literature review to identify ( 1 ) characteristics of effective campaigns , and ( 2 ) typical causes of unintended negative consequences of human rights awareness campaigns and ways to avoid them .

we found that evidence - building activities are fragmented within each of the five selected agencies and occur at multiple levels and entities within and across the agencies .

as illustrated in figure 4 , this fragmented approach to evidence - building includes separate component agencies or offices with responsibilities for building specific sources of evidence , such as performance information , evaluations , and statistical data .

for example , at the department of labor ( dol ) , different organizations at the department level are responsible for certain evidence - building activities .

this includes the bureau of labor statistics ( collecting statistical data ) , office of the chief evaluation officer ( conducting program evaluations ) and performance management center ( developing performance information ) .

in addition , some evidence - building activities are dispersed throughout agencies and occur at multiple organizational levels ( see figure 5 ) .

for example , at the department of health and human services ( hhs ) , evidence - building activities are generally managed at the component agency level ( referred to as divisions ) .

the divisions manage their own offices and programs , which include evidence - building responsibilities .

for instance , within the administration for children and families ( acf ) , an operating division within hhs — the office of planning , research , and evaluation — is responsible for acf - related evidence - building activities .

these activities include program evaluations , research syntheses , descriptive and exploratory studies , data analyses , and performance management activities .

officials at the selected agencies said that evidence - building activities are fragmented and occur at lower levels for a variety of reasons .

first , this approach helps ensure that decision makers at different levels within the organization have the evidence they need to inform decisions .

second , officials stated that many times these evidence - building activities have been undertaken in response to direction from congress — for example , through provisions in laws or related committee reports directed at a component agency or program .

third , agency officials said they have undertaken these activities based on omb direction , such as memorandums or budget guidance .

this has encouraged agencies to take actions at different organizational levels .

however , each of the selected agencies had established processes for coordinating their evidence - building activities .

for example , officials at each agency established one or more processes intended to regularly coordinate the assessment and prioritization of evidence needs across the agency , as described later in this report .

agency officials also described other efforts to coordinate evidence - building activities , but these efforts were either ad hoc ( i.e. , they did not occur regularly ) or not comprehensive in nature ( i.e. , they did not focus broadly across different sources of evidence or did not cover the entire agency ) .

for example , in august 2017 , the corporation for national and community service ( cncs ) published the results of an assessment of existing evidence — results from research and evaluation activities conducted between fiscal years 2015 and 2016 — in its state of the evidence report .

however , cncs has not conducted a similar analysis or issued a similar report since that time .

moreover , the assessment did not cover all of the agency's activities .

while the report included evidence related to its programs , cncs did not assess evidence related to other activities , such as internal management functions including information technology or human capital management .

we identified instances in which effective coordination helped selected agencies better manage their fragmented evidence - building activities .

for example , the u.s. agency for international development ( usaid ) developed an agency - wide private sector engagement learning agenda , published in may 2019 .

this learning agenda is intended to guide and coordinate crosscutting efforts to develop evidence of effective approaches for engaging the private sector to help partner countries meet development goals and ultimately move beyond the need for foreign assistance .

this learning agenda includes establishing performance measures to monitor progress on engagement with the private sector , and further evaluate the results of its activities .

the coordinated evidence - building approach established by this learning agenda can help usaid better focus limited resources on building new evidence in this crosscutting area for use across the agency , thereby reducing any unwarranted overlap or duplication of effort .

effectively - coordinated processes can help agencies ensure they are comprehensively and systematically looking across their organizations to leverage their existing evidence and focus limited resources on building new evidence .

they can also help agencies manage their fragmented evidence - building activities to improve effectiveness and reduce the potential for any unwarranted overlapping or duplicative efforts .

such processes can help ensure agencies are well positioned to meet forthcoming evidence act requirements related to assessing and prioritizing evidence across the entire agency .

each of the five selected agencies established a similar approach for assessing existing evidence and identifying gaps or other evidence needs across the agency .

agency officials said that these approaches responded to omb guidance for agencies to conduct annual strategic reviews .

specifically , in its guidance for implementing the gpra modernization act of 2010 , omb established an annual process in which each agency is to review progress in achieving strategic objectives — goals that reflect the outcome or impact the agency is seeking to achieve — established in its strategic plan .

according to omb's guidance , as a part of those reviews , the assessment of existing evidence should inform agency decisions about where to focus limited available resources to build new evidence to fulfill any identified needs .

omb's guidance encourages agencies to leverage existing decision - making processes , such as the budget development process , to implement these reviews .

each of the five selected agencies conducts strategic reviews and associated evidence assessments in similar ways , through a variety of existing decision - making processes: cncs and hhs use their budget formulation processes ; education incorporates strategic objective reviews into existing quarterly reviews of progress in meeting goals ; dol uses a stand - alone strategic review process ; and usaid leverages an existing review process conducted at lower levels ( i.e. , its missions ) .

officials at selected agencies identified instances in which they used their agency strategic reviews to ( 1 ) assess a variety of existing sources of evidence — a portfolio of evidence — to determine progress toward a strategic objective , and ( 2 ) identify the need for additional evidence , as illustrated by the following examples .

assessing a portfolio of evidence .

dol's guidance for its strategic review process directs its component agencies to assess a variety of evidence sources to determine results and risks or challenges that may affect future outcomes .

this includes performance information , program evaluations , risk assessments , and findings from reports by us and the department's office of inspector general ( oig ) , among other sources .

in its fiscal year 2018 annual performance report , dol identified different sources of evidence to demonstrate the effectiveness of some of its programs , and challenges related to others , for its strategic objective to create customer - focused workforce solutions for american workers .

for example , it cited statistics and performance data to provide context and some quantitative results related to this objective .

it also shared the results from several program evaluations , including a 2017 impact evaluation that suggested dol's adult and dislocated worker programs were effective at increasing participants' earnings and employment .

dol's performance report also highlighted that its oig identified aspects of several programs that support this objective as top management and performance challenges for fiscal year 2018 .

one of those challenges related to maintaining the integrity of foreign labor certification programs .

dol's performance report stated that balancing the quality review of applications with employers' needs for timely processing has been a challenge for years .

based on the totality of evidence , dol identified this strategic objective as a focus area for improvement for fiscal year 2018 .

identifying evidence needs .

in its strategic plan for fiscal years 2018-22 , education established a strategic objective to increase high - quality education options and empower students and parents to choose an option that meets their needs .

to implement this strategic objective , the strategic plan states that the department will encourage state and local education agencies to expand school choice by administering programs that increase education options , such as the charter schools program ( csp ) .

one of the performance measures education uses to assess the program and progress on this strategic objective is the aggregate number of charter schools that are open , operating , and supported by csp .

education officials told us that they identified limitations with this measure through the department's strategic review process , and the need for additional evidence .

as an aggregate count , the measure did not allow the department to accurately identify underlying changes in individual charter schools served by the program or the results and activities of csp .

for example , education officials set a goal to increase the number of csp - supported charter schools by 150 for the 2017-2018 school year .

however , education reported a decrease of four charter schools for this time period .

to better understand csp's performance , education officials told us they needed additional evidence to assess other aspects of the program's performance .

education officials identified additional sources of evidence within the department that they could use to understand the program's performance .

these included statistics from education's national center for education statistics ( nces ) on the total number of charter schools that opened and closed over the same time period , and annual performance reports from grantees .

according to information on performance.gov , these additional sources of information showed that , in the 2017-2018 school year , 134 new charter schools supported by csp opened , and 101 charter schools expanded under a csp grant .

these actions illustrate an instance of effective coordination of evidence - building activities to manage fragmentation and reduce the risk of duplication .

education officials looked across the agency and leveraged existing evidence generated by different organizational units — csp and nces — to better understand program performance .

had this not occurred , csp might have collected data that duplicated what was already generated by nces .

agencies' assessments of the sufficiency of their existing evidence — conducted via processes for their strategic reviews — reflect the four leading collaboration practices .

although omb's guidance provides flexibility in how the reviews are conducted , it also sets specific expectations for who should lead the process , who should participate in the process , and the types of roles and responsibilities for these individuals .

table 1 provides illustrative examples of the selected agencies' evidence assessment processes that reflect leading practices for collaboration .

unlike the similar processes they use for assessing existing evidence and identifying needs , the five selected agencies use a variety of processes to prioritize new evidence to generate .

agency officials told us that much of this prioritization takes place at lower organizational levels .

for example , at hhs , the department's component agencies — 11 operating divisions and 14 staff divisions — generally lead their own evidence - building processes , through which they prioritize which evidence to generate .

officials from hhs's office of the assistant secretary for planning and evaluation told us that this decentralized model is due to the size and complexity of the department , and that it respects the unique needs of the divisions .

according to these officials , a 2017 review by this office found variation in the processes that the components use for this purpose .

hhs officials said that most components prioritize their evidence needs through their budget formulation processes .

officials at each of the selected agencies identified one or more processes intended to coordinate the prioritization of evidence needs across the entire organization .

table 2 describes these processes .

we identified instances in which officials used these processes to more effectively focus limited resources to build new evidence through coordination across the agency .

for example , cncs officials described an instance in which agency leadership used the agency's budget formulation process to prioritize evidence - building activities to address knowledge gaps about the americorps national civilian community corps ( nccc ) program .

according to cncs officials , through the agency's evidence assessment processes , they found that the agency did not have evidence to fully assess the impact of nccc programs on members and communities .

moreover , existing evidence showed that nccc had experienced a decline in the number of qualified applicants and the retention of its members since 2014 .

to better understand the performance and results of this program , cncs officials told us that agency leadership approved funding in fiscal years 2018 and 2019 for nccc to undertake a multi - year impact evaluation .

this evaluation , which is being conducted in conjunction with cncs's office of research and evaluation and an independent contractor , is expected to examine the member retention , leadership development , and community impact of nccc programming .

officials at each of the selected agencies told us that they were considering how best to meet evidence act requirements to take a systematic and coordinated approach to prioritizing evidence - building activities , such as through learning agendas .

for example , as described in table 3 , education created a new body in march 2019 — the evidence leadership group — to coordinate its evidence - building activities .

education officials told us that in establishing this new group , they took into consideration our leading practices for collaboration .

as described in table 3 , all five selected agencies identified one or more leadership models for their evidence prioritization processes .

we found that all five of the selected agencies involved at least some relevant participants in their evidence prioritization processes , as summarized in table 4 .

our past work related to evidence - building activities identified a wide range of relevant participants to involve .

within agencies , these participants include agency leadership , program staff , and those with functional management responsibilities including budget , human capital , and information technology .

external stakeholders include congress , other federal agencies , state and local governments , grant recipients , and regulated entities .

the five selected agencies include a range of relevant internal participants , although the evidence prioritization process at cncs does not always include key internal stakeholders .

cncs's budget hearings involve discussions about prioritizing evidence , but primarily focus on budget formulation decisions .

therefore , agency leaders and budget officials are consistently involved in the hearings , but others , such as the director of the office of research and evaluation , are not .

involving all key internal stakeholders helps ensure that those involved in a collaborative effort can commit resources , make decisions , and share their knowledge , skills , and abilities .

this can also help ensure that the evidence that will be subsequently generated will be useful to decision makers across the organization .

education and usaid established expectations to seek input from external stakeholders in their evidence prioritization processes .

education's charter for its recently - established evidence leadership group states that the group is to engage a wide array of external stakeholders in its work .

similarly , for the evidence prioritization activities that occur through usaid's program cycle and learning agendas , related guidance sets expectations to involve or obtain the perspectives of external stakeholders .

as usaid developed its self - reliance learning agenda , it sought input from external stakeholders including officials from other federal agencies , organizations that implement usaid programs , and experts in international development , among others .

three of the selected agencies , however , do not always have mechanisms in place to involve , or consider the evidence needs of , a range of external stakeholders in their evidence prioritization processes .

officials at cncs , hhs , and dol told us that , because they consider their prioritization processes to cover internal management purposes and decisions , including external stakeholders is not appropriate .

officials at these three agencies described ways in which they sought input on evidence needs from some stakeholders , such as from interactions with grant recipients and external researchers .

however , these agencies have not developed an approach to collect and consider input on evidence needs from all relevant stakeholders to inform their prioritization processes .

our past work highlights the importance of engaging key external stakeholders , especially congress , to better understand and meet their evidence needs .

engaging external stakeholders can also create a shared understanding of competing demands facing the agency and ensure that their efforts and resources are targeted at the highest priorities across the agency .

moreover , through this engagement , agencies may find that external stakeholders have , or are aware of , existing evidence that helps the agency meet its needs or provide a fuller picture of performance .

involving a full range of relevant stakeholders in the process for prioritizing new evidence to generate would help each of the selected agencies ensure it is meeting the evidence needs of decision makers within and external to the agency .

four of the selected agencies — education , hhs , dol , and usaid — fully define roles and responsibilities for those involved in their evidence prioritization processes , while the process at cncs partially reflects this practice , as described in table 5 .

cncs officials said that the primary focus of the agency's process is budget formulation .

therefore , roles and responsibilities are generally related to that purpose instead of the evidence prioritization activities that also take place during that process .

clearly defining roles and responsibilities can ensure all participants are aware of and agree upon ( 1 ) who will have what responsibilities , ( 2 ) how they will organize their joint and individual evidence - building efforts , and ( 3 ) how they will make decisions .

as described in table 6 , education and usaid's processes reflect this practice , while those at cncs , dol , and hhs reflect it in part .

officials at cncs , hhs , and dol gave different reasons for why their written guidance and agreements related to evidence prioritization processes do not fully reflect this leading practice .

cncs's and hhs's written guidance primarily focuses on their budget formulation processes , since this is where their evidence prioritization activities take place .

thus , these guidance documents contain information on leadership , participants , and roles and responsibilities related to budget formulation activities , but not all relevant details related to evidence prioritization .

officials at dol stated that they do not want to take a “one - size - fits - all” approach to developing learning agendas within the department .

they told us they had not developed specific written guidance for that process to provide flexibility to component agencies to develop processes that work best for them in developing their learning agendas .

as we have previously found , documenting a clear and compelling rationale to work together — and how that work will be done and by whom — is a key factor in successful collaboration .

by incorporating this leading practice into their existing guidance , cncs , hhs , and dol would have greater assurance that they are effectively collaborating to prioritize evidence needs .

decision makers need evidence to help them address pressing governance challenges faced by the federal government .

agencies undertake a range of efforts at different organizational levels to build evidence to meet their own decision - making needs , as well as those of others , such as congress .

however , these evidence - building activities are fragmented within agencies .

through a more comprehensive and coordinated framework , evidence act implementation provides opportunities to improve the effectiveness of federal evidence - building activities .

the five selected agencies have taken steps to improve the coordination of evidence - building activities across their organizations , with education's and usaid's evidence - building activities reflecting the leading practices for collaboration .

cncs , dol , and hhs would have greater assurance that they are comprehensively considering evidence needs across their individual organizations by fully incorporating leading collaboration practices into their agency - wide efforts to prioritize new evidence to generate .

these actions could also help ensure these agencies are better managing fragmented evidence - building activities and more effectively focusing their limited resources to generate evidence to meet decision makers' needs .

in addition , improved coordination could reduce the potential for any unwarranted overlap and duplication in their efforts , and better position the agencies to meet the evidence act's requirements and related implementation actions outlined in omb's guidance .

we are making a total of seven recommendations , including three to cncs , two to hhs , and two to dol .

specifically: the chief executive officer of cncs should develop an approach to ensure that all relevant participants are involved in the agency - wide process for prioritizing evidence needs .

 ( recommendation 1 ) the chief executive officer of cncs should define roles and responsibilities for all relevant participants involved in the agency - wide process for prioritizing evidence needs .

 ( recommendation 2 ) the chief executive officer of cncs should revise written guidance for the agency - wide process for prioritizing evidence needs to ensure it identifies all relevant participants and their respective roles and responsibilities .

 ( recommendation 3 ) the secretary of health and human services should develop an approach to ensure that all relevant participants are involved in the department - wide process for prioritizing evidence needs .

 ( recommendation 4 ) the secretary of health and human services should revise written guidance for the department - wide process for prioritizing evidence needs to ensure it identifies all relevant participants and their respective roles and responsibilities .

 ( recommendation 5 ) the secretary of labor should develop an approach to ensure that all relevant participants are involved in the department - wide process for prioritizing evidence needs .

 ( recommendation 6 ) the secretary of labor should revise written guidance for the department - wide process for prioritizing evidence needs to ensure it identifies all relevant participants and their respective roles and responsibilities .

 ( recommendation 7 ) .

we provided a draft of this product for comment to omb and the five selected agencies — cncs , education , hhs , dol , and usaid .

cncs , education , hhs , dol and usaid provided written comments , which are summarized below and reproduced in appendixes v , vi , vii , viii , and ix , respectively .

in addition , cncs , education , hhs , usaid , and omb provided technical comments , which we incorporated as appropriate .

in its written comments , cncs neither agreed nor disagreed with the three recommendations we directed to it .

the agency stated that it believes the planned actions included in its strategic evidence plan , published in september 2019 , address those recommendations .

the plan includes a goal to strengthen how the agency prioritizes and uses evidence , and outlines various actions intended to achieve that goal .

the plan does not include sufficient details to enable us to assess the extent to which its implementation would fully address the issues identified in our review and covered by our recommendations .

education stated in its written comments that the department is committed to maximizing the performance of its programs , and it views building , using , and disseminating evidence as critical to those efforts .

education also outlined planned and proposed actions that it believes would further its evidence - building activities .

in its written comments , hhs did not concur with the two recommendations we directed to it .

in response to both recommendations , hhs stated that the department had developed an approach for including all relevant participants in its process for prioritizing evidence needs .

however , according to an hhs official in november 2019 , hhs had not yet finalized the approach , and therefore was unable to provide any additional information about it .

thus we could not assess the extent to which hhs's stated actions would address our recommendations .

dol agreed with the two recommendations we directed to it , and in its written comments described an action it plans to take to address them .

we will monitor dol's action , which we believe would likely address our recommendations , if effectively implemented .

usaid , in its written comments , reiterated the agency's commitment to a comprehensive and integrated approach for its evidence - building activities .

in the draft of this report we sent to usaid for its review in october 2019 , we included a recommendation to usaid that it ensure that all relevant participants are involved in agency - wide processes for prioritizing evidence needs .

usaid subsequently provided documentation that it had not provided previously that showed the agency had taken various steps to seek the input of a range of external stakeholders .

we determined that these actions addressed our draft recommendation .

thus , we removed the draft recommendation from our report .

we are sending copies of this report to the appropriate congressional committees , the director of the office of management and budget , the chief executive officer of the corporation for national and community service , the secretary of the department of education , the secretary of the department of health and human services , the secretary of the department of labor , the administrator of the u.s. agency for international development , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or sagerm@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix x .

this report responds to a request that we review the coordination of federal evidence - building activities .

this report ( 1 ) describes activities selected agencies have taken that align with congressional and office of management and budget ( omb ) direction to strengthen evidence - building , and ( 2 ) examines the extent to which selected agencies' processes for assessing and prioritizing evidence needs reflect leading practices for collaboration .

to address both objectives , we analyzed agency documents about federal evidence - building activities and interviewed relevant staff at omb and officials at five selected agencies: the departments of education , health and human services , and labor ; the corporation for national and community service ; and the u.s. agency for international development .

we selected these five agencies based on their experiences incorporating evidence - building activities into program design and implementation .

these experiences include evidence - based approaches such as pay for success projects , performance partnerships , and tiered evidence grants .

at the time we made our selection , these five agencies had designed or implemented evidence - based approaches to a greater extent than other agencies we identified .

the agencies we selected vary in size — as measured by budget authority and employees — and organizational structure ( see table 7 ) .

for the first objective , we reviewed information from the five selected agencies and identified examples of evidence - building activities within each agency since 2010 .

we then determined if these examples illustrated actions that aligned with evidence - building statutory requirements and directions from omb including guidance , memorandums , and activities outlined in the president's management agenda .

to do so , we reviewed relevant laws and omb guidance .

for the second objective , we evaluated processes each selected agency had established to take a coordinated approach to assessing and prioritizing evidence needs across the agency .

we compared these processes to four selected leading practices for collaboration identified in our prior work ( see table 8 ) .

we selected these four collaboration practices because our past work on evidence - building activities , such as analysis of performance information and program evaluations , has similarly identified them as key approaches related to evidence - building .

table 9 illustrates this alignment for selected past reports .

we conducted this performance audit from april 2018 to december 2019 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the office of management and budget ( omb ) has issued several memorandums and other key policy documents that encourage agencies to take actions to strengthen their capacity to build evidence .

for example , in a july 2013 memorandum , omb encouraged agencies to identify proposals for building evidence in their budget requests .

such proposals could be used to improve existing programs or inform decisions about new programs .

the omb guidance highlighted several evidence - based approaches for agencies to consider , including pay for success , performance partnerships , and tiered evidence grants , described further in the text box below .

examples of evidence - based program approaches identified in office of management and budget ( omb ) guidance pay for success .

pay for success is a contracting mechanism under which final payment is contingent upon achieving specific outcomes .

the government specifies performance outcomes in pay for success contracts and generally includes a requirement that contractors assess program outcomes or impacts through an independent evaluation .

the evaluators may also generate and analyze performance data to inform program management and improvement during implementation .

performance partnerships .

performance partnerships allow federal agencies to provide grant recipients flexibility in how they use funding across two or more programs along with additional flexibilities .

in exchange , the recipient commits to improve and assess progress toward agreed - upon outcomes by developing and using evidence .

tiered evidence grants .

tiered evidence grants seek to incorporate evidence of effectiveness into grant making .

federal agencies establish tiers of grant funding based on the level of evidence grantees provide on their approaches to deliver social , educational , health , or other services .

the grant generally requires grantees to evaluate their service models as a condition for the receipt of grant funds .

in addition , congress passed laws aimed at strengthening and better coordinating evidence - building activities , which omb reinforced through related guidance to implement those laws .

gpra modernization act ( gprama ) .

gprama established a framework aimed at taking a more crosscutting and integrated approach to improve government performance .

requirements included in that framework , such as cross - agency priority ( cap ) goals and strategic reviews , were intended to strengthen evidence - building activities and improve coordination .

cap goals .

at least every 4 years , omb is to coordinate with other agencies to develop and implement cap goals .

these goals are to address issues in a limited number of policy areas requiring action across multiple agencies , or management improvements that are needed across the government .

the president's management agenda , released in march 2018 , established the third set of cap goals since gprama was enacted .

implementation of each cap goal can involve evidence - building activities ; however , two goals in particular are to focus on them , as described further in the text box .

cross - agency priority ( cap ) goals focused on evidence - building leveraging data as a strategic asset .

the president's management agenda highlights several root causes for the challenges the federal government faces .

one root cause is that agencies do not consistently apply data - driven decision - making practices .

this agenda states that agencies need to make smarter use of data and evidence to orient decisions and accountability around service and results .

the administration established this cap goal to improve the use of data in decision - making to increase the federal government's effectiveness .

results - oriented accountability for grants .

according to the june 2019 update for this goal , the federal government uses grants to invest approximately $700 billion each year in mission - critical needs .

however , the report states that grant managers report spending 40 percent of their time using antiquated processes to monitor compliance instead of analyzing data to improve results .

the administration established this cap goal to maximize the value of grant funding by applying a risk - based , data - driven framework that balances compliance requirements with demonstrating successful results .

a strategic objective is a type of goal that reflects the outcome or impact the agency is seeking to achieve .

the agency is to identify the strategies — the portfolio of organizations , regulations , tax expenditures , programs , policies , and other activities — within and external to the agency that contribute to each strategic objective .

as a set , the agency's strategic objectives are to encompass all of its activities .

strategic reviews .

in its guidance for implementing gprama , omb established an annual process in which each agency is to review progress in achieving the strategic objectives established in its strategic plans ( see sidebar ) .

to do so , omb's guidance directs agencies to assess existing sources of evidence to understand the progress made toward each strategic objective and identify where additional evidence is needed to determine effectiveness .

in addition , omb's guidance states that another purpose of strategic reviews is to strengthen collaboration .

it notes that the reviews can do so by identifying and addressing crosscutting challenges and fragmentation .

the foreign aid transparency and accountability act of 2016 ( fataa ) .

among other things , fataa requires the president to establish guidelines for establishing measurable goals , performance metrics , and monitoring and evaluation plans for federal foreign assistance .

in january 2018 , omb issued guidelines for federal agencies that administer foreign assistance — which includes the departments of labor and health and human services , and the u.s. agency for international development .

among other things , the guidelines provide direction on strengthening evidence - building activities , such as establishing annual monitoring and evaluation plans , and disseminating findings and lessons learned .

agencies were directed to align their monitoring and evaluation policies with the guidelines by january 2019 .

the foundations for evidence - based policymaking act of 2018 ( evidence act ) .

in june and july 2019 , omb released its initial guidance on implementing the evidence act .

among other things , this guidance provides direction to agencies on developing evidence - building plans , also known as learning agendas ( see text box below ) .

according to omb , these plans will serve as the driving force for other evidence - building activities required by the evidence act .

prior to the enactment of the foundations for evidence - based policymaking act ( evidence act ) , both the office of management and budget ( omb ) and the commission on evidence - based policymaking highlighted and recommended the use of learning agendas by federal agencies to strengthen and coordinate their evidence - building activities .

according to omb's guidance for implementing the evidence act , a learning agenda is to define and prioritize relevant questions and identify strategies for building evidence to answer them .

a federal agency developing a learning agenda should involve key leaders and stakeholders to help ( 1 ) meet their evidence needs for decision - making , and ( 2 ) coordinate evidence - building activities across an agency .

omb's guidance stated that the evidence act emphasizes the need for collaboration and coordination of agency staff and activities to achieve successful implementation .

the guidance provides time frames for a phased approach to implement several evidence act requirements .

for example , although learning agendas are not required to be published until february 2022 , omb's guidance includes several interim milestones and deliverables to build toward the final published version .

5 u.s.c .

§§ 306 , 312 .

omb embedded portions of evidence act implementation guidance in its 2019 update to cir .

no .

a - 11 .

in it , omb noted that many of the evidence act's provisions support the federal performance framework for improving program and service delivery ( part 6 of the circular ) , which provides guidance for implementing gprama and other related laws and policies .

omb cir .

no .

a - 11 , at § 200.2 ( 2019 ) .

we identified 20 examples of the five selected agencies' incorporating evidence - based approaches in their program design and implementation .

table 10 describes each of these examples .

omb's july 2013 memorandum stated that agencies' use of evidence - based approaches could help strengthen agencies' abilities to improve program performance by using experimentation and innovation to test new approaches for service delivery .

in addition , it noted that these approaches can be used to ( 1 ) generate new knowledge , and ( 2 ) apply existing evidence about approaches found to be effective .

generate new knowledge .

omb guidance notes that new knowledge can be used to improve existing programs or inform decisions about new ones .

for example , education designed the first in the world program to generate evidence about effective strategies for improving college completion rates for underrepresented , underprepared , or low - income students .

program officials told us that , prior to the issuance of the 2014 grant solicitation for the program's first year , education had limited evidence of effective approaches .

as noted in the solicitation , education sought to expand its evidence base about effective approaches through the first round of grant awards .

using a tiered evidence approach , the program awarded grants to institutions of higher education to implement and evaluate the effectiveness of approaches , such as coaching or advisement services , intended to increase the number of these students who complete postsecondary education .

the first round awarded grant funds to projects in a single evidence tier to test and evaluate the effectiveness of approaches .

education officials told us that after the program's first year , they conducted a literature review to identify approaches that were supported by some evidence of their effectiveness .

using this evidence , education created a second tier for the 2015 grant awards , for which grantees could receive increased funding by implementing one of the program designs identified in the literature review .

officials told us they intend to publish the final results of first in the world grant recipient evaluations in education's what works clearinghouse .

evaluation results will not be available until after the completion of the grant periods , the first of which ended in september 2019 .

however , education officials told us that the evidence they have generated to date has improved their understanding of services that could potentially help at - risk students complete post - secondary education .

apply effective approaches .

to meet increased demand for services in a constrained resource environment , omb's guidance encourages agencies to allocate resources to programs and approaches backed by strong evidence of effectiveness .

in addition , omb's guidance encourages agencies to “scale up” effective program approaches by expanding them to a larger or different group of recipients .

for example , usaid created the development innovation ventures program in 2010 as a tiered evidence grant competition to create a portfolio of innovative approaches to reducing global poverty .

this program provides funding in three tiers , with greater funding provided to those approaches with greater evidence of effectiveness .

these three tiers ( which usaid referred to as stages ) were as follows: 1 .

proof of concept .

the program provided smaller grants to test the viability of an innovative approach ; 2 .

testing and positioning for scale .

grantees determined , through rigorous assessments , whether their approach could achieve greater results and also be implemented successfully at a larger scale ; and 3 .

scaling .

the program funded the expanded implementation of an effective approach within one country or replicated that approach in another country .

for example , from 2013 to 2015 , the development innovation ventures program awarded stage two funding to a nonprofit organization in india .

the organization designed a methodology to help primary school students improve reading skills by grouping students according to skill level , instead of age or grade and tailoring lessons to their learning level .

evidence generated through randomized control trials showed that the approach was effective .

based on that evidence , in 2017 , the program awarded stage three funding to replicate the approach in zambia .

earlier in this report , we discussed agency - wide evidence assessment and prioritization processes established by the five selected agencies .

in addition to those processes , officials described other actions they have taken to coordinate fragmented evidence - building activities across organizational levels ( see table 11 ) .

some of these actions were ad hoc ( i.e. , they did not occur regularly ) or not comprehensive in nature ( i.e. , they did not focus broadly across different sources of evidence or did not cover the entire agency ) .

in addition to the above contact , benjamin t. licht ( assistant director ) , daniel webb ( analyst - in - charge ) , amanda prichard , kelly turner , and brian wanlass made significant contributions to this report .

valerie caracelli , jacqueline chapin , ann czapiewski , steven putansu , and andrew j. stephens also made key contributions .

