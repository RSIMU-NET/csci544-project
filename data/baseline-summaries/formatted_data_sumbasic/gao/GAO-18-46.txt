the nation's maritime , surface , and aviation transportation systems and facilities are vulnerable and difficult to secure given their size , easy accessibility , large number of potential targets , and proximity to urban areas .

in order to help reduce the threats to these critical transportation systems and facilities , the transportation security administration ( tsa ) , a component of the department of homeland security ( dhs ) , conducts various security threat assessment screening and credentialing activities for millions of workers and travelers seeking access to the maritime , surface transportation , and aviation industries .

however , as we have previously reported , the effectiveness and the efficiency of the agency's threat assessments and credentialing programs have been hindered by stove - piped information technology ( it ) systems and duplicative processes which cannot accommodate growing enrollment demand .

moreover , the current stove - piped environment limits tsa's ability to effectively detect malicious individuals that apply for multiple transportation credentials to try to gain access through at least one of them .

in 2008 , tsa initiated the technology infrastructure modernization ( tim ) program to , among other things , enhance the sophistication of its security threat assessments and improve the capacity of its systems .

specifically , the program is intended to provide a modern and centralized end - to - end credentialing system that includes registration and enrollment , individual security threat assessments , adjudication , credential issuance / management , and revocation for transportation workers and travelers .

the program's initial baseline estimated that the program would be deployed in 2015 and cost about $631 million .

however , the program experienced significant cost and schedule overruns , a significant increase in its requirements , and critical system performance and technical issues during its initial limited deployment in 2014 .

as a result , in january 2015 , tsa suspended the program while it established a new strategy for developing and deploying the tim system .

the program was rebaselined in september 2016 and is now estimated to cost about $1.27 billion ( $639 million more than originally planned ) .

further , full operational capability for the system is now planned for fiscal year 2021 ( 6 years later than originally planned ) .

tsa officials decided to move away from implementing a commercial - off - the - shelf product for the program and instead planned to develop an open source system .

moreover , officials decided to use an agile software development approach , rather than the traditional waterfall development approach the program had been using .

given the issues that the tim program has faced in developing the system , you asked us to review the agency's current effort .

our objectives were to ( 1 ) describe tsa's past implementation efforts for the tim program and its new implementation strategy ; ( 2 ) determine the extent to which tsa's new strategy for the program addresses the challenges encountered during earlier implementation attempts ; ( 3 ) determine the extent to which tsa has implemented selected key practices for transitioning to an agile software development framework for the program ; and ( 4 ) determine the extent to which tsa and dhs are effectively overseeing and governing the tim program to ensure that it is meeting cost , schedule , and performance requirements .

to address the first objective , we reviewed program documentation , such as initial and current acquisition program baselines , initial and current life - cycle cost estimates , acquisition decision memorandums , and program plans documenting a new strategy for implementing the tim program .

we used the information in this documentation to summarize tsa's earlier attempts to implement tim capabilities and the program's new implementation strategy , including estimated costs , schedule , and key decisions made .

in this report , we use the tim program's objective estimated cost and schedule values ( targets that reflect the most likely cost and schedule ) , and not the threshold cost and schedule values ( ceilings which , if exceeded , initiate official replanning actions ) .

we also interviewed tsa officials , including the tim director and deputy director , on the status of the program office's efforts .

for the second objective , we reviewed program documentation on the challenges the tim program office faced when it experienced cost , schedule , and system performance issues and synthesized the information to identify a consolidated list of key challenges the program had previously faced .

we then reviewed documentation on the tim program's new implementation strategy .

we compared this new strategy to selected prior challenges and assessed the strategy against leading practices and guidance , such as dhs's systems engineering lifecycle guide and the software engineering institute's capability maturity model® integration for development .

we also conducted a site visit at the tsa adjudication center in reston , virginia , where we observed demonstrations of the information systems that are currently used to conduct security threat assessments .

to address the third objective , we first reviewed leading practices and guidance from , among others , the software engineering institute , the office of management and budget ( omb ) , and dhs , and identified those practices that are critical to establish when transitioning to an agile software development framework .

then , in consultation with gao's internal agile expert , we selected six practices that were most applicable to the status of the program .

we then reviewed relevant program documentation , such as agile training records , program plans , and status reports , and interviewed tsa and dhs officials to assess the extent that the program met these practices .

we also observed agile software development activities conducted at tsa facilities in annapolis junction , maryland , and at a contractor's facilities in beltsville , maryland .

to address the fourth objective , we reviewed leading practices and guidance from , among others , the software engineering institute , omb , dhs , and tsa , and identified four key practices in oversight and governance of programs using agile software development .

we then reviewed relevant tim program management and governance documentation , such as program management plans , agile contracts , schedules , cost estimates , program status reports , and artifacts from program oversight reviews .

we also interviewed tsa and dhs officials to determine the extent to which these officials were following the key practices .

to assess the reliability of the data that we used to support the findings in this report , we reviewed relevant program documentation to substantiate evidence obtained through interviews with agency officials .

we determined that the data used in this report were sufficiently reliable for the purposes of our reporting objectives .

we made appropriate attribution indicating the sources of the data .

a full description of our objectives , scope , and methodology can be found in appendix i .

we conducted this performance audit from september 2016 to october 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

as we have previously reported , transportation systems and facilities are vulnerable and difficult to secure given their size , easy accessibility , large number of potential targets , and proximity to urban areas .

tsa's mission is to protect the nation's transportation systems by providing effective and efficient security to ensure freedom of movement for people and commerce .

accordingly , tsa is responsible for managing vetting and credentialing programs to ensure that individuals that transport hazardous materials or have unescorted access to secure or restricted areas of transportation facilities at maritime ports and tsa - regulated airports do not pose a security threat .

in order to carry out this responsibility , tsa conducts background checks — known as security threat assessments — on individuals seeking an endorsement , credential , access , and / or privilege ( hereafter called a credential ) .

specifically , tsa reviews applicant information and searches government databases , such as criminal history records from federal , state , and local sources in the federal bureau of investigation's national crime information center database and terrorist screening database , which is the federal government's consolidated terrorist watchlist .

this information is used to determine whether the applicant has known ties to terrorism and whether the applicant may be otherwise precluded from obtaining a credential based on his or her immigration status and criminal history , among other factors .

if tsa determines that an applicant does not pose a security threat , a credential may be supplied by an issuing entity .

if it determines an applicant should be denied , the agency issues a preliminary determination of ineligibility letter to the applicant .

the applicant may seek redress by appealing the determination or requesting a waiver .

tsa's security threat assessments support over 30 credentialing programs in the maritime , surface , and aviation transportation segments .

the largest programs include the transportation worker identification credential program for maritime workers , hazardous materials endorsement program for commercially licensed drivers , the aviation worker program , and tsa pre® for travelers at airport checkpoints .

according to tim program officials , these transportation programs are collectively estimated to have processed about 12.8 million enrollments by october 2017 .

table 1 describes the largest transportation credentialing programs , by segment , and purpose of each .

tsa's legacy it systems that are currently used to help conduct its security threat assessment and credentialing functions are an aggregation of stove - piped solutions that were developed over a period of time to support individual transportation screening programs .

these systems are duplicative and lack needed sophistication to effectively detect , for example , if an individual is attempting to gain access to multiple facilities across different transportation programs in an effort to find any successful entry point .

early detection of this type of threat is difficult and time consuming because many aspects of the current systems are not fully automated .

additionally , we and the dhs office of inspector general ( oig ) have previously reported numerous shortfalls with tsa's security threat assessment and credentialing systems .

we reported in 2011 that the demand for security threat assessments is expected to continue to grow and the existing credentialing systems will not be able to accommodate this growing enrollment demand .

in july 2013 , we reported on functional limitations and technical problems with tsa's legacy credentialing systems that were to be addressed by the tim system .

these limitations included the inability to run reports to measure tsa response times to applicants , track adjudication of cases , and address case workload backlogs .

we also reported on delays in processing new cases .

we made recommendations to address these issues and dhs agreed with our recommendations .

dhs has taken several actions to implement the recommendations , such as establishing a process for developing accurate workload projections and hiring additional adjudicators .

in june 2015 , dhs's oig reported on issues with tsa's lack of continuous vetting once a credential was issued , referred to as recurrent vetting .

for example , the oig reported on the need for recurrent vetting of aviation workers .

specifically , it found that tsa did not have effective controls in place for ensuring that aviation workers had not committed crimes that would disqualify them from having unescorted access to secure areas of airports , and that they had lawful immigration status and were authorized to work in the united states .

instead , tsa depended on the commercial airports and air carriers to verify criminal histories of workers who already hold credentials , and on the credential holders themselves to report disqualifying crimes to the airports where they worked .

the dhs oig recommended that tsa pilot the federal bureau of investigation's rap back program and take steps to institute recurrent vetting of criminal histories at all commercial airports .

tsa concurred with the recommendation and stated that it planned to initiate a pilot rap back program to help ensure full implementation across all eligible tsa - regulated populations in the future .

in september 2016 , dhs's oig reported that , although tsa required transportation worker identification credential cardholders to self - report to the administration and surrender their card when charged with a disqualifying offense , this self - reporting occurred only once between 2007 and 2016 .

the report also stated that tsa was testing two methods to implement recurrent vetting into its credentialing programs — the federal bureau of investigation's rap back program to check for criminal violations and the use of dhs's automated biometric identification system to check for both criminal and immigration violations .

however , tsa's plans did not include a method for determining the best approach , and the oig reported that this would impede tsa's ability to implement recurrent vetting successfully and efficiently .

accordingly , the oig recommended that tsa establish measurable and comparable criteria to use in evaluating and selecting the best criminal and immigration recurrent vetting option , and tsa concurred with this recommendation .

also , in september 2016 , the dhs oig reported that the background checks for the transportation worker identification credential program were not as reliable as they could be .

for example , the oig found that tsa did not have processes in place to ensure the proper separation of duties for adjudicators , who had the ability to assign , review , and perform quality assurance on the same case .

the oig also found missing supervisory review controls in the terrorism vetting process .

accordingly , the oig recommended that tsa identify and implement additional internal controls and quality assurance procedures ; tsa agreed with the recommendation .

in response , tsa planned to make improvements to the tim system to include an additional quality assurance component in which the system would automatically select cases for senior adjudicators to review and to incorporate into the overall reporting and monitoring activities .

the tim system is intended to address the shortfalls identified in these prior reports by providing a modern and centralized end - to - end credentialing system .

the system is also intended to provide counter - terrorism and trend analytic capabilities to help identify unusual activities ( eg , credential shopping and using multiple aliases ) across the entire credentialing process and all transportation populations supported by tsa's security threat assessments .

in addition , the system is expected to enable automated recurrent vetting of individuals against criminal and immigration databases to ensure that a credential or endorsement is revoked if an individual commits a disqualifying act .

the planned credentialing process that is to be supported by the tim system includes: registration and enrollment: individuals seeking a credential or endorsement under one of the transportation programs supported by the system are expected to be able to apply for a security threat assessment at a universal enrollment center or via the system's online portal .

the biographic and biometric information collected from the applicant is to be received and processed by the system .

eligibility vetting and risk assessment: the system is to conduct automated vetting of the applicant's information against criminal , immigration , and terrorism watchlists to determine the security risk associated with allowing access privileges based on the criteria for the credential or endorsement that the individual is seeking to obtain .

if the results return a flag for a potentially disqualifying factor , the applicant's case is to be sent for adjudication .

tsa adjudicators are to use the system to review and adjudicate cases that did not pass automated vetting by comparing the applicant's information to the criteria for the credential or endorsement that the individual is seeking to obtain .

the adjudicators are to determine the applicant's eligibility for the credential or endorsement , and approve or deny the individual's application .

issuance: when an applicant is approved through eligibility vetting or adjudication , the system is to notify the applicant of approval and provide instructions on how to receive the credential , which is to be activated by the system and supplied by the issuing entity .

the applicant also is to be able to login to the online portal to view the status of the application .

verification and use: use of the credential in secured areas is to be verified , including determining that the credential is authentic , that the individual is the correct recipient of the credential , and that the credential's status is valid ( not revoked or expired ) .

revocation and expiration: the system is expected to conduct subsequent automated recurrent vetting of individuals who previously had been approved against criminal , immigration , and terrorist databases on an ongoing basis .

if , as a result of recurrent vetting or self - reporting , there is new information indicating that an applicant's credential should be revoked , the system is to alert the adjudicators who are then to determine if revocation is needed .

the system is to prompt credential expiration at the end of a specified period of time .

redress or waiver: an applicant that is denied a credential is to be able to apply to tsa to either appeal the decision , to include providing documentation to prove that he / she is eligible , or request a waiver from having to meet the eligibility criteria .

trend analytics: the system is to allow tsa's office of intelligence and analysis users to select from a standardized suite of analysis tools that would allow them to identify unusual activities across transportation populations .

a key objective would be to identify through analysis those adversaries and terrorists who may attempt to hide behind multiple personas and aliases .

figure 1 provides an overview of the intended future credentialing process which the tim system is expected to support .

tim program officials decided to adopt an agile software development approach — a type of incremental development — which calls for the rapid delivery of software in small , short increments rather than in the typically long , sequential phases of a traditional waterfall software development approach .

this decision is consistent with omb's guidance as specified in its it reform plan , as well as the legislation commonly referred to as the federal information technology acquisition reform act .

agile emphasizes early and continuous software delivery , as well as using collaborative teams and measuring progress with working software .

figure 2 provides a depiction of software development using the agile approach compared to a waterfall approach .

the agile approach significantly differs in several ways from traditional waterfall software development .

table 2 highlights major differences between the agile and waterfall software development approaches .

additionally , agile practices integrate planning continuously throughout the life - cycle .

although agile requires some high - level , up front planning , in general , planning in agile focuses on the near term of the next few software releases .

planning sessions are conducted to support each release , iteration , and every work day .

for example , development teams have daily meetings , where the team members discuss what they did yesterday and what they plan to do that day .

frequent planning is aimed at ensuring the program is delivering the needed capabilities to the end users .

as we have previously reported , numerous frameworks are available to agile practitioners to guide their agile software development activities .

scrum is one common framework , which is widely used in the public and private sectors and its terminology is often used in agile discussions .

the following are key scrum terminology and concepts: product owners represent the end user community and have the authority to set business priorities , make decisions , and accept completed work .

scrum iterations ( also called sprints ) are where development teams build a piece of working software during a short , set period of time ( eg , 2 weeks ) .

a collective set of sprints is bundled into a software release .

sprint teams ( or development teams ) conduct the agile software development and testing work .

these teams collaborate with minimal management direction , often co - located in work rooms .

they meet daily and post their task status visibly , such as on wall charts .

scrum masters , similar to project managers , are responsible for removing impediments to the sprint teams' ability to deliver the product goals and deliverables .

user stories convey the customers' requirements at the smallest and most discrete unit of work that must be done to create working software .

each user story is assigned a level of effort , called story points , which is a relative unit of measure used to communicate complexity and progress between the business and development sides of the project .

to ensure that the product is usable at the end of every iteration , teams adhere to an agreed - upon definition of what constitutes acceptable , completed work .

backlogs are lists of requirements , such as user stories , to be addressed by working software .

if new requirements or defects are discovered , these can be stored in the backlog to be addressed in future iterations .

velocity is a metric which is used to track the rate of work completed using the number of story points completed or expected to be completed in an iteration ( i.e. , sprint ) , or release .

for example , if a team completed 100 story points during a 4-week iteration , the velocity for the team would be 100 story points every 4 weeks .

another framework , referred to as the scaled agile framework ( safe ) , is a governance model for organizations to use to align and collaborate the product delivery for modest to large numbers of agile software development teams .

the framework is intended to be applied to several organizational levels , including the development team level , the program level , and the portfolio level .

it is also intended to provide a scalable and flexible governance framework that defines roles , artifacts , and processes for agile software development across all levels of an organization .

dhs has sought to establish agile software development as the preferred method for acquiring and delivering it capabilities .

specifically , in february 2016 , the dhs under secretary for management initiated an agile software development pilot to improve the execution and oversight of the department's it acquisitions .

the under secretary for management selected five dhs programs that were in various stages of the acquisition life - cycle , including the tim program , to be part of the pilot .

as part of this pilot initiative , dhs established integrated product teams designed to support each of the five programs in their efforts to adopt agile practices .

these teams were directed to focus on effectively planning and executing the pilot programs , as well as developing appropriate documentation to support program execution .

according to the under secretary for management , the department plans to use lessons learned from the pilots to develop and update policies and procedures for executing the pilot programs and future it acquisitions .

as of may 2017 , department officials had not determined a completion date for the pilot .

additionally , dhs established a headquarters - level agile team intended to collaborate across the department on improvements to policy , governance , and acquisition guidance .

this group is intended to support agile delivery ; codify and publicize process improvement artifacts generated by the program - level integrated product teams ; and eliminate redundancies and conflicting guidance so that oversight groups speak with one voice , reducing time through the acquisition process .

in addition to the use of agile software development principles , the tim program is subject to the department's oversight framework .

specifically , the program is to adhere to dhs's acquisition policy , including its systems engineering life - cycle framework , which is intended to support efficient and effective delivery of it capabilities .

the under secretary for management serves as the decision authority for the program , and is responsible for overseeing adherence to dhs's acquisition policies for the department's largest acquisition programs ( i.e. , those with life - cycle cost estimates of $1 billion or more ) .

the under secretary for management is supported by two offices within the department .

the first of these offices — the office of program accountability and risk management ( parm ) — is responsible for dhs's overall acquisition governance process .

parm is responsible for , among other things , periodically conducting program health assessments to evaluate acquisition programs , in terms of a program's management , resources , planning and execution activities , requirements , cost and schedule , and how these factors are impacting a program's ability to deliver a capability .

the other key supporting office — the dhs chief information officer ( cio ) — is responsible for , among other things , setting departmental it policies , processes , and standards .

the cio is also responsible for ensuring that acquisitions comply with the department's it management processes , technical requirements , and the approved enterprise architecture .

within the office of the chief information officer ( ocio ) , the enterprise business management office is to ensure that the department's it investments align with its missions and objectives .

as part of its responsibilities , this office periodically assesses investments to gauge how well they are performing through a review of program risk , human capital , cost and schedule , and requirements — referred to as the cio's program health assessment .

according to the cio , the chief technology officer , which is responsible for leading the development of it and standards across the department , and for management of the agile pilot initiative , offers guidance and assistance to programs to help improve their execution .

in addition , the director of the office of test and evaluation is to provide oversight of components' independent test and evaluation activities .

the dhs acquisition review board is chaired by the under secretary for management and is made up of many executive level members including the cio , the executive director of the office of parm , and the chief procurement officer .

the board is to meet periodically to oversee programs' business strategies , resources , management , accountability , and alignment to strategic initiatives .

additionally , the department has established executive steering committees , which generally are comprised of component and dhs executive - level members , such as the component cio and chief financial officer , as well as the dhs chief technology officer and the executive director of the office of parm .

the committees are to provide governance , oversight , and guidance to programs and their related projects and initiatives to help ensure successful development and operations .

figure 3 shows the organizational structure of the key dhs organizations with it acquisition management responsibilities .

the tim program office resides within the mission operations component of tsa's office of information technology .

the expected users of the tim system come from multiple offices under the office of intelligence and analysis , including the security threat assessment operations office , which is responsible for conducting the security threat assessments , and the program management office , which is responsible for managing tsa's maritime , surface , and aviation credentialing programs .

the tim program's executive steering committee is chaired by the tsa cio , who is the head of the office of information technology , and the tsa deputy component acquisition executive , and meets quarterly .

in addition , the tsa operational test agent is to perform operational testing and evaluation of the tim system's operational effectiveness , interoperability , cybersecurity , and suitability .

as previously mentioned , the dhs director of the office of test and evaluation is to provide oversight of these test and evaluation activities .

figure 4 shows the key tsa organizations involved with the tim program .

the tim program experienced significant cost , schedule , and performance issues during its initial implementation efforts .

specifically , in may 2014 , tsa launched an initial version of a commercial - off - the - shelf ( cots ) system for the maritime transportation segment of tim that was to support the transportation worker identification credential program .

however , as we previously reported , in september 2014 , tsa reported to dhs that the program had breached its baseline because it had significant cost , schedule , and performance issues due to , among other things , the addition of newly created credentialing programs that were added to the program's scope , such as tsa pre® and chemical facility anti - terrorism standards .

tim program officials also reported in the breach remediation plan other issues that led to the breach , including different expectations between tsa officials and the contractor regarding the extent of reuse of system functionality among the different transportation segments .

specifically , tsa expected that it would be able to reuse more of the maritime functionality for the surface and aviation populations , while the contractor expected there to be less reuse .

in january 2015 , the acting under secretary for management directed program officials to suspend all planning and development efforts related to the other two segments of the program — surface and aviation — until the issues with the maritime segment could be resolved .

in august 2015 , program officials prepared a revised life - cycle cost estimate which increased costs to approximately $1.34 billion ( about $713 million more than the original 2011 estimate ) , and delayed full deployment of the tim system ( to include all three transportation segments ) to fiscal year 2022 ( 7 years later than originally planned ) .

also , in september 2015 , the director of the office of test and evaluation issued a letter of assessment which concluded that initial operational testing of the cots system for the maritime segment had determined that the system was not operationally effective and not operationally suitable .

the under secretary for management directed the dhs cio to conduct a thorough review of the proposed plans for moving forward with the tim program .

after conducting the review , the cio did not support the program's proposal .

as a result , in november 2015 , the under secretary for management continued the suspension of all developmental efforts for the surface and aviation transportation segments , but authorized the program to continue resolving problems that were identified during initial operational testing for the cots system being used by the maritime segment .

the under secretary for management also directed the cio to form and lead an integrated product team with senior tsa representatives and the tim program office to develop a new strategy for the program .

in march 2016 , dhs and tsa officials completed a new strategy for delivering tim capabilities .

this strategy included the following changes: replace proprietary cots applications with custom - developed applications using open source code ; transition traditional , large development teams using a waterfall system development methodology to an agile software development framework to enable rapid , incremental development and deployment ; and migrate from a defined , fixed data center environment to a scalable federal risk and authorization management program ( fedramp ) certified cloud computing environment .

also , according to the new strategy , the move from the cots product to an open source solution is to include replacing the cots product that had already been deployed to the maritime segment with the open source solution .

it is also to include replacing the legacy systems that support the credentialing programs from the other two transportation segments ( surface and aviation ) with the open source solution .

tsa plans to incrementally transition the program from these legacy systems between fiscal years 2018 and 2021 .

additionally , the system is expected to interface with at least 19 other information systems , including the following key systems: tsa's transportation vetting system , which conducts initial and recurrent name - based matching against defined terrorist related data sets .

the federal of bureau of investigation's national crime information center , which is an electronic clearinghouse of crime data .

dhs's automated biometric identification system , also referred to as ident , which is the central dhs - wide system for storage and processing of biometric and associated biographic information for national security , law enforcement , immigration and border management , intelligence , and other background investigative purposes .

tsa's secure flight , which identifies individuals who may pose a threat to aviation or national security and designates them for enhanced screening or prohibition from boarding an aircraft , as appropriate .

the u.s .

citizenship and immigration service's systematic alien verification for entitlements , which is the primary data source for government agencies to verify legal entry and presence in the united states of a non - u.s. citizen or naturalized u.s. citizen .

in april 2016 , the under secretary for management approved the tim program's new strategy and , in september 2016 — almost 2 years after the program was initially suspended — the program was rebaselined to reflect the new strategy .

as we previously reported , the estimated cost and schedule in the revised baseline was significantly different than the initial baseline .

the revised baseline estimate was for about $1.27 billion ( a $74 million decrease from the previous 2015 cost estimate and an overall increase of $639 million from the original 2011 estimate ) , with full deployment planned for 2021 ( a 1-year acceleration from the previous 2015 schedule and an overall delay of 6 years from the original 2011 schedule ) .

table 3 shows the estimated costs and schedules reflected in the initial and revised estimates .

according to tim officials , in the program's first 8 years ( between october 2008 and september 2016 ) , tsa spent over $280 million to deploy the initial cots solution to the maritime segment and address critical fixes in the solution ( i.e. , the solution that tsa determined it needs to replace ) .

also during 2016 , tsa began transitioning to an agile software development framework .

in september 2016 , tsa issued two task orders to a contractor to provide agile software development services .

the orders were issued to the same design and development contractor that had assisted with the initial deployment of the tim cots solution .

from october 2016 to june 2017 , the program deployed four software releases using agile software development practices .

these releases were focused on , for example , deploying new functionality to the cots system to enhance the criminal and immigration vetting data provided to adjudicators .

in december 2016 , between the first and second agile releases , the program suspended new development for 1 month while officials reconsidered the order in which they would deliver functionality .

also during this period , the program developed and deployed a smaller release which program officials refer to as a “half release.” according to program officials , this release did not produce any new capabilities and instead addressed operations and maintenance - related fixes to the deployed cots system .

after development of the second software release , at the end of march 2017 , the program was reviewed by dhs's acquisition review board .

the purpose was to review the results of follow - on operational testing that was performed to determine whether the program had adequately addressed the prior system and usability issues and implementation of the program's new strategy .

the meeting was also intended to discuss the status of several action items from a prior review board meeting that occurred in september 2016 , such as finalizing a test and evaluation master plan , conducting a cybersecurity threat assessment , updating the program's mission needs statement and concept of operations , and establishing software development cost metrics .

implementation of the new strategy continues to be monitored by dhs and tsa oversight bodies .

the new strategy for the tim program addressed a number of major challenges that the program faced during earlier efforts to develop and deploy the system ; nevertheless , key challenges remain .

specifically , of the seven major challenges that the program faced during its initial implementation of a cots solution for the maritime segment , four challenges have been addressed related to ( 1 ) system performance and usability issues , ( 2 ) data migration issues , ( 3 ) information security testing , and ( 4 ) the inadequacy of the program's previous hosting facility .

however , the remaining three challenges regarding constraints with cots product , significant addition of new transportation programs ( eg , tsa pre® ) , and insufficient stakeholder coordination and communication have not been fully addressed .

according to dhs guidance , among other things , an operational test and evaluation examines systems for operational effectiveness .

specifically , it tests for the ability of a system to accomplish a mission when used by representative users in the expected environment .

the 2015 initial operational testing of the maritime segment ( supporting the transportation worker identification credential program ) found that the cots system was extremely unreliable due to frequent critical failures , and had several system performance and usability issues that limited users' ability to execute tasks in a timely and accurate manner .

these issues included lags , freezes , the need for excessive refreshes , inadequate reporting and case management functionalities , as well as an interface that was not user - friendly .

for example , the system was unable to produce accurate reports on case workload and status , so users expended significant effort creating spreadsheets to manually assign cases and manage their progress .

the system was also unable to perform certain waiver functions in a timely and complete manner , which resulted in a significant backlog .

the program office has addressed the issues identified in the initial operational test report by first identifying a list of over 900 action items .

according to tim officials , they validated this list with the operational test agent and prioritized the action items with the product owners ( i.e. , end users ) to identify which were the most critical to complete .

for example , critical items included addressing issues with the waiver functions , assigning cases , and issuing credentials .

the program implemented the critical fixes by developing seven software releases from september 2015 to october 2016 .

in january 2017 , the tsa operational test agent reported that follow - on operational testing of the cots system confirmed that the program had adequately addressed the prior system and usability issues .

as a result , according to the test agent , the program's previously deployed maritime segment of the system performed as intended .

according to leading practices , it programs should identify potential problems before they occur .

this allows programs to plan and execute activities to mitigate the risk of such problems having adverse impacts on the program .

when the tim program transitioned maritime users from the legacy system to the cots system , according to tsa's breach remediation plan , program officials found that cleaning and properly migrating data was very difficult and time consuming because the legacy systems were old and the data mapping information was not readily evident .

program officials stated that the data migration efforts were also difficult because of the proprietary nature of the cots product , which impacted the ability to effectively migrate data from legacy systems .

the additional time needed for data migration resulted in higher than anticipated costs for the maritime transportation segment .

program officials have taken action to better account for the tim program's future data migration efforts .

specifically , as part of the new strategy , the officials plan to defer legacy data migration until after system deployment efforts are complete to avoid disrupting deployment efforts .

the strategy focuses on the program migrating only closed case data from the legacy systems to the new system .

as such , adjudicators are to continue to complete and close any security threat assessment cases opened in the legacy system even after the new system is deployed , and the new system is to only handle newly opened security threat assessment cases .

once final disposition of the cases in the legacy system is complete , those cases would then be included in the closed case data migration effort , which is planned to occur at the end of development , around fiscal years 2020 to 2021 .

in addition , the new strategy includes streamlining the data migration by using the open source solutions to help simplify the migration of data on transportation populations from the legacy systems .

as a result of the new approach , the program should be better positioned to more effectively migrate data during future transitions between the legacy systems and new system .

according to dhs guidance , the operational test and evaluation also should examine the department's systems for operational suitability , which is the degree to which a system is deployable and sustainable .

the evaluation is to take into account factors such as reliability , maintainability , availability , and interoperability .

the 2015 initial operational testing of the cots system found that it was not suitable because the system had significant information security weaknesses .

specifically , the system inappropriately provided users with greater access than was necessary to do their jobs , which undermined the security benefits of controlling what different users were able to do in the system based on their role .

the cots system also contained critical and high - risk system security vulnerabilities which could result in the compromise of sensitive system information , such as passwords , and could hinder tsa officials' ability to effectively respond to incidents .

program officials took actions to address the security weaknesses previously identified .

for example , in response to the findings from the initial operational testing , between september 2015 and october 2016 , they developed and released fixes to the significant security weaknesses .

in april 2017 , the results of the follow - on operational testing confirmed that the cots system was free of critical or high - risk system security vulnerabilities and that it appropriately restricted access to the system by only allowing users to access areas of the system needed to support their specific business tasks .

in addition , critical steps to evaluate the system's cybersecurity have been planned , but not yet completed .

specifically , testing for realistic cybersecurity threats which is used to help categorize the system's risk - level in terms of confidentiality , integrity , and availability , was deferred until march 2018 .

program officials decided to defer this test until new hosting environments for tim are implemented , rather than testing tim in an environment that will soon be retired .

these environments are intended to enable the development , testing , and production of the system .

however , implementation of those environments has been delayed until december 2017 , and as a result , the cybersecurity vulnerability assessment has been deferred to march 2018 .

the identification of a time frame in which the program plans to conduct this important cybersecurity test is a step in the right direction , and avoiding additional delays will be important .

according to omb , a hosting facility or data center is to process or store data and must meet stringent availability requirements .

additionally , cloud computing can be used as a means for enabling on - demand access to shared and scalable pools of computing resources .

during the initial implementation of tim , the system was hosted in a cloud that operated out of a dhs data center ( referred to as dhs data center 1 ) .

however , the dhs cloud was higher in operations and maintenance costs than the program originally planned , which presented a challenge for the program .

to address this challenge , in 2016 , tim program officials decided to move the cots system that was previously deployed ( the maritime segment ) out of the dhs cloud and set it up in a public cloud environment .

they also planned to use the public cloud environment to develop , test , and operate the future tim open - source based system .

the officials planned to use a phased migration that consisted of first establishing hosting environments at two data centers — dhs data center 1 and tsa colorado springs operations center .

the officials planned to use the data centers for the development , testing , and production of the future tim open - source based system , and then eventually transition to a public or hybrid cloud once the system reaches full operational capability in fiscal year 2021 .

as part of this approach , officials planned to establish 10 development , testing , and production environments at these data centers from january to july 2017 , so that tim's development teams did not have to compete for the same environments during agile software development and testing efforts .

while the program experienced delays in setting up its production environment , officials recently took actions to address these delays .

specifically , the program was expected to have a new production environment available at the tsa colorado springs operations center by march 2017 ; however , it was delayed until may 2017 .

additionally , while migration of the tim system to the new hosting environments was planned to occur by september 2017 , it has been delayed .

these delays have contributed , in part , to delays in other aspects of the program , including the execution of the cybersecurity vulnerability assessment , as well as delays in the implementation of automated testing and deployment tools ( discussed later in this report ) .

in response to these delays , program officials recently established a revised schedule in may 2017 for setting up the new environments by december 2017 .

effectively executing against this updated schedule should help to keep the program on track with delivering these important environments and fully addressing the related challenge that the program experienced during its prior implementation efforts .

according to leading practices and guidance , technology decisions should seek to enable services to scale easily and cost - effectively and to avoid vendor lock - in by , for example , using open source solutions .

the benefits of using open source solutions can include improved software reliability and security through the identification and elimination of defects from continuous and broad peer review of publicly available source code that might otherwise go unrecognized by a more limited core development team ; unrestricted ability to modify software source code ; no reliance on a particular software vendor due to proprietary restrictions ; reduced software licensing costs ; and the ability to “test drive” the software with minimal costs and administrative delays in a rapid prototyping and experimentation environment .

also , according to leading practices , it programs should ensure that their plans include how they will transition from the current state to the final state of system operations .

such planning provides a mutual understanding to relevant stakeholders of how programs are to accomplish the transition .

according to tsa's breach remediation plan , the tim program's use of a cots solution led to several challenges .

for example , program officials reported that the cots product restricted their ability to make changes to the product to improve system usability and , as previously discussed , impacted the ability to effectively migrate data from legacy systems because of the proprietary cots product .

program officials also reported that they were highly dependent on the cots vendor to remediate compatibility issues and resolve problems , which required additional time .

the plan also stated that the cots product required a complex system architecture which prevented the program from implementing modern software development and testing tools .

finally , use of the cots product resulted in higher software licensing costs .

the tim program's new strategy is intended to address these challenges by moving away from using a cots product to a custom - developed open source solution .

however , the program's approach for developing and delivering this new solution has been in a continual state of fluctuation and implementation plans have not been defined .

as such , this challenge has yet to be fully addressed .

specifically , in september 2016 — after the 2-year pause in the program and completion of its extensive rebaselining effort — dhs and tsa officials decided that tsa would incrementally retire legacy systems as the transportation programs that use those systems are migrated to the open source solution ; they also decided to eventually replace the cots system that was previously deployed to support the maritime transportation worker identification credential program and migrate to the open source solution .

this was to be completed using a staged approach between the migrations , and also by using two versions of the cots system as well as the open source system .

however , the program lacked a plan detailing how it was going to migrate from the current legacy state , to the interim environment ( with the two versions of cots plus an open source system ) , to the final state .

as previously mentioned , in december 2016 , new development for the tim system was paused once again to , among other things , further evaluate the transitioning approach that was agreed to 3 months prior .

four months later ( in mid - march 2017 ) , program officials decided to continue pursuing the approach that was agreed to in september .

subsequently , the high - level implementation schedule was revised to adjust for delays that this most recent replanning effort contributed to ( other contributing factors for the delay are discussed later in this report ) .

the revised schedule delayed deployment of the initial pre® capabilities by 6 months and other key functionality up to 12 months .

further adding to the fluctuation in the program , at the end of march 2017 , the dhs acquisition review board requested that the program's implementation approach be revised to accelerate the delivery of the tim program's front - end interface for adjudication and redress functions .

however , it is unclear how the acceleration of the development and implementation of these functions will impact the delivery of the other planned functionality , and what tradeoffs the program will need to make .

program officials were expected to develop an overview of the acceleration efforts associated with cost , schedule , risk , and impacts on the program and deliver it to parm and the office of the chief technology officer in august 2017 .

as a result , while it has been 8 months since the tim program was rebaselined , the details of how the program will transition from its current state , to an interim state , then to the final state of full open source , have yet to be determined .

this is contrary to leading practices that we have previously identified , which state that when pursuing an it modernization effort , organizations should develop a plan for transitioning from the current to the target environment .

in response to our concerns , program officials stated that after they determine how they will adjust to incorporate the acquisition review board's recent acceleration request , they will determine the details of how the program will achieve the desired final state .

however , until the program establishes and implements specific time frames for determining key implementation details , including how it will transition the program from its current state to an interim state and to the final state , the tim program office , and tsa and dhs oversight bodies cannot be certain about how the program will ultimately deliver its complete open source solution .

according to leading practices , programs should manage changes to requirements as they evolve during the project .

programs should also ensure that planned schedules provide a realistic forecast for completion of activities , including providing reasonable slack ( i.e. , flexibility in the schedule ) .

after the tim program was initiated in 2008 , it experienced significant increases in scope , such as the addition of tsa pre® and chemical facility anti - terrorism standards populations in 2012 , which required more functionality and considerably more processing demands than originally planned .

the tim program was challenged to accommodate the additional work needed to incorporate these new transportation populations and capabilities , and , in part , contributed to a significant breach in its original cost and schedule estimates .

to address the challenge , the tim program incorporated the additional functionality and processing requirements into its cost and schedule rebaseline that was approved in september 2016 .

in addition , the program's new strategy addressed the need to be adaptable to accommodate any new transportation populations and capabilities that could be added in the future by taking an enterprise - level approach to providing capabilities .

nevertheless , while the tim program incorporated tsa pre® into its new plans , the implementation schedule for the program was very compressed and program officials did not establish a schedule that realistically forecasted when activities would be completed .

specifically , program officials planned to deploy initial tsa pre® capabilities by may 2017 without any slack in the schedule .

according to program officials , the reason for this approach , was because tsa pre® was considered a high priority for migrating from its legacy system in order to accommodate an expected influx of applicants during the summer months .

however , slack was not incorporated in the implementation schedule ; therefore , when the program experienced schedule delays , it resulted in the program missing the may 2017 implementation deadline and being rescheduled to november 2017 .

the 6-month delay in delivering initial pre® capabilities was due to the delays discussed in the prior section associated with replanning the strategy for transitioning to the open source system , as well as delays in onboarding additional development team members and setting up new development and production environments .

the delay in delivering pre® capabilities is especially problematic because program officials have reported that the legacy system is at risk of exceeding its processing capacity .

additionally , as previously mentioned , the program's revised schedule shows the delivery dates for almost all ( 8 of 10 ) capabilities being significantly pushed back — with 2 capabilities being delayed up to 12 months .

moreover , not only were the implementation dates delayed for these efforts , the time to complete a number of these efforts was reduced by about 1 to 12 months — thus further exacerbating our concerns about unrealistic schedules .

without a schedule that realistically forecasts when activities will be completed , tim program officials cannot ensure that they will meet the dates that they have committed to , such as when key capabilities for tsa pre® are to be deployed .

according to leading practices , programs should coordinate and collaborate with relevant stakeholders ( i.e. , those that are affected by or in some way accountable for the outcome of the program , such as program or work group members , suppliers , and end users ) .

stakeholder coordination includes , for example , involving stakeholders in reviewing and committing to program plans , agreeing on revisions to the plans , and identifying risks .

programs should also identify the needs and expectations of stakeholders and translate them into end user requirements .

however , during prior implementation efforts with the cots solution , the program experienced challenges with effectively coordinating and communicating with end - users .

for example , according to program documentation , it had not adequately collaborated with end users in developing and implementing business requirements and conducting post - deployment user satisfaction assessments .

this led to frustration among end users who felt inadequately informed and prepared for the new cots system .

to address this challenge , the tim program's new strategy includes establishing a product owner role , which , as previously mentioned , is intended to represent the end user community and have the authority to set business priorities , make decisions , and accept completed work .

the program's adoption of the agile software development approach has also significantly increased the frequency of the program's engagement with stakeholders to define , test , and implement software releases .

in addition , program officials established an organizational change management strategy in october 2016 that is intended to , among other things , focus broadly on establishing overall communication processes for program stakeholders .

this strategy identifies key steps such as , establishing a communication team and hiring a communication lead to oversee the development and execution of the communication action plans , establishing a communication working group , and serving as chair of the communication working group .

this group is to be responsible for developing four communication action plans for key stakeholder groups ( eg , new transportation populations , existing transportation populations , and management ) .

these particular steps were to be completed from november 2016 through january 2017 .

however , while as of may 2017 , the tim program had implemented certain steps from the organizational change management strategy , such as establishing a communication team , the program has been delayed in implementing other steps .

specifically , the communication lead position was to be filled in november 2016 .

however , in march 2017 tim program officials stated that the position had not yet been filled due to the federal hiring freeze .

additionally , because of the vacancy in the communication lead position , other key actions have been delayed , such as the development and execution of the communication action plans .

program officials have not established new time frames for completing the remaining steps outlined in the organizational change management strategy .

until these time frames are established and effectively executed , program officials will have less assurance that there will be effective communication with stakeholders and customers to ensure that the program is meeting their needs .

as discussed previously , transitioning a program from waterfall development to agile software development is a significant effort , and requires the implementation of fundamental practices to ensure that the transition is successful .

according to leading guidance , an organization transitioning to agile software development should establish critical practices to help ensure successful adoption of the agile approach , such as obtaining full support from leadership to adopt agile processes , enhancing agile knowledge , ensuring product owners are engaged with the development teams and have clearly defined roles , establishing a clear product vision , prioritizing backlogs of requirements , and implementing automated tools to enable rapid system development and deployment .

while the tim program has fully implemented the first two of these leading practices necessary to ensure the successful adoption of agile , the remaining four practices have not been fully implemented .

the gaps we have identified with the program's implementation of agile are concerning given that it did not follow key it acquisition best practices when using its waterfall development approach during the program's first 8 years and spent over $280 million on a system that tsa has determined it needs to replace .

according to leading practices and guidance , an organization transitioning to agile software development should get and maintain full support from the organization's leadership to adopt agile processes .

leadership support helps empower employees to continuously improve the use of agile software development practices .

dhs and tsa leadership have approved the tim program's adoption of agile software development , and continue to support the transition .

for example , the dhs ocio worked closely with tsa officials in 2015 and 2016 to develop the new strategy for the program which included moving away from a waterfall development approach to agile software development .

as previously mentioned , the under secretary for management selected the tim program to be part of the dhs agile pilot initiative in february 2016 and approved the program's new strategy in april 2016 .

moreover , the dhs office of the chief technology officer has continued to provide guidance and resources to the program since it adopted agile .

for example , tim program officials stated that the dhs chief technology officer added two of the office's full - time and one part - time staff members to the tim program .

dhs and tsa officials stated that the chief technology officer also provided an agile coach to assist the tim program manager about 3 days per week with establishing an agile governance framework .

finally , dhs established an agile integrated product team that is co - chaired by parm and the tim program manager .

the team meets bi - weekly to provide guidance on adopting agile processes .

as a result of the sustained leadership commitment , the program is better positioned to continuously improve its agile practices .

according to leading practices and guidance , an organization transitioning to agile software development should ensure that the entire program team receives agile training .

this allows organizations to achieve a faster shift away from the previous culture and processes and toward a more agile culture .

toward this end , the tim program requires its agile contractor to ensure that development teams are trained and skilled in agile methods , as well as in the specific agile frameworks the program has adopted , which include the scrum and safe frameworks .

additionally , the program provided initial agile training for key program staff when it began transitioning to agile software development .

specifically , the program provided a mandatory 2-day agile workshop in october and december 2016 which covered basic agile principles and the scrum and safe frameworks .

this training was provided to many key staff members , including contractor support staff , a contracting officer representative , and product owners .

further , in december 2016 , the program began providing training on the safe framework to its government employees .

this training was tailored based on different roles , such as agile practitioner , program manager or product owner , and scrum master .

the training courses were provided to key staff members , including tim program leadership , team leads , branch managers , and scrum masters .

as a result of providing agile training , the program's staff should be able to more effectively adopt and apply agile software development processes .

according to leading practices and guidance , an organization transitioning to agile software development should designate a product owner who represents the user community and establishes priorities based on business needs , approves user stories and their acceptance criteria , and decides whether completed work meets the acceptance criteria and can be considered done .

the product owner should also maintain close collaboration with the development teams by , among other things , providing daily support to help clarify requirements and attending key agile meetings , such as sprint - and release - level planning sessions and system demonstrations .

additionally , roles and responsibilities among relevant stakeholders , such as the product owner , should be clearly defined and documented by the organization that is transitioning to agile software development , so that the stakeholders are aware of their responsibilities and given the authority to perform their roles .

the tim program has two different groups of individuals that collectively share the responsibilities of product owner , and while these groups frequently engage with the development teams , program officials have not yet clearly defined the groups' roles and responsibilities .

specifically , according to program officials , the first group consists of five product owners that represent end users and are collectively responsible for supporting all development teams , attending all agile meetings , and prioritizing and approving planned and completed work .

in addition , according to program officials , these five individuals are also responsible for approving user stories associated with new system functionality .

the other group is referred to as the solutions team , which includes , for example , the tim chief architect and chief engineer .

according to program officials , the technical work ( which is to help enable the system functionality , such as ensuring network connectivity and proper software licenses ) is approved by the solutions team .

nevertheless , while program officials told us about these high - level roles and responsibilities , the program's documentation does not clearly define them among the five product owners and the solutions team .

moreover , program officials have not defined the rules of engagement for these product owners , such as how competing priorities among different product owners should be handled .

according to program officials , the lack of clearly defined roles and responsibilities has not been a problem for the program because the product owners and the solutions team regularly communicate and coordinate with each other , and thus far , have been in agreement on the priorities for the program .

however , the program recently scaled up the amount of work being conducted simultaneously , which adds to the volume of the decisions that need to be made and the coordination that has to occur among the five product owners and solutions team .

thus , even if the program has not yet experienced issues with coordination , without more clarity in the roles and responsibilities among the groups that are responsible for prioritizing and accepting work , the program risks facing challenges in establishing priorities , approving user stories , and deciding whether completed work meets the acceptance criteria .

according to leading practices and guidance , a program transitioning to agile software development should have a clearly defined vision .

this can be in the form of a product roadmap , to guide the development of the product and to help inform the planning and requirements development of agile software development releases .

consistent with leading practices , tsa established a vision for the tim program .

this vision is articulated in multiple documents — including the mission needs statement , concept of operations , and operational requirements document .

officials also use a strategic roadmap to articulate the program's vision , which specifies the high - level system capabilities that are to be deployed over the life - cycle of the program through 2021 .

however , the program's vision has not always informed the planning of requirements for the software releases , as intended by leading practices .

specifically , the capabilities outlined in the program vision documents , such as the strategic roadmap , do not consistently map to program requirements .

while 5 of the 10 capabilities in the strategic roadmap align to the high - level and large scope requirements , referred to as epics , the other half of the capabilities do not clearly align to the epics .

for example , the adjudication and redress capabilities that are in the strategic roadmap do not align to any epic .

in addition , the capability for public - facing portals does not clearly track to any epic .

tim officials recognized the alignment issues , and in august 2017 , stated that they are in the process of establishing alignment from the program's vision down to the lowest level of requirements , by refining the program's vision and requirements .

officials also stated that they expected this effort to be completed by 2018 .

effective execution of this effort should help ensure the program's vision is informing requirements planning .

according to leading practices and guidance , a program transitioning to agile software development should have a prioritized list of the requirements that are to be delivered — referred to as the backlog .

this backlog should be maintained so that the program can ensure it is always working on the highest priority requirements that will deliver the most value to the users .

in addition , according to tim agile management documentation and program officials , the program's backlog of features ( i.e. , mid - sized requirements ) is expected to represent the features that are to be delivered over the next several software releases .

these features are to be assigned priority levels to help determine which should be selected for development when planning the next release .

according to tim agile management documentation , the tim program is expected to manage a backlog for each software release , which is to identify the features and their derived user stories ( i.e. , the smallest and most detailed requirements ) that are to be delivered in a specific release .

the documentation also indicates that each feature and user story is to be assigned priority levels to determine which should be included in the development of the next release and associated sprint .

figure 5 illustrates the intended prioritization in the features , releases , and user stories backlogs .

however , as of july 2017 , the program's backlogs did not contain specific prioritization levels for each of the features and user stories , as called for in dhs guidance .

according to program officials , instead of assigning specific prioritization levels , they had more generally identified which features should be developed within the near - term ( eg , in the next several agile releases ) .

program officials recognized that they still needed to prioritize their backlogs by assigning priority levels to all features and user stories , but they did not have a time frame for completing this effort .

without ensuring full prioritization of current and future features and user stories , the program is at risk of delivering functionality that is not aligned with the highest needs of those that are responsible for conducting security threat assessments to protect the nation's critical transportation infrastructure .

according to leading practices and guidance , automating system development and deployment work and avoiding manual work is especially important for agile programs , as it enhances the ability for rapid development and delivery of high quality software .

specifically , a program transitioning to agile software development should use an automated tool for managing agile activities , such as maintaining the product backlog and tracking the status of completed work .

the program should also establish automated testing and deployment capabilities to improve the quality of the system .

for example , according a dhs's agile development instruction manual , the vast majority of software defects are discovered during system integration testing , and — if automated — this testing can be run multiple times on a sprint or release in order to identify more defects sooner .

in addition , automated tools can enable more efficient processes for frequently integrating computer code that is developed by different team members ( eg , hourly or daily ) , in order to quickly detect any code integration errors .

automation of testing can also help decrease the risk of introducing security flaws due to human error .

however , program officials deferred implementation of an automated agile program management tool and many other testing and deployment tools .

specifically , while the program had been using agile software development practices since october 2016 , the program has not used an automated management tool for tracking the status of completed work for its first three agile software releases .

instead the program has used spreadsheets that require tim program officials to manually populate and track large amounts of program status information .

program officials had planned to implement an automated management tool by october 2016 , but did not do so until the end of april 2017 .

according to the officials , the delay occurred because they were in the process of tailoring the safe governance framework and the management tool needed to be customized to reflect the tailored approach .

regarding tools for testing and deployment , as of may 2017 , the program was only using 4 of the16 automated tools that program officials planned to use .

these included tools that enable the management of software code development , defect tracking , and components of automated functional testing .

however , the remaining 12 testing and deployment tools had not yet been implemented .

these include , among others , tools that enable the automated building of software code , frequent merging of an individual piece of software code with the main code repository so that new changes are tested continuously ( referred to as continuous integration ) , small automated tests to verify that each individual unit of code written by the developer works as intended , and installation of application patches to protect against known vulnerabilities .

tim program officials stated that these testing and deployment tools are not expected to be implemented until the new development , testing , and production environments are set up .

however , as previously mentioned , the program has experienced challenges in implementing these environments .

as a result , the program's use of manual processes have been time consuming , impeded visibility into the process , and hindered software testing .

in addition , without automated tools , program performance metrics were being manually calculated and this increases the risk for incomplete and inaccurate data .

while the automated agile management tool has just been implemented , until the remainder of the automated agile testing and deployment tools are implemented , the program is likely to continue to operate at reduced efficiency levels , and be limited in its ability to ensure product quality .

according to leading practices , to ensure effective program oversight of cost , schedule , and performance , organizations should: ensure that corrective actions are identified and tracked until the desired outcomes are achieved , document relevant governance and oversight policies and monitor program performance and progress , and rely on complete and accurate data to review performance against expectations .

while tsa fully implemented the first practice , the remaining three practices were not fully implemented by dhs and tsa .

as a result , the effectiveness with which the governance bodies oversee and monitor the program has been limited .

according to leading practices , effective program oversight includes ensuring that corrective actions are identified and tracked until the desired outcomes are achieved .

in this regard , governance bodies should collect and analyze data on program risks and issues and determine corrective actions to address them and track them to completion .

tsa has established a process for ensuring that corrective actions are identified and tracked .

specifically , the program has a process for identifying corrective actions and monitoring the status of these actions in its weekly program status reviews .

the program also uses an automated tool to track and maintain a complete list of all actions that have been identified .

as of february 2017 , the list contained 89 actions and included the status of the actions — 83 of which had been tracked to completion .

as a result of the program having a process that can identify and track corrective actions , it is better positioned to address significant deviations in cost , schedule , and performance parameters .

according to leading practices , effective program oversight includes the use of documented policies and procedures for program governance and oversight , such as reporting and control processes .

these processes may include , among others , requiring programs to report on the status and progress of activities ; expected or incurred program resource requirements ; known risks , risk response plans , and escalation criteria ; and benefits realized .

oversight and governance documentation may also include threshold criteria to use when analyzing performance , and the conditions under which a program or project would be terminated .

tsa and dhs have documented selected policies and procedures for governance and oversight of the tim program .

specifically , dhs documented procedures for its acquisition review board and its executive steering committee for the tim program on how these governance bodies are to review the cost , schedule , and performance of the program .

for example , according to the committee's charter , it is responsible for assessing the health of the program and identifying major issues and risks , utilizing a standard reporting format at oversight meetings .

tsa has also documented processes for the program's agile milestone reviews , such as conducting workshops at the end of the release cycle to perform a system demonstration , review qualitative metrics , and promote continuous quality improvement .

tsa also developed a risk management plan tailored for the agile approach to guide tim staff members in identifying , managing , and mitigating risks and issues impacting cost , schedule , and performance of the program .

the agency also developed a test and evaluation master plan that outlines how it and dhs will conduct and oversee testing and evaluation of the program's capabilities under the new agile software development approach .

however , tsa and dhs have not developed or finalized other key oversight and governance documents .

specifically , three oversight and governance policies have not been finalized and / or appropriately updated: the tim program's tailoring plan for safe , a dhs - level oversight policy for agile programs , and dhs office of the chief technology officer's guidance for agile programs to use for collecting and reporting on performance metrics .

the tim program has not updated its systems engineering life cycle tailoring plan ( which outlines the agile governance process and all milestone reviews that are required for planning and deploying agile releases ) , to reflect changes in the way officials have reported using the safe governance framework .

as a result , there are inconsistencies in the governance documentation .

for example , the systems engineering life cycle tailoring plan describes four levels of governance — portfolio , value stream , program , and team — while program officials have reported omitting the value stream level from the governance framework .

according to tsa officials in may 2017 , they planned to update the systems engineering life cycle tailoring plan to reflect the revised governance framework , but they did not have a specific time frame for completing the revision .

until the tim program fully updates its systems engineering life cycle tailoring plan to reflect the revised governance framework , the program lacks a clearly documented and repeatable governance process to effectively oversee the program .

dhs officials stated that they plan to conduct biannual oversight reviews of the five agile pilot programs ( including tim ) , instead of the annual reviews that are typically conducted for traditional waterfall development programs .

according to the officials , the purpose of moving to biannual reviews is to better ensure cost , schedule , and performance remain on track for these agile programs .

however , officials in the office of the chief technology officer stated that dhs - level agile governance and oversight policies and procedures have not been revised to reflect this new oversight approach because consensus among dhs leadership on related changes needs to be established before this new oversight approach can be documented in the department's guidance .

as of may 2017 , officials had not specified a time frame for reaching such consensus .

until dhs leadership reaches consensus on needed oversight and governance changes , and then documents and implements associated changes , the program continues to plan as though it is undergoing annual oversight reviews , versus biannual reviews .

as of early may 2017 , officials in the office of the chief technology officer were also in the process of drafting guidance for agile programs to use for collecting and reporting on performance metrics , but did not know when this guidance will be finalized .

according to tsa officials , in the absence of complete agile guidance , the tim program receives support from dhs's agile team supporting the pilot initiative , which , as specified in the team's charter , is intended to help the program ( as well as the other four pilot programs ) facilitate agile software development .

however , this team is not intended to perform oversight functions to ensure that the program is meeting cost , schedule , and performance targets .

thus , until the office of the chief technology officer completes guidance for agile programs to use for collecting and reporting on performance metrics , tim program officials may not report the most informative agile performance metrics to oversight entities .

according to leading practices , effective program oversight includes monitoring program performance and progress by comparing actual cost , schedule , and performance data with estimates in the plan and identifying significant deviations from established targets or thresholds for acceptable performance levels .

program reviews are to be conducted at predetermined checkpoints or milestones in order to determine progress by measuring programs against cost , schedule , and performance metrics .

in addition , agile programs should be measured on , among other things , velocity ( i.e. , number of story points completed per sprint or release ) , development progression ( eg , the number of features and user stories planned and accepted ) , product quality ( eg , number of defects and unit test coverage ) , and user satisfaction .

the tim program management office conducts frequent and regular performance reviews and focuses on several important agile release - level metrics .

specifically , program management officials monitor tim's performance and progress during weekly program status review meetings and in periodic agile reviews that are conducted at the end of each release .

these reviews also include officials from the development teams and program stakeholders .

the reviews focus on , among other things , velocity , progress , and product quality .

they also include the status of key activities and risks impacting cost , schedule , and performance .

nevertheless , while the program management office uses performance metrics , the program has not established thresholds or targets for acceptable performance levels for these metrics .

for example , program status reports showed that about 47 percent of the work that was planned to be completed in the first agile release was accepted by the product owners .

while the program appears to have been improving in this metric — 74 percent was accepted in the second agile release and 94 percent in the third agile release — program officials have not established the thresholds or targets to determine the acceptable level of performance .

program officials stated that they considered the performance in the first agile release to be low , but they have not yet established targets or thresholds .

according to program officials , they planned to establish targets based on the capacity of work that development teams are expected to complete in a release , which can be better predicted as the teams spend more time together .

however , the program has since developed three releases and continues to lack performance thresholds and targets .

until program officials establish performance thresholds or targets , oversight bodies may lack important information to ensure the program is meeting acceptable performance levels .

in addition , the program management office's performance reviews have included limited information on program cost .

according to tim officials , the program manager holds weekly meetings with the contract , finance , and budget groups to review costs associated with tim's contracts .

however , management does not review or produce reports on overall life - cycle cost performance for the program or agile software development cost performance .

program officials said they have not yet determined how best to measure cost performance in an agile software development environment .

in september 2016 , the under secretary for management instructed the program to collaborate with dhs's cost analysis division and the headquarters - level agile integrated product team to establish agreed - upon software development cost metrics as well as a method for collecting and reporting on those metrics by the end of the march 2017 .

however , as of may 2017 , this effort was still in progress .

until the tim program begins collecting and reporting on agile - related cost , oversight bodies will have limited information by which to monitor tim costs .

department - level oversight bodies have focused on reviewing certain program life - cycle metrics for the tim program .

specifically , the dhs acquisition review board conducts periodic reviews of the program to monitor the program's performance and hold the program accountable .

since the program was rebaselined in september 2016 and transitioned to agile software development , the acquisition review board has conducted one review .

in addition , the executive steering committee , which is chaired by the tsa cio and deputy component acquisition executive , and includes representatives from the dhs chief technology officer and parm , reviews the program quarterly .

as of july 2017 , the executive steering committee had conducted three reviews of the tim program since implementing its new development approach .

these oversight bodies reviewed , for example , performance information such as comparisons of the dates that milestones were actually achieved , against the planned schedule , and the burnup charts for the program ( i.e. , graphical representations of accumulated story points planned and completed per release ) .

however , the acquisition review board and the executive steering committee have not been measuring the program against the rebaselined life - cycle costs , or important agile release - level metrics , which are essential for providing early indicators of issues with the program .

for example , these oversight bodies did not review the program's velocity , number of features / user stories planned and accepted , product quality , or agile software development cost metrics .

in addition , while we have previously reported that there was overlap in the dhs ocio's and the parm office's assessments of certain it programs , neither of these offices assessed the tim program's progress against key agile performance metrics or cost performance .

specifically , the dhs ocio and the parm office conducted periodic ( monthly or quarterly health assessments ) of the program that included , among other things , schedule and system performance indicators for the entire life - cycle of the program ( similar to what is used to review traditional waterfall programs ) .

while these metrics are useful for understanding the program's progress against the full schedule ( 60 months to full operational capability , or 30 agile releases ) , they do not offer insight into the progress of individual agile releases , which are deploying high - priority capabilities for the tim program every 2 months .

for example , as of april 2017 , these two oversight bodies did not include agile performance metrics which would have offered important insights into the progress of individual releases , such as velocity , progress metrics , quality metrics , post - deployment user satisfaction , or agile software development costs .

thus , until dhs - level oversight bodies review key agile performance and cost metrics and use them to inform management oversight decisions , the oversight bodies will be limited in their ability to obtain early indicators of any issues with the program , and to call for course correction , if needed .

recently , the tim program also began measuring user satisfaction .

specifically , in april 2017 , the dhs acting under secretary for management directed tsa's operational test agent to implement a continuous evaluation dashboard based on the results from the program's third agile release by the end of june 2017 .

this dashboard was to measure , among other things , post - deployment user satisfaction .

tsa subsequently implemented the continuous evaluation dashboard in june 2017 .

table 4 summarizes the extent to which performance metrics are reviewed by various oversight bodies .

according to leading practices , effective program oversight includes relying on complete and accurate data to review program performance against stated expectations .

complete and accurate data allow oversight bodies to have transparency into the performance of programs and helps them identify when course correction is needed .

however , tim's reported performance data were not always complete and accurate .

specifically , when reporting on the velocity ( i.e. , total number of story points completed per sprint and / or release across the development teams ) of tim's first release after it was deployed , program officials inconsistently reported velocity among the program's performance reports , thus calling into question the accuracy and completeness of the information .

since the data were being reported on a completed release , the velocity should have been reported as one consistent number that did not change .

according to program officials , the reason for inconsistent reporting was that , despite best practices , the program's methodology for measuring velocity was not consistent and was calculated differently each time .

for example , table 5 shows three different numbers that were to represent the collective velocity across the development teams , and that officials reported to program management after the deployment of the first software release .

while there was less variation in the velocity data reported after the second software release was deployed , discrepancies were still present .

for example , table 6 shows the different numbers that officials reported to tim program management after the deployment of the second software release .

program officials stated that the reason for the inconsistencies in reported velocity data was that during the first release they were still in the process of adapting agile and were working to determine how best to calculate velocity .

however , as shown in table 6 inconsistent data continued to occur beyond that first release .

these inconsistencies in reported data call into question the completeness and accuracy of the velocity numbers reported , and the potential impact on oversight bodies' ability to hold the program accountable .

for example , velocity is most useful when tracked over time to ensure consistent performance and for forecasting how quickly development teams can work through the items in a backlog .

however , without a complete and accurate velocity number from each release , it is difficult for oversight bodies to ensure the program is producing work at an acceptable pace to enable the program to meet its cost , schedule , and performance targets .

in addition , the program had been reporting inaccurate unit test coverage data using a manual measurement approach .

specifically , from december 2016 to march 2017 , program officials were reporting that , for each release , they tested every line of code , based on a manual estimate ( i.e. , 100 percent ) .

however , testing each line of code manually is unrealistic because with manual tests , it is difficult to determine which function , line of code , or logic decision is executed , and which is not .

as such , program officials were reporting that they were testing every line of code , even though they were unable to confirm that they were actually doing so , thus calling into question the reliability and accuracy of the data reported .

in response to our concerns , program officials acknowledged that they could not confirm whether they had tested every line of code .

accordingly , program officials stopped estimating this metric manually and stated that they planned to begin measuring unit test coverage again once lines of code could be tracked using automated tools .

as previously discussed , program officials stated that the testing and deployment tools are not expected to be implemented until the new development , testing , and production environments are set up .

however , until the program has complete and accurate unit test code coverage data , program officials will not know if portions of its code are going untested , which could lead to undetected issues and impact the quality of the product .

tsa's tim program has taken notable steps to address several of the major issues it faced during prior system development and deployment efforts , such as implementing system fixes to address critical performance and usability issues found in the maritime segment .

nonetheless , a number of significant challenges have not been fully addressed .

in particular , until the tim program establishes specific time frames for determining key implementation details , ensures its schedule provides planned completion dates based on realistic estimates , and establishes new time frames for implementing the actions identified in the strategy , it is at significant risk of repeating past mistakes and experiencing the same pitfalls as it did during its initial implementation attempts .

an indication of concern is that the program is currently experiencing a delay of at least 6 months in the rebaselined schedule for delivering tsa pre® capabilities .

while the program has also taken certain steps to successfully make the transition from a waterfall development approach to agile software development — a substantial and complex effort — tim has not defined key roles and responsibilities , prioritized features and user stories , or implemented automated capabilities that are essential to ensuring effective adoption of agile .

the gaps we identified with the program's implementation of agile are concerning given that it did not follow key it acquisition best practices when using its waterfall development approach , in which the program spent approximately 8 years and over $280 million on a system that tsa has determined it needs to replace .

while selected corrective actions have been taken , until the tim program is implemented in accordance with leading practices , the program will be putting at risk its ability to deliver a quality system that strengthens and enhances the sophistication of tsa's security threat assessment and credentialing programs .

in addition , while tsa and dhs have implemented certain practices for overseeing and governing the program , the lack of other practices has impeded their oversight effectiveness , including the lack of thresholds or targets for acceptable performance levels , the lack of reporting on agile - related cost metrics , and inconsistent measuring and reporting of program velocity and unit test coverage for software releases .

these gaps limit the ability of dhs oversight bodies to obtain early indicators of any issues with the program , and to call for course corrections , if needed .

further , until dhs leadership reaches consensus on needed oversight and governance changes related to agile programs , and then documents and implements associated changes to align oversight reviews with the timing of agile software releases , the department will not be well positioned to hold the program accountable .

moreover , until the office of the chief technology officer completes guidance for agile programs to use for collecting and reporting on performance metrics , and dhs - level oversight bodies require the tim program to report on key agile performance and cost metrics and use them to inform management oversight decisions , the department will also be limited in its ability to hold the tim program accountable and ensure that it is meeting its cost , schedule , and performance targets .

we are making the following 14 recommendations to dhs: the tsa administrator should ensure that the tim program management office establishes and implements specific time frames for determining key strategic implementation details , including how the program will transition from the current state to the final tim state .

 ( recommendation 1 ) the tsa administrator should ensure that the tim program management office establishes a schedule that provides planned completion dates based on realistic estimates of how long it will take to deliver capabilities .

 ( recommendation 2 ) the tsa administrator should ensure that the tim program management office establishes new time frames for implementing the actions identified in the organizational change management strategy and effectively executes against these time frames .

 ( recommendation 3 ) the tsa administrator should ensure that the tim program management office defines and documents the roles and responsibilities among product owners , the solution team , and any other relevant stakeholders for prioritizing and approving agile software development work .

 ( recommendation 4 ) the tsa administrator should ensure that the tim program management office establishes specific prioritization levels for current and future features and user stories .

 ( recommendation 5 ) the tsa administrator should ensure that the tim program management office implements automated agile management testing and deployment tools , as soon as possible .

 ( recommendation 6 ) the tsa administrator should ensure that the tim program management office updates the systems engineering life cycle tailoring plan to reflect the current governance framework and milestone review processes .

 ( recommendation 7 ) the tsa administrator should ensure that the tim program management office establishes thresholds or targets for acceptable performance - levels .

 ( recommendation 8 ) the tsa administrator should ensure that the tim program management office begins collecting and reporting on agile - related cost metrics .

 ( recommendation 9 ) the tsa administrator should ensure that the tim program management office ensures that program velocity is measured and reported consistently .

 ( recommendation 10 ) the tsa administrator should ensure that the tim program management office ensures that unit test coverage for software releases is measured and reported accurately .

 ( recommendation 11 ) the secretary of homeland security should direct the under secretary for management to ensure that appropriate dhs leadership reaches consensus on needed oversight and governance changes related to the frequency of reviewing agile programs , and then documents and implements associated changes .

 ( recommendation 12 ) the secretary of homeland security should direct the under secretary for management to ensure that the office of the chief technology officer completes guidance for agile programs to use for collecting and reporting on performance metrics .

 ( recommendation 13 ) the secretary of homeland security should direct the under secretary for management to ensure that dhs - level oversight bodies review key agile performance and cost metrics for the tim program and use them to inform management oversight decisions .

 ( recommendation 14 ) .

dhs provided written comments on a draft of this report , which are reprinted in appendix ii .

in its comments , the department concurred with all 14 of our recommendations and described actions it has planned or taken to address them .

for example , with regard to recommendation 6 , which calls for dhs to implement automated agile management testing and deployment tools , the department stated that tsa plans to implement such tools by june 30 , 2018 .

additionally , for recommendation 14 , the department stated that dhs intends to ensure that oversight bodies review key agile performance and cost metrics for the tim program by june 30 , 2018 .

if implemented effectively , these actions should address the weaknesses we identified .

the department also described recent actions that it and tsa had taken to address three of the recommendations , and requested that we consider these recommendations resolved .

specifically , in response to recommendation 9 , calling for tsa to ensure that the tim program management office begins collecting and reporting on agile - related cost metrics , the department stated that the program is now reporting these metrics on a monthly basis .

in response to recommendation 10 , calling for tsa to ensure that the program's velocity is measured and reported consistently , the department stated that velocity is now being reported consistently and in accordance with dhs guidelines .

further , in response to recommendation 13 , which calls for dhs to complete guidance for agile programs to use for collecting and reporting on performance metrics , the department stated that the guidance had recently been published and provided to us .

however , to date , we have received only draft versions of the guidance .

we will work with the department to obtain finalized documentation related to the three recommendations , to determine if the recent actions fully address the recommendations .

in addition to the aforementioned comments , we received technical comments from dhs and tsa officials , which we incorporated , as appropriate .

we are sending copies of this report to the secretary of homeland security and interested congressional committees .

in addition , the report will be available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-4456 or harriscc@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iii .

our objectives were to ( 1 ) describe the transportation security administration's ( tsa ) past implementation efforts for the technology infrastructure modernization ( tim ) program and its new implementation strategy ; ( 2 ) determine the extent to which tsa's new strategy for the program addresses the challenges encountered during earlier implementation attempts ; ( 3 ) determine the extent to which tsa has implemented selected key practices for transitioning to an agile software development framework for the program ; and ( 4 ) determine the extent to which the tsa and the department of homeland security ( dhs ) are effectively overseeing and governing the tim program to ensure that it is meeting cost , schedule , and performance requirements .

to address our first objective , we reviewed program documentation , such as initial and current acquisition program baselines , initial and current life - cycle cost estimates , acquisition decision memorandums , and program plans documenting a new strategy for implementing the program .

we used the information in this documentation to summarize the program's earlier attempts to implement tim capabilities and its new implementation strategy for delivering the program , including estimated costs , schedule , and key decisions made .

we also interviewed tsa officials , including the tim director and deputy director , on the status of tim program office efforts .

to determine the extent to which the tim program's new strategy addresses the challenges encountered during earlier implementation attempts , we reviewed documentation on the challenges the tim program faced when it breached cost and schedule thresholds and experienced system performance issues , such as those described in initial operational test reports , the breach remediation plan , and the results of a technical evaluation of program challenges .

we synthesized the information in these documents to identify a consolidated list of key challenges the program had faced .

we did not include challenges that were already being evaluated as part of other objectives , such as the use of the waterfall software development approach .

we then reviewed documentation on the program's new strategy , such as plans documenting the new strategy , follow - on operational test reports , program schedules , program status reports , and identified risks .

we assessed the extent to which the new strategy outlined in these documents addressed the prior challenges by comparing them against criteria identified in leading practices and guidance , such as dhs's systems engineering lifecycle guide and the software engineering institute's capability maturity model® integration for development .

in addition , we conducted a site visit at the tsa adjudication center in reston , virginia .

during this site visit , we observed demonstrations of the current commercial - off - the - shelf system and legacy systems for tsa pre® and aviation workers , and we interviewed adjudicators and supervisors on current security threat assessment processes and limitations .

further , we interviewed tsa officials , including the tim director and deputy director , on the program office's efforts to address prior challenges .

to determine the extent to which the program has implemented selected key practices for transitioning to an agile software development framework , we identified leading practices and guidance outlined in the following sources: gao , software development: effective practices and federal challenges in applying agile methods software engineering institute , agile readiness and fit techfar handbook tsa agile scrum guidance cmmi® for development , version 1.3 software engineering institute , agile metrics after reviewing the sources listed , in consultation with our internal expert , we grouped practices that were identified as being critical to establish when transitioning to an agile software development framework , and selected the practices that were most relevant based on the status of the program's transition and we discussed the practice areas with tsa officials .

the practices included: full support from leadership to adopt agile processes , enhancing agile knowledge , ensuring product owners are engaged with the development teams and have clearly defined roles , establishing a clear product vision , prioritized backlogs of requirements , and implementing automated tools to enable rapid system development and deployment .

we reviewed program management documentation against these practices , such as agile training records , agile contracts , program roadmaps , backlogs , test plans , agile release artifacts , program status reports , and identified risks .

additionally , we observed agile release and sprint development activities at tsa facilities in annapolis junction , maryland , and at a contractor's facilities in beltsville , maryland , and we observed a demonstration of how user stories map from high - level capabilities and tracked through development and testing .

we also interviewed tsa officials , including the tim director and deputy director and the five tim product owners , on their efforts to transition the program to an agile software development framework .

further , we interviewed dhs officials , including the chief technology officer , on their efforts to conduct an agile pilot to assist programs like tim in adopting agile software development processes .

we assessed the evidence against leading practices to determine the extent to which tsa met the practices .

to determine the extent to which tsa and dhs are effectively overseeing and governing the program to ensure that it is meeting cost , schedule , and performance requirements , we identified leading practices and guidance outlined in the following sources: tsa agile scrum guidance cmmi for development , version 1.3 software engineering institute , agile metrics after reviewing the sources listed , we grouped practices related to oversight and governance for programs using agile software development into four key practice areas and we discussed the practices with dhs and tsa officials .

these areas included: document relevant governance and oversight policies and procedures .

monitor program performance and progress .

rely on complete and accurate data to review performance against expectations .

ensure that corrective actions are identified and tracked until the desired outcomes are achieved .

to assess the extent that tsa and dhs had addressed these key practices , we reviewed the most current program management and governance documentation as of april 2017 .

specifically , we analyzed documentation on program management processes , such as tim's systems engineering life cycle tailoring plan , tim agile and technical strategy , tim agile software development contract , and draft dhs agile acquisition program delivery metrics playbook ; and artifacts from tim's program execution and review , such as agile release artifacts , program status reports , contractor status reports , program schedules , life - cycle cost estimates , risk registers , tsa executive steering committee reviews , dhs program health assessments , dhs agile pilot integrated product team meetings , dhs office of the chief technology officer agile pilot reviews , and dhs acquisition review board reviews .

additionally , we interviewed tsa officials , including the tim director and deputy director , on their efforts to oversee tim's development .

further , we interviewed dhs officials , including the chief technology officer , on their efforts to oversee the program's agile software development activities .

we compared this evidence against leading practices to determine the extent to which tsa and dhs met the practices .

to assess the reliability of the data that we used to support the findings in this report , we reviewed relevant program documentation to substantiate evidence obtained through interviews with agency officials .

we determined that the data used in this report were sufficiently reliable for the purposes of our reporting objectives .

we made appropriate attribution indicating the sources of the data .

we conducted this performance audit from september 2016 to october 2017 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , the following staff made key contributions to this report: shannin g. o'neill ( assistant director ) , jeanne sung ( analyst in charge ) , jennifer beddor , rebecca eyler , bruce rackliff , and dwayne staten .

