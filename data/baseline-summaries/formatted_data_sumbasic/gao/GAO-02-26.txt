this report provides you with the results of our review of the u.s. census bureau's accuracy and coverage evaluation ( a.c.e. ) .

person interviewing operation .

a.c.e .

used an independent sample survey to assess the quality of the population data collected by the 2000 census by estimating the number of people missed , counted more than once , or otherwise improperly counted in the census .

partly on the basis of the a.c.e .

results , the acting director of the census bureau recommended on march 1 , 2001 , that the 2000 census tabulations for purposes of redrawing the boundaries of congressional districts not be adjusted , and on october 16 , 2001 , he similarly recommended that unadjusted census data be used for nonredistricting purposes .

these decisions will have far - ranging implications because census data are used to distribute billions of dollars in federal funding , guide public and private investment decisions , and provide a baseline for a number of other statistical measurement programs .

in addition , the results of a.c.e .

are expected to play an important role in the bureau's research and preparation for the 2010 census .

person interviewing was a critical component of a.c.e .

because it was used to collect the sample data used to evaluate the nationwide headcount .

it was conducted from april 24 through september 11 , 2000 .

in our prior work , we noted that the bureau's plans for assessing the quality of census population data faced several methodological , technological , and quality control challenges .

this report is the latest in our series of reviews that examine the results of key census - taking operations and highlight opportunities for reform .

 ( see app .

iv for a list of products issued to date. ) .

as agreed , our objective for this report was to identify the challenges , if any , that the bureau confronted during person interviewing and the degree to which the bureau successfully addressed any challenges .

the census bureau conducted a.c.e .

on a sample of areas across the country to estimate the number of people and housing units missed or counted more than once in the census and to evaluate the final census counts .

the statistical methodology underpinning a.c.e .

assumes that the chance that a person is counted by the census is not affected by whether he or she is counted in a.c.e. , or vice versa .

violating this “independence” assumption can bias the estimate of the number of people missed in the census and thus either overstate or understate the census undercount .

the bureau's procedures called for it to go to great lengths to maintain this independence .

as illustrated in figure 1 , the bureau developed separate address lists — one for the entire nation of about 120 million housing units and one for a.c.e .

sample areas — and collected data through two independent operations .

for the census , the bureau mailed out forms for mail - back to most of the housing units in the country ; hand - delivered mail - back forms to most of the rest of the country ( in an operation called update / leave ) ; and then carried out a number of other follow - up operations , the largest of which was called nonresponse follow - up .

a.c.e .

collected its response data during person interviewing from april 24 through september 11 , 2000 , with telephone calls or visits to about 314,000 housing units .

a.c.e .

person interviewing was managed directly out of 12 a.c.e .

regional offices , independent of the 12 regional census centers from which the census was managed .

a.c.e .

regional offices managed person interviewing workflow at the geographic level of the local census office area out of convenience .

there were 520 local census offices operating during the census , including 9 in puerto rico managed out of the boston regional office , and person interviewing took place in the area of each .

the person interviewing operation had three phases: early telephone interviewing ; then interviewing conducted by personal visits ; and finally nonresponse conversion , when difficult cases were reassigned to the operation's best interviewers to reduce the number of noninterviews .

in each phase , interviewers relied on an automated survey instrument and databases stored on laptop computers assigned to each interviewer .

by having the interviewers use laptops to dial in to the bureau's servers , the bureau could manage cases automatically and remotely .

in its initial design for the 2000 census , the bureau planned a “one - number” census that would have integrated the results of a survey similar to a.c.e .

with the traditional census to provide one adjusted set of numbers by december 31 , 2000 .

however , the u.s. supreme court ruled in january 1999 that statistical sampling could not be used to generate population data for reapportioning the house of representatives .

following that ruling , the bureau abandoned its plans to conduct a one - number census using statistical methods integrated into the final census counts .

on december 28 , 2000 , the bureau delivered its population counts for purposes of reapportioning seats in the house of representatives to the secretary of commerce for his transmission to the president .

on march 1 , 2001 , a committee of senior bureau executives recommended that unadjusted census data be released as the bureau's official redistricting data .

the acting director of the u.s. census bureau concurred , and the secretary of commerce announced on march 7 , 2001 , his decision to release the unadjusted data .

on october 16 , 2001 , the acting director of the census bureau decided that unadjusted data should be used for nonredistricting purposes as well as for postcensus population estimates and for benchmarks for other federal surveys .

the bureau is continuing to investigate issues related to a.c.e .

and the census , and the results of that investigation are expected to influence the bureau's planning for the 2010 census .

to meet our objectives and review the implementation of person interviewing , we examined relevant bureau program and research documents , such as procedures memorandums and analysis of census tests .

further , we reviewed data from the bureau's “cost and progress” management information system , which bureau officials used to monitor the conduct of census and a.c.e .

operations .

to help validate and expand on the cost and progress data , we interviewed bureau headquarters and regional officials .

we also interviewed key bureau officials from headquarters and , where applicable , regional officials responsible for the planning and implemention of the person interviewing operation .

although we verified with bureau officials that the data were final , we did not independently verify data contained in the bureau's cost and progress management information system .

to obtain a local perspective on how person interviewing was implemented , we interviewed temporary a.c.e .

workers in 12 locations , covering over 60 local census office areas ( out of the 520 in the united states and puerto rico ) and corresponding to 8 of the 12 census regions ( atlanta , boston , charlotte , chicago , dallas , denver , los angeles , and seattle ) .

to provide further context , we also interviewed a.c.e .

managers of seven of these regions .

we selected these areas primarily for their geographic dispersion , variation in type of enumeration area , and their proximity to our field offices .

the results of the visits could not be generalized to all person interviewing .

in addition to these field locations , we performed our audit work on eight of the bureau's a.c.e .

regions at bureau headquarters in suitland , md ; as well as in washington , dc , from june 2000 through january 2001 , in accordance with generally accepted government auditing standards .

on september 7 , we requested comments on a draft of this report from the secretary of commerce .

on october 5 , 2001 , the secretary of commerce forwarded written comments from the bureau ( see appendix i ) , which we address in the “agency comments and our evaluation” section of this report .

the bureau appears to have generally completed person interviewing according to its operational schedule .

failure to collect data in a timely manner could have reduced the interview completion rate or increased the bureau's dependence on less reliable sources of data , such as proxy data , thus reducing the quality of data collected .

in addition , the bureau believes that the more time that passes from census day ( april 1 ) to the time of the survey interview , generally the more likely that the survey respondent will err in his or her recall of census day information .

finally , data processing and other operations depended on the data from person interviewing , and could have been delayed had person interviewing not been completed on time .

about 84 percent — 439 — of 520 local census office areas completed all of their fieldwork at least 2 weeks before the end of the operation in their areas .

by the deadline for completing all person interviewing , september 1 , 2000 , only 45 cases ( out of over 314,000 nationally ) remained to be completed , and they were all in the area of a single local census office , which had received an extension of its deadline .

the timely completion of person interviewing was due in part to the bureau's ability to conduct a much higher share of the person interviewing caseload by telephone than it had anticipated .

although the bureau anticipated that about 40,000 cases ( about 13 percent ) of the person interviewing caseload would be completed by telephone , the actual amount was much higher — about 90,000 cases ( over 28 percent ) .

bureau officials informed us that more people provided their telephone numbers on their census returns and more people returned their census forms than the bureau had anticipated .

the telephone phase of the operation was limited to cases in which households had provided telephone numbers with their census responses — about 40 percent of the roughly 314,000 total person interviewing cases .

as figure 2 illustrates , the share of the workload completed by telephone varied across regions , ranging from 19 to 34 percent .

it also varied across local census office areas , ranging from less than 1 to over 55 percent .

bureau officials explained that this variation was related to the eligibility criteria , which further limited telephoning to households with city - style addresses ( for example , 123 main street ) that were not in small multiunit dwellings .

completing interviews over the telephone reduces the travel time for interviewers and can thus decrease the cost of each interview completed .

person interviewing did not progress equally in all local census office areas .

about 3 percent of the national person interviewing caseload had to be reassigned to nonresponse conversion , which took place in the final 2 weeks of interviewing in a given area .

this compares to the about 2 percent of the person interviewing that was completed during an ad hoc nonresponse conversion operation during 1990 .

as shown in figure 3 , about 36 areas out of the 520 had over 10 percent of their cases reassigned to this last phase , and 9 areas had more than one - fifth of their caseloads reassigned to this phase .

the new york region had almost 13 percent of its caseload referred to nonresponse conversion .

bureau officials told us that , in response , the new york region brought in professional interviewers from other nondecennial survey work to conduct its nonresponse conversion , hoping to ensure a high - quality interview process .

during the person interviewing operation , the bureau carried out a quality assurance program , which focused primarily on detecting interviews falsified by interviewers .

bureau officials designed the person interview quality assurance program to detect when interview results had been submitted but the interview had not been done , because bureau research suggested that the most common type of falsification was the falsification of an entire interview .

outside the quality assurance program , a number of operational indicators associated with data quality in the past suggest that data quality may have varied locally .

under the bureau's quality assurance program , regional offices were to telephone or visit a 5-percent random sample of all person interview cases to determine whether an initial person interview had actually taken place .

further , according to headquarters quality assurance managers , regional quality assurance managers were to select about another 5 percent relying on automated “outlier reports” and other criteria .

for example , supervisors were required to select additional cases for quality review when outlier reports showed that an interviewer had a relatively high percentage of vacant housing units or interviews conducted at unusual hours and thus might be falsifying data .

every interviewer was to have at least one case covered by quality assurance .

as an additional check , the bureau provided quality assurance supervisors with reports on respondents' names so that they could look for indicators of possible falsification by interviewers , such as names of famous characters / people or multiple respondents with the same name .

figure 4 illustrates the cumulative share of each of the 12 regions' person interviewing caseload that was reviewed by the quality assurance program .

by the end of the operation , the national share exceeded 11 percent , and each of the 12 regions was near or exceeded the target ratio of about 10 percent of the person interview workload .

headquarters a.c.e .

quality assurance managers said that the percentage of the interview caseload selected for quality assurance review was expected to vary depending on a number of local circumstances .

for example , where quality reviews raised the suspicion of fraudulent interviews , supervisors were to select more of those interviewers' cases for review , further increasing the percentages reviewed by quality assurance in those areas .

as figure 4 illustrates , in some of the regions with higher percentages of cases suspected of falsification by interviewers , supervisors did indeed select a higher percentage of cases for review .

nationally , less than 3 percent of the quality assurance cases were suspected of falsification , although across regions the percentage varied from about 1.3 to 5 percent .

in comparison , bureau evaluations of a 1998 dress rehearsal of person interviewing reported suspected falsifications of from about 0.9 to 3.1 percent of the quality assurance cases at the three different rehearsal sites .

although the sites of the dress rehearsal were not representative of the whole nation , they provide a reasonable benchmark for the 2000 census .

as the bureau noted in its response to our draft report , none of the sites were exceptionally hard - to - enumerate areas , which tend to have higher rates of falsification .

so , that 6 of the 12 regions , each with hard - to - enumerate areas , had their rates fall into this range , demonstrates in part that the bureau's person interviewing experienced low rates of suspected falsification in 2000 .

each case suspected of falsification was to be reinterviewed , as was the entire workload of any interviewer found to have actually falsified a case .

a total of 1,004 cases ( 2.8 percent of the quality assurance caseload ) had their data replaced by these reinterviews .

after further investigating the cases suspected of falsification , bureau officials believed that about 0.1 percent of the randomly selected quality assurance caseload stateside ( 0.2 percent including puerto rico ) was falsified and assumed that this percentage is generalizable to the entire a.c.e .

sample .

the percentage of the replaced interviews that contained errors due to honest interviewer mistakes , poor respondent recall , or reasons other than falsification was not reported by the bureau's quality assurance program as part of the failure rate .

but data that the bureau provided later show that 2.1 percent of all randomly selected cases stateside ( and 2.1 percent including puerto rico ) were replaced .

we discussed the utility of the bureau defining , measuring , and reporting a broader measure of quality assurance failure — including failure for reasons other than falsification — with the associate director for decennial , and he concurred that the bureau should consider this in the future .

all cases in the final phase of person interviewing , nonresponse conversion , were excluded from the quality assurance program .

headquarters officials said that because ( 1 ) the quality assurance was intended primarily to identify falsified interviews , ( 2 ) only the most experienced workers were used in the final phase of interviewing , and ( 3 ) the most experienced workers falsify less , no quality assurance was deemed necessary for that phase .

they also pointed out that there would not have been time to check the cases completed on the last days of the operation , and a relatively small percentage of the total person interview caseload fell into this phase of the operation .

according to bureau data , the nonresponse conversion phase had a workload of about 10,000 cases , or about 3 percent of person interviewing cases nationwide .

according to bureau data , person interviewing collected information on current residents in almost 100 percent of the cases for housing units that existed , were inhabitable , and were not vacant .

following the change in the census design after the 1999 supreme court ruling , the bureau no longer specified a goal or target for person interviewing response rate .

however , this response rate exceeded the 95 percent expectation expressed by the bureau's field directorate in internal memorandums in 1997 , as well as the 98 percent target the bureau set prior to the supreme court ruling and the response rate of 98.6 percent during a similar survey during 1990 — the 1990 post - enumeration survey .

all regions exceeded 99.7 percent , and only three local census office areas had person interview response rates lower than 98 percent .

however , not all of these interviews obtained complete information on the household .

about 5.1 percent of cases nationally were recorded as “partial” interviews — interviews missing either the age , sex , or census day residency status for one or more household members .

the regions varied in their rate of partial interviews from about 1 percent to over 8 percent .

when interviews were not complete , the missing data were to be provided through statistical methods , and this would be a source of error in the resulting a.c.e .

calculations .

and , as we have reported previously , low interview completion rates could have resulted in some segments of the population being underrepresented in the a.c.e .

data , adversely affecting the accuracy of any a.c.e. - based adjustments .

bureau officials believe that numerous studies over the years have shown that their procedures for dealing with missing data have acceptable error levels .

furthermore , bureau data show that about 5 percent of the household interviews were completed with proxy respondents , such as neighbors .

according to bureau research , proxy interviews do not generally provide information as reliable as interviews with household members , and this can be a source of error in a.c.e .

calculations .

proxy interviews are also more likely to provide only partial information .

the 2000 proxy rate exceeded the 2-to - 3-percent proxy rate experienced during the 1990 post - enumeration survey .

the boston region reported as little as 2 percent of its caseload completed by proxy , and the new york region had over 8 percent completed by proxy .

local variations in data quality may affect the accuracy of a.c.e .

results for some segments of the population .

although national level data are important for determining broad trends , they often mask implementation challenges occurring at the local level .

for example , as figure 5 shows , most local census office areas relied less on proxies than the national effort did , but 49 had to complete over 10 percent of their total caseloads with proxy respondents .

bureau officials said that the local areas with the highest proxy rates tended to be dense urban areas , such as in new york city , where buildings may have had restricted access , and interviewers had to rely on apartment managers for information .

similarly , most local census office areas had rates of partial interviews near or below the national rate of about 5.1 percent , but 37 had rates exceeding 10 percent , as shown in figure 6 .

bureau officials explained that many of these areas with the highest partial interview rates were areas with higher proxy rates as well because proxy interviews are less likely to provide complete data .

most local areas were near or below the nation's 0.1 percent final nonresponse rate on person interviewing cases , although one area had a nonresponse rate of 2.3 percent , and nine local areas exceeded 1 percent .

most local census office areas completed less than or about the nationwide 3 percent of their caseload during nonresponse conversion .

as shown in figure 3 , the percentage of the person interviewing workload completed during nonresponse conversion exceeded 10 percent in 36 of the 520 local census office areas and 20 percent for 9 areas .

as discussed earlier , nonresponse conversion was not subject to the quality assurance program , although the bureau relied on its best workers for this stage of interviewing .

the bureau reports that quality assurance was not done during nonresponse conversion because that stage involved getting cooperation from uncooperative respondents , and the later a.c.e .

field operation , person follow - up , would serve as a form of quality assurance on these interviews .

early in the person interviewing operation , the bureau experienced and resolved problems with a critical function in its automated work management system that was to allow supervisors to selectively reassign work among interviewers .

the software was to enable supervisors to reassign cases that had either been sent to them for review or that needed to be reassigned from a laptop computer that was either broken or had been issued to someone who was no longer interviewing .

for cases being reassigned that had not been flagged for supervisory review , the software was to ask the supervisor whether to disable the cases on the original laptop .

if the cases were being reassigned to a different interviewer , supervisors were to disable the cases .

however , according to bureau officials , the software contained errors in two different places .

one error resulted in cases not being disabled from an interviewer's laptop even after the supervisor attempted to disable them .

another error resulted in certain cases being reassigned automatically , again without them being disabled on the original laptop .

both problems resulted in duplicate records on the laptops , which required supervisors to individually review and delete cases .

according to bureau officials , this confused some temporary staff and their supervisors and created some work inefficiencies .

for example , some households received unplanned multiple visits by different interviewers .

however , according to bureau a.c.e .

officials , the bureau addressed the underlying programming error within 2 weeks , and the operation proceeded without a reported recurrence of this problem .

an accurate address list avoids unnecessary and costly efforts to locate nonexistent residences .

a measure of address list quality is the percentage of addresses that are nonexistent or “undeliverable” because interviewers were unable to locate housing units at the listed addresses .

during person interviewing , 1.4 percent of housing units to be interviewed were deemed to not exist , and this was less than for other major census questionnaire delivery operations .

table 1 illustrates that the two primary census questionnaire delivery operations both experienced initial undeliverability rates greater than the share of nonexistent housing units during person interviewing .

in comparison , person interviewing during the 1990 post - enumeration survey also encountered a higher share of its caseload being undeliverable than did person interviewing in 2000 .

the list of addresses visited during person interviewing benefited from earlier a.c.e .

operations that ( 1 ) independently canvassed all addresses in a.c.e .

areas , ( 2 ) compared the initial a.c.e .

address list to the initial census address list , ( 3 ) reconciled any differences by field visits , ( 4 ) flagged nonexistent addresses in a.c.e .

sample areas , and ( 5 ) entirely relisted some areas that were canvassed improperly .

person interviewing attempted to visit only addresses that had been confirmed to exist in a.c.e .

sample areas .

as noted earlier , the bureau's procedures called for it to go to great lengths to ensure that a.c.e .

operations were kept independent of the census operations to avoid biasing a.c.e .

estimates .

for person interviewing , this meant conducting the operation after the bureau completed nonresponse follow - up activities in a local census office area ; implementing controls to prevent their overlap ; sharing status information about nonresponse follow - up with a.c.e .

managers ; and managing field activities out of 12 separate regional census offices , independent of the 12 regional census centers managing the rest of the census .

however , in response to the 1999 supreme court ruling against the planned use of sampling to generate population data for reapportioning the house of representatives , the bureau reintroduced a census follow - up operation intended to improve census coverage in part by sending enumerators to households that were added to the census address list late and thus may have been missed by earlier census operations .

the schedule of this operation , known as “coverage improvement follow - up,” overlapped the beginning of person interviewing , thus increasing the risk that it would violate the independence assumption .

according to bureau officials , a similar operation had overlapped person interviewing for the post - enumeration survey in 1990 , and delaying person interviewing further from census day would have increased the risk that respondents would not reliably recall their census day data .

the risk of violating the independence assumption was increased further when the workload of the follow - up operation increased over what was projected .

in december 1999 the estimated workload volume for coverage improvement follow - up was about 7.7 million addresses .

most of these were to have verified the vacancy or nonexistence of housing units previously marked vacant or for deletion .

but up to about 0.8 million addresses were more likely to have enumeration interviews take place .

a.c.e .

designers believed that the number of coverage improvement cases in a given area would not be enough to affect a.c.e .

data collection .

on the basis of the actual number of addresses being covered by the operation , by june 2000 this number had risen to about 2.3 million addresses .

in addition , after person interviewing had begun , the bureau decided to revisit every census household for which the population count was unknown .

according to bureau sources , there were about 0.7 million such households .

although the bureau had strict controls to prevent person interviewing from going door - to - door in areas where census nonresponse follow - up — the primary census field follow - up operation — was still under way , the controls did not apply to other census follow - up operations , such as coverage improvement follow - up .

automated work management rules were to prohibit person interviewing field visits from beginning in a local census office area prior to the earlier of either ( 1 ) 100 percent completion of nonresponse follow - up in that local census office area or ( 2 ) 1 week after 90 percent completion of nonresponse follow - up in all a.c.e .

clusters in that local census office area .

in addition , a.c.e .

management had access to “early warning reports” that provided the daily status of nonresponse follow - up in each area .

according to the bureau , exceptions to the start rules had to be approved in headquarters , and the only software that would allow an earlier start to personal visits was located at headquarters .

the bureau also informed us that the regional offices did not have the ability or the authority for exceptions to be implemented , as any changes required at least assistant division chief level approval .

according to the assistant field division chief for evaluation and research , to his knowledge , no such approvals had been given .

he said that these rules did not apply to other follow - up operations .

each of the eight regional directors ( there are 12 in all ) or their deputies with whom we spoke regarding a.c.e .

independence said that no significant overlap occurred between concurrent census follow - up and person interviewing .

however , most of them believed that some overlap was likely , and none of them could be certain of the extent of any actual overlap .

moreover , all of these regional directors or their a.c.e .

management staffs also reported not having any communication from the census side of their operations to the a.c.e .

operations on the status of these follow - up operations beyond that on the nonresponse follow - up work , underscoring their inability to control the possible overlap of census and a.c.e .

fieldwork .

prior bureau research on sample data collection detected few possible effects of overlap ; and , at the time the coverage improvement follow - up operation was reintroduced , bureau officials concluded that there was no significant risk to independence .

the chief of the bureau's decennial statistical studies division said that on the basis of his experience and understanding of prior bureau research , the small risk of compromising independence was worth taking to reduce the risk of increased errors from delaying person interviewing until the bureau completed coverage improvement follow - up .

he and other headquarters officials we interviewed were unaware of any bureau attempts to determine the extent of any possible interview overlap in 2000 , which might demonstrate whether a.c.e .

assumptions were operationally supported .

the bureau recently completed , as part of its census 2000 evaluation program , a study intended to detect significant differences between the census responses in comparable a.c.e .

and non - a.c.e .

blocks .

the study found no differences it deemed significant .

the bureau largely overcame significant challenges that could have undermined the person interviewing operation .

notably , the bureau completed the person interviewing data collection on schedule and in accordance with its general guidelines for quality assurance coverage .

the bureau also demonstrated its ability to overcome the limited technical challenges it confronted .

furthermore , the series of a.c.e .

address operations , as designed , appeared to effectively remove nonexistent housing units and addresses from the person interviewing caseload , thus reducing an otherwise inefficient use of interviewing time and resources .

still , the bureau's experience in implementing person interviewing highlights areas where additional research might lead to improvements if the bureau conducts a similar operation for the 2010 census .

for example , certain operational challenges may have contributed error to final a.c.e .

estimates of census undercounts and overcounts .

the bureau experienced variation at the local level in how person interviewing was carried out , in terms of response rates , proxy rates , and partial interview rates .

as we have reported before , if the local census office areas with the worst values of each of these measures have populations that are typically hard to count in the census , these segments of the population may be underrepresented in the a.c.e .

data , possibly leading to inaccurate reflections of these population segments in a.c.e. - based adjustments .

the bureau plans to evaluate the relationship between operational measures , such as proxy rates , and how well a.c.e .

data match to census data .

the results of these evaluations , and others , will provide an important basis for planning an improved 2010 census and evaluation survey .

further , although bureau data show that the person interviewing quality assurance program met its objectives , the program focused primarily on identifying falsification and reported failure rates based solely on cases believed to have been falsified .

as the bureau looks to improve its interviewing experience further , a broader definition of quality assurance failure to include interviews the bureau reinterviewed and replaced for other reasons would provide a more complete measure of interviewing quality .

finally , the same controls and sharing of status information to ensure independence between the census nonresponse follow - up operation and a.c.e .

person interviewing were not applied or did not take place with other census follow - up operations , thus increasing the risk of compromising the independence of a.c.e .

a relatively small part of the census follow - up workload was not subject to control over its possible overlap with a.c.e .

person interviewing and thus the magnitude of this influence may have been small nationally ; however , it could potentially have been significant in some local areas .

to that extent , the a.c.e assumptions may not apply equally in all areas or for all segments of the population , with possible adverse effects on the accuracy of a.c.e .

calculations .

since the bureau will likely use an evaluation survey in 2010 , perhaps similar to a.c.e. , it will be important for the bureau to learn the lessons from the 2000 census that can be incorporated into the planning for 2010 .

as the bureau documents its lessons learned from the 2000 decennial census and as part of its planning efforts for 2010 , we recommend that the secretary of commerce conduct research that determines the relationship , if any , between operational measures of person interviewing , such as proxy rates , and the accuracy of a.c.e .

estimates of census undercounts as planned ; determines how best to define , measure , and report interview quality failure rates that include interviews rejected for all reasons , and not just for a subset of reasons such as falsification ; determines and documents the extent , if any , of the actual overlap between census follow - up operations and a.c.e .

person interviewing in 2000 ; determines whether sufficient overlap may have occurred to violate the determines whether increasing the flow of status data on specific decennial follow - up operations to the managers of independent surveys can help ensure the independence of such surveys , particularly when such operations are scheduled to overlap in the field ; and determines what additional steps or controls to preserve the independence of census follow - up and person interviewing , if any , could be implemented for other census follow - up operations that collect enumeration data and are scheduled contemporaneously with person interviewing .

the secretary of commerce forwarded written comments from the bureau on a draft of this report .

 ( see appendix i ) .

the bureau provided minor technical corrections and additional information .

the bureau also offered clarification on some of our key points and recommendations , which we have reflected in this final report and comment on in more detail in appendix i .

regarding our finding that census follow - up operations overlapped with person interviewing in the field , the bureau provided additional information on its decision to permit overlap between census coverage improvement follow - up and the a.c.e .

person interviewing operations .

we recognized this context , and revised the draft to better reflect it .

nevertheless , while the bureau's response justifies its not adding any controls or communications or changing any procedures when it noticed that the increase in the follow - up workload was over what was projected , as we note in the report , the increase in workload increased the risk that the independence assumption was violated .

there may still be opportunities to implement steps in the future to help ensure the independence of such surveys .

the bureau commented that our conclusion linking variations in data quality to possible effects on the accuracy of a.c.e .

results was unsubstantiated and suggested wording it as a question .

in our draft report , we had raised the link as a possibility and then recommended that the relationship , if any , be determined between operational measures and the accuracy of a.c.e .

estimates .

we believe that this conclusion is logical given other bureau reporting linking data quality measures such as missing data rates to possible errors in a.c.e .

results .

we have also reported on this issue in the past .

the bureau said that our conclusion that the influence of census and a.c.e .

overlap may have been significant in some local areas was unsubstantiated .

we were unable to conclude whether significant overlap had occurred or not .

as we noted in our report , the bureau officials we interviewed were unaware of any bureau attempts to determine the extent of any possible interview overlap in 2000 , which might have demonstrated whether a.c.e .

assumptions were operationally supported in the field .

without such evidence regarding the extent of the overlap , and given the anecdotal evidence , which the bureau cites in its response and we mentioned in the draft report , that some overlap did occur , we view the conclusion that the overlap may have been significant in some areas as appropriate .

we revised the text to more clearly state , however , that the effect of the overlap is a potential one .

in responding to our recommendations ( 1 ) to determine the relationships , if any , between operational measures and the accuracy of a.c.e .

estimates , as planned , and ( 2 ) to determine and document the extent of overlap between census and a.c.e .

in 2000 , the bureau acknowledged the importance of extensive evaluations of a.c.e. , and referred to the evaluation it is undertaking .

we look forward to reviewing this evaluation when it is complete .

since receiving comments from the bureau , we added one additional recommendation .

the basis for this new recommendation centered on our finding that the bureau's quality assurance program did not report fully on the percentage of the interview workload replaced by the quality assurance interviews .

although we recognize that the quality assurance program was designed primarily to detect falsification , the definition of quality assurance “failure” used by the bureau excluded the sources of error other than falsification .

after receiving the bureau's response , we discussed this with the associate director for decennial at the bureau , who concurred that the bureau should consider a broader definition in the future .

we have added an additional recommendation for executive action accordingly .

in responding to our recommendation to determine whether sufficient overlap occurred to violate the independence assumptions , the bureau referred to its recent evaluation of the possible contamination of census data collected in a.c.e .

blocks , as well as several other similar studies throughout the decade .

some of these studies find weak or only limited indications of contamination of census data in prior censuses , and they all conclude that there was no systemic contamination of census data .

the bureau's most recent evaluation , which is consistent with our recommendation , was released after our audit work was completed .

we have revised the draft accordingly .

we are sending copies of this letter to other interested congressional committees .

please contact me on ( 202 ) 512-6806 if you have any questions .

other key contributors to this report are included in appendix ii .

the following are gao's comments on the department of commerce's letter dated october 5 , 2001 .

the bureau generally provided minor technical corrections and additional information .

the bureau also clarified some of our key points and recommendations , which we have reflected in this report and comment on further below .

1 .

the bureau noted that figures used throughout the draft report appeared to be inconsistent .

we met with bureau officials and determined that the apparent discrepancies were due to several factors , including the following: ( 1 ) our data included puerto rico while bureau data covered only the 50 states , ( 2 ) the bureau initially miscounted the total number of local census offices in its comparison , ( 3 ) bureau results include data from 1,004 quality assurance interviews that replaced the data for the initial interviews , and ( 4 ) an error exists in how the bureau's cost and progress data , upon which we relied , report the number of proxy interviews .

in some cases , the bureau provided us with additional data , and this final report reflects minor changes based on that new information .

none of these data changes were significant enough to affect either our conclusions or recommendations .

see also comment 13 .

2 .

the bureau suggested that additional detail be included in figure 1 to indicate that both housing unit matching and person matching operations comprised separate clerical and field follow - up components .

we recognize the complexity of those matching operations and will be issuing a separate report on the person matching operation soon .

however , to maintain clarity in the figure , we chose not to include such additional detail on the a.c.e .

operations that were not the subject of this report .

3 .

throughout its response the bureau suggested various revisions , technical corrections , and clarifications .

we revised the report accordingly .

4 .

the bureau provided additional information on its decision to permit overlap between census coverage improvement follow - up and the a.c.e .

person interviewing operations .

the bureau pointed out that ( 1 ) the decision was a conscious one , made in advance of person interviewing , ( 2 ) delaying person interviewing until after coverage improvement follow - up was completed in a local census office area would introduce considerable risk , ( 3 ) managing person interviewing on levels of geography below the local census office area would have been impossible , ( 4 ) a.c.e .

designers believed that the number of coverage improvement cases in an area would not have an effect on a.c.e .

data collection , ( 5 ) there was never any intent to have operational controls in place between these two operations , and ( 6 ) ad hoc procedures without prior headquarters approval were prohibited for a.c.e .

we recognize this context , and revised the main body of the report to better reflect it .

nevertheless , while the bureau's response helps explain why it did not change its procedures or add any new controls or communications when it noticed the increase in the follow - up workload over what was projected , as we note in the report , the increase in workload increased the risk that the independence assumption was violated .

there may still be opportunity to implement steps in the future to help ensure the independence of such surveys .

5 .

the bureau's st. louis , missouri , office should be deleted from the list of census bureau regional offices and replaced with kansas city , kansas .

we revised the text accordingly .

6 .

the bureau noted that factors other than the number of people entering their phone numbers on census forms could have accounted for the higher share of person interviewing completed by telephone than was expected .

the bureau suggested that the higher than expected mail return rate of census forms was also a likely factor in the higher telephone interview rate , since this also could have increased the pool of census forms possibly having telephone numbers recorded on them .

our explanation in the draft report was based on interviews with senior bureau staff in the field division .

however , we agree that the mail return also helps explain the higher telephone interviewing rate , and revised the draft accordingly .

7 .

the bureau noted that proxy interviews are known indicators of insufficient data quality and not fabrication , as our draft report had suggested .

the bureau noted that indicators of falsification included missing telephone numbers and work days with more than 13 cases .

our draft report was based on language in bureau training documents for field managers ; however , we revised the text to reflect the bureau comment .

8 .

the bureau objected to our comparison of suspected falsification rates in the 2000 census to those obtained at the three 1998 dress rehearsal sites , since the three sites were not representative of the nation .

while we agree that the sites are not representative of the nation , and we revised the report to clarify this , we believe that the dress rehearsal comparison can provide a reasonable benchmark for regions since , as the bureau notes , none of the dress rehearsal sites were “exceptionally hard - to - enumerate,” and the bureau believes that hard - to - enumerate areas tend to have higher rates of falsification .

we revised the draft to note that half of the regions had falsification rates that fell into the low range of the dress rehearsal sites , even though each region contained hard - to - enumerate areas .

9 .

the bureau commented that figure 4 , which illustrates the regional rates of quality assurance coverage compared to regional rates of suspected falsifications , was “very misleading.” the bureau said that the figure appeared to attempt to demonstrate whether supervisors were properly following up on cases suspected of falsification .

that is not our intent , and our draft did not contain such an implication .

we noted in our draft report that the percentage of the interview caseload selected for quality assurance review was expected to vary depending on a number of local circumstances .

we reported data on falsification rates only as an example , and because they had been cited as a primary local circumstance during earlier interviews with bureau staff .

10 .

the bureau commented that its quality assurance program did in fact measure whether cases contained errors due to honest interviewer mistakes , poor respondent recall , or reasons other than falsification .

the bureau noted that for all replacement cases , it determined whether cases were falsified or fell into the other categories .

we revised our report accordingly .

however , the quality assurance failure rate that the bureau calculated and reported includes only those interviews replaced for falsification .

we recognize that the quality assurance program was designed primarily to detect falsification , but this definition of “failure” excludes the other sources of rejected interviews and thus understates the rate at which interviews failed to meet bureau quality standards .

after receiving the bureau's response , we discussed this with the associate director for decennial at the bureau , who concurred that the bureau should consider the broader definition in the future .

we have added an additional recommendation for executive action accordingly .

11 .

the bureau commented that while imputation for missing data undoubtedly leads to some error , the bureau had numerous studies over the years showing that its imputation procedures had acceptable error levels .

while this explains why the presence of missing data in person interviewing should not by itself be alarming , it does not justify ignoring levels of missing data , the operational quality measures that contribute to missing data , or the methods chosen by the bureau to deal with missing data .

for example , the bureau recently reported that a variety of alternative statistical models for dealing with missing data gave a wide range of results implying widely varying effects on a.c.e .

estimates .

the same report suggested that further research was needed to study these effects .

the bureau commented that local variability in data quality indicators is unavoidable given variations in local populations and localities .

we agree and in our draft report noted that data quality did in fact vary during person interviewing .

the bureau commented that for most surveys , comparing quality indicators of certain regions would be of little value .

the regional comparisons that we made in our report are across the 12 census regions .

with the exception of our inclusion of puerto rico data with the boston region data — the census region in which data collection in puerto rico was managed and under which census operational data was tabulated in data provided to us by the bureau — the bureau reported comparisons of data across the same regions , and of many of the same variables , in a technical memorandum it published in march 2001 .

the bureau suggested that comparisons controlling for demography and geography would be more appropriate to assess the extent of local quality variability .

given the bureau's acknowledgment that quality variability is unavoidable and that our presentation of local census office area data corroborates that subregional variability exists , we believe that additional comparisons like those suggested by the bureau are unnecessary to make the general point that variation exists .

12 .

the bureau believed that our conclusion linking variations in data quality to possible effects on the accuracy of a.c.e .

results was unsubstantiated and suggested wording it as a question .

in our draft report we raised the link as a possibility and then recommended that the relationship , if any , be determined between operational measures and the accuracy of a.c.e .

estimates .

we believe that the conclusion is logical given our prior work linking data quality measures such as missing data rates to errors in a.c.e .

results , and provided additional support for making this link .

13 .

the bureau noted that there were 52 local census office areas that had to complete over 10 percent of their total caseload with proxy respondents , and not the 42 that we had reported in our draft report .

before receiving the secretary's response , the bureau had provided us with additional data .

based on that later data , we counted 49 local census office areas that had to complete over 10 percent of their total caseload with proxy respondents , and we revised the draft text and related figure 5 accordingly .

see also comment 1 .

14 .

the bureau said that our conclusion that the influence of census and a.c.e .

overlap may have been significant in some local areas was unsubstantiated .

as we noted in the draft report , the bureau officials we interviewed were unaware of any bureau attempts to determine the extent of any possible interview overlap in 2000 .

such data , if available , might demonstrate whether a.c.e .

assumptions were operationally supported in the field .

we saw no data on the impact of the overlap that occurred , and we revised the text to state more clearly that the influence of the overlap was a potential one .

see also comment 15 .

15 .

the bureau acknowledged the importance of extensive evaluations of the a.c.e. , and referred to the evaluation it is undertaking .

we look forward to reviewing this evaluation when it is complete .

16 .

the bureau referred to its recent evaluation of the possible contamination of census data collected in a.c.e .

blocks , as well as several other similar studies throughout the decade , and saw no need for further work .

some of these studies find weak or only limited indications of contamination of census data in prior censuses , and they all concluded that there was no systemic contamination of census data .

the bureau's most recent evaluation , which is consistent with our recommendation , was released after our audit work was completed .

we have revised the draft accordingly .

17 .

the bureau said that it was reassessing its approach to coverage measurement .

the bureau gave assurance that it would appraise these recommendations with respect to the approaches under consideration .

we look forward to reviewing this appraisal when it is complete .

in addition to those named above , ty mitchell , lynn wasielewski , angela pun , richard hung , janet keller , lara carreon , and staff from our denver , los angeles , norfolk , and seattle offices contributed to this report .

