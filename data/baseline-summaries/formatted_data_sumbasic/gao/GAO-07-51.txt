the navy marine corps intranet ( nmci ) program is a multiyear information technology ( it ) services program ; its goals are to provide information superiority and to foster innovation via interoperability and shared services .

the navy awarded the nmci services contract — currently valued at $9.3 billion — to electronic data systems ( eds ) in october 2000 .

the contract calls for eds to replace thousands of independent networks , applications , and other hardware and software with a single , internal communications network ( intranet ) , and associated desktop , server , and infrastructure assets and services for navy and marine corps customers ( end users , network operators , and commanders ) .

because of the size and importance of nmci , as well as continuing widespread congressional interest , we prepared this report under the comptroller general's authority as part of a continued effort to assist congress and reviewed ( 1 ) whether the program is meeting its strategic goals , ( 2 ) the extent to which the contractor is meeting its service level agreements ( sla ) , ( 3 ) whether customers are satisfied with the program , and ( 4 ) what is being done to improve customer satisfaction .

to accomplish these objectives , we reviewed program documentation , analyzed performance data ( including those related to slas and customer satisfaction surveys ) , reviewed collection processes and results , met with customers at several large nmci sites ( navy shipyards and air depots ) to discuss their level of satisfaction , and interviewed officials from the program office , the navy's chief information officer's ( cio ) office , and eds .

we performed our work from april 2005 to august 2006 , in accordance with generally accepted government auditing standards .

details on our objectives , scope , and methodology are in appendix i .

the department of the navy is a large and complex organization with a wide range of mission operations and supporting business functions .

for example , the navy has about 350,000 active duty officers and enlisted personnel , 130,000 ready reserve , and 175,000 civilian employees .

navy's fleet operations involve approximately 280 ships and 4,000 aircraft operating throughout the world .

further , the navy's annual operating budget is about $120 billion and is used to fund such things as ship and aircraft operations , air depot maintenance , and marine corps operations .

the department's primary organizational components are the secretary of the navy , the chief of naval operations , and the commandant of the marine corps .

the structural relationships among these components are summarized later and in figure 1 .

secretary of the navy: department of the navy headquarters recruits , organizes , supplies , equips , trains , and mobilizes , naval forces .

among other things , this includes construction , outfitting , and repair of navy and marine corps ships , equipment , and facilities .

it also includes formulating and implementing policies and programs .

naval and marine corps operating forces: the operating forces commanders and fleet commanders have two chains of command .

administratively , they report to the chief of naval operations , and are responsible for providing , training , and equipping naval forces .

operationally , they provide naval forces and report to the appropriate unified combatant commanders .

the operating forces include a variety of organizations with diverse missions , such as the atlantic and pacific fleets , naval network warfare command , and naval reserve forces .

naval shore establishment: the navy shore establishment includes facilities and activities for repairing machinery , electronics , ships , and aircraft ; providing communications capabilities ; providing training ; providing intelligence and meteorological support ; storing repair parts , fuel , and munitions ; and providing medical support .

it consists of organizations such as the naval sea systems command ( which includes shipyards ) , naval air systems command ( which includes aviation depots ) , space and naval warfare systems command , navy personnel command , naval education and training command , and the office of naval intelligence .

the navy's many and dispersed organizational components rely heavily on it to help them perform their respective mission operations and business functions .

for fiscal year 2006 , the navy's it budget was about $5.8 billion , which included funding for the development , operation , and maintenance of navy - owned it systems , as well as funding for contractor - provided it services and programs , such as nmci .

the assistant secretary of the navy for research , development and acquisition is responsible for navy acquisition programs .

reporting to the assistant secretary are numerous entities that have authority , responsibility , and accountability for life - cycle management of acquisition programs within their cognizance .

these entities include certain program managers , system command , and program executive officers .

the navy chief information officer ( cio ) is responsible for developing and issuing it management policies and standards in coordination with the above assistant secretary , the system commands , and others .

the navy cio is also responsible for ensuring that major programs comply with the clinger - cohen act ( 1996 ) and for recommending to the secretary of the navy whether to continue , modify , or terminate it programs , such as nmci .

nmci is a major , navy - wide it services program .

its goals are to provide information superiority — an uninterrupted information flow and the ability to exploit or deny an adversary's ability to do the same — and to foster innovative ways of operating through interoperable and shared network services .

the program is being implemented through a multiyear it services contract that is to provide desktop , server , infrastructure , and communications - related services at navy and marine corps sites located in the united states and japan .

through this contract , the navy is replacing independent local and wide area networks with a single network and related desktop hardware and software that are owned by the contractor .

among other things , the contractor is to provide voice , video , and data services ; infrastructure improvements ; and customer service .

this type of contract is commonly referred to as “seat management.” generally speaking , under seat management , contractor - owned desktop and other computing hardware , software , and related services are bundled and provided on the basis of a fixed price per unit ( or seat ) .

in october 2000 , the navy's goal was to have between 412,000 and 416,000 seats operational by fiscal year 2004 .

as of june 2006 , the navy reported that about 303,000 seats were operational at about 550 sites .

according to the navy , initial delays in meeting deployment schedules were due to underestimates in its existing inventory of legacy applications that needed to be migrated to nmci .

subsequent delays were attributed to developing and implementing a certification and accreditation process for all applications , as well as legislation requiring certain analyses to be completed before seat deployment could exceed specific levels .

the number of seats at each site ranges from a single seat to about 10,000 .

these sites include small sites , such as office facilities located throughout the united states , and large sites , such as shipyards and air depots , which use unique software to assist in repair work .

various organizations in the navy are responsible for nmci management and oversight ( see fig .

2 ) .

the program executive officer for enterprise information systems ( peo - eis ) along with the nmci program manager are responsible for nmci acquisition and contract management .

the program is also overseen and supported by several groups .

one is the navy's information executive committee , which provides guidance for , and oversight of , nmci and other information issues .

the committee is made up of cios from a range of navy commands , activities , offices , and other entities within the navy .

another is the nmci executive committee , which includes representatives of the heads of a broad cross section of organizations throughout the navy , and the contractor .

its mission is to help in the review , oversight , and management of the navy's implementation of nmci , as well as to assist in identifying and resolving process and policy impediments within the navy that hinder an efficient and effective implementation process .

additionally , the network warfare command ( netwarcom ) and the marine corps network operations and security command ( mcnosc ) , are the two entities primarily responsible for network operations management in the navy and marine corps , respectively .

the navy cio is responsible for overall it policy .

on october 6 , 2000 , the navy awarded a 5-year contract for nmci services to a single service provider — eds — for an estimated 412,000 to 416,000 seats and minimum value of $4.1 billion .

the original contract also included a 3-year option for an additional $2.8 billion in services , bringing the potential total contract value to $6.9 billion .

the department and eds subsequently restructured the contract to be a 7-year , $6 billion contract with a 3-year option for an additional $2.8 billion beginning in fiscal year 2008 .

following further contract restructuring and the navy's decision to exercise the 3-year option , the total contract period and minimum value is now 10 years and about $9.3 billion .

figure 3 illustrates the value of the nmci contract .

the nmci contract type is commonly referred to as seat management because pricing for the desktop services is based on a fixed price per “seat.” seats include desktop computers , as well as other devices , such as cellular phones .

pricing for these seats varies depending on the services provided .

for example , having classified connectivity , mission - critical service , additional user accounts , or additonal software installation increases the amount paid per seat .

the nmci contract is performance - based , which means that it contains monetary incentives to provide services at specified levels of quality and timeliness .

the contract includes several types of incentives , including incentives tied to sla performance , and customer satisfaction surveys .

the contract currently specifies 23 slas divided into three tiers: 100 slas , 200 slas , and 300 slas .

the 100 tier is referred to as base agreements , the 200 as transitional agreements , and the 300 as additional agreements .

examples of agreements for each tier are provided below .

100 — end user services ( sla 103 ) 200 — web access services ( sla 206 ) 300 — network management services ( sla 328 ) slas are further categorized as enterprisewide , site - specific , or both .

unlike site - specific slas , enterprisewide slas are not analyzed on a site - by - site basis .

see table 1 for a list of agreements organized by tier and category .

each agreement has one or more performance categories .

for example , sla 102 has 1 performance category ( network problem resolution ) , while sla 107 has 3 performance categories ( nmci intranet availability , latency / packet loss , and voice and video quality of service ) .

collectively , there are 51 performance categories .

each performance category has specific performance targets that the contractor must reach in order for the category to be met .

an example of a target is providing e - mail server services to users 99.7 percent of the time that they are supposed to be available .

the contract currently specifies two levels of performance to be used in determining , on a site - by - site basis , what performance - based payment incentives , if any , eds will earn in a given quarter ( 3-month period ) .

if either of these levels of performance is not met , the contractor is to be paid 85 percent of the amount allowed under the contract for each seat that has been cut over ( i.e. , is operational ) .

1 .

full payment .

to achieve this level for a given seat , the contractor must meet 100 percent of the applicable slas for that seat , and 50 to 90 percent of the planned seats at the site must be cut over .

meeting a quarterly agreement is defined as performance at or above the applicable target ( s ) for either ( 1 ) 2 out of the 3 months preceding an invoice or ( 2 ) the current month of the invoice .

if these conditions are met , the contractor is paid 100 percent of the amount allowed per seat .

if , in subsequent months , the contractor fails to achieve 100 percent of the agreements , the amount paid is 85 percent of the amount allowed per seat .

2 .

full performance .

to achieve this level for a given seat , the contractor must meet 100 percent of the applicable slas for that seat , and over 90 percent of the planned seats at the site must be cut over .

meeting an agreement is defined as performance at or above the target ( s ) for either ( 1 ) 2 out of the 3 months preceding a quarterly invoice or ( 2 ) the current month of the invoice .

if these conditions are met , the contractor is paid 100 percent of the amount allowed per seat .

once a site has achieved full performance , it remains eligible for full payments , regardless of changes to the numbers of seat orders .

however , the contractor is required to provide “financial credits” to the navy in the event that the agreements are not met at some future time .

the contract also provides for administration of three customer satisfaction surveys: end user , echelon ii / major command , and network operations leaders .

these surveys and their related financial incentives are discussed below .

the contractor began conducting quarterly satisfaction surveys of navy end users in june of 2002 and marine corps end users in march 2005 .

these surveys are administered to a different mix of 25 percent of eligible users each quarter , with nearly all users being surveyed each year .

since march 2004 , the survey has consisted of 14 questions , all relating to satisfaction with the nmci program and 10 focusing on satisfaction with eds .

for each question , users are asked to indicate their level of dissatisfaction / satisfaction according to a 10-point scale , with 1-5 denoting levels of dissatisfaction , and 6-10 denoting levels of satisfaction .

the navy considers end users to be satisfied in general , with the program , or with the contractor , if the average response across the 14 , 4 , or 10 questions , respectively , is 5.5 or higher .

the survey instrument also includes space for additional comments and asks the end users to identify and rank reasons for dissatisfaction or suggestions for improvements .

see table 2 for a list of the 14 questions .

based on the quarterly survey results , the contractor is eligible for an incentive payment of $12.50 per seat if 85 to 90 percent of the average responses is 5.5 or higher , and $25 per seat if greater that 90 percent respond in this way .

no incentive is to be paid if fewer than 85 percent respond as being satisfied .

echelon ii ( navy ) and major command ( marine corps ) commander survey and network operations leader survey in october 2004 , the navy designated two additional categories of customers — commanders and network operations leaders — and developed separate satisfaction surveys for each .

in general , the commander survey focuses on whether nmci is adequately supporting a command's mission needs and strategic goals ; the network operations leader survey focuses on whether the contractor is meeting certain operational network requirements .

the surveys are administered every 6 months .

the latest commander survey was distributed to the heads of 23 navy and marine corps command units .

the network operations leader survey was distributed to netwarcom and mcnosc .

both surveys are organized by major topic and subtopic .

for the commander survey , the major topics and subtopics are as follows: warfighter support — including classified network support , deployable support , and emergent requirement support .

cutover services — including planning , preparation , and execution .

technical solutions — including the new service order and delivery process , and technical performance .

service delivery — including organizational understanding , customer service , and issue management .

for the network operations leader surveys , the major topics and subtopics are as follows: mission support and planning — including interoperability support , continuity of operations , future readiness , and public key infrastructure .

network management — including network status information , information assurance , urgent software patch implementation , and data management .

service delivery — including organizational understanding , communications , issue management , and flexibility and responsiveness .

appendix ii provides a complete listing of the questions included in the commander survey and the network operations leader survey .

responses to the questions in both surveys are solicited on a scale of 0-3 , with 0 being dissatisfied , and 3 being extremely satisfied .

to aggregate the respective surveys' results , the navy averages the responses by command units , and network operations units .

based on the 6-month survey results , the contractor is eligible for an incentive payment of up to $50 per seat , with average scores of less than 0.5 receiving no incentive , 0.5 to less than 1.5 receiving 25 percent of the incentive , between 1.5 to less than 2.25 receiving 50 percent of the incentive , and at least 2.25 receiving 100 percent of the incentive .

we have reported on a number of nmci issues since the program's inception .

for example , in march 2000 , we reported that the navy's acquisition approach and implementation plan had a number of weaknesses , and thus introduced unnecessary program risk .

in particular , we said that the navy lacked a plan for addressing many program requirements and information on nmci's potential impacts on navy personnel .

in october 2002 , we reported that nmci's transition costs for shipyards and air depots was unclear , which in turn limited the ability of such industrially funded entities to set the future rates that they would charge their customers .

accordingly , we recommended that the program , in collaboration with the naval sea systems command and the naval air systems command , systematically and expeditiously resolve implementation issues that affect the ability of shipyards and depots to plan and budget .

in response to these recommendations , the navy took a number of actions , including establishing an executive customer forum to , among other things , adjudicate issues requiring collaborative decision making among navy component cios , including those from the naval sea systems command and the naval air systems command , which represent navy shipyards and air depots , respectively .

in april 2003 , we reported on the extent to which five dod it services projects , including nmci , had followed leading commercial outsourcing practices .

for nmci , we found that while the navy had employed most of these practices , it did not follow the key practice related to establishing an accurate baseline of the existing it environment , choosing instead to rely on a preexisting and dated inventory of its legacy applications .

because of this , we concluded that the navy substantially underestimated the number of legacy applications that needed to transition to nmci , in turn causing the program's time frame for transitioning to slip considerably .

we recommended that dod take steps to learn from such lessons , so that such mistakes are not repeated on future it outsourcing projects .

consistent with relevant laws and guidance , the navy defined strategic goals for its nmci program and developed a plan for measuring and reporting on achievement of these goals .

however , the navy did not implement this plan , choosing instead to focus on defining and measuring contractually specified slas .

according to navy officials , implementing the goal - oriented plan was not a priority , compared with swiftly deploying nmci seats and measuring satisfaction of contract provisions .

while program officials told us that nmci has produced considerable mission value and achieved much , they did not have performance data to demonstrate progress in relation to either the program's strategic goals or nine performance categories that its plan and related efforts defined relative to these goals .

given this , we mapped slas to the nine performance categories and two strategic goals , which prompted the navy to do the same .

the navy's mapping shows that nmci has met few of the categories' performance targets , and thus has yet to meet either of the strategic goals .

this means that the mission - critical information superiority and operational innovation outcomes that were used to justify investment in nmci have yet to be attained .

without effective performance management , the navy is increasing the risk that the program will continue to fall short of its goals and expected results .

various laws — such as the government performance & results act and clinger - cohen act — require federal agencies to identify and report on mission and strategic goals , associated performance measures , and actual performance .

federal it guidance also recognizes the importance of defining program goals and related measures and performance targets , as well as determining the extent to which targets , measures , and goals are being met .

in initiating nmci , the navy established two strategic goals for the program .

according to the navy , the program's primary goal is to support “information superiority,” which it characterizes as “providing the capability to collect , process , and disseminate an uninterrupted flow of information while exploiting or denying an adversary's ability to do the same.” in this regard , nmci was to create an integrated network in which connectivity among all parts of the shore establishment , and with all deployed forces at sea and ashore , enables all members of the network to collaborate freely , share information , and interoperate with other services and nations .

the second goal is to “foster innovation” by providing an interoperable and shared services “environment that supports innovative ways of integrating doctrine and tactics , training , and supporting activities into new operational capabilities and more productive ways of using resources.” related to these goals , the navy also cited significant benefits that were to accrue from nmci , including ( 1 ) an uninterrupted flow of information ; ( 2 ) improvements to interoperability , security , information assurance , knowledge sharing , productivity , and operational performance ; and ( 3 ) reduced costs .

to determine its progress in meeting these program goals and producing expected benefits , the navy included a performance measurement plan in its “2000 report to congress” on nmci .

according to the navy , the purpose of this 2000 performance measurement plan was to document its approach to ensuring that key nmci outcomes ( i.e. , results and benefits ) and measures were identified and collected .

in this regard , the plan identified eight strategic performance measurement categories , and related them to the nmci strategic program goals .

subsequently , the navy added a ninth performance category .

according to program office and the navy cio officials , the nine performance categories are all relevant to determining program performance and strategic goal attainment .

moreover , the plan states that these categories provide for making nmci an integrated portion of the navy and marine corps strategic vision , support the principles of using it to support people , and focus on the mission value of technology .

these nine categories , including the navy's definition of each , are as follows: interoperability: ability to allow navy systems and applications to communicate and share information with , and for providing services to and accepting services from , other military services .

security and information assurance: compliance with relevant dod , navy , and marine corps information assurance policies and procedures .

workforce capabilities: ability to ( 1 ) increase people's access to information , ( 2 ) provide tools and develop people's skills for obtaining and sharing information , and ( 3 ) support a knowledge - centric and – sharing culture that is built on mutual trust and respect .

process improvement: role as a strategic enabler for assessment and benchmarking of business and operational processes , and for sharing of data , information , applications , and knowledge .

operational performance: ability to support improved mission ( operational and business ) performance .

service efficiency: economic effectiveness ( i.e. , its cost versus services and benefits ) .

customer satisfaction: key stakeholders ( eg , end users , ) degree of satisfaction .

program management: ability to ( 1 ) meet the seat implementation schedule and the nmci budget , ( 2 ) achieve specified levels of network performance , and ( 3 ) proactively manage program risks .

network operations and maintenance: includes such things as virus detection and repair , upgradeability , scalability , maintainability , asset management , and software distribution .

the performance plan also included metrics , targets , and comparative baselines that were to be used for the first annual performance report , although it noted that progress in meeting some performance targets would not be measured until after contract award and that some of the cited measures could at some point cease to provide useful information for making decisions , while others may need to be collected continuously .

the plan also stated that the navy would fully develop performance measures for each of the categories and that it would produce an annual report on nmci's performance in each of the categories .

however , the navy has not implemented its 2000 performance management plan .

for example , the navy did not develop performance measures for each of the performance categories and has not reported annually on progress against performance targets , categories and goals .

instead , navy officials told us that they focused on defining and measuring progress against contractually specified slas , deploying nmci seats , and reducing the number of navy applications that are to run on nmci workstations .

according to these officials , measuring progress against the program's strategic goals was not a priority .

because measurement of goal attainment has not been the navy's focus to date , when we sought ( from both the program office and the navy cio office ) performance data demonstrating progress in meeting nmci's strategic goals and performance categories , the navy was unable to provide data in this context .

instead , these officials said that data were available relative to contract performance , to include sla performance levels and customer satisfaction survey results .

given this , we mapped the available contract - related performance data to the nine performance categories and targets and provided our analysis to the program office and the navy cio office .

the navy provided additional performance data and revisions to our mappings .

our analysis of the navy - provided mapping , including associated fiscal year 2005 data , is discussed in the next section .

the navy has not fully met any of its performance categories associated with achieving nmci strategic goals and realizing program benefits .

for example , the performance category of “program management” has four performance targets relative to cost , schedule , performance , and risk .

for fiscal year 2005 , the nmci program met one of the performance targets .

it did not meet the other three targets and thus did not meet this performance category .

overall , the navy defined 20 targets for the 9 performance categories .

of these 20 , the navy met 3 , did not meet 13 , and was unable to determine if it met 4 .

the specific performance targets for each performance category are described below , along with performance in fiscal year 2005 against each target .

table 3 summarizes the number of targets met and not met for each category .

interoperability: the navy defined information systems interoperability , critical joint applications interoperability , and operational testing targets as its measures of this category .

for fiscal year 2005 , it met the information systems interoperability target .

however , it did not meet the critical joint applications interoperability target , and it could not determine whether it met the operational testing target because of insufficient data .

information systems interoperability: the target was to be level 2 on the dod levels of information systems interoperability ( lisi ) scale .

the navy reports that nmci was a level 2 .

critical joint applications interoperability: the target was for all critical joint applications to be interoperable with nmci .

in fiscal year 2005 , the navy did not transition all of its critical joint applications to nmci .

moreover , of the 13 applications that were fully or partially transitioned , one was determined not to be interoperable .

operational testing: the target was to be “potentially operationally effective” and “potentially operationally suitable.” however , navy reported that the joint interoperability test command operational testing did not produce sufficient data to determine this .

security and information assurance: the navy identified slas and information assurance incentive targets as its measures of this category .

for fiscal year 2005 , it did not meet either target .

slas: the target was to meet 100 percent of all security - related agreements .

the navy reported that it met this target during 4 months of the fiscal year but did not meet it for 8 months , including the last 6 months of the fiscal year .

information assurance incentives: the target was to have the contractor earn 100 percent of the incentive each year .

however , the contractor did not earn 100 percent of the incentive for the last 6 months of this fiscal year .

workforce capabilities: the navy defined the reduction of civilian it workforce , percentage of workforce with access to nmci , and the amount of professional certifications as its measures of this category .

for fiscal year 2005 , it reported that it met the reduction of civilian it workforce target but did not meet the percent of workforce with access target and could not determine whether it met the professional certifications target .

reduction of civilian it workforce: the target was to have a zero reduction in its civilian it workforce .

the navy reported that it met this target .

percent of workforce with access: the target was for 100 percent of its workforce to have access .

as of september 30 , 2005 , 82 percent of the applicable workforce had a seat .

amount of professional certifications: while navy officials stated that the target is professional certifications , they could not provide a measurable target .

therefore , it cannot be determined whether the target was met .

process improvement: the navy defined certain customer survey and technology refreshment targets as its measures of this category .

for fiscal year 2005 , the navy did not meet the leadership survey target and could not determine whether it met the technology refreshment target .

information from customer surveys: the target was to have the contractor earn 100 percent of the echelon ii survey and the network operations leaders' survey incentives .

however , the contractor earned 25 percent of the incentive for the echelon ii survey , and 0 percent of the incentive for the network operations leaders' survey in fiscal year 2005 .

technology refreshment: while navy officials stated that the target is technology refreshment , they could not provide measurable targets .

therefore , it cannot be determined whether the target was met .

operational performance: the navy identified information from the network operations leaders' survey as its target for measuring this category .

for fiscal year 2005 , it did not meet this target .

network operations leaders' survey: the target was for the contractor to earn 100 percent of the network operations leaders' survey incentive .

the contractor earned 0 percent of the incentive in fiscal year 2005 .

service efficiency: the navy defined sla performance and cost / service ratio per seat targets as measures of this category .

for fiscal year 2005 , the navy did not meet the sla performance target , and it could not determine if it met the cost / service ratio per seat target .

sla performance: the target was to have 100 percent of seats at the full performance or full payment level .

as of september 2005 , the navy reported that 82 percent of seats achieved full payment or full performance .

this is down from march 2005 , when the navy reported that 96 percent of seats achieved full payment or full performance .

cost / service per seat: the target was to have the cost / service ratio per seat to not exceed what it was prior to nmci .

according to the navy , while the per seat cost for nmci is higher , the service level is also higher .

however , the navy did not have sufficient information to determine if the target was met .

customer satisfaction: the navy identified information from the end user satisfaction survey as a target for measuring this category .

it did not meet this target in fiscal year 2005 .

customer satisfaction survey: the target was to have 85 percent of nmci end users satisfied .

however , the percentage of users reported as satisfied from december 2004 through september 2005 ranged from 75 to 80 percent .

program management: the navy defined cost , schedule , performance , and risk - related performance targets as measures of this category .

for fiscal year 2005 , it reports that it met the cost target because it did not obligate more than 100 percent of available nmci funding but did not meet the schedule , performance , and risk targets .

cost: the target was to obligate up to 100 percent of program funds on nmci in fiscal year 2005 .

the navy reports that it obligated 97 percent of these funds in this fiscal year .

program officials stated that the other 3 percent was spent on legacy it infrastructure .

schedule: the target was to deploy all seats that were scheduled for deployment in fiscal year 2005 .

the navy reports that it deployed 77 percent of these scheduled seats .

performance: the target was to have 100 percent of eligible seats at full payment or full performance .

the navy reports that , as of september 2005 , 82 percent of the seats achieved full payment or full performance .

risk: the target is to be “green” in all risk areas .

the navy reports that it was “yellow” in several risk areas , such as schedule and organizational change management .

network operations and maintenance: the navy defined sla performance , leadership survey results , and technology refreshment targets for measuring this category .

for fiscal year 2005 , it did not meet the sla performance or the leadership survey results targets .

further , it could not determine if it met the technology refreshment target .

sla performance: the target was to have 100 percent of eligible seats at either full payment or full performance .

as of september 2005 , the navy reported that 82 percent of seats were achieving full payment or full performance .

this is down from march 2005 , when the navy reported that 96 percent of seats achieved full payment or full performance .

leadership survey results: the target was to have the contractor earn 100 percent of both the echelon ii and network operations leaders' survey incentives .

through september 30 , 2005 , the contractor earned 25 percent of the echelon ii incentive , and 0 percent of the operator's incentive .

notwithstanding the above described performance relative to performance category targets and strategic goals , navy cio and program officials described the program as a major success .

cio officials , for example stated that nmci has significantly improved the navy's it environment , and will increase productivity through greater knowledge sharing and improved interoperability .

they also stated that a review and certification process for all applications deployed on the network has been implemented and thus compliance with security and interoperability requirements has been ensured .

according to these officials , nmci's value has been demonstrated repeatedly over the last few years .

in this regard , they cited the following examples but did not provide verifiable data to support them .

improved security through continuous security assessments , a centralized distribution of vulnerability information , configuration control of critical servers , and an improved response to new vulnerabilities / threats .

improved continuity of operations ( eg , the navy reports that it had no prolonged disruptions due to recent hurricanes and fires on the west coast ) .

increased personnel training and certification by increasing the amount of offerings .

identified opportunities for improving efficiency through the use of performance metrics .

improved software and hardware asset management and implementation of standard and secure configurations .

provided pier - side ( waterfront ) connectivity and navy - wide public key infrastructure .

the navy's mapping of fiscal year 2005 data to performance categories and targets as summarized above shows that the nmci program has not yet met either of its strategic goals .

specifically , the information superiority and innovation goals that were used to justify the program have yet to be attained .

further , although the navy developed a plan to measure and report on nmci progress in meeting the strategic goals , this plan was not implemented .

as a result , the development and reporting of program performance relative to strategic goals has not occurred .

our analysis of navy contractor performance data since september 2004 shows that the extent to which the site - specific agreements have been met for all operational seats ( regardless of site ) varies widely by individual agreement , with some always being met but others having varied performance over time and by seat type .

our analysis also showed that , although the contractor has met most of the enterprisewide agreements during this time period , it has not met a few .

the navy's analysis and reporting of contractor performance relative to the slas , using data for the same time period , showed that the percentage of operational seats meeting the agreements averaged about 89 percent from march 2005 to september 2005 , then declined to 74 percent in october 2005 and averaged about 56 percent between november 2005 and march 2006 .

these differences in how sla performance can be viewed illustrate how contractor performance against the agreements can be viewed differently depending on how available data are analyzed and presented .

they also illustrate the importance of having a comprehensive , transparent , and consistent approach to program performance management that considers a range of perspectives and metrics .

for the period beginning october 2004 and ending march 2006 , the contractor's performance relative to site - specific slas has varied , with certain agreements consistently being met regardless of seat type , other agreements being met to varying degrees over time , and still others largely not being met for certain seat types .

variability in performance has also occurred for enterprisewide agreements , although most have been met .

between october 2004 and march 2006 , the contractor has met , or usually met , the agreement for each seat type for many slas .

for example , the contractor met sla 324 , which covers wide area network connectivity , for all seat types all of the time .

also , sla 325 , covering network communication services , and sla 332 , measuring application server connectivity , were met for all seat types over the same time period .

sla 225 , which measures base area network and local area network performance , was met for essentially all seat types ( see fig .

4 ) .

similarly , sla 328 , which measures the time to implement new seats and application servers , was met for 94 percent or more of deployed seat types in january 2005 through march 2006 ( see fig .

5 ) .

 ( see app .

iii for descriptions of each sla and figures illustrating levels of performance relative to each applicable seat type. ) .

the contractor has not consistently met certain agreements between october 2004 and march 2006 .

for example , satisfaction of sla 102 , which covers response time for network problem resolution , has ranged from a high of 100 percent in march 2005 and june 2005 to a low of 79 percent in february 2006 .

as of march 2006 , this sla was met by 97 percent of all seat types ( see fig .

6 ) .

also , satisfaction of sla 107 , which is a measure of network performance in areas of availability , latency / packet loss , and quality of service in support of videoconferencing and voice - over - ip , has varied over time .

specifically , satisfaction has ranged from a high of 99 percent in january 2006 to a low of 71 percent in january 2005 .

as of march 2006 , this agreement was met by 90 percent of all seat types ( see fig .

7 ) .

between october 2004 and march 2006 , the contractor has not met certain agreements for all seat types .

for example , for sla 101 , which is a measure of the time it takes to resolve nmci user issues , the percentage of seats meeting the agreement has widely varied .

specifically , the percentage of mission - critical seats that met the agreement has been consistently and significantly lower than was the case for the basic or high end seats .

in particular , as of march 2006 , sla 101 was met for about 90 percent of basic seats , 77 percent of high end seats , and 48 percent of mission - critical seats ( see fig .

8 ) .

similarly , for sla 103 , which is a measure of performance of end user services , the percentage of basic seats that met the agreement was consistently and significantly lower than that of high end or mission - critical seats .

in march 2006 , sla 103 was met for about 63 percent of basic seats , 74 percent of high end seats , and 86 percent of mission - critical seats ( see fig .

9 ) .

the contractor generally met most of the slas that have enterprisewide applicability .

in particular , of the 13 such sla's , 8 were met each month between october 2004 and march 2006 , and another was met all but 1 month during this time period .

further , a tenth sla was met for 14 out of the 18 months during this period .

however , the contractor has not consistently met 3 of the 13 enterprisewide slas .

specifically , sla 103 , which covers end user services , was not met 12 of the 18 months .

sla 104 , which covers the help desks , was not met 11 out of the 18 months , including 8 out of the last 9 months of this period .

sla 106 , which covers information assurance services including identifying incidents , responding to incidents , and configuration of nmci , was not met for 11 out of 18 months , including the last 9 months of the period .

 ( see fig .

10 for a summary of the months in which the contractor met and did not meet the enterprisewide slas. ) .

nmci program officials told us that they measure the contractor's sla - related performance in terms of the percentage of eligible seats that have met the contractual definitions of full payment and full performance .

more specifically , they compare the number of seats on a site - by - site basis that have met these definitions with the number of seats that are eligible .

as discussed earlier , full payment means that the contractor has met 100 percent of the applicable agreements at a given site , and 50 to 90 percent of the planned seats at that site have been cut over ( i.e. , are operational ) .

full performance means that the contractor has met 100 percent of the applicable agreements at a given site , and over 90 percent of the planned seats at that site have been cut over .

in effect , this approach focuses on performance for only those seats that are at sites where at least 50 percent of the planned number of seats are actually operating .

it excludes performance at sites where less than 50 percent of the ordered seats are operating .

moreover , it combines the results for all slas and , therefore , does not highlight differences in performance among service areas .

for the period beginning in october 2004 and ending in march 2005 , the contractor's performance in meeting the agreements from a contractual standpoint increased , with the percentage of operational seats that met either performance level having jumped markedly between october and december 2004 ( about 5 to 65 percent ) , then generally increasing to a high of about 96 percent in march 2005 .

since then , the percentage of seats meeting either of the two performance levels fluctuated between 82 and 94 percent through september 2005 and then decreased to 74 percent in october 2005 .

from november 2005 through march 2006 , the percentage of seats meeting either performance level decreased to 55 percent .

 ( see fig .

11 for the trend in the percentage of operational seats meeting either the full payment or full performance levels ; see fig .

12 for the number of seats achieving either performance level versus the number eligible for doing so for the same time period. ) .

the preceding descriptions of sla performance illustrate that contractor performance against the agreements can be viewed differently depending on how relevant data are analyzed and presented .

further , they illustrate the importance of considering different perspectives and metrics in order to have a comprehensive , transparent , and consistent approach to program performance management .

the navy's three groups of nmci customers — end users , organizational commanders , and network operators — vary in the extent to which they are satisfied with the program , but collectively these customers are generally not satisfied .

with respect to end users , the navy reports that overall satisfaction with nmci improved between 2003 and 2005 ; however , reported satisfaction levels have dropped off since september 2005 .

in addition , while the navy reports that this overall level of end user satisfaction with contractor provided services has averaged about 76 percent since april 2004 , this is below the navy - wide target of 85 percent and includes many survey responses at the lower end of the range of scores that the navy has defined “satisfied” to mean .

with respect to commanders and network operations leaders , neither is satisfied with nmci .

in addition , officials representing each of the customer groups at five shipyard or air depot installations that we visited expressed a number of nmci concerns and areas of dissatisfaction with the program .

without satisfied customers , the navy runs the risk that nmci will not attain the widespread acceptance necessary to achieve strategic program goals .

despite reported improvements in end user satisfaction levels since 2002 , end user responses to quarterly satisfaction surveys have been consistently at the low end of the range of scores that the navy defines the term “satisfied” to mean , and the percentage of end users that navy counts as being “satisfied” have consistently been below the navy's satisfaction target level .

specifically , although the navy's satisfied users dropped from about 66 percent in june 2002 to around 54 percent for the next two quarters ( september and december 2002 ) , satisfaction reportedly rose steadily from march 2003 through september 2005 , peaking at that time at about 80 percent .

since then , the percentage of end users that the navy reports to be satisfied has declined , leveling off at around 76 percent over the next several months .

this means that even with the navy's forgiving definition of what constitutes a satisfied end user , at least 24 percent of end users are dissatisfied with nmci .

 ( see fig .

13 for the trends in end user satisfaction with the program and the contractor. ) .

exacerbating this long - standing shortfall in meeting end user satisfaction expectations is the fact that the navy considers a “satisfied” end user to include users that are at best marginally satisfied and arguably somewhat dissatisfied .

that is , the navy uses an average score of 5.5 or greater ( on its 10-point satisfaction scale , where 1 is dissatisfied , and 10 is satisfied ) as the threshold for categorizing and counting end users as satisfied .

this means that users counted as satisfied may include a large contingent that are at the low end of the satisfaction range ( eg , between 5.5 and 7 ) .

when the results of the march 2006 survey are examined in this context , we see that this is the case .

for example , we see that 8 of the 14 questions received an average score below 7.0 .

additional insights into the degree and nature of end user satisfaction ( and dissatisfaction ) are apparent when the reported percentage of satisfied users are examined from different perspectives , such as by ( 1 ) individual survey questions and ( 2 ) organizational units .

for example , navy - reported end user satisfaction survey results for the quarter ending march 31 , 2006 , show that while the percentage of users deemed satisfied with the program averaged about 74 percent , the percentage reported as satisfied relative to each survey question ranged from a low 52 to a high of 87 percent .

these insights into end user sources of satisfaction and dissatisfaction are summarized as follows: variations in satisfaction levels by question .

while the percentage of end users who are categorized as satisfied with the program and the contractor do not significantly differ ( 74 versus 76 percent , respectively ) , variations do exist among the percentage satisfied with the 14 areas that the questions address .

for example , far fewer ( 66 percent ) were satisfied with the reliability of the nmci network than were satisfied with the professionalism of eds personnel ( 87 percent ) .

 ( see table 4 for the percentage of users satisfied and dissatisfied according to each of the 14 survey questions. ) .

variations in satisfaction levels by organizational unit .

the percentage of end users who were categorized as being satisfied with the nmci program varied by organizational unit as much as 18 percentage points .

for example , about 66 percent of users in the naval sea systems command were deemed satisfied with the program as compared with about 84 percent in the commander of navy installations .

similarly , the percentage of end users who were categorized as satisfied with the contractor also varied by 17 percentage points , with the naval sea systems command and naval air systems command having about 69 percent of its users viewed as satisfied and the commander of navy installations having about 86 percent .

 ( see tables 5 and 6 for percentages of satisfied end users by navy and marine corps , respectively , organizations as of march 31 , 2006. ) .

the navy conducted surveys of commander and network operations leader units in september 2005 and in march 2006 .

overall , survey results show that neither commanders nor operators are satisfied with nmci .

the results from the two commander satisfaction surveys conducted to date show that the customers are not satisfied , with nmci .

specifically , on a scale of 0-3 with 0 being not satisfied , and 1 being slightly satisfied with the contractor's support in meeting the mission needs and strategic goals of these organizations , the average response from all organizations was 0.65 and 0.76 in september 2005 and march 2006 , respectively .

the latest survey results show minor differences in the degree of dissatisfaction with the four types of contractor services addressed ( cutover services , technical solutions , service delivery , and warfighter support ) .

 ( see table 7 for results of the september 2005 , and march 2006 , commander satisfaction surveys. ) .

the navy - reported results of the two network operations leader satisfaction surveys conducted to date show that these customers are also not satisfied with nmci .

specifically , on a scale of 0-3 with 0 being not satisfied and 1 being slightly satisfied with the contractor's support in meeting the mission needs and strategic goals of these two organizations , the average of the responses from netwarcom in september 2005 was 0.33 , rising to 0.67 in march 2006 .

for mcnosc , the average of the responses to both surveys was 0.00 .

 ( see table 8 for these results. ) .

of the three types of contractor services addressed in the survey ( mission support and planning , network management , and service delivery ) , network management services , which includes information assurance and urgent software patching , received a score of 0 from both organizations on both surveys .

consistent with the results of the navy's customer satisfaction surveys , officials representing end users , commanders , and network operations personnel at five shipyards or air depots that we interviewed cited a number of concerns or sources of dissatisfaction with nmci .

the anecdotal information that they provided to illustrate their concerns are described in the next section .

shipyard and air depot officials for all five sites told us that they have continued to rely on their legacy systems rather than nmci for various reasons .

for example , officials at one air depot stated that nmci provided less functionality than their legacy systems and thus they have continued to use these legacy systems to support mission operations .

also , officials at one shipyard told us that site personnel lack confidence in nmci and thus they continue to use legacy systems .

officials at the other two shipyards voiced even greater concerns , with officials at one saying that only nmci seats ( i.e. , workstations ) are running on the nmci intranet ( their servers are still running on their legacy network ) , and officials at the other saying that nmci does not support their applications and thus they primarily use it for e - mail .

similarly , officials at an air depot stated that nmci workstations are not capable of supporting certain applications , such as high - performance modeling , and thus they operate about 233 other workstations to support their needs .

according to a memo from the commander of one shipyard to the naval sea systems command dated december 2005 , ncmi software updates adversely affect the operation of network applications .

consistent with this , officials at two of the sites stated that nmci is hurting workforce productivity , with officials at one shipyard saying that system downtime , particularly as it relates to major applications , has deteriorated and is unacceptable , and officials at another shipyard said that nmci response time is slow both on - and off - site .

to illustrate , officials at one air depot said that personnel cannot download more than one file at a time , while officials at shipyards stated that “reach back” to legacy systems through nmci is slow , sometimes taking 45 minutes to open a document .

further , officials at shipyards complained that users' profiles do not follow the user from one workstation to another , causing users to recreate them , while officials at one air depot stated that nmci does not provide them the capability to monitor employees' inappropriate use of the internet ( eg , excessive use or accessing unauthorized sites ) .

both air depot and shipyard officials described their respective work environments as dynamic , meaning that they are frequently changing , and thus require flexibility in moving and configuring workstations .

further , shipyards operate at the waterfront , which we were told is an environment that requires quick responses to changing needs .

for example , ships come in , barges are created to service them , and these barges must be outfitted with computers .

decisions occur in a short amount of time regarding new barge set ups and equipment movements .

according to shipyard officials , nmci has not been able to support these barge - related requirements because it is not flexible enough to quickly react to shifting work priorities .

as a result , officials with one shipyard stated that they have had to provide their own waterfront support using legacy systems .

similarly , officials with the air depots stated that the nmci contractor has a difficult time moving seats fast enough to keep up with changing needs .

limitations in help desk support officials from each of the shipyards and air depots voiced concerns and dissatisfaction with help desk assistance .

according to officials with the air depots , the quality of help desk support is inconsistent , and thus they have had to assume more of the burden in dealing with it system problems since they transitioned to nmci .

shipyard officials were even more critical of help desk support .

according to officials at one shipyard , help desk support is not working , as it is almost impossible to get a help desk call done in 1 hour .

similarly , officials at another shipyard told us that help desk responsiveness has been poor because it takes hours , if not days , to get problems fixed .

the previously cited memo from the commander of one shipyard to the naval sea systems command cited an average time of 2.4 days to respond to customer inquiries .

officials from all five sites expressed concerns with the manner in which they were prepared for transitioning to nmci .

according to officials at one air depot , certain seat management requirements were overlooked , and nmci users have struggled with understanding the contract processes that govern , for example , how to order new software and hardware , or how to relocate machines , because the contractual terms are difficult to follow , and training was not adequate .

in particular , they said users do not understand with whom they should talk to address a given need , and officials with one air depot noted that nmci has no solution for their electronic classroom needs .

officials at one shipyard attributed the lack of nmci site preparation to insufficient planning prior to deploying nmci and a lack of transparency in how nmci was being managed , including how deployment issues were to be resolved .

as stated by officials at another shipyard , the transition to nmci was difficult and very disruptive to operations because they had no control over the contractor transition team .

nmci program officials told us that they are aware of the concerns and sources of dissatisfaction of shipyard and air depot customers , however , they added that many of them are either not supported by data or reflect customers' lack of familiarity with the services available under the contract .

in particular , they said that they have not been provided any data showing a drop in workforce productivity caused by nmci .

they also said that continued reliance on legacy systems illustrates a lack of familiarity with the contract because provisions exist for moving legacy servers onto nmci and supporting certain applications , such as high - performance modeling .

further , they said that the contract supports monitoring internet usage , provides waterfront support to shipyards , and provides help desk service 24 hours a day , 7 days a week .

nevertheless , they acknowledged that both a lack of customer understanding , and customer perceptions about the program are real issues affecting customer satisfaction that need to be addressed .

the nmci program office reports that improving customer satisfaction is a program priority .

accordingly , it has invested and continues to invest time and resources in a variety of activities that it associated with customer satisfaction improvement , such as holding user conferences and focus groups .

however , these efforts are not being guided by a documented plan that defines prioritized improvement projects and associated resource requirements , schedules , and measurable goals and outcomes .

given the importance of improved customer satisfaction to achieving nmci program goals and benefits , it is important for the navy to take a structured and disciplined approach to planning its improvement activities .

without it , the program office cannot adequately ensure that it is effectively investing scarce program resources .

as we have previously reported , effectively managing program improvement activities requires planning and executing such activities in a structured and disciplined fashion .

among other things , this includes developing an action plan that defines improvement projects and initiatives , assigns roles and responsibilities , sets priorities , identifies resource needs , establishes time lines with milestones , and describes expected results in measurable terms .

the software engineering institute's idealsm model , for example , is one recognized approach for managing process improvement efforts .

according to this model , improvement efforts should include a written plan that serves as the foundation and basis for guiding improvement activities , including obtaining management commitment to and funding for the activities , establishing a baseline of commitments and expectation against which to measure progress , prioritizing and executing activities and initiatives , determining success , and identifying and applying lessons learned .

through such a structured and disciplined approach , improvement resources can be invested in a manner that produces optimal results .

without such an approach , improvement efforts can be reduced to trial and error .

the nmci program office identified seven initiatives that are intended to increase customer satisfaction with the program .

according to program officials , the initiatives are ( 1 ) holding user conferences , ( 2 ) conducting focus groups , ( 3 ) administering diagnostic surveys , ( 4 ) strengthening help desk capabilities , ( 5 ) expanding network services ( eg , adding broadband remote access ) , ( 6 ) assessing infrastructure performance , and ( 7 ) initiating a lean six sigma effort .

following are descriptions of each initiative: user conferences .

the program office has conducted semiannual nmci user conferences since 2000 .

according to program officials , these conferences provide a forum for users to directly voice to program leaders their sources of dissatisfaction with nmci .

during the conferences , users ask questions , participate in issue - focused breakout sessions , and engage in informal discussions .

we attended the june 2005 user conference and observed that navy and contractor program officials provided information , such as updates on current and planned activities and capabilities , while users had opportunities to provide comments and ask questions .

according to program officials , the conferences are useful in making program officials aware of customer issues and are used to help diagnose nmci problems .

focus groups .

according to program officials , they conduct user focus groups to , among other things , solicit reasons for customer dissatisfaction and explore solutions and to test newly proposed end user satisfaction survey questions .

the focus group sessions include invited participants and are guided by prepared scripts .

the results of the sessions are summarized for purposes of identifying improvements such as revisions to user satisfaction survey questions .

diagnostic surveys .

the program office performs periodic surveys to diagnose the source of user dissatisfaction with specific services , such as e - mail , printing , and technical support .

according to program officials , these surveys help identify the root causes of user dissatisfaction and support analysis of areas needing improvement .

however , they could not identify specific examples of where such causes have been identified and addressed and measurable improvements have resulted .

help desk improvement team .

the program office established a team to identify the reasons for declining end user satisfaction survey scores relative to the technical support services provided by the help desk .

according to program officials , the team traced declining satisfaction levels to such causes as help desk agents' knowledge , training , and network privilege shortfalls .

to address these limitations , the program office reports that it has redesigned and restructured help desk operations to organize help desk agents according to skills and experience , route calls according to the skill level needed to address the call , target needed agent training , hold daily meetings with agents to apprise them of recent issues , and monitor help desk feedback .

however , program officials could not link these efforts to measurable improvements in help desk performance , and nmci customers that we interviewed during our visits to shipyards and air depots voiced concerns with help desk support .

expanded network services .

nmci program officials stated that a key improvement initiative has been expanding the scope of network - related services that are available under the contract .

in particular , they point to such new services as broadband remote access for all laptop users , antispam services for all e - mail accounts , and antispyware services for all accounts as having improved customer satisfaction .

further , they said that the planned addition of wireless broadband access will increase customer satisfaction .

however , they could not provide data showing how these added services affected customer satisfaction , or how future services are expected to affect satisfaction .

infrastructure performance assessment .

working with eds , the program office undertook an nmci network infrastructure assessment that was intended to identify and mitigate performance issues .

this assessment included establishing metrics and targets for common user functions such as opening a web site , then determining actual network performance at the washington navy yard and marine corps installations in quantico , virginia .

according to program officials , assessment results included finding that network performance could be improved by balancing traffic among firewalls and upgrading wide area network circuits .

as a result of this initial assessment , the program has begun adjusting network settings and upgrading hardware at additional nmci sites .

further , program officials said they are expanding their use of network infrastructure metrics to all sites .

however , they neither provided us with a plan for doing so , nor did they demonstrate that these efforts have affected customer satisfaction .

lean six sigma .

program officials said they are applying lean six sigma techniques to improve customer satisfaction .

in particular , they have established a customer satisfaction workgroup , which is to define a process for identifying customer problems and prioritizing improvement projects .

they said that , for each project , they will perform concept testing using pilot projects and focus groups .

they also said that they plan to establish a steering committee that includes representatives from the navy and the contractor .

the officials told us that they have initiated seven projects using lean six sigma techniques , although they did not provide us with any information about the results of these projects or their impact on customer satisfaction .

while any or all of these initiatives could result in improvements to customer satisfaction , the program office could not demonstrate that they have produced or will produce measurable improvements .

moreover , the latest customer satisfaction data provided to us show that satisfaction levels are not improving .

further , it is unclear how these various initiatives relate to one another , and various aspects of these initiatives appear redundant such as multiple teams and venues to identify root causes and propose solutions .

one reason for this lack of demonstrable improvements and redundancy is the way in which the program office has pursued its improvement initiatives .

in particular , they have not been pursued as an integrated portfolio of projects that were justified and prioritized on the basis of relative costs and benefits .

further , they have not been guided by a well - defined action plan that includes explicit resource , schedule , and results - oriented baselines , as well as related steps for knowing whether expected outcomes and benefits have actually accrued .

rather , program officials stated that customer satisfaction improvement activities have been pursued as resources become available and have been in reaction to immediate issues and concerns .

without a proactive , integrated , and disciplined approach to improving customer satisfaction , the navy does not have adequate assurance that it is optimally investing its limited resources .

while the lean six sigma techniques that program officials told us they are now applying to customer satisfaction improvement advocate such an approach , program officials did not provide us with documentation demonstrating that they are effectively planning and executing these projects .

it service programs , like nmci , are intended to deliver effective and efficient mission support and to satisfy customer needs .

if they do not , or if they are not being managed in a way to know whether or not they do , then the program is at risk .

therefore , it is important for such programs to be grounded in outcome - based strategic goals that are linked to performance measures and targets , and it is important for progress against these goals , measures , and targets to be tracked and reported to agency and congressional decision makers .

if such measurement does not occur , then deviations from program expectations will not become known in time for decision makers to take timely corrective action .

the inevitable consequence is that program results will fall short of those that were promised and used to justify investment in the program .

the larger the program , the more significant these deviations and their consequences can be .

nmci is an enormous it services program and thus requires highly effective performance management practices .

however , such management , to include measurement of progress against strategic program goals and reporting to key decision makers of performance against strategic goals and other important program aspects , such as examining service level agreement satisfaction from multiple vantage points and ensuring customer satisfaction , has not been adequate .

one reason for this is that measurement of progress against strategic program goals has not been a priority for the navy on nmci , giving way to the navy's focus on deploying nmci seats to more sites despite a long - standing pattern of low customer satisfaction with the program and known performance shortfalls with certain types of seats .

moreover , despite investing in a range of activities intended to improve customer satisfaction , plans to effectively guide these improvement efforts , including plans for measuring the success of these activities , have not been developed .

given that the navy reports that it has already invested about 6 years and $3.7 billion in nmci , the time to develop a comprehensive understanding of the program's performance to date , and its prospects for the future , is long overdue .

to its credit , the navy recognizes the importance of measuring program performance , as evidenced by its use of service level agreements , its extensive efforts to survey customers , and its various customer satisfaction improvement efforts .

however , these steps need to be given the priority that they deserve and be expanded to obtain a full and accurate picture of program performance .

doing less increases the risk of inadequately informing ongoing nmci investment management decisions that involve huge sums of money and carry important mission consequences .

to improve nmci performance management and better inform investment decision making , we recommend that the secretary of defense direct the secretary of the navy to ensure that the nmci program adopts robust performance management practices that , at a minimum , include ( 1 ) evaluating and appropriately adjusting the original plan for measuring achievement of strategic program goals and provides for its implementation in a manner that treats such measurement as a program priority ; ( 2 ) expanding its range of activities to measure and understand service level agreement performance to provide increased visibility into performance relative to each agreement ; ( 3 ) sharing the nmci performance results with dod , office of management and budget , and congressional decision makers as part of the program's annual budget submissions ; and ( 4 ) reexamining the focus , scope , and transparency of its customer satisfaction activities to ensure that areas of dissatisfaction described in this report are regularly disclosed to the aforementioned decision makers and that customer satisfaction improvement efforts are effectively planned and managed .

in addition , we recommend that the secretary of defense direct the secretary of the navy , in collaboration with the various navy entities involved in overseeing , managing , and employing nmci , to take appropriate steps to ensure that the findings in this report and the outcomes from implementing the above recommendations are used in considering and implementing warranted changes to the nmci's scope and approach .

in its comments on a draft of this report , signed by the deputy assistant secretary of defense ( command , control , communications , intelligence , surveillance , reconnaissance & information technology acquisition programs ) and reproduced in appendix iv , dod agreed with our recommendations and stated that it has implemented , is implementing , or will implement each of them .

in this regard , the department stated that the report accurately highlights the need to adjust the nmci strategic goals and associated measures , and it committed to , among other things , sharing additional nmci performance data with decision makers as part of the annual budget process .

notwithstanding this agreement , dod also commented that the navy believes that our draft report contained factual errors , misinterpretations , and unsupported conclusions .

we do not agree with the navy's position .

the navy's points are summarized below along with our response .

the navy stated that our review focused on navy shipyards and air depots to the exclusion of marine corps sites .

we disagree .

as the objectives , scope and methodology section of our report points out , the scope of our review covered the entire nmci program and extended to navy and marine corps sites based on data we obtained from program officials .

for example , our work on the extent to which nmci had met its two strategic goals was programwide , and our work on sla performance and customer satisfaction surveys included navy and marine corps sites at which nmci was operating and navy and marine corps customers that responded to the program's satisfaction surveys .

the navy stated that nmci is a strategic success , noting that the program is meeting its goals of providing information superiority ( as well as information security ) and fostering innovation .

as part of these statements , the navy cited such things as the number of users supported and seats deployed , the types of capabilities fielded , and contracting actions taken .

in addition , the navy stated that nmci has thwarted intrusion attacks that have penetrated other dod systems , and it concluded that nmci represents a major improvement in information superiority over the navy's legacy network environment in such areas as virus protection and firewall architecture .

it also noted that more naval commands now have access to state - of - the - art workstations and network services , which it concluded means that nmci is fostering innovation .

while we do not question these various statements about capabilities , improvements , and access , we would note they are not results - oriented , outcome - based measures of success .

moreover , we do not agree with the statements about nmci meeting its two strategic goals and being a strategic success .

as we show in our report using the navy's own performance categories , performance targets , and actual sla and other performance data , nmci met only 3 of the 20 performance targets spanning nine performance categories that the navy established for determining goal attainment .

concerning these results , the navy stated that our report's use of sla performance data constitutes a recommendation on our part for using such data in determining program goal attainment , which the navy said is “awkward” because slas “do not translate well into broad goals.” we do not agree that our report recommends the use of any particular performance data and targets for determining program goal attainment .

our report's use of these data and targets is purely because the nmci program office provided them to us in response to our inquiry for nmci performance relative the nine navy - established performance categories .

we are not recommending any particular performance targets or data .

rather , we are recommending that the approach for measuring achievement of strategic goals be reevaluated and adjusted .

accordingly , we support dod's comment that the navy needs to adjust the original nmci strategic goals and associated measures .

the navy stated that we misinterpreted sla data as they relate to the contractor performance categories of full payment and full performance .

we disagree .

the report presents a navy - performed analysis of sla data relative to the full payment and full performance categories that offers no interpretation of these data .

however , because the navy's analysis of sla data is an aggregation , we performed a different analysis to provide greater visibility into individual sla performance that the navy's full payment and full performance analyses tends to hide .

our analysis also avoids the bundling and averaging concerns that the navy raised .

the navy stated that some of our customer satisfaction conclusions were unsupported .

specifically , the navy said that the way it collects end - user satisfaction responses , 5.5 or higher on a scale of 10 indicates a satisfied user , and such a scale is in line with industry practice .

therefore , the navy said that user satisfaction survey responses do not “break out” in a way that supports our conclusion that scores of 5.5 through 7 are marginally satisfied users .

we do not agree .

while we recognize that the navy's 1-10 scale does not differentiate between degrees of satisfaction , we believe that doing so would provide insight and perspective that is lacking from merely counting a user as satisfied or not satisfied .

when we analyzed the responses to individual questions in terms of degrees of satisfaction , we found that average responses to 10 of 14 survey questions were 5.5 to 7 , which is clearly close to the lower limit of the satisfaction range .

also , with regard to customer satisfaction , the navy stated that our inclusion in the report of subjective statements from shipyard and air depot officials did not include any data to support the officials' statements and thus did not support our conclusions .

we recognize that the officials' statements are subjective and anecdotal , and our report clearly identified them as such .

nevertheless , we included them in the report because they are fully consistent with the customer satisfaction survey results and thus help illustrate the nature of nmci user concerns and areas of dissatisfaction that the survey results show exist .

the navy stated that nmci provides adequate reports to key decision makers .

however , we disagree because the reporting that the navy has done has yet to disclose the range of performance and customer satisfaction issues that our report contains .

our message is that fully and accurately disclosing program and contractor performance and customer satisfaction to the various entities responsible for overseeing , managing , and employing nmci will serve to strengthen program performance and accountability .

the navy also provided various technical comments , which we have incorporated as appropriate .

we are sending copies of this report to interested congressional committees ; the secretary of defense ; the secretary of the navy ; the commandant of the marine corps ; and the director , office of management and budget .

we also will make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you have any questions concerning this information , please contact me at ( 202 ) 512-6256 or by e - mail at hiter@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix v .

our objectives were to review ( 1 ) whether the navy marine corps intranet ( nmci ) is meeting its strategic goals , ( 2 ) the extent to which the contractor is meeting its service level agreements ( sla ) , ( 3 ) whether customers are satisfied with the program , and ( 4 ) what is being done to improve customer satisfaction .

to determine whether nmci is meeting its strategic goals , we reviewed documents provided by department of the navy describing the mission need for nmci , strategic goals , performance measures , and data gathered on actual performance , conducted interviews with officials from the offices of the department of defense chief information officer ( cio ) , department of the navy cio , and assistant secretary of the navy for research , development , and acquisition , including officials in the nmci program office , identified the nmci strategic goals , related performance categories , associated performance targets , and actual performance data through document reviews and interviews , developed an analysis showing nmci's performance relative to the strategic goals , performance categories , and targets based upon available actual performance data , and shared our analysis with program officials and adjusted the analysis based on comments and additional data they provided .

to determine the extent to which performance expectations defined in nmci slas have been met , we conducted interviews with nmci program office and contractor officials to gain an understanding of available sla performance data and potential analysis methods , obtained data on actual sla performance that are used by the navy as the basis for making performance - based payments to the contractor and , for each sla , these data indicated whether one or more measurement ( s ) were taken and if so , whether the measure was met or not , for each seat type ( i.e. , basic , high end , and mission - critical ) , at every site for each month from october 2004 through march 2006 , analyzed data for site - specific slas by calculating the number of seats that met each agreement at each site for each month and when measurement data were available according to seat type , we calculated the number of seats that met each agreement for each seat type .

otherwise , we calculated the total number of seats that met each agreement .

we counted an agreement as met at a site if all of the agreement's measured targets were met at the site for a given month .

to calculate the percentage of seats for which an agreement was met , we added the total number of seats at all sites for which an agreement was met , and divided it by the total number of seats at all sites for which measurements were made , analyzed data for enterprisewide slas by determining whether an agreement was met at all navy ( excluding the marine corps ) and all marine corps sites for each month , and we counted an agreement as met if all of the agreement's measured targets were met for a given month , compared our site specific and enterprisewide sla analyses across months to identify patterns and trends in overall sla performance and in situations were an sla is composed of site specific and enterprisewide measures , we did not aggregate our site specific and enterprisewide results .

thus , an sla could have been met at the site level but not at the enterprisewide , and vice versa , and described our analysis method and shared our results with program office and contractor officials and made adjustments based on their comments .

to determine whether nmci customers are satisfied , we obtained and analyzed results of end users surveys conducted from june 2002 through march 2006 and commanders and network operations leaders surveys from september 2005 through march 2006 , conducted interviews with nmci program office and contractor officials to gain an understanding of how the surveys were developed and administered and their procedures for validating and auditing reported results , analyzed data in the survey reports by comparing actual with desired results , and we also analyzed the data to identify trends in satisfaction levels over time and variation in satisfaction by question , organization , and type of service , and conducted interviews with a broad range of nmci users at navy sites: portsmouth naval shipyard , norfolk naval shipyard , puget sound naval shipyard , jacksonville naval air depot , and north island naval air depot .

we selected these sites because they are among the largest , include diverse user communities , and represent different stages of program implementation .

participants in the interviews included officials from the offices of the commander , cio , information technology and communications services , end users relying on nmci desktop services in day - to - day operations , and the contractor .

to determine what has been done to improve customer satisfaction , we interviewed program office and contractor officials to identify and develop an understanding of customer satisfaction improvement efforts .

to determine the results and impact of each effort , and we interviewed program officials and obtained and analyzed relevant documentation , researched best practices into effective management of improvement activities and compared the program office's approach with the practices we identified to evaluate the overall effectiveness of the customer satisfaction improvement activities , and attended the june 2005 nmci enterprise conference to observe the proceedings .

we performed our work from april 2005 to august 2006 in accordance with generally accepted government auditing standards .

this appendix includes the questions used in the three customer satisfaction surveys: end user customer satisfaction survey , navy echelon ii and marine corps major command commander's incentive survey , and navy and marine corps network operations leader's survey .

the end user customer satisfaction survey consists of 14 questions , 10 of which are tied to incentives .

users are asked to think only of the experiences they have had with the services during the prior 3 months .

if a question is not relevant to their experience , they are asked to indicate that it is not applicable .

otherwise , they are asked to score it on a 1-10 scale with 1-5 being levels of dissatisfaction , and 6-10 being levels of satisfaction .

users are also currently asked demographic information in the survey , as well as suggestions for improvement , and sources of dissatisfaction .

table 9 lists the end user customer satisfaction survey questions .

the commander's customer satisfaction incentive survey consists of four topics ( warfighter support services , cutover services , technology solutions , and service delivery ) corresponding to key mission and / or business objective - related services or capabilities .

each topic is broken down into a number of subtopics .

under each subtopic , the survey asks commanders to indicate whether they agree , disagree , or have no basis to respond to a series of statements about eds's performance .

the survey also asks commanders to rate their overall satisfaction with each topic as “extremely satisfied,” “mostly satisfied,” “slightly satisfied,” “not satisfied,” or “no basis to respond.” the last section of each topic contains two open - ended questions soliciting feedback on satisfaction with nmci services .

table 10 is a condensed version of the commander's customer satisfaction survey that includes each of the subtopics , statements about eds's performance , the overall topic satisfaction question , and the two open - ended questions .

the network operations leaders' customer satisfaction incentive survey consists of three topics ( mission support and planning , network management , and service delivery ) corresponding to key mission and / or business objective - related services or capabilities .

each topic is broken down into a number of subtopics .

under each subtopic , the survey asks the leaders to indicate whether they agree , disagree , or have no basis to respond to a series of statements about eds's performance .

the survey also asks the leaders to rate their overall satisfaction with each topic as “extremely satisfied,” “mostly satisfied,” “slightly satisfied,” “not satisfied,” or “have no basis to respond.” the last section of each topic contains two open - ended questions soliciting feedback on satisfaction with nmci services .

table 11 is an abbreviated version of the network operations leader's surveys that includes each of the subtopics , statements about eds's performance , the overall topic satisfaction question , and the two open - ended questions .

this appendix contains descriptions and performance trends for nmci's service level agreements .

slas are measured at site level , enterprisewide , or both the site and enterprisewide .

site level sla performance is based on the percentage of operational seats that met the sla , meaning that all performance targets for a given sla were met for a particular month .

where applicable , the percentage of seats meeting an sla was analyzed by seat type ( i.e. , basic , high end , and mission - critical ) .

enterprisewide sla performance is based on whether the sla was met for a given month , meaning that all performance targets for a given sla were met for a particular month .

sla 101-end user problem resolution: this sla measures the percentage of all resolved nmci problems against identified performance target values .

figure 14 portrays the contractor's historical site level performance with sla 101 .

sla 102-network problem resolution: this sla measures the resolution of problems associated with the contractor provided network devices and connections .

figure 15 portrays the contractor's historical site level performance with sla 102 .

sla 103-end user services: this sla measures performance with end user services , including e - mail , web and portal , file share , print , network logon , access to government applications , and ras services .

figure 16 portrays the contractor's historical site level performance with sla 103 .

figure 17 portrays the contractor's historical enterprisewide performance with sla 103 .

sla 104-help desk: this sla measures help desk services including , average speed of answer , average speed of response , call abandonment rate , and first call resolution .

figure 18 portrays the contractor's historical enterprisewide performance with sla 104 .

sla 105-move , add , change ( mac ) : this sla measures the time to complete mac activity , from the receipt of the mac request from an authorized government submitter to the completion of the mac activity .

macs include activities such as moving a seat from one location to another and adding seats at a location .

figure 19 portrays the contractor's historical site level performance with sla 105 .

sla 106-information assurance ( ia ) services: this sla measures the contractor's ia services , including security event detection , security event reporting , security event response , and ia configuration management .

figure 20 portrays the contractor's historical enterprisewide performance with sla 106 .

sla 107-nmci intranet: this sla measures performance of the nmci intranet in areas of availability , latency / packet loss , and quality of service in support of videoteleconferencing and voice - over - ip .

figure 21 portrays the contractor's historical site level performance with sla 107 .

sla 203-e - mail services: this sla measures the performance of e - mail transfers .

figure 22 portrays the contractor's historical enterprisewide performance with sla 203 .

sla 204-directory services: this sla measures the availability and responsiveness of directory services .

directory services include supporting the management and use of file services , security services , messaging , and directory information ( eg , e - mail addresses ) for users .

figure 23 portrays the contractor's historical site level performance with sla 204 .

figure 24 portrays the contractor's enterprisewide performance with sla 204 .

sla 206-web access services: this sla measures the performance of user access to internal and external web content .

figure 25 portrays the contractor's historical site level performance with sla 206 .

figure 26 portrays the contractor's historical enterprisewide performance with sla 206 .

sla 211-unclassified but sensitive internet protocol router network ( niprnet ) access: this sla measures the performance of niprnet access , including latency and packet loss .

figure 27 portrays the contractor's historical site level performance with sla 211 .

figure 28 portrays the contractor's historical enterprisewide performance with sla 211 sla 225 – base area network / local area network ( ban / lan ) communications services: this sla measures ban / lan performance , including availability and latency .

figure 29 portrays the contractor's historical site level performance with sla 225 .

sla 226-proxy and caching service: this sla measures the availability of the proxy and caching services .

proxy servers are located between a client and a network server and are intended to improve network performance by fulfilling small requests .

figure 30 portrays the contractor's historical enterprisewide performance with sla 226 .

sla 231-system service - domain name server: this sla measures the availability and latency of domain name server services .

the domain name server translates domain names to ip addresses and vice versa .

figure 31 portrays the contractor's historical site level performance with sla 231 .

figure 32 portrays the contractor's historical enterprisewide performance with sla 231 .

sla 324-wide area network connectivity: this sla measures the percent of bandwidth used to provide connection to external networks .

figure 33 portrays the contractor's historical site level performance with sla 324 .

sla 325-ban / lan communication services: this sla measures the percent of bandwidth utilized on shared network segments .

figure 34 portrays the contractor's historical site level performance for sla 325 .

sla 328-network management service – asset management: this sla measures the time it takes to implement new assets , such as seats , and application servers .

figure 35 portrays the contractor's historical site level performance with sla 328 .

sla 329-operational support services: this sla measures the effectiveness of nmci's disaster recovery plan .

figure 36 portrays the contractor's historical enterprisewide performance with sla 329 .

sla 332-application server connectivity: this sla measures both the time it takes for the contractor to implement the connectivity between the network backbone and an application server and the percentage of available bandwidth from an application server to the local supporting backbone .

figure 37 portrays the contractor's historical site level performance with sla 332 .

sla 333-nmci security operational services – general: this sla measures the percentage of successful accreditations on the first attempt , based on compliance with dod certification and accreditation policies and procedures .

figure 38 portrays the contractor's historical enterprisewide performance with sla 333 .

sla 334-information assurance operational service – pki: this sla measures the timeliness of revoking a pki certificate when required , ability of a nmci user to obtain the dod pki certificate of another nmci user , and the time it takes for user registration of dod pki within nmci .

figure 39 portrays the contractor's historical enterprisewide performance with sla 334 .

sla 336-information assurance planning services: this sla measures the time it takes to distribute new or revised security products ( hardware and software ) .

figure 40 portrays the contractor's historical enterprisewide performance with sla 336 .

in addition to the individual named above , mark bird , assistant director ; scott borre ; timothy case ; barbara collier ; vijay d'souza ; neil doherty ; jim fields ; mike gilmore ; peggy hegg ; wilfred holloway ; george kovachick ; frank maguire ; charles roney ; sidney schwartz ; karl seifert ; glenn spiegel ; dr. rona stillman ; amos tevelow ; and eric winter made key contributions to this report .

