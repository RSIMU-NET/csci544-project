the federal government awards hundreds of billions of dollars in grants and contracts annually .

to help ensure compliance with applicable laws and regulations , the recipients of these funds are required to report spending and other information .

however , grant recipients and federal contractors often face challenges related to duplicative and burdensome reporting .

effective implementation of the digital accountability and transparency act of 2014 ( data act ) offers the promise of addressing some of the challenges of duplicative and burdensome reporting as it requires the office of management and budget ( omb ) and department of the treasury ( treasury ) to establish standardized government - wide financial data standards .

a key lesson learned from our prior reports on the american recovery and reinvestment act of 2009 is that standardized data could decrease the burden on federal fund recipients and increase the accuracy of the data reported .

in addition , section 5 of the federal funding accountability and transparency act of 2006 ( ffata ) , as amended by the data act , provides another opportunity for simplifying reporting for federal contracts , awards , and subawards .

it directs omb , or a federal agency designated by omb , to establish a pilot program to develop recommendations for eliminating unnecessary duplication in financial reporting and reducing compliance costs for federal award recipients .

this report is our latest work in response to a statutory provision to review data act implementation .

this report ( 1 ) describes the administration's approach to the section 5 pilot requirements , ( 2 ) assesses whether current activities and plans that were available during the review period will likely allow omb and its partners to meet requirements and time frames established under the section 5 pilot , and ( 3 ) evaluates the extent to which the pilot design are consistent with leading practices .

to address these objectives , we assessed pilot activities by reviewing design documentation from the department of health and human services ( hhs ) and omb's office of federal procurement policy ( ofpp ) .

our reviews were based on the latest design plans available at the time — specifically , hhs's draft plan dated november 16 , 2015 , and ofpp's working draft dated november 28 , 2015 .

we interviewed omb , hhs , and general services administration ( gsa ) officials responsible for implementing section 5 of the act .

we also interviewed officials from organizations representing key nonfederal stakeholders , including state governments , private - sector contractors , and other federal fund recipients .

to identify leading practices for pilot design , we reviewed past work evaluating and assessing pilots as well as relevant studies from academia and other entities .

we plan to conduct a review on the implementation of the pilot and the extent to which its design will allow omb to develop recommendations for reducing recipient reporting burden later this year .

additional details regarding our objectives , scope , and methodology are provided in appendix i .

we conducted this performance audit from may 2015 to april 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the data act became law in may 2014 and holds considerable promise for shedding more light on how federal funds are spent .

to improve the transparency and quality of the federal spending data made available to the public , the data act directed omb and treasury to establish government - wide data standards that include common data elements for reporting financial and payment information by may 2015 .

under the act , federal agencies must begin reporting financial spending data using these standards by may 2017 and publicly post spending data in a machine - readable format by may 2018 .

the data act also requires that omb , or an agency it designates , establish a pilot program to facilitate the development of recommendations to ( 1 ) standardize reporting elements across the federal government , ( 2 ) eliminate unnecessary duplication in financial reporting , and ( 3 ) reduce compliance costs for recipients of federal awards .

the act established reporting requirements and timeframes for implementation of the pilot .

see figure 1 for a timeline of these deadlines .

the data act also sets specific requirements related to the pilot's design .

first , the pilot must collect data during a 12-month reporting cycle .

the pilot must also include a diverse group of recipients such as awardees receiving a range of awards as long as the total value of the awards falls within the statutory range .

to the extent practicable , the pilot is to include recipients who receive federal awards from multiple programs across multiple agencies .

finally , the pilot must include a combination of federal contracts , grants , and subawards with an aggregate value between $1 billion and $2 billion .

in addition , omb must review the information recipients are required to report to identify common reporting elements across the federal government , unnecessary duplication in financial reporting , and unnecessarily burdensome reporting requirements for recipients of federal awards .

this review is to be done in consultation with relevant federal agencies and recipients of federal awards , including state and local governments and institutions of higher education .

a well - developed and documented pilot program can help ensure that agency assessments produce information needed to make effective program and policy decisions .

such a process enhances the quality , credibility , and usefulness of evaluations in addition to helping to ensure that time and resources are used effectively .

we have identified five leading practices that , taken together , form a framework for effective pilot design .

to identify these practices , we reviewed our prior work as well as academic literature related to the design of pilot and evaluation programs .

by following these leading practices , agencies can promote a consistent and effective pilot design process .

we shared these practices with omb , hhs , and gsa staff , who found them to be reasonable and appropriate , and applicable to the section 5 pilot .

1 .

establish well - defined , appropriate , clear , and measurable objectives .

such objectives should have specific statements of the accomplishments necessary to meet the objectives .

clear and measurable objectives can help ensure that appropriate evaluation data are collected from the outset of pilot implementation so that data will subsequently be available to measure performance against the objectives .

broad study objectives should be translated into specific , researchable questions that articulate what will be assessed .

2 .

clearly articulate assessment methodology and data gathering strategy that addresses all components of the pilot program and includes key features of a sound plan .

key features of a clearly articulated methodology include a strategy for comparing the pilot implementation and results with other efforts , a clear plan that details the type and source of the data necessary to evaluate the pilot , and methods for data collection including the timing and frequency .

3 .

identify criteria or standards for identifying lessons about the pilot to inform decisions about scalability and whether , how , and when to integrate pilot activities into overall efforts .

the purpose of a pilot is generally to inform a decision on whether and how to implement a new approach in a broader context .

therefore , it is critically important to consider how well the lessons learned from the pilot can be applied in other , broader settings .

to assess scalability , criteria should relate to the similarity or comparability of the pilot to the range of circumstances and population expected in full implementation .

the criteria or standards can be based on lessons from past experiences or other related efforts known to influence implementation and performance as well as on literature reviews and stakeholder input , among other sources .

the criteria and standards should be observable and measureable events , actions , or characteristics that provide evidence that the pilot objectives have been met .

choosing well - regarded criteria against which to make comparisons can lead to strong , defensible conclusions .

4 .

develop a detailed data - analysis plan to track the pilot program's implementation and performance and evaluate the final results of the project and draw conclusions on whether , how , and when to integrate pilot activities into overall efforts .

a detailed data - analysis plan identifies who will do the analysis as well as when and how data will be analyzed to measure the pilot program's implementation and performance .

the results will show the successes and challenges of the pilot , and in turn , how the pilot can be incorporated into broader efforts .

some elements of a detailed data - analysis plan include talking to users , managers , and developers ; evaluating the lessons learned to improve procedures moving forward ; and other appropriate measures .

5 .

ensure appropriate two - way stakeholder communication and input at all stages of the pilot project , including design , implementation , data gathering , and assessment appropriate two - way stakeholder communication and input should occur at all stages of the pilot , including design , implementation , data gathering , and assessment .

failure to effectively engage with stakeholders , and understand and address their views can undermine or derail an initiative .

to that end , it is critical that agencies identify who the relevant stakeholders are , and communicate early and often to address their concerns and convey the initiative's overarching benefits .

omb has established a section 5 pilot with two primary focus areas — one on federal grants and another on federal contracts ( procurement ) .

omb's office of federal financial management is responsible for the grants portion of the pilot and has designated the department of health and human services ( hhs ) to serve as its executing agent .

on the contracting side , omb's ofpp is responsible for leading the procurement portion and is working with various entities including 18f and the chief acquisitions officers' council ( caoc ) .

specifically , 18f is designing the system to be tested as part of the pilot .

gsa's office of government - wide policy is responsible for providing federal register notices ; and its integrated award environment provides guidance and technical considerations .

omb launched a number of pilot - related initiatives in may 2015 and expects to continue activities until at least may 2017 .

as the executing agent for the grants portion of the pilot , hhs has developed six “test models” that will evaluate different approaches to potentially reducing grantee reporting burden .

these six models are the specific grants tools , forms , or processes that will be tested and analyzed under the pilot to determine if adopting these changes will actually contribute to the program's objectives of reducing reporting burden , duplication , and compliance costs .

taken as a whole , the six test models examine a variety of grant reporting issues that hhs has identified as presenting challenges .

hhs officials told us that they have received comments through the national dialogue , a website for grant recipients and contractors to discuss issues including compliance costs , reporting burden , eliminating duplication , and standardizing processes .

in addition , the officials obtained feedback on areas of concern from grantees involved in earlier hhs efforts to streamline grants reporting .

they used that information to inform the development of the six test models .

officials from advocacy groups representing grant recipients and federal contractors told us that they initially expected the grants portion of the pilot to be an extension of the grants reporting information project ( grip ) proof of concept that was launched following the enactment of the american recovery and reinvestment act of 2009 rather than the six test models .

hhs officials told us they would have liked to more fully replicate the grip , however , that would have required broader participation from agencies than was available for the section 5 pilot .

the following provides high - level summaries of each of the six test models .

for additional details , see appendix ii .

hhs intends to assess whether an online and searchable repository for data standards will facilitate grant reporting .

to do this , hhs developed the common data element repository ( cder ) library , which is intended to be an authorized source for data elements and definitions for use by the federal government and recipients reporting grant information .

the cder library is also intended to encourage the use of common definitions for grants - related terms by nonfederal stakeholders and federal agencies .

as of march 2016 , the publicly - available version of the cder library contained 112 data elements from a variety of sources , including the federal acquisition regulation ( far ) , omb circular a - 11 , and the uniform grant guidance .

it also included several data elements standardized in accordance with data act requirements .

hhs has developed a version of the cder library , accessible only to federal agencies , that contains a much more detailed database of more than 9,000 elements .

this federal - agency - only version of the cder library also identifies which grant reporting forms these data elements come from so that users can see how many forms require the same data element and which agencies request that information from grantees .

hhs officials told us that they believe the cder library has the potential to be a powerful tool for streamlining definitions and forms .

hhs intends to test whether it will be possible to use a consolidated federal financial report ( ffr ) to allow grantees to submit multiple reporting forms into one system .

the ffr , reported on the standard form 425 , is used for reporting grants expenditures for the recipients of federal assistance .

hhs believes that a consolidated ffr will allow participants to submit complete information once instead of through multiple entry points .

a consolidated ffr could provide a single point of data entry , earlier validation of ffr data , and potential future streamlining of the grants close - out process .

according to hhs officials , this test model is intended to be a continuation of the grip launched during the american recovery and reinvestment act of 2009 .

the aim of that effort was to determine the feasibility of developing a centralized government - wide collection system for federal agencies and recipients of federal awards .

hhs is examining ways to reduce duplicate and redundant information contained in single audit forms .

the single audit act requires states , local governments , and nonprofit organizations expending $750,000 or more in federal awards in a year to obtain an audit in accordance with the requirements set forth in the act .

hhs intends to test whether some grant forms related to the single audit could be combined .

hhs plans to examine whether a consolidated notice of award coversheet might reduce reporting burden by allowing grant recipients to locate required reporting data in one place , rather than attempting to find information on coversheets that differ by agency .

hhs added a new section to the grants.gov website , called learn grants , intended to make it easier for stakeholders to find , learn about , and apply for federal grants .

the learn grants website provides links to grant policies , processes , funding , and other grant lifecycle information .

hhs officials said they want to use this test model to determine whether the learn grants site could effectively engage stakeholders and provide training early in the grants lifecycle process that , in turn , would have a positive effect on recipient compliance during post - award activities .

the procurement portion of the pilot will be focused on examining the feasibility of centralizing the reporting of certain required information .

depending on the contract , there may be many types of information contractors must report .

ofpp staff told us the pilot will initially focus on the reporting of certified payroll .

this is one specific far requirement only applicable to contracts for construction within the united states .

specifically , ofpp has identified opportunities to improve upon the current unstandardized reporting format under which some employers report data electronically while others use manual paper processes .

further , ofpp intends to identify which data elements would be included in reporting , the method of data transmission , and other related details .

this narrow approach stands in contrast to the grants portion of the pilot where hhs has a broader , more comprehensive plan to explore several areas where grantee reporting burden might be reduced .

ofpp staff explained that its decision to focus on certified payroll reporting arose out of feedback from the procurement community .

they also noted that the section 5 pilot is one of a number of government - wide initiatives to reduce contractor burden and streamline procurement processes , such as gsa's integrated award environment initiative to integrate acquisition systems into one streamlined environment .

to better understand the issue of certified payroll reporting and its potential suitability as a subject for the procurement portion of the section 5 pilot , the caoc engaged gsa's 18f through an interagency agreement to interview contractors , contracting officers , business owners , government employees , and subject - matter experts ( sme ) .

as a result of that effort , 18f identified major categories of burdens and constraints related to certified payroll reporting and potential recommendations on how to address them .

ofpp staff said they once again worked with 18f in winter 2016 to gather requirements for building a prototype system to centralize the reporting of certified payroll data .

the 18f staff we spoke with noted that they will build a prototype to explore potential solutions for reducing contractor burden through user research and testing .

ofpp staff will develop and evaluate metrics for the pilot .

ofpp intends to test the system in summer 2016 .

in may 2015 , omb , caoc , gsa , and hhs launched the national dialogue , a website for grant recipients and federal contractors to discuss issues including compliance costs , reporting burden , eliminating duplication , and standardizing processes .

omb staff told us that they used the national dialogue as a feedback mechanism for the grants and procurement portions of the pilot .

this was one of the first publicly announced pilot - related activities .

the website will accept comments through may 2017 .

omb and gsa staff told us that they plan to actively review and address the input they receive .

the website is intended to be a useful tool for obtaining information about issues of concern to their respective communities .

discussions related to grantee reporting have been significantly more active than those focused on procurement .

although the comments vary widely in topic , there are a number of substantive suggestions for how grantee reporting burdens can be reduced .

while hhs officials told us that the dialogue was intentionally designed so that feedback would be submitted anonymously , some commenters have self - identified the institution they represent , including the council on governmental relations , association on american universities , association of public and land - grant universities , and coalition for government procurement .

if hhs effectively implements its stated plans for the grants portion of the section 5 pilot , it is likely that the grants portion of the pilot will comply with the act .

these requirements call for the grants portion's design to include the following: data act requirement 1: collect data during a 12-month reporting cycle .

hhs's november 2015 design documentation shows that it will begin collecting data for these six test models by may 2016 .

this would allow for data to be collected on these test models during a 12-month reporting cycle before may 2017 , when the pilot is required to terminate .

we believe these timeframes should provide sufficient time for hhs to incorporate public comments by may 2016 and allow for a full 12-month data collection cycle .

data act requirement 2: include a diverse group of federal award recipients and , to the extent practicable , recipients who receive federal awards from multiple programs across multiple agencies .

hhs officials told us that they have developed a detailed plan to select participants , which will include state and local governments , universities , and other types of grant recipients .

hhs officials explained that the grants portion of the pilot will include recipients who received a range of federal funding amounts and will not be limited to one agency or grant program .

hhs officials initially told us that they could not provide us with the revised plan because it was still under review by omb .

we did receive a copy of the revised plan at the end of march 2016 , but because of the timing we were unable to fully review it in time for the release of this report .

we will provide our assessment of the plan as part of future work as we continue to monitor the design and implementation of the section 5 pilot .

data act requirement 3: include a combination of federal contracts , grants , and subawards , with an aggregate value of not less than $1 billion but not more than $2 billion .

hhs officials told us that they are still determining how to meet the requirement for total award value because they want to ensure the pool of pilot participants is as diverse and large as possible while still being legally compliant .

specifically , one of their selection considerations is the award value of grants received by awardees .

further , hhs officials have explored strategies to ensure that they do not exceed the maximum dollar amount threshold .

hhs officials told us that they expect to make decisions related to how to meet this requirement in early 2016 .

we have concerns about the extent that the design of the procurement portion of the pilot reflects the requirements specified in the data act .

ofpp's plans to address those statutory design requirements discussed below reflect the status of the procurement portion of the pilot described by ofpp staff and related documents we reviewed .

data act requirement 1: collect data during a 12-month reporting cycle .

the design of the procurement portion of the pilot is at risk of not including data collected during a 12-month reporting cycle in a meaningful way .

to meet this requirement , ofpp and gsa would need to begin collecting data no later than may 9 , 2016 .

when we spoke with ofpp staff , they stated that by launching the national dialogue in may 2015 , they believe they will have met the act's requirements that data collection take place during a 12-month reporting cycle .

further , staff also considered comments received from other efforts including the open dialogue on improving how to do business with the federal government conducted in 2014 to meet this requirement .

however , neither of these dialogues included comments that specifically mentioned the issue of certified payroll .

as a result , we do not believe those comments provide meaningful and relevant data on the effectiveness of a centralized portal for certified payroll reporting .

as a result of design and development delays , ofpp will not be able to collect meaningful and useful data for the procurement portion of the pilot until summer 2016 , when it expects to complete the development of a centralized portal through which participants will submit certified payroll data .

ofpp started exploring ways to streamline certified payroll reporting in spring 2015 .

ofpp said that due to staffing challenges , work on designing a prototype for a system to be tested under the pilot did not begin until late february 2016 .

at that time , the caoc signed an agreement with gsa's 18f to begin what it expected to be a 10-week design period .

cognizant staff expect this design work will take place between march and may 2016 .

however , a contractor cannot begin building an actual “production” version of the system to be tested under the pilot until 18f designs the prototype , which is expected to be completed by the beginning of may 2016 .

therefore , this leaves at most a few weeks to develop the centralized reporting portal before may 9 , 2016 — the date which the pilot must begin for meaningful and useful data to be collected in a full 12- month period .

ofpp staff told us that they do not intend to begin testing a centralized reporting portal until late summer 2016 .

according to ofpp and gsa staff , they were faced with delays due to bid protests related to the contracting mechanism gsa intends to use to select a contractor to build the portal to be tested under the pilot .

however , as of march 2016 , these bid protests have been resolved and no longer present a barrier in awarding the contract .

while we agree that these protests could pose a barrier to awarding the contract to develop the testing portal , we do not believe that ofpp needed to wait until they were resolved before moving forward with 18f's development of a prototype for the portal .

given the resolution of these bid protests , ofpp staff said that they are working with 18f to assess the feasibility of expediting project timelines to launch the prototype sooner than expected so that they could potentially collect 10 months of data through the certified payroll reporting portal .

given the weekly or bi - weekly reporting of certified payroll , this approach may result in a sufficient amount of meaningful and useful data on which ofpp can base conclusions related to its hypothesis .

however , it is important that ofpp clearly conveys and documents its rationale for how its approach will contribute to the collection of meaningful and useful data consistent with the timeframes established under the act .

data act requirement 2: include a diverse group of federal award recipients and , to the extent practicable , recipients who receive federal awards from multiple programs across multiple agencies .

ofpp and gsa do not yet have a detailed plan for selecting participants that will result in a diverse group of recipients with awards from multiple programs and agencies .

however , there is some documentation related to ofpp's approach for selecting participants in the project plan and in a federal register notice issued on november 24 , 2015 .

for example , the draft plan identifies the federal procurement data system - next generation as the mechanism that will be used for identifying which contracts and contractors to include in the pilot .

ofpp staff also told us that they intend to cover both large and small industries .

while valuable information , these documents do not clearly convey how the procurement portion of the pilot would specifically contribute to meeting the act's requirement regarding diversity of participants .

ofpp staff told us that for the purposes of meeting the pilot requirements they consider any individual or group that provided information to the national dialogue to be a participant in the pilot .

however , as previously mentioned , individuals and groups that have commented on the national dialogue did not provide any comments related to certified payroll .

therefore , it is unclear how they could be considered pilot participants .

additionally , ofpp staff were unable to tell us how they plan to count commenters that are not contract awardees , but instead are organizations representing groups of federal contractors .

it is unclear how ofpp can ensure the universe of commenters is diverse because it does not control who comments on the dialogue .

ofpp staff stated that they also intend to select participants for testing their prototype system using a nongeneralizable sample of contractor data reported through the federal procurement data system - next generation .

however , they did not provide us with specific information on how they would ensure that the sample met all requirements under the act , nor did they provide a detailed , documented sampling plan equivalent to the grants portion of the pilot .

as a result , it will be important for ofpp to clearly document its rationale for how its approach will allow for the inclusion of a diverse group of federal contractors , as required by the act .

data act requirement 3: include a combination of federal contracts , grants , and subawards , with an aggregate value of not less than $1 billion but not more than $2 billion .

ofpp staff told us omb could meet this dollar range requirement through the grants and procurement portions of the pilot collectively .

under such an approach , it would be important for each portion of the pilot to know how much it is contributing to meet the required award range .

our understanding of the grants portion of the pilot suggests that that it has a plan for doing this .

less apparent are the specifics of how the procurement portion of the pilot would do so .

we assessed the designs of the grants and procurement portions of the pilot against leading practices that we identified from our prior work and other sources .

in continuation of our constructive engagement approach for working with agencies implementing the data act , we shared the results of our analysis with hhs and ofpp staff who told us that they will consider our input as they continue to update and revise their plans .

hhs's november 2015 design for the grants portion of the pilot generally applied leading practices .

as noted above , while we have received a revised plan for the design of the grants portion , we were unable to fully review it in time for the release of this report .

we will provide our assessment of that plan in a forthcoming review that will focus on the pilot's implementation .

data act grants test models under the office of management and budget's ( omb ) direction , the department of health and human services ( hhs ) intends to develop recommendations for reducing grantee reporting burden by testing different areas .

hhs will develop and test: an online repository for data elements and definitions that is intended to be an authoritative source for data elements and definitions , called the common data element repository ( cder ) library .

a federal agency - only version of the cder library containing more than 9,000 grants data elements that identify which specific grant forms these data elements come from , so that users can see how many forms require the same data element and which agencies request that information .

leading practice 1: establish well - defined , appropriate , clear , and measurable objectives .

each of the six grants test models at least partially met the leading practice that pilots have well - defined , appropriate , clear , and measurable objectives .

for example , one of the single audit test models has the clearly defined objective of testing whether two forms containing duplicative information can be combined to reduce recipient reporting burden .

this objective is measurable and appropriately linked to the purposes of the section 5 pilot overall , which include eliminating unnecessary duplication in financial reporting and reducing compliance costs for recipients of federal awards .

in another example , one of the cder library test models has a clearly established objective of determining whether access to an authoritative source for common data element definitions would help grant recipients complete necessary forms accurately and in a timely manner .

the cder library test model also identifies specific metrics that would allow them to measure whether they are able to achieve its stated objectives .

in our initial review of these test models , we provided feedback to hhs that the other cder library test did not have a clear , fully established objective .

in response , hhs officials explained that the objective of that test model is to compare data elements and forms used across the federal government with the goal of consolidating these forms and ultimately passing on reporting efficiencies to grant recipients .

leading practice 2: clearly articulate an assessment methodology .

five of the six test models did not clearly articulate an assessment methodology .

in contrast , for the learn grants test model , hhs described how it planned to use webinars , conference presentations , and other events to increase awareness inside and outside of government about the grants - related resources available on grants.gov .

the plan also includes a detailed timeline for executing the test model , as well as hhs's methodology for conducting pre - and post - tests of pilot participants .

hhs officials told us that they worked with a federal sme with previous experience working on grants.gov to help develop and refine the assessment methodology .

the remaining five test models have less clearly articulated assessment methodologies .

for example , for the consolidated ffr test model , hhs said it will survey grant recipients on their experiences when submitting their reports into one system rather than multiple entry points ; but we found that the plans lacked detail about how surveys will be designed and administered .

in addition , the plan did not provide specific information about the participants hhs intends to survey , nor did it provide details regarding how hhs will compare survey results for recipients in the pilot versus those not participating in the pilot .

in meetings with senior hhs officials , we raised these and similar concerns about the notice of award test model and one of the cder library models .

for the other cder library test model , we found that hhs's plans did not identify the data sources or metrics that would be used in the assessment methodology .

in those feedback meetings , hhs officials said many of the concerns have been addressed in the revised plan .

leading practice 3: ensure scalability of pilot design .

hhs documented an overall structure for how each test model is integrated into the overall grants portion of the pilot .

however , the documented design lacks specific details about how hhs intends to evaluate the performance of each test model to inform decisions about scalability .

specifically , five of the six test models include either no or few specifics about how any observed reduction in burden could be generalizable beyond the context of the pilot .

for example , hhs's plan for the consolidated ffr test model indicates that it will be tested using grantees who receive awards from the administration for children and families ( acf ) , a subunit of hhs .

however , the plan does not specify how acf will select participants or how results from acf grant recipients can be applied government - wide .

hhs officials told us that acf has a list of potential participants .

given the size and complexity of acf's grant recipients , the officials believed that these participants would provide a good basis for scalability should the ffr test model prove to be successful .

according to hhs officials , they have developed a comprehensive sampling plan for selecting participants for each of the six test models .

they will reach out to selected participants to begin data collection in may 2016 .

we have recently been provided with the draft sampling plan and will provide our assessment of it in our forthcoming review on the implementation of the section 5 pilot .

leading practice 4: develop a plan to evaluate pilot results .

the design for five of hhs's six test models provides some level of detail on how it plans to evaluate pilot results .

for instance , hhs's learn grants test model provides a description of a methodology to measure knowledge about the grants lifecycle .

it will compare a group of recipients that has access to certain grant resources contained in a public on - line portal to another group of recipients that does not .

hhs's plans indicate that the results from both tests will be analyzed to evaluate knowledge gained by participants to draw conclusions about the effectiveness of the learn grants tab on the grants.gov website .

however , the documented pilot design lacks specific detail on how hhs plans to analyze the data it gathered and how it will draw conclusions about integrating the pilot activities into overall grant reporting efforts .

for example , both cder library test models reference an analysis plan for evaluating to see if burden has been reduced .

the plans do not indicate how hhs would determine if a particular time threshold represents a true reduction in burden and whether that burden is measured in minutes , hours , or some other unit of analysis .

similarly , the single audit and notice of award test models indicate that hhs will use results from surveys and focus groups , including documenting benefits and challenges raised by participants ; yet hhs's plans for these two test models do not specify how hhs will compile these results and distill them into actionable recommendations .

hhs officials told us that their revised planning documents are to include this additional level of detail to address our concerns .

leading practice 5: ensure appropriate two - way stakeholder communication .

hhs has engaged in two - way stakeholder communications for all six of its test models .

it also has taken a number of actions to obtain input from grant recipients including posting questions on the national dialogue to solicit feedback on how to ease grantee reporting burden .

further , hhs has been involved in a number of outreach activities including presentations at conferences , town hall events , and webinars to identify areas of reporting burden and duplication , and to collect ideas to streamline reporting .

hhs also used these forums to provide updates on the progress of the design and specific information on the six test models .

hhs supplemented input received through the national dialogue with feedback from sme to help design the test models .

an hhs official told us they identified smes based on their experience working with federal grants , grant recipients , and systems being tested .

hhs officials provided several examples of how they engaged in two - way communication with stakeholders when developing their test models .

for example , hhs consulted with a federal official who used to work for grants.gov to help develop the learn grants test model and the pre - and post - test evaluations associated with it .

for the ffr test model , hhs consulted with officials who work in acf and the payment management system .

hhs also worked with other smes from across the federal government to develop other test models .

according to a hhs official , smes were asked to critically assess the methodology for each of the models with the intent of making each model more effective .

more recently , in january 2016 , hhs pre - tested proposed section 5 pilot test models and obtained feedback on ways to improve them with advocacy groups representing those in the grants recipient communities including state and local governments as well as research universities .

also included were representatives from the auditing and software development industries .

hhs officials told us that they have made significant revisions in response to the pre - tests and feedback to their documented design .

however , hhs has additional opportunities to foster two - way dialogue with recipients of federal funds .

officials from advocacy groups representing federal funding recipients told us that they are still waiting for information about how their membership can be more engaged in the pilot process .

for example , an official from the national association of state auditors , comptrollers , and treasurers told us that following a webinar for their membership hosted by the association of government accountants in november 2015 on the section 5 pilot , they collected the names of more than 20 state and local government representatives who were interested in participating in the grants portion of the pilot .

this official said the names were given to hhs , but the association has not received any information on how these volunteers can participate in the pilot .

hhs officials said that once they receive omb approval on their sampling methodology for selecting participants , they will be able to reach out to those who expressed interest in being a part of the pilot .

we provided our assessment of the design of the grants portion of the pilot to hhs officials , who told us that they generally concurred with our analysis and had updated their plan to address many of these concerns .

as noted above , we did not have time to review this update in this report because we did not receive the plan in time .

for details of our assessment of the design of the six grants test models , see appendix ii .

based on our review of the working draft plan for the procurement portion of the pilot dated november 2015 , related documents , and interviews with cognizant staff , we found that the design did not reflect leading practices for pilot design .

further , while the plan included some information regarding responsibilities of stakeholders involved in the procurement portion of the pilot , specific roles and deliverables were not clearly described for all phases of the pilot .

for example , the written draft plan listed broad areas of responsibilities — such as “manage funding” or “federal register notice” — but did not detail what stakeholders would be working on related to those activities .

ofpp staff described additional actions to supplement the information contained in the draft plan .

this information included their decision to initially focus the design of the procurement pilot on testing the feasibility of centralizing certified payroll reporting by contractors subject to the davis - bacon and related acts because of public feedback on the need to reduce duplicate reporting .

however , even after taking this additional information into account , we found that the design was neither well - developed nor documented in accordance with leading practices to allow for the development of effective recommendations to simplify reporting for contractors , as described below .

leading practice 1: establish well - defined , appropriate , clear , and measurable objectives .

the working draft plan provided by ofpp does not include specifics pertaining to the proposed focus of certified payroll reporting .

ofpp staff told us that they believe submitting certified payroll information through a centralized portal would reduce contractor reporting burden .

they explained that this topic was selected because they learned that it was a particular pain point for contractors as a result of various outreach efforts including 18f's discovery process .

the draft plan also does not provide specifics regarding the particular objectives and hypothesis that will be tested by the pilot .

ofpp staff stated that , consistent with their view of agile practices , they intend to further refine their approach as 18f develops its prototype and additional work proceeds with the pilot .

leading practice 2: clearly articulate an assessment methodology .

the draft plan we reviewed did not include detailed information on the methodology , strategy , or types of data planned to be collected .

the draft plan referenced an information - gathering effort conducted by gsa's 18f to discover challenges and develop recommendations for burden reduction .

however , ofpp staff could not provide any evidence that this effort resulted in specific methodologies or data - collection strategies related to centralizing certified payroll reporting .

according to 18f staff , a second phase of the procurement portion of the pilot will begin in march 2016 .

ofpp staff said that during this phase , 18f will research , design , and test a prototype that will become a basis for the centralized portal that will be tested under the pilot .

this prototype will be vetted in workshops with stakeholders who will test , among other things , the metrics , functionality , and accessibility of the prototype and any needed changes .

18f expects the second phase to be completed by may 2016 , after which ofpp will begin the third phase of the pilot later this summer .

in that phase , a contractor will develop a centralized portal based on 18f's design that could be used to test the submission and review of certified payroll data .

additionally , ofpp staff told us that they intended to collect data in accordance with far requirements and would compare the information collected in the portal with that being submitted through other methods .

however , ofpp was not able to provide specific details on its pilot methodology , such as how it intends to compare results of contractors that use the prototype and those that do not , identify the type and source of data necessary to evaluate the pilot , and establish the timing and frequency of the data to be collected .

without these details , the procurement methodology design does not address all components of a pilot program nor does it include key design features that would meet leading practices .

leading practice 3: ensure scalability of pilot design .

the draft design of the procurement portion of the pilot that we reviewed did not address the issue of scalability or efforts to ensure that conclusions and recommendations resulting from the pilot could be applied government - wide .

however , ofpp staff indicated that they plan to develop a sampling approach that will allow them to collect data from a population that is representative of federal contractors .

specifically , they said that they will select a diverse group of participants by potentially pulling data from the federal procurement data system - next generation .

using that database , they expect to be able to select a range of small and large contractors that are required to report certified payroll under davis - bacon and related acts .

however , without documentation providing details of a sampling methodology , measures , and a data analysis plan , the design cannot ensure the scalability of the results or findings from the pilot .

leading practice 4: develop plan to evaluate pilot results .

the draft procurement plan does not indicate how data will be evaluated to track program performance , how final results will be evaluated , or conclusions drawn .

ofpp staff told us that although they believe it is early in the process to have finalized evaluation plans , they are considering a number of options for evaluating whether a centralized certified payroll portal would cost more or less than current reporting approaches .

specifically , they said that they expect to have some quantifiable data to allow for straightforward analysis and will evaluate the qualitative data from the certified payroll portal as well as the national dialogue .

however , the absence of a detailed data analysis plan suggests that ofpp lacks a sound approach to evaluate pilot results .

leading practice 5: ensure appropriate two - way stakeholder communication .

ofpp's plans for obtaining stakeholder input and fostering two - way dialogue have not yet been developed to engage public participation and feedback on its approach for designing and implementing the procurement portion of the pilot .

similar to the approach taken by hhs , ofpp staff told us that they used comments posted on the national dialogue to inform the design of the procurement portion .

however , as previously mentioned , we have concerns about the usefulness of that approach because none of the three comments they received on the dialogue were related to certified payroll .

ofpp staff said they also used comments posted on the 2014 open dialogue on improving procurement processes to inform their pilot design .

from commentary posted on both sites , ofpp identified certified payroll reporting as a pain point that could be further explored through the pilot project .

ofpp staff told us that they engaged gsa's 18f to conduct the discovery phase of the pilot design to better understand areas of significant reporting burden related to certified payroll with a select group of stakeholders that included contractors , federal agency officials , and contracting officers .

a federal register notice was also issued on november 24 , 2015 to solicit public comments on the reporting burden of the procurement portion of the pilot under the paperwork reduction act .

although ofpp obtained stakeholder input to identify areas of focus for the design of the procurement portion of the pilot , it has not engaged them to solicit input on other stages of the pilot , including design , implementation , data gathering , and assessment .

further , ofpp has not released specific information about the design of the pilot , nor has it made information about pilot participation available to stakeholders despite repeated requests for information from those participating in monthly calls hosted by the association for government accountants and treasury .

in addition to being a leading practice for pilot design , our previous work examining grants management streamlining initiatives found that stakeholder communication is not just “pushing the message out,” but should also facilitate a two - way , honest exchange and allow for feedback from relevant stakeholders .

we found that a lack of opportunities to provide timely feedback resulted in poor implementation and prioritization of streamlining initiatives and limited recipients' use and understanding of new systems .

as such , it will be important for ofpp to engage with the procurement community on its pilot design so that it can be improved based on public input .

in addition , more effective two - way communications could also be a strategy for recruiting participants for the procurement portion of the pilot .

in crafting the data act , congress sought to reduce the burden and cost of reporting for the recipients of federal funds .

toward that end , omb , partnering with other federal agencies , has taken steps to design the section 5 pilot that will explore potential ways to reduce the burden and cost of reporting on federal funds for both the federal grantee and procurement communities .

however , we found uneven progress in the grants and procurement portions of the pilot .

omb and hhs have made considerable progress designing an overall approach that will examine a variety of potential ways to simplify reporting for grant recipients .

in addition to generally being on track to meet the specific requirements set out in the act , we found that the proposed design of the grants portion of the pilot partially adheres to leading practices .

in contrast , our review of the design of the procurement portion of the pilot raises several concerns .

in the absence of a detailed design and risk management plans for executing the pilot moving forward , it is unclear how the design of the procurement portion will reflect the requirements set forth by section 5 of the act .

because of project delays to date , it will be especially important for omb to communicate to congress and interested stakeholders how it plans to address key aspects of these requirements , such as the collection of meaningful and useful data over a 12-month reporting cycle and including a diverse group of participants with federal contracts totaling from $1 billion to $2 billion .

moreover , the design we reviewed for the procurement portion of the pilot did not reflect leading practices to allow for the development of effective recommendations to simplify reporting for contractors .

moving forward , given the tight timelines set out in the act , it will be important for omb to redouble its focus on the design and implementation of the procurement portion .

without a sound design that applies leading practices , the recommendations to congress for reducing reporting burden for contractors coming out of this effort may be late , of limited use , or incomplete .

1 .

to help ensure and more clearly convey how the procurement portion of the pilot will contribute to meeting the section 5 pilot design requirements , we recommend that the director of omb determine and clearly document ( 1 ) how it will collect certified payroll data over a 12-month reporting cycle , ( 2 ) ensure the diversity of pilot participants , and ( 3 ) how the inclusion of federal contracts will contribute to an aggregate amount of $1 billion to $2 billion .

2 .

to enable the development of effective recommendations for reducing reporting burden for contractors , the director of omb should ensure that the procurement portion of the pilot reflects leading practices for pilot design .

we provided a draft of this report to omb , hhs , and gsa for review and comment .

omb and hhs provided technical comments that we have incorporated throughout the report , as appropriate .

omb and hhs did not offer a view on our recommendations .

gsa did not have any comments .

we are sending copies of this report to the director of omb , secretary of hhs , administrator of gsa , and appropriate congressional addressees .

in addition , the report is available at no charge on the gao website at https: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me on ( 202 ) 512-6806 or by email at sagerm@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix iii .

this review ( 1 ) describes the administration's approach to the section 5 pilot ; ( 2 ) assesses whether current activities and plans will likely allow the office of management and budget ( omb ) and its partners to meet requirements and time frames established under the section 5 pilot ; and ( 3 ) evaluates the extent to which the design for the pilot is consistent with leading practices .

to describe the administration's approach to the pilot , we assessed documents related to pilot activities and interviewed omb , department of health and human services ( hhs ) , and general services administration ( gsa ) officials and staff responsible for implementing the section 5 pilot .

specifically , we reviewed documentation from hhs and omb's office of federal procurement policy ( ofpp ) .

our reviews were based on the latest design plans available at the time .

we also interviewed officials from organizations representing key non - federal stakeholders including state and local governments , private - sector contractors , and other federal fund recipients .

to assess whether the section 5 pilot design would be likely to meet the statutory design requirements , we reviewed section 5 of the federal funding accountability and transparency act of 2006 , as added by the digital accountability and transparency act of 2014 ( data act ) to understand the deadlines and design requirements .

we reviewed the draft design documents to assess omb and its partners' plans for meeting these requirements .

to supplement our review of those plans , we also spoke with cognizant staff implementing these pilots at omb , hhs , and gsa .

to identify and analyze leading practices for pilot design , we reviewed our past work evaluating and assessing pilots .

additionally , we also relied on our technical guidance on designing evaluations .

further , we reviewed relevant studies from academia as well as other entities , such as the brookings institution and the federal demonstration partnership .

we reviewed reports from organizations that have expertise on conducting pilot programs and experience in scaling pilot results that could be applied government - wide .

we also shared these leading practices with the agencies in this review during our audit work .

to assess the extent to which the section 5 pilot design adhered to these leading practices , we reviewed documented designs and plans for both the grants and procurement portions of the pilot .

to evaluate the grants portion of the pilot , we focused on a draft design document from november 2015 .

hhs officials told us that they have updated that plan .

because we did not receive this update until the end of march 2016 , we did not have time to include its content for this report .

as such , our assessment is based on the november 2015 plan .

we intend to review the updated plan as we continue our work on data act implementation .

we have supplemented our assessment with information hhs officials provided to us during subsequent interviews , as appropriate .

for the procurement portion , we reviewed a working draft plan from november 2015 .

while it is unclear whether there has been an updated version , we have also provided additional details from discussions with ofpp officials , as appropriate .

to evaluate the grants and procurement portions of the pilot , we applied the five leading practices we identified to omb and hhs's design documents .

each of those assessments were subsequently verified by another individual .

we determined that the design met the criteria when we saw evidence that all aspects of a leading practice were met .

when we were unable to assess whether all aspects of a leading practice were met without additional information , we determined that the design partially met the criteria .

finally , when we saw no evidence of a leading practice , we determined that the criteria was not met .

in continuation of our constructive engagement approach on the data act for working with agencies implementing the act , we provided hhs and omb with feedback on the design of the grants and procurement portions of the pilot during our review .

these officials generally accepted our feedback and , in some instances , noted that they have or would make changes to their design as a result of our input .

we conducted this performance audit from may 2015 to april 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

this appendix provides detailed information regarding our assessment of the pilot design for the grants portion of the section 5 pilot .

we assessed each of the department of health and human services's ( hhs ) six test models against the five leading practices for pilot design described in the report .

using hhs's november 2015 design plans and relevant supporting information available during the preparation of this report , we determined whether each test model met , partially met , or did not meet those leading practices .

in addition to the contact named above , j. christopher mihm ( managing director ) , peter del toro ( assistant director ) , shirley hwang ( analyst - in - charge ) , aaron colsher , kathleen drennan , jason lyuke , kiran sreepada , and david watsula made major contributions to this report .

other key contributors include lisette baylor , brandon booth , jenny chanley , robert gebhart , donna miller , carl ramirez , andrew j. stephens , and tatiana winger .

additional members of gao's data act internal working group also contributed to the development of this report .

