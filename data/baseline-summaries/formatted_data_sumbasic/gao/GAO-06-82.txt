performance data are becoming increasingly significant in helping policy makers and program managers assess progress of federal programs in meeting their long - term goals and in helping to make a variety of programmatic and budget decisions .

yet our previous work has identified limitations in the ability of federal agencies to produce credible performance data .

in particular , federal programs that are carried out in partnership with states and localities continually balance the competing objectives of collecting uniform performance data at the national level with giving states and localities the flexibility they need to implement programs .

the workforce investment act ( wia ) of 1998 — the centerpiece of the nation's employment and training system — established three programs that rely on states and localities to work together to track and report on participant outcomes in areas of job placement , retention , earnings , and skill attainment , as well as customer satisfaction .

wia , implemented in july 2000 , has resulted in a major shift from predecessor programs , including the job training partnership act ( jtpa ) program , by offering a broader array of services to the general public and no longer using income to determine eligibility for all program services .

wia also changed the way performance is measured , including establishing new performance measures that assess outcomes over time , requiring the use of unemployment insurance ( ui ) wage data to track outcomes , and requiring states to negotiate expected performance levels with the department of labor ( labor ) .

states are held accountable for achieving their performance levels through financial incentives and sanctions .

these changes have had profound implications for the way wia performance data are collected and reported .

given the magnitude of these changes , the potential impact such changes can have on data quality , and the importance of having meaningful performance data , we examined ( 1 ) the data quality issues that have affected states' efforts to collect and report wia performance data , ( 2 ) states' actions to address them , and ( 3 ) the actions labor is taking to address data quality issues and the issues that remain .

to learn more about states' experiences implementing data collection and reporting system changes for wia , their implementation of labor's data validation requirements for wia , and state and local efforts to address the quality of wia data , we conducted a web - based survey of workforce officials in 50 states and received a 100 percent response rate .

we did not include the district of columbia and u.s. territories in our survey .

in addition , we conducted site visits in california , new york , texas , west virginia , and wyoming , where we interviewed state officials and visited two local areas in each state .

we selected these states because they represent a range of information technology ( it ) systems — statewide comprehensive systems versus local systems with a state reporting function , include states with single and multiple workforce areas , and are geographically diverse .

we also collected information on the quality of wia data through interviews with labor officials in headquarters and all six regional offices , and nationally recognized experts , and reviewed relevant research literature .

our work was conducted between june 2004 and september 2005 in accordance with generally accepted government auditing standards .

 ( for a complete description of our scope and methodology , see app .

i. ) .

labor required states to implement major provisions of wia by july 1 , 2000 , although some states began implementing provisions of wia as early as july 1999 .

wia replaced the job training partnership act ( jtpa ) program and requires that many federal programs provide employment and training services through one - stop centers .

services funded under wia represent a marked change from those provided under the previous program , allowing for a greater array of services to the general public .

wia is designed to provide for greater accountability than under previous law: it established new performance measures and a requirement to use unemployment insurance ( ui ) wage data to track and report on outcomes .

program services provided under wia represent a marked change from those provided under jtpa .

when wia was enacted in 1998 , it replaced the jtpa programs for economically disadvantaged adults and youth and for dislocated workers with three new programs — adult , dislocated worker , and youth — that provide a broad range of services to the general public , no longer using income to determine eligibility for all program services .

the wia adult and dislocated worker programs no longer focus exclusively on training , but provide for three tiers , or levels , of service: core , intensive , and training .

core services include basic services such as help with job searches and providing labor market information .

these activities may either be self - service or require some staff assistance .

intensive services include such activities as comprehensive assessment of jobseekers' skill levels and service needs and case management — activities that typically require greater staff involvement .

training services include such activities as occupational skills development or on - the - job training .

labor's guidance specifies that monitoring and tracking for the adult and dislocated worker programs should begin when jobseekers receive core services that require significant staff assistance .

jobseekers who receive core services that are self - service or informational in nature are not counted in the performance measures .

in addition to those services provided by the three wia funded programs , wia also requires that states and local areas use the one - stop center system to provide services for many other employment and training programs .

seventeen categories of programs funded through four federal agencies are now required to provide services through the one - stop center under wia .

table 1 shows the programs that wia requires to provide services through the one - stop centers ( also known as mandatory programs ) and the federal agencies that administer these programs .

wia is designed to provide for greater accountability than its predecessor program by establishing new performance measures , a new requirement to use ui wage data to track and report on outcomes , and a requirement for labor to conduct at least one multi - site control group evaluation .

according to labor , performance data collected from the states in support of the measures are intended to be comparable across states in order to maintain objectivity in determining incentives and sanctions .

the performance measures also provide information to support labor's performance goals under the government performance and results act ( gpra ) , the budget formulation process using the office of management and budget's ( omb ) program assessment rating tool ( part ) , and for program evaluation required under wia .

in contrast to jtpa , under which data on outcomes were obtained through follow - ups with job seekers , wia requires states to use ui wage records to track employment - related outcomes .

each state maintains ui wage records to support the process of providing unemployment compensation to unemployed workers .

the records are compiled from data submitted to the state each quarter by employers and primarily include information on the total amount of income earned during that quarter by each of their employees .

although ui wage records contain basic wage information for about 94 percent of workers , certain employment categories are excluded , such as self - employed persons , independent contractors , federal employees , and military personnel .

according to labor's guidance , if a program participant does not appear in the ui wage records , states may then use supplemental data sources , such as follow - up with participants and employers , or other administrative databases , such as u.s. office of personnel management or u.s. department of defense records , to track most of the employment - related measures .

however , only ui wage records may be used to calculate the earnings change and earnings replacement performance measures .

 ( see table 2 for a complete list of wia performance measures. ) .

unlike jtpa , which established expected performance goals using a computer model that took into account varying economic and demographic factors , wia requires states to negotiate with labor to establish expected performance levels for each measure .

states , in turn , must negotiate performance levels with each local area .

the law requires that these negotiations take into account differences in economic conditions , participant characteristics , and services provided .

to derive equitable performance levels , labor and the states use historical data to develop their estimates of expected performance levels .

these estimates provide the basis for negotiations .

wia holds states accountable for achieving their performance levels by tying those levels to financial sanctions and incentive funding .

states that meet their performance levels under wia are eligible to receive incentive grants that generally range from $750,000 to $3 million .

nineteen states were eligible to apply for incentive grants in program year 2003 .

states that do not meet at least 80 percent of their wia performance levels are subject to sanctions .

if a state fails to meet its performance levels for 1 year , labor provides technical assistance , if requested .

if a state fails to meets its performance levels for 2 consecutive years , it may be subject to a 5 percent reduction in its annual wia formula grant .

no states received financial sanctions in program year 2003 .

labor determines incentive grants or sanctions based on the performance data submitted by states each october in their annual reports .

states also submit quarterly performance reports , which are due 45 days after the end of each quarter .

in addition to the performance reports , states submit updates for their workforce investment act standardized record data ( wiasrd ) in mid - october .

wiasrd is a national database of individual records containing characteristics , activities , and outcome information for all enrolled participants who receive services or benefits under wia .

all three submissions primarily represent participants who have exited the wia programs within the previous program year .

the process of collecting and reporting wia data involves all three levels of government .

participant data are typically collected by frontline staff in local areas and entered into a state or local it system .

in some states , local area staff may enter data directly into a statewide it system ; in other states , local areas may use their own individualized it system to enter data , from which staff can extract and compile the necessary information for state submission .

after the state receives data from local areas , this information is compiled and formatted for various submissions to labor , including the state's wiasrd file , quarterly report , and annual report .

during the data compilation process , state agencies administering wia typically match participant records to their state's ui wage record system to obtain wage records and employment status .

in addition , states may use the wage record interchange system ( wris ) to match participant records to other state's ui wage records or use other databases such as that of the u.s. office of personnel management to fill gaps in the ui wage records .

states may also link participant records to partner programs' it systems to track activities across programs or to determine outcomes such as attaining high school diplomas , degrees , and certificates .

for the quarterly and annual report , states use software to calculate their performance measures .

states generate the required wia performance reports and electronically submit them to labor's regional offices using the enterprise business support system ( see fig .

1 ) .

internal controls comprise the plans , methods , and procedures an organization uses to meet its missions , goals , and objectives .

internal controls used by government agencies may include guidance that defines the specific data to be collected and any documentation needed to support the data and safeguards to ensure data are secure .

some key aspects of internal controls for collecting and reporting data include: guidance: guidance should clearly and consistently define all data elements required for reporting , and effectively communicate this information to states and local areas .

if definitions are vague or inconsistent , then program staff may interpret them incorrectly , resulting in more errors to the data .

additionally , any guidance and documentation from the national office to states and local areas must be clear and free of any conflicting or contradictory instructions .

if reporting instructions are misinterpreted by program staff , then the data may not be useful to assess program performance .

data entry procedures and edit check software: data entry procedures and edit check software can help ensure data entering the designated reporting system are accurate and consistent .

written guides establishing who is responsible for each step in data creation and maintenance , and how data are transferred from initial to final formats can ensure data are consistently reported .

additionally , using electronic data management and processing software programs to conduct automated checks on data values and integrity can limit errors when data are reported at a later date .

monitoring: monitoring can ensure reported data are accurate and complete .

common monitoring practices may include formal on - site reviews of individual case files and source documentation at both the state and local levels , and assessments of issued guidance to ensure that information collected nationwide is consistent with existing policies and in compliance with laws and regulations .

three key issues — flexibility in federal guidance , major changes to states' information technology ( it ) systems and limited monitoring efforts — have compromised states' early efforts to collect and report wia performance data .

the guidance available to states at the time of implementation allowed flexibility in key definitions and contributed to inconsistency in the way the data are collected and reported .

the transition from jtpa to wia required states to make major changes to their it systems and in some cases , the transition led to problems with the data .

states used a variety of strategies to make the necessary system changes , some used the software they had used to report under jtpa , and others developed new software for wia .

more than three - fourths of the states told us that they had made major modifications to their wia it systems since implementation .

one - third of these states reported that when these modifications were made , they experienced significant problems that affected the quality of the data .

lack of oversight at the local , state , and federal levels made it difficult to ensure that early wia performance data were accurate .

the guidance available to states at the time of implementation was open to interpretation in key terms and contributed to inconsistency in the way that data are collected and reported .

labor allowed states and local areas flexibility in determining when to register a jobseeker in wia and when participants leave the program ( see table 3 ) .

registration .

when and who is registered affects all wia performance measures for adults and dislocated workers because performance data are only collected for those job seekers who are registered under wia — a process that occurs when they begin receiving services that require significant staff assistance .

labor has provided detailed written guidance to states on who should be registered under wia and when this registration should occur , but the guidance is open to interpretation in some areas .

the guidance provides examples of when to register job seekers , but it sometimes requires staff to make subtle and subjective distinctions .

for example , those who receive initial assessment of skill levels and the need for supportive services are not to be registered ; those requiring comprehensive assessment or staff - assisted job search and placement assistance are to be registered .

in an earlier report , we found that local areas differed on when they registered wia jobseekers , raising questions about both the accuracy and comparability of states' performance data , and we recommended that labor provide clearer guidance .

inconsistencies in when states register participants could lead some states to register fewer participants than others do , which could affect the reported outcomes .

exit .

determining when a participant leaves the program — or exits — affects nearly all wia performance measures because jobseekers must exit the program in order to be counted in the performance measures .

while labor's guidance explains when an exit occurs , it also has allowed two different kinds of exits — the hard exit and the soft exit .

a hard exit occurs when a participant has a specific date of case closure , program completion or known exit from wia - funded or one - stop partner - funded services .

a soft exit occurs when a participant does not receive any wia - funded or partner - funded service for 90 days and is not scheduled for future services except follow - up .

furthermore , labor's guidance on wia did not clearly specify which services are substantial enough to delay exiting a participant , and local areas define these services differently .

in a recent review we found considerable variation in exit practices at the state and local levels .

for example , one local area defined exit as occurring when participants are finished with their wia services ; another local area defined exit when participants have found a new job and the wages for their new job are considered acceptable ( regardless of the number of days that have passed since their last service ) .

in addition to allowing states the flexibility to define some performance elements , the initial guidance failed to specify other key elements necessary to ensure data quality .

for example , the guidance did not specify which source documentation was to be collected and maintained to support entries into the it system .

in the absence of guidance , some states continued to collect source documentation similar to that collected under jtpa , other states moved to paperless systems and did not collect and retain any source documentation .

without consistent source documentation , there is no assurance that the data in the it system are accurate .

the transition from jtpa to wia required states to make significant changes to their it systems , and in some cases , problems during the transition led to data errors .

for example , several data elements required in wiasrd — the file of individual exiters that states submit to labor every year — were similar to those collected under jtpa , but the data definitions were slightly changed .

this sometimes led to miscoded or missing data — especially for those participants who were carried over from jtpa into wia .

in addition , new data sources were used to measure outcomes , and the calculations for the measures were complex .

some states integrated their it systems so that the system that is used for wia data collection is used for tracking participation in other partner programs as well .

these changes required major modifications to the it systems .

states used a variety of strategies to make the necessary system changes , often facing challenges in fully implementing wia's requirements .

for example , 22 states reported that they used the same software they had used under jtpa to report on wia performance , but 15 of these states later converted to different software for wia .

twenty - six states used new software for wia at implementation , but almost one - third of them replaced that system when it became clear that the new system was not sufficient to meet wia reporting requirements .

the time needed to make system changes varied across states .

while nearly half of the states reported that they were able to implement their it system changes in 1 year or less , the other half reported that it took more than one year , and as long as 3 years ( see fig .

2 ) .

m. m. .

 .

 .

 .

thirty - nine states reported to us that they had made major modifications to their wia it systems since implementation , such as converting to internet - based systems or adding new capabilities such as case management tracking .

thirteen of these states reported that when they made these modifications , they experienced significant problems that affected the quality of the data , including lost data and difficulties in combining or reconciling data from the multiple systems they had used .

while 8 of the states reported that these issues have been resolved , 5 told us that they are still trying to resolve these data quality concerns .

some of the remaining 11 states that did not report making major changes to their it systems since wia implementation reported that they made minor changes , such as adding or deleting data elements and adding reporting capabilities .

in addition to collecting and reporting the performance data , it systems must also be able to calculate the performance measures .

however , states are not all using the same methodology to calculate these measures .

the calculations for the measures are complex and sometimes confusing .

for example , in calculating some of the measures for the adult program , states must consider ( 1 ) whether the jobseeker is employed at registration , ( 2 ) whether he or she is employed at both the first and third quarters after exit , and ( 3 ) what data were used to confirm employment .

this information results in 14 different ways that adult participants can be grouped together in order to calculate the measures .

labor does not mandate which software package states must use to calculate their performance measures , and at the 5 states we visited , each used a different approach — commercially available software , software developed by the state , or one of two different software packages developed under contract with labor .

these software packages can use slightly different formulas to calculate the measures and , as a result , produce differences in the outcomes reported .

lack of oversight at the local , state , and federal levels made it difficult to ensure that early wia performance data are accurate or verifiable .

during the first year of wia implementation , labor's inspector general ( ig ) found insufficient documentation of verification procedures at the state and local levels .

the same report questioned the lack of formal federal monitoring to gauge the progress of state efforts to ensure the quality of the data .

furthermore , the report noted that labor and states lacked adequate monitoring procedures and little was being done to monitor performance data at the case file level .

in a previous study , we reported that labor did not have a standard data monitoring guide in place , and regional officials — who have primary responsibility for monitoring — followed various oversight procedures .

table 4 summarizes wia's data quality issues .

states have made efforts to address data quality concerns and improve the quality of wia performance data .

most states have taken actions to clarify labor's guidance to help local areas determine who should be tracked in the performance measures .

almost all states reported on our survey that they have controls for it systems , such as edit checks or reports to help screen for errors or missing data .

in addition , most states reported to us that they monitor local areas to ensure data quality and consistency by assessing local procedures and policies .

states have taken some steps to provide additional clarity to help local areas adhere to federal guidance .

over 40 states reported to us that they provide guidance to help local areas determine which jobseekers should be tracked — or registered — for wia and when participants leave — or exited — services , and therefore get counted in the performance measures .

for example , a west virginia state official said the state developed a list of staff - assisted services that should trigger registration under wia .

most states also provide technical assistance and training on registration and exit policies ( see fig .

3 ) .

some states take other steps to help local areas adhere to federal policies .

for example , california state officials attempt to prevent local areas from keeping participants enrolled in the program once they have exited services by incorporating a capability in their it system that will automatically exit a person who has not had any service for 150 days .

states have made efforts to reduce the errors in their wia performance data .

almost all states reported on our survey that they have controls for it systems , such as edit checks or reports to help screen for errors or missing data .

forty - six states screen for missing values , and 44 states screen for errors such as data logic inconsistencies ( see fig .

4 ) .

for example , if an individual is registered in the youth program , but the birth date indicates that the person is 40 years old , this case would be flagged in an error report checking for inconsistencies between these two data elements .

some of the states we visited told us they allow local areas flexibility in deciding who should enter data and how it gets done .

in some locations , a case manager who works with the participant may enter data , and sometimes the case manager completes forms that are given to a data entry specialist .

despite these differences , most states have implemented edit checks and other controls in their it systems to detect and control for errors .

for example , state officials we met with in west virginia said that the state created screen edits and drop - down menus to guide case managers as they enter data .

if a case manager does not enter the necessary data , the system will not let the data entry process go forward until the data are entered .

state officials acknowledged that people entering data can still make mistakes if they choose the wrong option on a drop down menu , but they told us they try to minimize these mistakes by conducting training sessions to acquaint staff with the right techniques .

states also address data entry errors by running error reports .

in new york , state officials told us that they produce error reports for each local area to show where data are missing , meeting with local officials to discuss these reports every 6 weeks .

labor officials in most of labor's six regions told us that states have made improvements to their it systems since wia was first implemented .

for example , labor officials in one region said that they identified data quality issues related to states' it systems in 10 of the 11 states in that region in program year 2000 and found similar issues in only 5 states in the region between program years 2002 and 2004 .

a labor official in another region told us that initial data collection efforts were poor because states were largely focused on getting wia up and running and had not developed adequate it system instructions .

now , most states have developed it system manuals with clear instructions .

some regional officials told us that they provided technical assistance and closely monitored states that had early problems with their it systems .

most states told us they monitor local areas to ensure data quality and consistency by assessing local procedures and policies .

thirty - eight states reported to us that they monitor data collection at the local level .

at least 33 states also reported to us that they conduct monitoring of local policies and procedures on registrations and exits and data entry ( see fig .

5 ) .

state officials at the sites we visited generally said that they conduct annual monitoring visits to local areas or one - stop centers , and some conduct more frequent monitoring visits .

texas state officials we visited told us that the state monitors each local area once a year that includes reviewing participant files to assess eligibility decisions and ensure that outcomes are documented .

in new york , state officials said that they have monitoring teams located in five regions across the state who visit the local areas within their regions about once a month .

initially , these visits focused on program compliance , but they have recently been expanded to include data quality .

labor recently began addressing data quality issues , however , some data quality issues remain .

in 2004 , labor addressed some data quality concerns by implementing new data validation requirements that called for states to review samples of participant files and provided software to help states ensure that the performance measures are computed accurately .

most states reported on our survey that labor's new requirements are having positive effects on states' and local areas' attention to data quality .

however , labor does not currently have methods in place to review states' data validation efforts and hold states accountable to the data validation requirements .

labor's guidance requiring states to implement common performance measures on july 1 , 2005 , clarified some key data elements that had been problematic with regard to the wia performance measures , but it does not address all the issues .

further , labor has some federal monitoring processes in place but lacks a standard monitoring guide to address data quality .

to address data quality concerns , labor required states to implement new data validation procedures for wia performance data in october 2004 .

this process requires states to conduct two types of validation: ( 1 ) data element validation — reviewing samples of wia participant files , and ( 2 ) report validation — assessing whether states' software accurately calculated performance outcomes .

these requirements addressed a gap in earlier guidance by providing instructions for collecting and retaining source documentation to verify that the reported data are accurate .

this includes specifying which documentation is acceptable and what should be maintained in participant files .

for example , to document that a participant is placed in post program employment , states must show that the information was obtained from the ui wage records or wage record interchange system or other sources such as a pay stub , a 1099 form , or telephone verification with employers .

labor's data validation process requires states to monitor local areas to compare data elements that were reported to the state against source documentation to verify that the data are accurate .

labor selected data elements for validation based on factors such as feasibility and risk of error .

for example , self - reported data elements , such as race and ethnicity , are not validated because it is not feasible to locate the participant to verify these items .

data elements needing independent documentation , such as the use of supplemental data sources to determine employment , are assumed to be at higher risk of error than from using the ui wage records .

labor provided software to help states select a sample of files to be validated that includes participants from each group reported on in the performance measures — adults , dislocated workers , older youth , and younger youth .

states are required to conduct monitoring visits to the local areas selected for validation and compare data elements for each participant in the sample to source files to ensure accuracy , but labor does not have a standard process to verify that states did this correctly .

state monitors record whether each data element is supported by source documentation , and therefore passes , or whether the documentation shows the element was incorrect or was not supported with source documentation , and , therefore , fails the element .

states use labor's software to total error rates for each population group and states submit these data to labor .

to address inconsistencies in calculating the performance measures , labor's report validation software verifies the accuracy of outcomes reported by states .

states can use labor's software in two ways: they can use the software to compute the state's performance measures or they can use the software to check the calculations computed by their state's software to make sure that the measures were calculated accurately .

according to labor , about 20 states are currently using its software to compute their states performance measures .

the remainder of states use their own or commercially available software to compute outcomes .

these states must submit validation reports to labor to show any differences between their calculations and the outcomes computed with labor's software .

since initiating data validation , labor made a number of modifications to its software , and states reported on our survey that they experienced some challenges in using the software .

most states reported that they experienced only minor difficulties or had no problems in using labor's software for both data element validation and report validation ( see fig .

6 ) .

however , some states did report major difficulties .

for example , seven states reported that they initially had major difficulties with report validation , such as resolving discrepancies or errors in labor's software .

states also reported concerns that they were not always informed when labor made updates to the software and did not always receive adequate time to work with the software before the results were due to labor .

in addition , some states reported on our survey that conducting data element validation was time consuming .

half the states that were able to estimate the time it took to complete data element validation said it took 60 days and half said that it took more than 60 days .

the majority of states told us that labor's guidance , training , and technical assistance on data validation were sufficient ( see fig .

7 ) .

it is too soon to fully assess whether labor's efforts have improved data quality , however , at least 46 states reported on our survey that labor's new requirements have helped increase awareness of data accuracy and reliability at the state and local level ( see fig .

8 ) .

a new york state official told us that the federal requirements helped local staff better understand the connection between the data that get entered and how these data affect performance levels .

in addition , over 30 states said that the new requirements have helped them in their monitoring of outcomes and eligibility .

some states and local areas we visited reported finding errors in their data through the data validation process and have made modifications to state and local procedures to enhance data quality as a result .

for example , a local area in california started doing monthly spot checks of files to identify and correct errors on an ongoing basis .

in new york , a local area told us that it added a new staff person , developed new forms and procedures , and centralized data entry to have more control over data quality as a result of the federal data validation process .

while either centralized or decentralized data entry may be effective , experts in wia performance data told us that one of the most important factors to avoid human error is for program managers and staff who enter data to understand how the data are used .

while labor's data validation requirements are having some positive effects on states and local areas , labor currently has no mechanism to hold states accountable for complying with the data validation requirements .

labor has plans to develop accuracy standards for report validation and to hold states accountable to these standards in about 3 years .

initially , labor planned to use program year 2003 — july 1 , 2003 until june 30 , 2004 — as a base year for developing accuracy standards on report validation .

however , as a result of reporting changes for the common measures , labor has postponed the development of these standards until program year 2006 , beginning july 1 , 2006 .

at this time , labor does not have plans to develop accuracy standards for the data element validation portion of its requirements .

in addition , labor does not conduct its own review of a sample of wia participant files verified by states as part of data validation to ensure that states did this process correctly .

table 5 provides a summary of data quality concerns and how labor's data validation efforts affect these concerns .

in response to an omb initiative , labor recently began requiring states to implement common performance measures for wia programs .

omb established a set of common measures to be applied to most federally funded job training programs that share similar goals .

labor further defined the common measures for all of its employment and training administration programs and required states to implement these measures beginning july 1 , 2005 .

in addition , labor is replacing the definitions for the wia measures that are similar to the common measures with the new definitions for common measures ( see table 6 ) .

moving to the common measures may increase the comparability of outcome information across programs and make it easier for states and local areas to collect and report performance information across the full range of programs that provide services in the one - stop system .

many federal job training programs had performance measures that track similar outcomes but have variations in the terms used and the way the measures are calculated .

for example , wia's adult program uses a different time period to assess whether participants got a job than the wagner - peyser funded employment service does .

wia's adult program looks at whether participants get a job by the end of the first quarter after exit , whereas the employment service looks at whether participants get a job in the first or second quarter after registration .

under common measures , both programs use the same time period for this measure .

labor's new guidance for common measures requires states to collect a count of all wia participants who use one - stop centers .

this can help provide a more complete picture of the one - stop system , but it does not clarify when participants should be registered for wia and tracked in the performance measures .

therefore , it raises questions about both the accuracy and comparability of wia's outcomes for adults and dislocated workers .

under common measures , states are being required to begin collecting and reporting a quarterly count of all jobseekers who receive services at one - stop centers .

to track these jobseekers , labor suggested that states collect a valid social security number , but allowed states to exclude individuals who do not wish to disclose their social security numbers .

in addition , labor is encouraging states to voluntarily report performance information on all jobseekers that are counted in one - stops .

however , it is not clear how many states have the capability to track jobseekers who receive only self - service and informational activities .

while 30 states reported on our survey that they have a state system to track all jobseekers , some officials we visited told us they do not require local areas to collect and report this information to the state .

given this , implementing the new requirement may take time and early data collection efforts may be incomplete .

labor's guidance on common measures provides for a clearer understanding of when wia participants should be exited from the program than did earlier wia guidance .

first , the guidance provides a more uniform definition of exit .

in the past , local areas could use a hard exit — when a participant has a known date of completion or exit from services or a soft exit — when a participant has not received any services for 90 days .

under the new guidance , only soft exits will be allowed and states will no longer be able to report a hard exit .

second , labor clarified that some services are not substantial enough to keep a participant from being exited from wia .

for example , if a case manager is only making phone calls to the participant to see if he or she has a job or needs additional services or income support payments , those phone calls are not considered a service ( see table 7 ) .

this new clarification may help prevent local areas from keeping wia participants enrolled long after they have completed their last valid service .

in a previous study , however , we cautioned that rushed implementation of these reporting changes may not allow states and local areas enough time to fully meet the requirements and could negatively affect the data quality of the information reported .

in addition to data validation , labor has some limited federal monitoring processes in place to oversee state and local performance data .

labor's regional offices — with primary responsibility for oversight — conduct a limited review of wia report data to review quarterly and annual wia performance reports .

this generally involves identifying outliers or missing data and comparing the data with data in previous reports .

if labor regional officials identify basic problems with the data , they contact states to reconcile concerns .

labor's headquarters implemented an electronic system to manage grant oversight and track activities throughout the program year — called grants e - management system ( gems ) .

this system provides automated tools for conducting grant monitoring activities , including performing risk assessments and generating reports .

labor developed the risk assessment to help determine the programs and grant projects most in need of monitoring .

the risk assessment assigns a risk level to each state based on past performance and other criteria .

for example , for the wia program , a state may be considered at risk if it failed to meet its performance levels in the prior year .

however , regional officials can override the risk assessment if they are aware of other information that may not be captured in gems .

labor also implemented a core monitoring guide in spring 2005 to ensure that certain basic parameters are being followed during monitoring visits across all regions , but this guide does not provide for a standard analysis of data quality issues .

according to labor officials , they are developing program supplements for this guide that will address other issues specific to various programs .

one regional office developed an extensive monitoring guide to review state and local guidance , procedures used for data entry , it systems , and other data quality factors .

this guide has been used since 2003 to review the eight states in its region .

in addition , labor officials said that several regional offices are using this guide and they plan to develop a similar guide that will be used across all regions .

wia overhauled the way federally funded employment and training services are provided to jobseekers and employers , and introduced changes that significantly affected the way performance data are collected and reported for wia .

making this shift has taken a long time and some trial and error on the part of labor , states , and localities .

the magnitude of changes required considerable retooling of states' it systems , which had a negative effect on the integrity of wia performance data during the initial years of implementation .

since then , states have made progress in addressing challenges they faced in modifying or developing new it systems and have invested considerable effort establishing controls for it systems to minimize data errors .

in addition , labor's recent efforts to implement common performance measures across many of the wia partner programs and its revised wia reporting requirements have helped to address the concerns about when participants complete services and should be tracked in the performance measures .

the new requirement for states to capture limited data on all wia participants is an important step to better determine the full reach of wia .

however , this change still does not address the long - standing challenge labor has faced in clearly defining which participants should be counted in the performance measures .

without clear guidance , the wia performance data will continue to be inconsistent , even if the other data quality safeguards in place at the federal , state , and local level improve the quality of each state's and local area' s data .

labor's implementation of new data validation requirements is a major step toward addressing concerns about data quality resulting from the limited guidance and monitoring of wia performance data in the past .

by providing additional guidance and software to help states calculate the performance measures in a more uniform manner and requiring states to compare data reporting with participant case files , labor has gone a long way toward helping ensure the consistency and comparability of the data .

most notably , these requirements have significantly raised awareness of data quality at the state and local levels , which is an essential part of ensuring data quality .

however , more time is needed to fully assess the impact these new requirements are having on data quality .

in addition , labor does not currently review a sample of the participant files verified by states , nor does it have a mechanism to hold states accountable for meeting the data validation requirements .

further , labor has not developed a standard monitoring guide to more uniformly assess state and local data collection and processing to ensure data quality .

without a standard monitoring guide and a means to hold states accountable to the data validation requirements , it will be difficult to assure decision makers that the data is of sufficient quality for applying incentives and sanctions , and making budget decisions .

to address the inconsistencies in determining when participants should be registered and counted in the performance measures , we recommend that the secretary of labor determine a standard point of registration and monitor states to ensure that the policy is consistently applied .

to enhance the data validation requirements , we recommend that the secretary of labor: conduct its own review of the wia participant files validated by states to ensure that states did this correctly , and ensure that steps are taken to hold states accountable to both the report validation and data element validation requirements .

to address variations in federal monitoring practices , we recommend that the secretary of labor develop a standard comprehensive monitoring tool for wia performance data that is used across all regions , including monitoring the new guidelines for determining when participants end services .

we provided a draft of this report to labor for review and comment .

labor agreed with our findings and recommendations .

labor agreed that the lack of a standard point of registration and exit prevents comparisons across states and leads to performance outcome information that is arbitrary and inconsistent .

labor also agreed that steps are needed to increase the integrity of the data validation requirements and to improve the completeness and consistency of oversight .

a copy of labor's response is in appendix ii .

in response to our recommendations , labor noted that it plans to implement a policy prior to the start of program year 2006 to clarify the point of registration and exit .

in addition , labor plans to modify the current data validation procedures to begin reviewing a sample of states' validated files and plans to hold states accountable for data validation results by program year 2006 .

further , labor told us that it is taking steps to develop a comprehensive monitoring guide for performance data and plans to provide training on this new guide to help improve the completeness and consistency of oversight .

we are sending copies of this report to the secretary of labor , relevant congressional committees , and others who are interested .

copies will also be made available to others upon request .

the report is also available on gao's home page at http: / / www.gao.gov .

if you or members of your staff have any questions about this report , please contact me at ( 202 ) 512-7215 .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributors to this report are listed in appendix iii .

we examined ( 1 ) the data quality issues that have affected states' efforts to collect and report workforce investment act ( wia ) performance data ; ( 2 ) states' actions to address them ; and ( 3 ) the actions the department of labor ( labor ) is taking to address data quality issues , and the issues that remain .

to learn more about states' experiences implementing data collection and reporting system changes for wia , their implementation of labor's data validation requirements for wia , and state and local efforts to address the quality of wia data , we conducted a web - based survey of state workforce officials and conducted site visits in five states , where we interviewed state officials and visited two local areas or one - stop centers in each state .

we also collected information on the quality of wia data through interviews with department of labor officials in headquarters and all six regional offices , nationally recognized experts , and reviewed relevant research literature .

our work was conducted between june 2004 and september 2005 in accordance with generally accepted government auditing standards .

to determine the factors that affect the quality of wia performance data , we conducted a web - based survey of state workforce officials .

these officials were identified using a gao - maintained list of state wia officials .

we e - mailed the contacts , and they confirmed that they were the appropriate contact for our survey or identified and referred us to another person at the state level .

survey topics included ( 1 ) the changes made to data collection and reporting during the transition from the job training partnership act to wia , ( 2 ) the current status of wia data collection and reporting systems , ( 3 ) implementation of the u.s. department of labor's data validation requirements , and ( 4 ) state and local efforts to ensure the accuracy and reliability of wia data .

the survey was conducted using a self - administered electronic questionnaire posted on the web .

we contacted respondents via e - mail announcing the survey , and sent follow - up e - mails to encourage responses .

the survey data were collected between february and may 2005 .

we received completed surveys from all 50 states ( a 100 percent response rate ) .

we did not include washington , d.c.,and u.s. territories in our survey .

we worked to develop the questionnaire with social science survey specialists .

because these were not sample surveys , there are no sampling errors .

however , the practical difficulties of conducting any survey may introduce errors , commonly referred to as nonsampling errors .

for example , differences in how a particular question is interpreted , in the sources of information that are available to respondents , or how the data are entered into a database can introduce unwanted variability into the survey results .

we took steps in the development of the questionnaires , the data collection , and data analysis to minimize these nonsampling errors .

for example , prior to administering the survey , we pretested the content and format of the questionnaire with several states to determine whether ( 1 ) the survey questions were clear , ( 2 ) the terms used were precise , ( 3 ) respondents were able to provide the information we were seeking , and ( 4 ) the questions were unbiased .

we made changes to the content and format of the final questionnaire based on pretest results .

in that these were web - based surveys whereby respondents entered their responses directly into our database , possibility of data entry errors was greatly reduced .

we also performed computer analyses to identify inconsistencies in responses and other indications of error .

in addition , a second independent analyst verified that the computer programs used to analyze the data were written correctly .

we visited five states — california , new york , texas , west virginia , and wyoming , — and traveled to two local areas or one - stop centers in each of these states .

we selected these states because they represent a range of it systems — statewide comprehensive systems versus local systems with a state reporting function , include single and multiple workforce areas , and are geographically diverse .

from within each state , we judgmentally selected two local boards .

in the case of our single workforce area state , we visited two one - stop centers ( see table 8 ) .

in each state visited , we obtained general information about the state's implementation of wia , an overview of the state's wia administrative structure , the management information system and reporting processes in place to meet the federal requirements , data quality practices at the state and local levels , implementation of labor's data validation requirements .

we interviewed state officials responsible for local areas' wia programs and analyzing and reporting on the state's wia performance data , as well as other state wia and information technology ( it ) officials and staff of the state's workforce investment board .

at the local areas , we interviewed wia officials and staff , including service providers , staff responsible for performance management issues , it staff , case managers and other frontline staff , as well as staff of the local area workforce investment board .

the state and local interviews were administered using a semi - structured interview guide .

information that we gathered on our site visits represents only the conditions present in the states and local areas at the time of our site visits , from august 2004 through march 2005 .

we cannot comment on any changes that may have occurred after our fieldwork was completed .

furthermore , our fieldwork focused on in - depth analysis of only a few selected states and local areas or sites .

on the basis of our site visit information , we cannot generalize our findings beyond the states and local areas or sites we visited .

dianne blank , assistant director laura heald , analyst - in - charge in addition , the following staff made major contributions to this report: melinda cordero , vidhya ananthakrishnan , and leslie sarapu served as team members ; jennifer miller assisted with early data collection .

carolyn boyce advised on design and methodology issues ; susan bernstein advised on report preparation ; jessica botsford advised on legal issues ; avrum ashery and robert alarapon provided graphic design assistance ; and bill hutchinson and daniele schiffman verified our findings .

workforce investment act: substantial funds are used for training , but little is known nationally about training outcomes .

gao - 05-650 .

washington , d.c.: june 29 , 2005 unemployment insurance: better data needed to assess reemployment services to claimants .

gao - 05-413 .

washington , d.c.: june 24 , 2005 workforce investment act: labor should consider alternative approaches to implement new performance and reporting requirements .

gao - 05-539 .

washington , d.c.: may 27 , 2005 workforce investment act: employers are aware of , using , and satisfied with one - stop services , but more data could help labor better address employers' needs .

gao - 05-259 .

washington , d.c.: february 18 , 2005 workforce investment act: states and local areas have developed strategies to assess performance , but labor could do more to help .

gao - 04-657 .

washington , d.c.: june 1 , 2004 .

workforce investment act: labor actions can help states improve quality of performance outcome data and delivery of youth services .

gao - 04-308 .

washington , d.c.: february 23 , 2004 .

workforce investment act: one - stop centers implemented strategies to strengthen services and partnerships , but more research and information sharing is needed .

gao - 03-725 .

washington , d.c.: june 18 , 2003 .

older workers: employment assistance focuses on subsidized jobs and job search , but revised performance measures could improve access to other services .

gao - 03-350 .

washington , d.c.: january 24 , 2003 workforce investment act: youth provisions promote new service strategies , but additional guidance would enhance program development .

gao - 02-413 .

washington , d.c.: april 5 , 2002 .

workforce investment act: better guidance and revised funding formula would enhance dislocated worker program .

gao - 02-274 .

washington , d.c.: february 11 , 2002 .

workforce investment act: improvements needed in performance measures to provide a more accurate picture of wia's effectiveness .

gao - 02-275 .

washington , d.c.: february 1 , 2002 .

