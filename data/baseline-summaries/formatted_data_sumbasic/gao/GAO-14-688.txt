the department of homeland security ( dhs ) is the third - largest cabinet - level department in the federal government , with over 230,000 employees doing diverse jobs in areas such as aviation , border security , emergency response , cybersecurity analysis , and chemical facility inspection .

to address increasingly complex national security challenges , it is important that dhs have a workforce with the skills and expertise to fulfill its mission .

training and development programs are one way to help ensure personnel have the necessary skills and to prevent competency gaps .

these programs can include a set of courses using a variety of approaches , including classroom training , e - learning , webinars , coaching , practical exercises , and rotational assignments .

effective training and development programs for dhs's mission - critical functions , such as law enforcement , inspections , and screening , are important for enhancing dhs's ability to retain employees with the skills and competencies needed to achieve results .

according to dhs officials , dhs spent about $1.1 billion on training and development programs in fiscal year 2012 and about 7,000 staff are dedicated to training and development activities across the department .

in addition , our work in identifying high - risk areas in the federal government has identified dhs management , including the function of human capital management , as a high - risk area .

dhs's management of human capital has been on our high - risk list since 2003 because , among other things , the department has not fully implemented a mechanism to assess education , training , and other development programs and opportunities to help employees build and acquire needed skills and competencies .

in addition , our high - risk work has also identified the need for dhs to improve employees' opinions of the quality of departmental leadership as reflected in dhs's scores on the office of personnel management's ( opm ) federal employee viewpoint survey .

dhs uses training as one of its tools for enhancing departmental leadership .

as we have reported since march 2004 , using training evaluations to demonstrate how training efforts help develop employees , improve agencies' performance , and inform decision making on investments in training is a leading practice for ensuring agencies are being good stewards of their training and development resources .

given today's fiscal realities and the need to deliver cost - effective training and development programs without sacrificing quality or training effectiveness , you asked us to evaluate dhs's training practices , as well as efforts to ensure training is efficient and effective in developing its next cadre of leaders .

this report will address the following two questions .

1 .

to what extent does dhs have documented processes to evaluate training and development programs and reliably capture costs ? .

2 .

what leader development programs has dhs implemented , what are stakeholders' perspectives on them , and to what extent does dhs measure program performance ? .

to understand training programs at dhs , we obtained information from the dhs office of the chief human capital officer ( ochco ) , and five selected components: the federal law enforcement training center ( fletc ) , u.s. customs and border protection ( cbp ) , u.s. immigration and customs enforcement ( ice ) , the transportation security administration ( tsa ) , and the u.s. coast guard .

we selected these components to represent different dhs mission areas , workforce sizes , training costs , and number of career senior executive service ( ses ) personnel .

to further our understanding of training at the component level , we also interviewed training officials at each of the selected components and identified these individuals based on their knowledge , experience , and leadership roles .

the perspectives of dhs ochco and the selected components provided are not generalizable to all training programs at dhs , but provided helpful insights into the selected components' specific training and development programs at dhs .

to address the first question regarding the extent to which dhs has documented processes to evaluate its training and development programs , we reviewed documented policies and procedures related to the evaluation of training programs for the five selected dhs components , as well as completed training evaluations .

we also conducted semistructured interviews with officials responsible for conducting training evaluation at each of the five components to understand the evaluation process that each component follows and how evaluation feedback is used .

we then assessed the documented processes from each of the selected components against attributes of training evaluation processes identified by opm , dhs , and gao to determine the extent to which the documents include select attributes of evaluation processes .

we selected the attributes for our analysis by including six that were consistently identified in relevant criteria documents related to training evaluation , such as the dhs learning evaluation guide , the opm training evaluation field guide , and gao's prior work on training and development .

these attributes also align with those identified in standards for internal control in the federal government for the plans , methods , and procedures used to accomplish missions , goals , and objectives and support performance - based management practices .

the six training evaluation process attributes include assessing whether each component's documented process ( 1 ) establishes goals about what the training program is supposed to achieve , ( 2 ) indicates which training programs are being evaluated , ( 3 ) explains the methodology used to conduct the evaluation , ( 4 ) presents timeframes for conducting the evaluation , ( 5 ) presents roles and responsibilities for evaluation efforts , and ( 6 ) explains how the evaluation results will be used .

we assessed each component's documented evaluation process to determine the extent to which the attributes were included and gave a component a rating indicating that the attribute was fully met , a component partially met the attribute but did not fully or consistently meet all parts , or the component did not include any information to meet the attribute .

to address the extent to which dhs ensures training costs are reliably captured , we reviewed relevant documentation on processes and steps taken to examine budget and cost documentation .

as part of our review of cost tracking at dhs , we observed methods ochco and the components used for identifying efficiencies in training that were used to identify cost savings .

we also conducted semistructured interviews with dhs and component officials responsible for administering training programs and tracking costs to understand how dhs and components identified and captured costs , and any challenges they may have in doing so in a reliable manner .

accordingly , through our review of cost - saving documentation and interviews with dhs and component officials , we sought illustrative examples to understand how ochco and the selected dhs components identified potential efficiencies and steps planned or already taken to achieve them .

we assessed the reliability of the reported cost savings relevant to these illustrative examples and we replicated cost - saving calculations provided by components , including estimates for training equipment , salaries , and benefits .

we determined through analysis of cost - saving estimates and interviews with knowledgeable officials at dhs and the select components that the cost - saving data provided and reported in this product for the illustrative examples from fiscal years 2011 through 2013 were sufficiently reliable for the purposes of illustrating the types of cost efficiencies that may be achieved .

the cost - saving examples dhs and components provided are not generalizable to all of dhs , but provided helpful insights into cost - saving efforts identified to date at dhs .

to address the second question about leader development programs dhs has implemented , we reviewed program documentation relevant to leadership training programs .

in addition , we obtained and analyzed data from ochco and the selected components on the number of participants in the leader development programs they provided during fiscal years 2012 and 2013 .

we assessed the reliability of these data by interviewing agency officials familiar with the sources of the data regarding internal controls built into the information systems and stand - alone spreadsheets in which the data are stored , and quality assurance steps performed after data are entered into the systems or documents .

we determined that the data were sufficiently reliable for the purpose of reporting the approximate number of program participants .

we also interviewed officials from ochco and the selected components regarding implemented and planned leader development programs .

to assess the extent to which dhs measures the performance of leader development programs , we reviewed program documentation from ochco and the selected components , including performance measurement requirements and guidance .

in addition , we interviewed cognizant officials about what performance measurement information they collect and how they use the information .

through these efforts we determined that the leader development program ( ldp ) office uses performance measures to assess the ldp's impact .

we assessed these measures against three of nine selected key attributes for performance measures identified in prior gao work that we identified as relevant given the maturity level of the ldp .

in particular , given that the ldp is a relatively new program , we focused our analysis on three attributes that we identified as foundational — having linkage between performance measures and division - and agency - wide goals , being clear , and having measurable targets .

additional details on our scope and methodology can be found in appendix i .

we conducted this performance audit from july 2013 to september 2014 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

dhs's ochco is responsible , in broad terms , for the strategy , oversight , and planning of dhs employee training and development .

at the same time , each dhs component , such as cbp and tsa , also has its own human capital office and training and development functions .

in practice , dhs's ochco focuses on department - wide efforts while each component focuses on ensuring its employees are trained and developed to meet its specific mission needs .

in addition , fletc , a component of dhs , offers and delivers law enforcement training to dhs components , including those in our review — cbp , ice , tsa , and the coast guard .

fletc also serves as an interagency law enforcement training organization for more than 90 federal partner organizations , as well as state , local , tribal , and international entities .

table 1 provides a summary of training and development responsibilities at ochco and dhs components selected for our review .

in 2009 , opm developed and published regulations that require agencies to regularly evaluate training programs .

among other things , these regulations require agencies to evaluate their training programs annually to determine how well such plans and programs contribute to mission accomplishment and meet organizational performance goals .

the training and development process can loosely be segmented into four broad , interrelated elements: ( 1 ) planning / front - end analysis , ( 2 ) design / development , ( 3 ) implementation , and ( 4 ) evaluation .

the four elements help to produce a strategic approach to federal agencies' training and development efforts .

one commonly accepted training evaluation model , which is endorsed by opm in its training evaluation guidance , is known as the kirkpatrick model .

this model is commonly used in the federal government , including at dhs .

the kirkpatrick model consists of a four - level approach for soliciting feedback from training course participants and evaluating the impact the training had on individual development , among other things .

the following is a description of what each level within the kirkpatrick model is to accomplish: level 1: the first level measures the training participants' reaction to , and satisfaction with , the training program .

a level 1 evaluation could take the form of a course survey that a participant fills out immediately after completing the training .

level 2: the second level measures the extent to which learning has occurred because of the training effort .

a level 2 evaluation could take the form of a written exam that a participant takes during the course .

level 3: the third level measures how training affects changes in behavior on the job .

such an evaluation could take the form of a survey sent to participants several months after they have completed the training to follow up on the impact of the training on the job .

level 4: the fourth level measures the impact of the training program on the agency's mission or organizational results .

such an evaluation could take the form of comparing operational data before and after a training modification was made .

figure 1 highlights the elements of the training development process , from the planning stage through the implementation and evaluation of training , and depicts how the kirkpatrick model complements the training development process .

in addition to utilizing this training development process , agencies may also seek federal law enforcement training accreditation ( fleta ) for some or all of their training programs and academies .

by attaining fleta accreditation for their training academies or programs , agencies provide assurance that they have voluntarily submitted to a process of self - regulation and have successfully achieved compliance with a set of standards that demonstrate their adherence to quality , effectiveness , and integrity .

fleta accreditation also helps maintain training standards by ensuring that training programs are comprehensively evaluated , using kirkpatrick levels 1 through 3 or an equivalent approach , within a 5-year period .

in october 2010 , dhs issued its dhs learning evaluation guide .

dhs created the guide to help the department's learning and development community evaluate the effectiveness of training activities in a diverse organization with varied training needs .

among other things , the guidance gives an overview of best practices and provides components with tools they can use to implement the kirkpatrick model in their training evaluations .

in addition , the guide highlights the need for a training evaluation plan to identify and address ( 1 ) what is being evaluated , ( 2 ) how it is being evaluated , ( 3 ) when it is being evaluated , and ( 4 ) the factors involving stakeholder expectations , such as agency policies and procedures .

in 2004 , the secretary of homeland security announced the “one dhs” policy , which identified the need for a common leadership competency framework across the department , as well as a unified training curriculum for current and future leaders .

accordingly , dhs established the ldp office in may 2010 under the office of the chief human capital officer to design , develop , and execute a department - wide leadership program that would strengthen leadership at all levels of the dhs workforce .

through the ldp , all dhs components are to invest in developing leaders with skills that transfer across the department , yet retain the agility to balance this with their own mission - focused leader development needs .

in january 2011 , dhs also developed the leader development framework to serve as a 3- to 5-year strategic roadmap for implementing the ldp .

this framework consists of five tiers of leader development programs for employees of different levels , such as the executive and supervisory levels .

in february 2013 , dhs issued a directive — directive 258-02: leader development — formally establishing responsibilities and policies related to leader development at dhs through the ldp , as well as instructions for implementing the directive .

this directive specifies that the ldp is , among other things , to delineate requirements and activities to be implemented by components .

the ldp is also to develop and manage centrally coordinated and high - potential programs for developing employees to fill future leader positions .

dhs components have documented processes in place for evaluating their training programs and have used evaluation feedback to improve their training offerings ; however , their documented processes varied on the extent to which selected attributes of an effective training evaluation process were included .

further , dhs identified opportunities for efficiencies and cost savings , but varying approaches for capturing training costs across the department affect dhs's ability to reliably capture and track its training costs department - wide .

the five dhs components in our review have a documented process in place for evaluating their training programs using the kirkpatrick four - level model .

however , their documented evaluation processes varied on the extent to which they included selected attributes of an effective evaluation process .

components use the results of their evaluations to make improvements to the training programs and assess training needs .

for example , components used evaluation feedback to improve the delivery of training content , such as through additional hands - on training , and the use of e - learning .

table 2 provides such an example for each component .

opm guidance on training evaluation , dhs's learning evaluation guidance , and our prior work on effective training and development programs identify various attributes for effective training evaluations .

consistent with these criteria , the attributes of an effective training evaluation process include communicating ( 1 ) the goals the training programs are supposed to achieve , ( 2 ) which training programs will be evaluated , ( 3 ) the methodology for conducting the evaluations , ( 4 ) timeframes for conducting the evaluation , ( 5 ) roles and responsibilities for evaluation efforts , and ( 6 ) how the evaluation results will be used .

all dhs components in our review reflected a number of the attributes of an effective training evaluation process in their documentation .

for example , all components included information on identifying goals that the training programs are to achieve , identifying which training programs are to be evaluated , and explaining how the evaluation results will be used .

table 3 presents information on the extent to which each component's documented evaluation process includes these attributes , and additional details about component ratings are explained in appendix ii .

however , components varied on the extent to which they included information in their documentation about evaluation methodologies , timeframes , and roles and responsibilities for evaluation .

for example , evaluation methodologies: each component's documentation indicates that its training programs are to be evaluated using the kirkpatrick model .

however , only ice's and the coast guard's documentation specify the methods to be used when performing each kirkpatrick level of evaluation .

the other three components' documentation does not specify how each kirkpatrick level of evaluation is to be performed in practice .

timeframes: ice and fletc's documented evaluation processes fully presents timeframes for evaluations , but documentation for the remaining three components does not .

for example , all of the components' documentation identified the timeframes for the initial steps of completing evaluation surveys and collecting data .

however , ice and fletc's documentation also communicated timeframes for the subsequent steps for analyzing the results of the evaluation .

roles and responsibilities: we found that three of five components did not consistently outline the roles and responsibilities for the evaluation efforts .

for instance , two components , cbp and ice , communicate information on roles and responsibilities for some parts of the process , but do not present this information for others .

tsa's documentation did not communicate information on the specific roles and responsibilities within the office of training and workforce engagement ( otwe ) for evaluation activities .

officials at one component , cbp , told us that since training at cbp is more decentralized through separate academies that follow their own processes , their documentation did not include some attributes of effective training and development programs , as their training standards were intended to be more of a “how - to guide” rather than a formal step - by - step methodology .

according to tsa officials , they did not fully include certain attributes such as explaining the methodology to be used to conduct the evaluations and defining roles and responsibilities because their process is still under development and agreement on this information has not yet been reached internally .

tsa officials stated that their documentation is to be finalized by the first quarter of fiscal year 2015 .

officials at the coast guard told us that their documentation did not include information on the timeframes for analyzing the results , but they plan to rectify this with their current effort in fiscal year 2014 to revise their evaluation processes .

two of the five components – ice , and fletc — did not provide a reason for why their documentation did not include all of the attributes of effective training and development programs .

all dhs components agreed that having a documented training evaluation process provides benefits , such as helping to ensure consistency and transparency across the organization .

accordingly , as previously noted , tsa is working to finalize its training evaluation process .

in addition , officials from two components , the coast guard and cbp , stated they plan to revise their documented processes in the near future .

specifically , according to coast guard officials , revisions to the training evaluation process are being made to more explicitly communicate their process , enhance standardization , and facilitate prioritizing its training evaluation efforts to focus on the most mission - critical training needs .

as the process is under way , coast guard officials were not able to provide an estimate for when these revisions should be completed .

similarly , according to cbp officials , they plan to review and revise cbp's training evaluation process to ensure consistency across the component .

cbp believes this is necessary given that its training functions have become more decentralized since its last training evaluation process came into effect in 2008 .

according to cbp officials , the target completion of revisions is fiscal year 2015 , after the reorganization of cbp's office of training and development is complete .

by ensuring that the components' documented evaluation processes fully address attributes for effective training as they are drafted , updated or revised , dhs would have better assurance that the components have complete information to guide its efforts in conducting effective evaluations .

moreover , such documentation could help ensure that evaluation processes for assessing whether training programs appropriately support component and dhs needs can be repeated and implemented consistently .

as components draft , update , and revise their documented evaluation processes , incorporating or more fully addressing the aforementioned attributes of effective training evaluations could help to ensure that components clearly communicate all aspects of their evaluation processes and that employees can consistently implement them .

all dhs components reported reviewing the merits of different delivery mechanisms ( eg , classroom or computer - based training ) to determine which mix would be the most efficient for at least one of their training programs .

in addition , four of five components — cbp , the coast guard , ice , and tsa — provided at least one illustrative example of how they used a mix of webinars , online learning , and classroom instructor - led training to develop a blended learning approach that improved cost - effectiveness .

for example , in june 2013 , tsa implemented webinar training for sensitive security information ( ssi ) , which helped tsa avoid travel costs and led to an estimated $855,026 in cost savings .

in addition , ice developed online training for fourth amendment instruction , which helped reduce the course length by 1 week , contributing to opportunity cost savings and savings in room and board totaling about $4.8 million over a 5-year period .

the components we reviewed that used webinars or online learning stated that these delivery mechanisms did not adversely affect the quality of training offered in these instances .

furthermore , all dhs components reported that they have evaluated at least one training program to determine how to streamline or consolidate the training to make it more cost - effective .

for example , fletc adopted firearms simulation technology and more cost - effective ammunition , and according to our analysis of fletc data , led to about $2.2 million in cost savings .

see figure 2 for a photographic example of training using firearms simulation .

similarly , at the department level , ochco has taken steps to streamline mandatory department - wide training requirements for counterintelligence and records management training .

for example , based on review of legal requirements , ochco found that it could streamline mandatory records management training by consolidating multiple annual training requirements into a single course .

according to ochco officials and our analysis of ochco data , this effort could lead to a potential cost saving of about $57.1 million over a 5-year period .

table 4 provides illustrative examples of actions ochco and selected components have taken to improve the cost - effectiveness of training and the estimated cost savings from these actions over a 5-year period , according to our analysis of ochco and dhs components .

starting in fiscal year 2014 , cbp adopted a new approach to improve the identification of efficiencies in training , including regular reviews of training justification , cost , and prioritization of training needs .

though the process is not yet documented in policy , directives , or standard operating procedures , cbp officials report that it has allowed them to identify training cost discrepancies more consistently and efficiently .

for example , according to cbp officials , their approach includes tracking key cost elements , such as travel , lodging , meal , rental vehicle requirements ; duration of training ; instructor costs ; and contract costs , for all training courses under separate codes .

further , according to these officials , this allows cbp to better compare costs for course execution , such as the cost of course resources ( eg , delivery location , equipment , etc. ) .

and related travel , if any , from year to year .

in addition , cbp uses these data to challenge requests for training and identify possible alternatives for delivering existing courses at a reduced cost .

the process also provides cbp with a vehicle for better projecting training costs .

according to cbp officials , the new approach created requirements that internal offices provide more precise estimates of the number of participants attending training , which reportedly helped cbp more efficiently allocate about $5.8 million in fiscal year 2013 .

before the new process was implemented , internal offices could not commit to filling training courses and slots , resulting in cbp's office of training and development overprojecting about $7 million in training costs in fiscal year 2012 , which was returned to cbp .

according to cbp officials , leadership change , overestimation of training funding needs , and spending reductions under sequestration in fiscal year 2013 were some of the key reasons for adopting this new approach .

cbp officials reported that by refining cost projections , cbp improved their ability to approve more training within allocated budgetary resources .

for example , based on the new approach in fiscal year 2014 , cbp identified surplus training funds from unfilled training slots and class cancellations early enough to enhance training programs .

in addition , although dhs and components provided illustrative examples of efficiencies in training and cost savings , dhs uses different methods to capture training costs .

dhs , through ochco , has worked to capture the cost and delivery of dhs's training and development programs .

however , at dhs headquarters and at the component level , there are inconsistencies in how training costs are captured across the department that have made it a challenge to accurately and reliably capture these costs across dhs .

for example , ochco officials explained that the lack of a centralized funding source and disparate financial management systems used by components created challenges in reliably capturing training costs .

components also often capture training costs differently from one another , which can contribute to inconsistencies among training costs captured at dhs .

training costs may , for example , include expenses for instructional development , participant and instructor salary and benefits , equipment costs , and travel and per diem expenses .

accordingly , ochco officials report that some components include conferences as a training cost while others do not , and some components did not include mission - critical law enforcement training costs when they provided department - wide training costs .

in fiscal year 2012 , the dhs undersecretary for management requested that ochco collect training cost data from components .

during this process , ochco relied on senior - level data requests to retrieve annual training expenditure information from components .

according to ochco officials , the senior - level data call process revealed that training cost data had limited reliability because some components were not consistent in determining the types of mission - critical training costs they provided , among other things .

given ongoing concerns about data reliability , ochco officials noted that it would be difficult to update and reliably aggregate department - wide training costs for fiscal year 2013 .

according to ochco officials , given budget constraints , it is difficult for ochco to make good investment decisions about training when they do not know how components spend their training dollars .

in addition , according to discussions with ice officials , we found that the cost of ice's training and development programs may not be consistently and accurately captured .

for example , ice officials stated that participant and instructor salaries are consistently tracked as part of training costs , but travel expenses are less consistently tracked for all ice internal training programs .

further , according to ice officials , incomplete definitions of training and inconsistency in how costs are tracked also contribute to shortfalls in reliably capturing training costs .

ice officials reported , for example , that they cannot reliably capture training costs from one of ice's internal departments and , instead , need to rely on sporadic data calls to retrieve training budget and expenditure information from its departments .

ice officials reported concerns about the reliability of this process , partly because of concerns about inconsistent coding schemes for tracking similar training activities and the lack of third - party checks on the reliability of how training information is coded .

as of august 2014 , ice officials report that their office of the chief financial officer is working to standardize its coding schemes — or object class reporting — across ice programs and plans to implement the revised coding standards in fiscal year 2015 .

ochco and ice officials we met with acknowledged that the department has not identified all challenges that prevent dhs from accurately capturing training costs department - wide , but they have taken some preliminary steps toward more consistently defining training and capturing costs .

for example , while dhs has not issued central guidance on what should be included in training costs , ochco officials noted that they provided a glossary of terms to components in december 2007 to help establish an initial definition of training department - wide .

although the glossary clarifies a number of training - related terms , it does not provide requirements for tracking training costs consistently across components .

for example , the glossary notes that training program costs are calculated differently on a component basis .

further , according to ice and ochco officials , dhs discussed the issue of accurately and reliably capturing training and development costs across the department as part of its training leaders council in may 2014 .

ochco officials reported that the use of a standard form for requesting training , known as the federal government's standard form 182 , authorization , agreement , and certification of training , may be one method for improving the tracking of training costs .

for example , the form 182 may help provide for consistent definitions and methods for capturing certain training costs .

however , while use of the standard form 182 would be a positive step , it may not address certain reliability concerns associated with capturing training costs at dhs .

for example , the approach may not prevent the duplicative capturing of procurement - related training costs or shortfalls in how training information is entered and captured in each component's systems .

according to the dhs chief learning officer , requiring the use of the form 182 dhs - wide is still in the preliminary stages of consideration and would require accompanying policy changes .

as dhs has not yet made a decision on whether to require use of the form 182 , it does not yet have timeframes for implementing this proposal .

one leading training investment practice is that agencies should capture the cost and delivery of their training and development programs .

our prior work has also shown that agencies need reliable information on how much they spend on training and for what purposes .

to capture the cost and delivery of training and development programs , agencies need credible and reliable data from learning management systems as well as accounting , financial , and performance reporting systems .

to the extent possible , agencies also need to ensure data consistency across the organization ( such as having data elements that are pulled from various systems representing the same type of information ) .

variations in the methods used to collect data can greatly affect the analysis of uniform , high - quality data on the cost and delivery of training and development programs .

given today's budgetary constraints and the need to effectively utilize and account for all federal dollars , identifying existing challenges that prevent dhs from accurately capturing training costs department - wide and , to the extent that the benefits exceed the costs , implementing corrective measures to overcome these challenges , could enhance dhs's resource stewardship .

dhs's leader development program office is in the process of implementing a department - wide , five - tier leader development framework to build leadership skills across all staff levels .

while dhs components generally stated that the ldp framework is beneficial , they raised concerns about its training requirements , which the ldp office's planned evaluation efforts may address .

further , the ldp office has developed a program - wide assessment approach to analyze the impact of the ldp that includes tracking 12 performance measures over time .

however , the ldp office could strengthen its performance measurement efforts by clearly identifying its program goals and better incorporating key attributes of successful performance measures we have previously identified .

dhs has implemented programs in support of two of five tiers within its department - wide leader development framework , and the selected components in our review also deliver additional leader development programs for supervisors , managers , and executives .

as previously discussed , dhs established the ldp office in may 2010 to design , develop , and execute a department - wide program to strengthen leadership at all levels of the dhs workforce .

in january 2011 , dhs approved the leader development framework as a 3- to 5-year strategic roadmap for implementing the ldp .

this leader development framework consists of five tiers that identify envisioned leader development programs for employees of different levels .

these tiers , and the employees they include , are the following: executive ( members of the senior executive service , coast guard admirals , and selected other leaders ) , manager ( nonexecutive employees who supervise other supervisors , lead through subordinate supervisors , and formally supervise at least one supervisory employee ) , supervisor ( employees who accomplish work through , and are directly responsible for , the work of nonsupervisory employees , and who formally supervise only nonsupervisory employees ) , team lead ( nonsupervisory employees formally designated as such or tasked to guide a group of people to results on a program , project , initiative , or task force ) , and team member ( nonsupervisory dhs employees ) .

the ldp office has implemented programs within two of the five leader development framework tiers ( supervisor and executive ) , initiated program development within two tiers ( team lead and team member ) , and plans to begin program development within one tier during fiscal year 2014 ( manager ) .

according to the ldp manager , the office prioritized implementation of the supervisor tier at the direction of the then deputy secretary , who identified supervisors as a critical nexus between strategic leadership and employee performance .

the ldp manager stated that the office also prioritized implementation of the executive tier because ochco officials were familiar with best practices for instruction for new executives and the then deputy secretary identified particular value in providing new executives with consistent instruction .

within the supervisor tier , the ldp office has established the cornerstone program , which consists of a set of baseline requirements for new and seasoned supervisors at all levels .

dhs components may fulfill cornerstone program requirements through new or existing training programs , cross - component programming , or a combination thereof .

within the executive tier , the ldp office centrally administers the 3-week capstone cohort program , which includes discussion forums , operational site visits , and learning activities intended to address real - world strategic issues .

as the program is currently implemented , whereas components may elect to send participants to the capstone program , they are required by dhs to implement cornerstone program requirements .

in addition to implementing programs to support the supervisor and executive leader development framework tiers , the ldp office has assumed responsibility for administering two preexisting dhs programs , the senior executive service candidate development program and dhs fellows .

these programs are not a part of any one tier , as their intended participants may span framework tiers .

for example , the senior executive service candidate development program is designed for senior executive service candidates who aspire to transition into the executive tier .

figure 3 , an interactive graphic , describes the development and implementation status of the programs that support each tier as of august 2014 .

see appendix iii for a print version of this figure .

ldp office officials anticipate fully implementing all five tiers of the leader development framework before the end of fiscal year 2016 .

move mouse over the program names for more information .

for a text version please see app .

iii .

component program ( s ) the five dhs components selected for our review have all participated in the ldp department - wide programs .

in particular , according to ldp office and component data and officials , all five components have programs in place , as required by dhs , intended to meet the cornerstone program requirements .

for instance , data from the selected components demonstrate that the fundamentals of dhs leadership courses they delivered — one of four program segments of the cornerstone program — from fiscal year 2012 through fiscal year 2013 had more than 3,600 participants .

according to the ldp manager , the ldp office has sought to avoid duplication of effort and costs in components' implementation of the cornerstone program .

for example , the ldp office coordinated the development of instructional materials for all components to use to meet requirements for the understanding the dhs leadership commitment segment of the cornerstone program , which is for individuals considering the supervisory path .

in addition , information that a dhs working group collected from components during initial development of the cornerstone program indicated that most components could utilize existing programs to help meet program requirements .

specifically , six of the seven components that provided information to the working group indicated that they had an existing training program that they could use to provide instruction to first - time supervisors .

for instance , fletc uses two preexisting programs to meet fundamentals of dhs leadership requirements — the fletc new supervisor training program and the law enforcement supervisor leadership training program .

in addition to maintaining programs to meet cornerstone program requirements , each of the selected components has elected to participate in the three department - wide programs administered centrally by the ldp office — the capstone cohort program , senior executive service candidate development program , and dhs fellows .

in particular , these programs had a total of approximately 60 participants from the selected components from fiscal year 2012 through fiscal year 2013 , according to ldp office data .

table 5 summarizes approximate participation in leader development framework programs that were provided by the selected components to meet department - wide requirements or centrally administered by the ldp from fiscal year 2012 through fiscal year 2013 , according to ldp office and component data .

in addition to the programs administered under the leader development framework , the components in our review also deliver various additional leader development programs .

for example , coast guard delivers a 1- week course for newly selected executives focused on change management issues and tsa delivers a program for supervisors over a period of 18 to 24 months that includes training , shadowing , mentoring , and other developmental leadership opportunities .

table 6 provides examples of leader development programs delivered by these components at the executive , supervisor , and manager levels .

as shown in table 7 , according to data provided by the selected components , leader development programs they delivered for supervisors , managers , and executives , independent of department - wide programming , from fiscal year 2012 through fiscal year 2013 had a total of more than 10,000 participants .

officials from the components in our review generally stated that it is beneficial for their components to provide leader development programs in addition to the ldp department - wide training because , whereas ldp training focuses on more general leadership skills and competencies , component - level training is tailored to the components' unique missions .

for example , tsa officials explained that tsa leader development programs focus on developing individuals for tsa's mission - critical occupational areas ( eg , federal security directors and federal air marshals ) , which require proficiency in a set of leadership and technical competencies unique to tsa .

in addition , tsa leader development programs utilize examples that are readily applicable to day - to - day tsa operations , according to these officials .

similarly , according to coast guard officials , their leader development programs afford the coast guard the opportunity to provide instruction using case studies and in - class discussions on how to lead a coast guard workforce in a coast guard context .

component officials we spoke with generally agreed that the ldp is helpful in providing a common framework for leader development training .

however , officials from three of the five components we met with raised concerns about the applicability or clarity of certain learning objectives the ldp requires they teach when implementing the cornerstone program .

officials from one component stated that the ldp has established policy and procedures , which help to ensure all components are informed and have consistent definitions and policies related to leader development .

components also identified other benefits of the ldp , such as bringing focus across the department to leader development , allowing for collaboration on leader development activities , and having experienced staff who work solely on leader development issues and programs .

as previously discussed , the cornerstone program consists of requirements in four segments , one of which is a course on the fundamentals of dhs leadership that is required for all first - time federal supervisors .

in order to fulfill ldp requirements for this course , all components must provide instruction on more than 200 learning objectives that identify content the course must cover .

for example , these learning objectives include encouraging respect for individual differences and determining appropriate tasks to delegate .

according to dhs guidance on the ldp , all dhs components are to develop leaders with skills that transfer across dhs , yet retain the agility to advance their own unique mission - focused leader development needs .

however , officials from two components raised concerns about the applicability of certain objectives that the ldp requires them to teach .

for example , officials from one component stated that the learning objective involving supervising a workforce of federal employees and contractors is not universally applicable because supervisors in their component do not supervise contractors .

officials from another component stated that one objective related to supervisors' knowledge of the hiring process does not pertain to new supervisors within their component .

according to these component officials , requiring them to teach objectives that are not pertinent to tasks supervisors must perform takes away from instructional time that they could use to teach more relevant content .

according to the ldp manager , the ldp office established learning objectives in order to meet dhs's direction to ensure sufficient consistency in leader development investment across components , but components may adapt the objectives , as appropriate .

for example , tsa requested a waiver from teaching learning objectives focused on title 5 of the u.s. code , from which tsa is generally exempt .

the ldp manager granted the waiver , and suggested that tsa replace instruction focused on title 5 with instruction on related subjects applicable to tsa .

however , officials from the two components that raised concerns were not aware that cornerstone program requirements provided them with this flexibility .

officials from two components raised concerns that some of the learning objectives required to be taught under the fundamentals of dhs leadership course do not clearly articulate what the training must cover , and that they are not written with standards that can be measured or observed .

for example , according to officials from one component , some fundamentals of dhs leadership objectives are not consistent with their component's standards , which require that performance objectives include condition , measurable performance behavior , and a standard that specifies the degree of quality expected in performance .

for instance , one of the learning objectives that officials identified as not meeting these requirements states , “recognize a recent study that reported 48 percent of workers surveyed responded to job pressure by performing illegal or unethical activities ; 58 percent considered acting illegally or unethically.” a senior official from this component explained that this can result in implementation and evaluation challenges — if it is unclear what the outcome of an objective is supposed to be , it is difficult to know how to implement it or evaluate its implementation .

this official also stated that officials from his component voiced their concerns about the clarity of learning objectives to dhs headquarters , but dhs did not change them .

according to ldp office officials , they solicited input from components on cornerstone program requirements and adopted selected changes .

in particular , according to the ldp manager , the ldp office solicited input from components during four informal and two formal reviews of cornerstone program requirements .

the ldp office has also awarded a contract for an assessment beginning in february 2014 that includes evaluation of the fundamentals of dhs leadership's learning objectives .

scheduled for completion by september 2014 , this assessment may help to address concerns raised by components .

this assessment is to determine the cornerstone program's overall implementation status , determine the effectiveness of the program's products and elements , evaluate the efficacy of the fundamentals of dhs leadership's learning objectives , and recommend specific tactical and strategic changes for improving program effectiveness .

the ldp has developed a program - wide assessment approach intended to analyze the impact of the ldp over time and assess whether the program is targeting the right things in the right way .

however , the ldp office could strengthen this assessment approach by more clearly identifying its program goals and ensuring its 12 performance measures incorporate key attributes of successful performance measures we have previously identified .

the ldp's assessment approach applies to all ldp program elements , including capstone , cornerstone , and other programs .

the approach consists of ( 1 ) biannually collecting and analyzing completion rate data for all ldp programs implemented by components , ( 2 ) collecting and analyzing responses to six core evaluation questions immediately following each developmental activity and 6 months later , and ( 3 ) tracking 12 program performance measures .

table 8 provides some examples of these 12 measures .

for more detailed information about the ldp's assessment approach , see appendix iv .

developing this assessment approach is a positive step toward assessing the effectiveness of the ldp .

however , the ldp office has not clearly identified goals for the program , and the 12 measures that the office has developed to assess its performance do not consistently exhibit attributes we have previously identified as key for successful measurement .

these key attributes include having linkage with division - and agency - wide goals , being clear , and having measurable targets .

table 9 presents definitions of these attributes along with potentially adverse consequences of not meeting them .

performance measurement is the ongoing monitoring and reporting of program accomplishments , particularly progress toward preestablished goals .

we have previously reported that performance measurement allows organizations to track progress in achieving their goals and gives managers crucial information to identify gaps in program performance and plan any needed improvements .

in addition , according to standards for internal control in the federal government , managers need to compare actual performance against planned or expected results and analyze significant differences .

we observed the following when assessing the ldp's performance measures against these selected key attributes: linkage: the ldp's 12 performance measures do not clearly link with program goals and linkage is not clearly communicated throughout the organization .

the ldp has identified working program goals , but they are disparately documented and not clearly identified as goals .

when we asked ldp office officials to identify the ldp's goals , the ldp manager referred us to statements in directive 258-02: leader development , and provided us with a list of working program goals assembled from statements in various ldp materials .

these working program goals included , for example , using best practices to maximize effectiveness and elevating the importance of developing leaders department - wide .

however , neither the directive nor ldp materials clearly identify or refer to the statements the manager directed us to as goals .

given that the ldp's identified working program goals are disparately documented and not clearly identified as goals , it is unclear whether the ldp's 12 performance measures align with the statements and working program goals the ldp manager identified .

we tried to identify linkage between the ldp's 12 performance measures and the informal goals the ldp manager identified , but could not determine definitively how they relate .

according to the ldp manager , the ldp office did not clearly document the program's goals in one place or record their linkage to the program's performance measures because it established the goals as it developed programs to support the leader development framework .

in addition , the ldp manager stated that dhs's strategic human capital – related plans — which include goals — were under development when the office developed the measures .

the ldp manager explained that , in developing the performance measures , an ldp program official used dhs department - wide strategic plans , direction from dhs officials and stakeholders , and guidance from the dhs learning evaluation guide — which provides guidance for evaluating the effectiveness of training activities — to determine categories for the performance measures and then developed measures pertaining to each category .

in addition , dhs uses data that the ldp office collects for its performance measures for strategic planning and reporting , according to the ldp manager .

for example , dhs uses data the ldp office collects to measure progress against two goals established in the dhs workforce strategy for fiscal years 2011- 2016 .

we agree that data collected to track the measures may provide information for measuring progress against some department - wide goals established in strategy documents ; however , it is not evident how these or other ldp performance measures link to goals specific to the ldp .

we have previously found that linkages between goals and measures are most effective when they are clearly communicated to all staff within the agency so that everyone understands what the organization is trying to achieve .

explicitly identifying program goals , creating an evident link between performance measures and program goals , and clearly communicating the linkage could help ensure that the behaviors and incentives created by the ldp's 12 performance measures support the ldp's intended outcomes , and that they are appropriate measures for the program .

clarity: not all ldp performance measures possess clarity because some measures include terms that are ambiguous and for which the ldp office has not documented definitions .

for example , one of the ldp's measures is the percentage of developmental activities that fulfill ldp requirements delivered with shared resources .

however , it is not clear what constitutes a “developmental activity” for the purpose of calculating this measure ( eg , a course or unit within a course ) , and the value could be different depending on the definition of “activity” used in the measure's calculation .

as a result , this measure could be confusing and misleading to users , such as dhs leadership and congressional constituents , by leading them to think that performance was better or worse than it actually was .

according to the ldp manager , the ldp office has not documented definitions for terms used in its performance measures because components may elect to fulfill ldp requirements through varied approaches and this makes how terms such as “developmental activity” are defined contextually driven .

while we recognize that components may use varied approaches to fulfilling ldp requirements , it is important that terms the ldp office uses in its performances measures are clear so that users understand what the measures mean .

ldp officials also stated that components are able to contact the ldp office with questions about how to calculate the measures , and that the ldp office will work with components to provide any requested clarification .

providing support to components is a positive step , but documenting definitions for ambiguous terms used in the measures could help ensure the meaning of their values is clear to stakeholders .

targets: the ldp's performance measures do not have measurable targets .

according to the ldp manager , the ldp has not set targets for its 12 program performance measures because it is too early in the process , as ldp officials have just established a baseline with fiscal year 2013 data .

the ldp manager anticipates developing ldp targets in the future and stated that program officials will consider doing so once they collect more data .

the ldp office does not have a definitive plan or time frame for setting targets , but according to the ldp manager , expects to do so using fiscal year 2014 data .

we agree that , consistent with key attributes of performance measures , developing measurable targets could help dhs determine whether the program's performance is meeting expectations .

to set appropriate targets , however , it will be important for the ldp office to first clearly identify program goals and ensure its performance measures link to the goals .

dhs leadership has previously identified implementation of leader development programs as important to the department's success and a means by which to improve its human capital management .

for example , in february 2012 , the then deputy secretary of homeland security stated that leader development is critical to dhs's growth and long - term success and must be a strategic mission investment priority .

in addition , dhs has identified implementation of the ldp among the actions it is taking to address dhs's high - risk designation with respect to human capital management .

by clearly identifying program goals and ensuring ldp performance measures include key attributes , such as linkage , clarity , and measurable targets , the ldp could strengthen its performance measurement , consequently producing actionable information for ldp management to use in identifying the need for , and making , program improvements .

as dhs faces increasingly complex national security challenges , it is important that it support employees with effective training and development programs to meet its mission requirements .

evaluating training and development programs is important for ensuring that such programs are cost - effective and continue to be relevant for the department .

by updating dhs components' documented training evaluation processes to more fully address key attributes for effective training evaluation , dhs components could have better assurance that the components have more complete information to guide their efforts in conducting effective evaluations .

such documentation can further help ensure that processes for assessing whether training programs support component and dhs needs are repeatable and consistently implemented .

further , given limited budgetary resources , by identifying existing challenges that prevent dhs from accurately capturing its training costs department - wide and , to the extent that the benefits exceed the costs , implementing corrective measures to overcome these challenges , dhs could improve its awareness about the actual costs of its training programs , and enhance its ability to consistently and reliably capture training costs dhs - wide , thereby enhancing its resource stewardship .

in addition , dhs is in the process of implementing a department - wide leader development program to build leadership skills across all staff levels .

the effectiveness of this program is particularly important given that dhs leadership has identified leader development as critical to the department's success .

as dhs begins to assess the impact of the ldp program , clearly identifying ldp goals and ensuring that ldp performance measures possess key attributes , including ( 1 ) linkage to the program's goals , ( 2 ) clarity , and ( 3 ) measurable targets by which to assess the measures could help provide dhs with the actionable information it needs to identify and make program improvements .

to ensure effective evaluation of federal training programs and enhance dhs's stewardship of resources for federal training programs , we recommend that the secretary of homeland security take the following two actions: direct dhs components to ensure that their documented training evaluation processes fully address attributes for effective training evaluation processes as they are drafted , updated , or revised and identify existing challenges that prevent dhs from accurately capturing training costs department - wide and , to the extent that the benefits of addressing those challenges exceed the costs , implement corrective measures to overcome these challenges .

in addition , to produce actionable information for dhs's ldp management to use in identifying the need for , and making , program improvements , we recommend that the secretary of homeland security direct the chief human capital officer to clearly identify ldp goals and ensure ldp performance measures include key attributes , such as linkage , clarity , and measurable targets .

we provided a draft of this report to dhs for review and comment .

dhs provided written comments , which are reprinted in appendix iv , and technical comments , which we incorporated as appropriate .

dhs agreed with all three of the recommendations and outlined steps to address them .

if fully implemented , these actions will address the intent of our recommendations .

with respect to the first recommendation , dhs agreed to ensure that effective training evaluation processes are documented and in place at components by incorporating a review of component training evaluation documents into the dhs chief human capital officer's audit of human resource functions .

dhs reports that a full review of components should be completed with the fiscal year 2019 audit cycle .

regarding the second recommendation , dhs agreed to resolve the issue of capturing training costs department - wide .

for example , dhs plans to establish a team jointly chaired by the dhs chief human capital officer and the chief financial officer , and comprised of representatives from both financial and training offices of each operational component and headquarters , that is to deliver a methodology to track and report training costs across dhs by june 30 , 2015 .

dhs anticipates the new methodology will be implemented across all components by january 31 , 2016 .

in response to the third recommendation , dhs agreed to publish clear dhs leader development program goals and performance measures that include key attributes , such as linkage , clarity and measurable targets on the dhs intranet website by december 31 , 2014 .

we are sending copies of this report to appropriate congressional committees and the secretary of homeland security .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

should you or your staff have any questions concerning this report , please contact me at ( 202 ) 512-9627 or by e - mail at maurerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix vi .

david maurer director , homeland security and justice .

our objectives for this report were to address the following questions: 1 .

to what extent does the department of homeland security ( dhs ) have documented processes to evaluate training and development programs and reliably capture costs ? .

2 .

what leader development programs has dhs implemented , what are stakeholders' perspectives on them , and to what extent does dhs measure program performance ? .

to understand training programs at dhs , we obtained information from the dhs office of the chief human capital officer ( ochco ) , and five selected components: the federal law enforcement training center ( fletc ) , u.s. customs and border protection ( cbp ) , u.s. immigration and customs enforcement ( ice ) , the transportation security administration ( tsa ) , and the united states coast guard .

we selected these components to represent different dhs mission areas , workforce sizes , training costs , and number of career senior executive service ( ses ) personnel .

in addition , these components have a mix of new and more established training programs .

when examining training programs at selected components , we reviewed component - level training evaluation and strategic plans when available ; training budget requests ; cost and expenditure documents ; training procedures , policies , and organizational charts ; and policies for identifying and prioritizing training programs ; selected training course materials , and other relevant documents .

to further our understanding of training at the component level , we also interviewed training officials at each of the selected components and identified these individuals based on their knowledge , experience , and leadership roles .

we conducted our interviews at component headquarters located in the washington , d.c. area , or field offices .

in addition , as part of our review of dhs's delivery of mission - critical law enforcement training across components , we observed training at fletc's glynco , georgia facilities .

the perspectives dhs ochco and the selected components provided are not generalizable to all of dhs , but provided helpful insights into the selected components specific training and development programs at dhs .

to address the first question , regarding the extent to which dhs has documented processes to evaluate training and development programs , and ensure training costs are reliably captured , we reviewed dhs and component - specific documents and interviewed relevant officials at dhs ochco and each of the components .

specifically , to determine the extent to which dhs has documented processes to evaluate its training and development programs , we reviewed policies and procedures related to the evaluation of training programs , such as component - specific standard operating procedures and training development standards .

we then assessed the documented processes from each of the selected components against attributes of training evaluation processes identified by the office of personnel management ( opm ) , dhs , and gao to determine the extent to which the documents include selected attributes of evaluation processes .

we selected the attributes for our analysis to include attributes that were consistently identified in relevant criteria documents related to training evaluation , such as the dhs learning evaluation guide , the opm training evaluation field guide , and gao's prior work on training and development , specifically the guide for strategic training and development efforts in the federal government .

these attributes also align with those identified in standards for internal control in the federal government , which calls for agencies to document the plans , methods , and procedures used to achieve missions , goals , and objectives and support performance - based management practices .

from these sources , we identified six attributes of a training evaluation process to conduct our analysis: ( 1 ) establishes goals about what the training program is supposed to achieve , ( 2 ) indicates which training programs are being evaluated , ( 3 ) explains the methodology used to conduct the evaluation , ( 4 ) presents time frames for conducting the evaluation , ( 5 ) presents roles and responsibilities for evaluation efforts , and ( 6 ) explains how the evaluation results will be used .

we assessed each component's documented evaluation process to determine the extent to which the attributes were included and gave a component a rating indicating that the attribute was fully met , a component partially met the attribute but did not fully or consistently meet all parts , or the component did not include any information to meet the attribute .

we also conducted semistructured interviews with officials responsible for conducting training evaluation at each of the five components to understand the evaluation process that each component follows and how evaluation feedback is used .

to assess the extent to which dhs ensures training costs are reliably captured , we reviewed information and relevant documentation on processes and steps components took to examine available budget and cost information .

we further reviewed documentation on the process of capturing training costs from each of our selected components , including financial audit reports .

as part of our review of cost tracking at dhs , we observed methods components used for identifying efficiencies in training to identify cost savings and employ more cost - effective alternatives .

we also conducted semistructured interviews with dhs and component officials responsible for administering training programs and tracking costs to understand how dhs and components identified and captured costs , and any challenges they may have in doing so in a reliable manner .

through our review of cost - saving documentation and interviews with dhs and component officials , we sought illustrative examples to understand how ochco and the selected dhs components identified potential efficiencies and steps planned or already taken to achieve them .

accordingly , ochco and dhs component officials identified examples of cost savings realized in selected training programs from fiscal year 2011 through fiscal year 2013 , and we reviewed the reliability of the related cost - saving estimates .

for example , we interviewed knowledgeable officials who provided cost estimates , reviewed the estimates related to cost savings , and replicated cost - saving calculations provided by components , including estimates for training equipment , salaries , and benefits .

we determined through analysis of cost - saving estimates and interviews with knowledgeable officials at dhs and the selected components that the cost - saving data provided and reported for the illustrative examples in this product were sufficiently reliable for the purposes of illustrating the types of cost efficiencies that may be achieved .

the cost - saving examples dhs ochco and components provided are not generalizable to all of dhs , but provided helpful insights into cost - saving efforts identified to date at dhs .

to address the second question , about leader development programs dhs has implemented , we reviewed program documentation , analyzed participant data , and interviewed officials from ochco and the selected components .

in particular , to determine what leader development programs dhs has implemented , we reviewed ochco leader development program ( ldp ) curricula and requirements documentation , such as the senior executive service candidate development program candidate guide and the cornerstone program requirements and accountability guide , and documentation of leader development programs provided by the selected components , such as program descriptions and evaluations .

in addition , we obtained and analyzed data from ochco and the selected components on the number of participants in the leader development programs they provided during fiscal years 2012 and 2013 .

we assessed the reliability of these data by interviewing agency officials familiar with the sources of the data regarding internal controls built into the information systems and stand - alone documents in which they are stored and quality assurance steps performed after data are entered into the systems or documents .

in addition , we compared participant completion data for the fundamentals of dhs leadership segment of the cornerstone program — one of dhs's leader development programs — for similar time periods that components provided to us and had previously reported to the ldp office .

where we identified discrepancies , we interviewed officials to determine their cause and the correct values .

we determined that the data were sufficiently reliable for the purpose of reporting the approximate number of program participants .

we also interviewed officials from ochco and the components regarding implemented and planned leader development programs .

to determine officials' perspectives on dhs leader development programs , we obtained ochco and component officials' views on the development and implementation of leader development programs and the programs' strengths and weaknesses .

the perspectives the interviewees provided are not generalizable to all dhs officials , but provided helpful insights into strengths and weaknesses of leader development programs .

to assess the extent to which dhs measures the performance of leader development programs , we reviewed program documentation from ochco and the selected components , including performance measurement requirements and guidance .

in addition , we interviewed cognizant officials about what performance measurement information they collect and how they use the information .

through these efforts , we determined that the ldp office uses 12 performance measures to assess the ldp's impact .

we assessed these measures against three of nine selected key attributes for performance measures identified in prior gao work that we identified as relevant given the maturity level of the ldp .

in particular , given that the ldp is a relatively new program , we focused our analysis on three attributes that we identified as foundational and — having linkage with division - and agency - wide goals , being clear , and having measurable targets .

we selected linkage because aligning measures with division - and agency - wide goals and mission helps ensure that the behaviors and incentives created by the measures support the division - or agency - wide goals or mission .

once the measures' relevance to a program is ensured through linkage , then assessment of more detailed aspects of the measures , such as reliability , is more relevant .

similarly , we selected having measurable targets because , without measurable targets , it may not be evident whether performance is meeting expectations .

with regard to clarity , if a measure is not clearly stated and the name and definition are not consistent with the methodology used to calculate it , performance data could be confusing and misleading to users , such as department leadership and congressional constituents .

we conducted this performance audit from july 2013 to september 2014 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

component u.s. customs and border protection ( cbp ) u.s. immigration and customs enforcement ( ice ) coast guard transportation security administration ( tsa ) federal law enforcement training center ( fletc ) explains how the evaluation results will be used   .

: the component's documented evaluation processes fully included information to meet the attribute for all aspects of its evaluation process .

: the component's documented evaluation processes included some information to address a given attribute but did not include information to fully and consistently meet all parts of the attribute .

this includes , for example , incomplete evaluation processes or incomplete information to address a given attribute for certain levels of the evaluation .

: the component's documented evaluation processes did not include information to address the attribute for the evaluation process .

cbp's documentation presents ways cbp can implement the kirkpatrick model but does not indicate the actual process that will be used .

cbp's documentation outlines when the various levels of evaluations are supposed to be administered , but does not present a timeframe for cbp to analyze the evaluation feedback .

cbp's documentation identifies some cbp entities that perform the evaluations and receive the evaluation information , but does not do so consistently for each level of evaluation .

ice's documentation outlines who is responsible for the various level 1 evaluation activities and what ice stakeholders should be involved in the process , but does not provide this information consistently for evaluation levels 2 and 3 .

the coast guard's documentation provides guidance on when to administer the evaluation surveys , but it does not specify timeframes for the coast guard to analyze the evaluation data .

tsa's documentation states that it will evaluate training programs using the kirkpatrick model and discusses the process in a very general sense .

however , the documentation does not indicate specifically how tsa will conduct each level of evaluation or the circumstances in which a certain approach will be used .

fletc's documentation indicates that it will evaluate its training programs using the four - level kirkpatrick model .

however , the documents do not consistently indicate how fletc will develop , administer , and analyze the evaluation data for each level .

for example , for level 3 evaluations , fletc's documentation includes some policies and procedures that govern the evaluations , but these procedures do not provide specifics on the process such as how the surveys are developed and deployed , and how the surveys are sent to a sample of students , among others .

the following information appears as interactive content in figure 3 in the report body when viewed electronically .

the department of homeland security's ( dhs ) leader development program ( ldp ) office has developed a program - wide assessment approach intended to analyze the impact of the ldp over time and to assess whether the program is targeting the right things in the right way .

this assessment approach , which applies to all ldp program elements — including capstone , cornerstone , and other programs — consists of ( 1 ) biannually collecting and analyzing completion rate data , ( 2 ) collecting and analyzing responses to six core evaluation questions , and ( 3 ) tracking 12 program performance measures .

table 11 provides more detailed information about this approach .

in addition to the contact named above , joseph p. cruz , assistant director ; chuck bausell ; gary bianchi ; gustavo crosetto ; peter deltoro ; michele fejfar ; eric hauswirth ; adam hoffman ; susan hsu ; kirk kiester ; tracey king ; taylor matheson ; signora may ; linda miller ; julia vieweg ; and yee wong made key contributions to this report .

