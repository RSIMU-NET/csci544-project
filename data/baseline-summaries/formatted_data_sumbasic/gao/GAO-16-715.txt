on august 10 , 2005 , a truck carrying over 35,000 pounds of explosives overturned and caught on fire on a highway near spanish fork , utah .

the fire resulted in an explosion that caused $2 million in property damage and 11 injuries .

although this type of incident is rare , it underscores the risk involved with transporting explosives .

explosives accounted for 4 million tons of the total 2.6 billion tons of hazardous materials ( hazmat ) transported in the u.s. by land , air , and water in 2012 .

explosives are used in a variety of industries , including the mining , oil and gas , and aerospace industries .

federal regulations require that explosives be classified before being transported .

classifications denote the relative risk posed by the explosive and dictate transportation requirements , such as by which modes the explosive can travel .

for example , only specific classifications of explosives , which are determined to pose lower risks , are permitted to travel by aircraft .

the vast majority of explosives are transported by truck .

the department of transportation's ( dot ) pipeline and hazardous materials safety administration ( phmsa ) is responsible for regulating the transport of hazmat , including explosives .

according to regulations , new explosives must be classified according to risk level before they can be transported .

under the classification process described in the regulations , a phmsa - approved third - party test lab must examine the new explosive and recommend a classification .

phmsa must then review the test lab's report and assign an approved classification .

phmsa processes over a thousand applications for classification of new explosives every year .

in 2010 , the dot's office of inspector general ( oig ) identified weaknesses in phmsa's oversight of the classification of new explosives .

since then , phmsa has made changes to improve its oversight .

the fixing america's surface transportation act included a provision for us to review phmsa's oversight of the classification of new explosives .

this report addresses ( 1 ) how phmsa oversees the classification of new explosives , including the performance of test labs , and selected stakeholder views on these oversight processes and ( 2 ) phmsa's efforts to improve the oversight process and what challenges , if any , phmsa faces .

we presented our findings to committee staff on june 1 , 2016 .

to respond to these objectives we collected and analyzed phmsa data on explosive classification applications processed from 2006 through 2015 and transportation incidents involving explosives from 2005 through 2015 .

through interviews with phmsa officials on how this information was collected and stored , we determined these data were sufficiently reliable for our purposes .

we also reviewed phmsa documentation related to its oversight of the classification of explosives , including regulations , procedures , and policy memos and interviewed phmsa officials on their oversight process , improvement efforts , and associated challenges .

to gather stakeholder views on phmsa's oversight , we interviewed associations representing explosive manufacturers ( institute of makers of explosives and american pyrotechnics association ) , hazmat carriers ( association of american railroads , american trucking associations , owner operator independent drivers association , international air transport association , and air line pilots association ) , and a hazmat education organization ( dangerous goods advisory council ) .

we also interviewed all six third party test labs currently approved by phmsa and five explosives manufacturers .

we selected the manufacturers to represent a mix of industries with a preference toward those that have used more than one test lab , which we determined using the test labs' 2015 annual reports submitted to phmsa .

views of those selected are not generalizable to all explosive manufacturers .

we conducted this performance audit from january 2016 to july 2016 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

explosives , like all hazardous materials ( hazmat ) , are subject to regulations to ensure safe handling and transportation , among other things .

hazmat regulations are coordinated with international standards and generally govern the labeling , packaging , and transportation of hazmat in commerce .

explosives are one of nine classes of hazmat .

in order to be transported , explosives must be assigned a classification .

the classification , which includes a number that denotes the risk level of the explosive ( from most to least hazardous ) , dictates associated transportation requirements , such as by which transportation modes the explosives can travel and how they are packaged .

for example , class 1.1 explosives , which pose a mass explosion hazard , cannot travel by aircraft but can travel by truck .

meanwhile , certain class 1.4 explosives , which pose a minor explosion hazard , that meet specific requirements can travel by aircraft or u.s .

postal service .

classifications also include “compatibility groups” that denote which explosives can be transported together .

for example , the regulations do not allow blasting detonators , which are used to trigger an explosive device , to be transported in the same truck as primary explosive substances .

unlike some other classes of hazmat that can be self - classified ( meaning the shipper classifies the material ) , in order to be classified , explosives must be first examined by one of six phmsa - approved third party test labs .

the explosives manufacturer , which ultimately submits an application to phmsa for classification , first selects and hires one of the test labs and makes a sample of the explosive available to the test lab for examination .

the test lab uses international standards to test the material and to recommend a classification .

phmsa then reviews the manufacturer's application , including the test lab's report and recommended classification , and approves a classification for the explosive ( see fig .

1 ) .

in 2010 and 2014 , the dot oig reported weaknesses in phmsa's management and oversight of the approvals process .

for example , in 2010 , the dot oig raised questions about the effectiveness of phmsa's oversight and found that phmsa had not inspected any of the test labs in 10 years and that test labs did not always submit annual reports as required .

in 2014 , the dot oig reported that phmsa had addressed these issues .

however , the 2014 report found phmsa evaluation forms missing for many explosives classification reviews , a situation that dot oig noted was an internal control weakness .

as a result , the dot oig recommended phmsa require the use of evaluation forms to document its review of explosive classification applications .

according to a dot oig official , phmsa effectively addressed this recommendation in 2015 .

phmsa's oversight of the classification of new explosives includes two key parts — ( 1 ) approving and monitoring the test labs and ( 2 ) reviewing and approving the manufacturers' applications for classification of a new explosive and the labs' test classification recommendations .

phmsa has several activities to approve and oversee test labs , but its efforts to ensure consistency are limited by phmsa's lack of a systematic approach to developing and issuing guidance for these labs .

in response to the dot oig's findings of oversight weaknesses from 2010 , phmsa strengthened its activities to approve and monitor test labs .

specifically , to ensure that test lab examiners meet the requirements specified in regulation , phmsa established an approval process that includes interviewing examiners and reviewing test lab and examiner qualifications and recertifying test labs every 5 years through an on - site inspection .

phmsa officials stated that in addition to ensuring compliance with regulations , an important goal of phmsa's oversight of test labs is to promote consistency across test labs .

phmsa officials stated that efforts to promote consistency are particularly important given turnover — half of test labs were approved in 2012 or later .

however , while phmsa works to promote consistency through various types of communications with test labs , it lacks a systematic approach to determining what guidance is needed and to issuing such guidance .

internal control standards state that agencies should communicate quality information so that external parties can help the agency achieve its objectives and address related risks .

likewise , we have previously reported that agencies benefit from procedures that continually reassess and improve guidance processes and documents to respond to the concerns of regulated entities .

according to phmsa officials , regulations and international standards described in a united nations test manual set forth certain requirements for testing new explosives .

however , phmsa officials told us they give test labs flexibility in how to apply these requirements on a case - by - case basis when testing new explosives to recommend classifications .

phmsa officials told us that granting this flexibility allows test labs to use their expertise and professional judgment .

for example , phmsa officials noted examiners can deviate from the proscribed tests in the test manual if they justify their reasoning for doing so based on their expertise , and officials at one test lab stated that , given the variation in the specific attributes of different new explosives , such flexibility can improve their ability to effectively test a new explosive .

however , phmsa and some test lab officials also stated that this flexible approach can lead to inconsistencies across test labs , such as a test lab examiner being unaware of a common deviation from the proscribed test or similar types of explosives being subject to different tests .

for example , one examiner reported learning of an “unwritten rule” that test labs can use alternative tests in cases where multiple samples of the explosive cannot be destroyed due to costs , such as for large and expensive explosives .

four of the six test labs we spoke to said written guidance from phmsa could help address “unwritten rules” such as commonly used modifications to the test manual .

although four of the five manufacturers we spoke to said the labs are consistent in quality of testing , two noted that test lab report quality can vary .

phmsa has ongoing efforts to promote consistency across test labs .

specifically , phmsa officials stated that they: discuss issues , best practices , and phmsa's recommended approaches with test lab examiners during in - person annual meetings and quarterly teleconferences .

phmsa also distributes agendas and minutes associated with these teleconferences and meetings .

five of the six test labs we spoke to said phmsa's quarterly teleconferences and annual meetings are helpful to share issues and good practices .

however , three test labs noted that test lab examiners are hesitant to volunteer information or ask questions during these meetings since the test labs compete with one another for business .

as one test lab noted , if one test lab has information that the others do not , this test lab has an incentive not to mention this information in order to have a business advantage over competitors .

issue letters of interpretation to communicate phmsa's views on specific issues in response to questions or concerns , which are posted on phmsa's website .

however , phmsa officials and one test lab stated that while the letters of interpretation include topical guidance , they are hard to find , are not organized by topic , and can contradict each other .

as described above , phmsa's approach to providing guidance is not systematic , and therefore phmsa may be missing areas where more guidance would be beneficial to helping test labs understand phmsa's expectations and to improving phmsa's ability to reach its objective of consistency among test labs .

moreover , phmsa officials acknowledge that they currently do not have a comprehensive written document that encompasses all phmsa guidance for test labs .

they stated that they are currently evaluating whether to issue such a document .

phmsa officials stated that the development of such a document could involve compiling the content of existing written communication such as the letters of interpretation , which , as described above , phmsa officials stated are not currently easily accessible .

however , phmsa officials have not specified a systematic approach for these efforts , including an effort to identify test labs' needs such as explaining the “unwritten rules” that affect phmsa's expectations for test labs .

without such an approach to improving its guidance , phmsa may not be providing test labs with the information needed to effectively meet phmsa's goal of promoting consistency .

phmsa has a multi - part application review and approval process for the classification of new explosives , including a check for completeness by a program officer , two levels of technical review , and the completion of an evaluation form ( see fig .

2 ) .

according to phmsa officials , phmsa's role in approving classifications , which is outlined in regulation , is essential to fulfilling its role in regulating hazmat transportation in the u.s .

furthermore , phmsa officials say its application review process serves quality assurance purposes , since test labs , which are paid by the manufacturers , compete for business and may be pressured by manufacturers to provide a specific classification .

two of the six test labs we spoke to reported facing pressure from manufacturers on how to conduct tests or which classification to recommend .

a third test lab also stated that manufacturers often aim to have their explosive classified as class 1.4 since , as mentioned previously , such explosives can be transported by aircraft .

one manufacturer noted that having an explosive classified as a 1.4 to travel by aircraft makes the product more competitive since air travel is the fastest option to get the product to an overseas customer .

phmsa officials stated that they are checking for multiple things in their technical reviews , and that in addition to ensuring that explosives are classified correctly , these technical reviews also help phmsa to improve test labs' performance .

specifically , phmsa officials stated that they keep notes in an excel spreadsheet on recurring issues identified in application reviews , which are then used to inform topics for annual and quarterly meetings with the test labs .

according to phmsa officials , its overarching goal in reviewing applications is to ensure that every new explosive is classified correctly , and as a result , application review processing times can vary greatly depending on many factors , such as the quality of the application ; the complexity of the application and the explosive device ; and the timeliness of test - lab response when phmsa technical reviewers reach out with questions or requests for additional information .

phmsa officials also noted that technical reviewers may give some applications by more recently approved test labs or examiners a closer look to develop a “comfort level” with their testing procedures , which can slow the review .

similarly , officials noted there has been turnover among phmsa's technical reviewers , which can slow the process since the second - level technical reviewer provides guidance and feedback to the first - level reviewers as part of their training .

phmsa officials also noted that they began emphasizing the process step of completing an evaluation form for each application in response to dot oig's 2014 report , which affected application review times .

other stakeholders we spoke with had varying views on phmsa's review process .

the manufacturers and the explosives manufacturing association we spoke to generally had two key complaints about phmsa's process , seeing it as overly time consuming and opaque .

time - consuming: industry stakeholders noted that the time required for phmsa's review can be lengthy .

specifically , four of the five manufacturers and an explosives industry association said phmsa's review takes too long , while one manufacturer noted that turnaround times have recently improved .

for example , three manufacturers told us that in their experience phmsa's review often takes up to 6 months .

manufacturers and the association stated that phmsa's review process delays manufacturers' ability to get a return on investments made in developing a new explosive .

opaque: some manufacturers noted that phmsa's review process is opaque , leaving the manufacturer unsure of their application's status .

in particular , four of the five selected manufacturers and an explosives manufacturing industry association we spoke to stated that phmsa could better communicate where applications are in the review process .

manufacturers and the explosives industry association stated that the uncertainty surrounding phmsa's review process creates business planning uncertainties , such as when to allocate staff and resources to manufacturing and sales .

furthermore , some manufacturers and the manufacturers' association questioned the value of what they see as an overly time - consuming and opaque process and suggested that phmsa should place more trust in the test lab results to reduce the amount of time phmsa takes to review applications .

for example , one manufacturer noted that phmsa's review adds little value since , in the manufacturer's experience , phmsa rarely changes a classification from the one recommended by the test lab .

this manufacturer noted that phmsa's efforts would be best suited to overseeing the test labs rather than reviewing applications and that the test lab should have the final determination on the classification .

in contrast to some manufacturers' views , other stakeholders , including carrier associations and test lab examiners , were supportive of phmsa's oversight role , including the review and approval of test labs' classification recommendations .

one air carrier association stated that phmsa's oversight is critical given what the association believes to be the test labs' potential conflict of interest since test labs are paid by manufacturers and since explosives , if misclassified , could pose a major risk during air transport .

the carrier associations we spoke to that represent air , rail , and trucking modes noted that phmsa's oversight is effective insofar as explosives are classified correctly and incidents are rare .

specifically , according to phmsa data , phmsa receives about 16,000 hazmat incident reports each year , and of that number , an average of about 35 per year involved explosives between 2005 and 2015 .

a total of 388 such incidents occurred during that time period .

over the same time period , excluding fireworks , only two incidents involving explosives resulted in injuries , but no such incidents resulted in fatalities .

in addition , all of the six test labs we spoke with were generally supportive of phmsa's oversight role .

for example , one test lab noted that phmsa's review is important for liability protection for the test labs , and expressed discomfort at the notion of test labs becoming responsible for assigning approvals without phmsa's review , noting that while such a change could speed the approval process , it could also decrease quality and consistency across test labs .

recently , phmsa has taken some steps to respond to manufacturers' concerns about the uncertainty and lack of transparency of the process .

phmsa established an internal goal of 120 days for average application processing times and , since the second quarter of fiscal year 2015 , has posted quarterly average application processing times on its website .

for the second quarter of fiscal year 2016 , phmsa reported the average was 99 days .

in addition , in a february 2016 memo on reforming the explosive classification approval review process , phmsa noted that to increase transparency , it had increased the information provided to manufacturers in its online status reports so that manufacturers could have better visibility into where each application is in the process .

representatives from one manufacturer we spoke to stated they appreciated the new ability to better track the status of applications through phmsa's review process .

in its february 2016 memo , which cited manufacturers' concerns with the approval process , phmsa outlined improvement efforts that align with goals in phmsa's office of hazardous materials safety 2013-2016 strategic plan .

these goals include increasing outreach , streamlining the regulatory system , and enhancing risk management .

to increase outreach , as described above , phmsa increased the amount of information about applications online .

phmsa also described several efforts that align with the goals of streamlining the regulatory system and enhancing risk management .

for example , in the memo , phmsa stated it would immediately begin a one - level technical review for certain applications , including explosives classified as 1.1 .

although these explosives pose the greatest risk of mass explosion , phmsa officials stated that the review is relatively low risk since these explosives have the most stringent packaging and transportation requirements .

to be classified as 1.1 , the explosive must meet the standard of being stable and not forbidden from transport but does not need to undergo further testing and scrutiny that would be required to determine whether an explosive could be classified at a lower risk that , for example , might allow it to go on an aircraft .

in the reform memo , phmsa also indicated that it would continue to look for future opportunities for streamlining .

in particular , phmsa said it would look for ( 1 ) other types of applications where a streamlined one - level technical review is appropriate and ( 2 ) specific types of explosive approvals that could be standardized in the regulations , which could allow for self - classification , meaning the manufacturer could determine the classification .

phmsa officials stated that identifying areas for self - classification involves research conducted either by phmsa or by the industry to determine whether the standard is sufficient to reduce the amount of time phmsa takes for its review without increasing the risk of misclassifying explosives .

despite these recent reform efforts , phmsa officials stated that limited staff resources create challenges for application turnaround .

according to data provided by phmsa , between 2006 and 2015 , on average , phmsa officials reviewed 1,700 applications for new explosive classifications annually .

as mentioned previously , each of these applications is subject to a multi - step review process , including a completeness check by a project officer , a two - level technical review , and approval or rejection letter signed by an approving official .

as noted in figure 3 , as of the time of our review , there were seven phmsa officials involved with this review process — two project officers , four technical reviewers ( three first level and one second level ) , and one primary approving official .

furthermore , these seven officials have other responsibilities outside of reviewing explosives classifications .

for example , technical reviewers assist with high priority hazmat issues such as crude oil by rail and lithium ion batteries , and the second level technical reviewer also supervises the work of the first level reviewers and represents phmsa in international working groups .

according to phmsa officials , although these activities are important to the agency , they can take time away from explosives approvals .

another challenge that affects phmsa's improvement efforts is a lack of sufficient data and data planning .

internal control standards for the federal government state that management should design the entity's information system to achieve objectives and respond to risks .

these standards state that an information system includes both manual and technology - enabled information processes and represents the life cycle of information used for the entity's operational processes .

to effectively design an entity's information system , the standards state that entities should consider the information requirements , including the expectations and needs of both internal and external users .

currently , according to phmsa officials , phmsa stores classification application information in a system — called the fyi system — that is a document management system , not a database .

phmsa officials stated that the fyi system does not allow for standard fields to be entered that could then be easily analyzed across applications .

instead , phmsa reviewers complete a separate microsoft word document evaluation form for each approval that is filed electronically .

phmsa officials noted that in addition to not being designed for evaluation or analysis across applications , the manual process of filling out the evaluation form can cause delays in the application review process .

due to the limitations of this current data system , phmsa was unable to provide us data that we requested in several areas .

specifically , phmsa was unable to provide data on the following: the number of applications in which phmsa approved a classification that was different from the one recommended by the test lab .

phmsa officials stated that to compile this data would be a manual process requiring staff to go through each application file and manually compare the recommended classification from the test lab to the final approved classification .

phmsa officials stated that as a result of these limitations , phmsa does not know how many applications required such a change .

a manufacturer's association suggested that information on how often phmsa changes a test lab's recommended classification could help inform the extent to which phmsa's final review adds value to the process .

phmsa officials stated they did not think that such information would be instructive in the level of value the final phmsa review adds since currently , the quality of test labs' recommendations may be influenced by the knowledge that phmsa will be providing a final review .

however , phmsa officials stated that they would like to be able to analyze this type of information .

the amount of time applications spent in each part of phmsa's review process .

phmsa officials stated that although they had developed a method to track information on applications' total time in phmsa's review process , to obtain more detailed information on how long applications spent in each part of the process would be difficult and time consuming given the limitations of the current system .

the amount of time different types of applications spent in phmsa's review process .

because of the effort involved in this type of analysis , phmsa officials stated that phmsa does not have historical information on timeframes for different types of explosives applications .

however , officials stated they had implemented a software fix that would allow them to obtain this information going forward .

the number of applications for which phmsa had to request additional information from test labs , which phmsa officials cited as a common reason for extended timeframes of application reviews , or any information on the reasons phmsa had to request additional information from test labs .

such information could potentially help phmsa target guidance efforts to labs .

the extent to which the length of phmsa's review of new applications varied by test lab , which could potentially help phmsa analyze the consistency of test lab reports and target guidance efforts to labs .

phmsa officials described several ongoing efforts to improve the agency's data systems .

phmsa officials stated that phmsa's goal is to develop a risk - based , data - driven system that allows phmsa to use data to identify potential risks , to address workflow process weaknesses , and to use resources more effectively .

moreover , phmsa officials stated that fiscal year 2016 funds have been designated to transition the fyi system to a phmsa - wide portal to better capture information in applications and develop the ability to analyze information across applications .

in addition , phmsa officials stated that a statistician has joined their staff to assist with data analysis , and phmsa has three contracts with outside organizations to improve phmsa's use of data for analytics , enhanced risk management , and quality assurance .

while phmsa has taken these steps towards improving its data system , and officials described some desired information fields and capabilities , the agency does not have a plan documenting the data fields or information needs required to reach its stated goals .

phmsa officials stressed one narrower aim for the new system — to provide greater transparency and predictability in the review process to better meet the needs of explosives manufacturers .

for example , phmsa officials stated that they would like the ability to track and report average processing times for each review process step .

phmsa officials stated that processing time information could also help the agency manage its staffing resources .

while phmsa officials discussed their desire for this added capability , they did not describe in a systematic way and have not documented in a plan how an improved information system could potentially help meet their goals of streamlining the review system and enhancing risk management , or how it could help them meet their stated goals of using data to identify potential risks , address workflow process weaknesses , and use resources more effectively .

for example , while , as described above , phmsa identified several factors , such as an application's quality and complexity of the application and the explosive device , that can result in a longer application review time , phmsa did not discuss or provide documentation considering how an improved information system could allow it to better track and analyze these issues .

similarly , while phmsa officials stated that test labs' quality and experience could affect the length of its review , phmsa officials have not developed data fields that could highlight such information in applications and be incorporated into the new system , potentially improving phmsa's ability to analyze the quality and consistency both across test labs and the reviewers .

this could help the agency target efforts to streamline its process .

finally , as mentioned previously , phmsa has recently identified reforms such as a one - level technical review and proposed standardization for certain types of explosives , and has noted that it would like to explore new areas for these opportunities .

without defining the necessary data elements to capture during the classification application review process , phmsa may be unable to analyze the effects of previous reform efforts or identify opportunities for further reform .

without a systematically developed plan , phmsa may miss opportunities to create an information system that allows phmsa to better meet the expectations of both external stakeholders , such as manufacturers , and internal stakeholders — i.e. , phmsa officials themselves .

although phmsa has taken steps to strengthen its oversight of the explosives classification process that align with its strategic goals , phmsa's ability to be responsive to stakeholder concerns and to overcome challenges may be limited without a more systematic approach to improvements in two areas: guidance to test labs and information systems .

without systematically determining what guidance would most benefit test labs and how to best communicate this guidance to test labs , phmsa's efforts to support increased consistency among test labs and respond to test labs' desire for clearer guidance in certain areas may fall short .

similarly , without developing a data plan that clearly defines what information phmsa most needs to meet its various objectives , the effectiveness of phmsa's data improvement efforts to meet the expectations of internal and external users may be reduced .

in contrast , a carefully developed and implemented data plan could potentially help phmsa respond to manufacturers' concerns about the timeliness of phmsa's review process and mitigate phmsa's challenges related to limited staff resources while also helping it meet its goals related to outreach , transparency , and developing a risk - based approach .

to improve phmsa's oversight of the explosives classification process , the secretary of transportation should direct the phmsa administrator to take the following two actions: 1 .

develop and implement a systematic approach for improving the guidance phmsa provides test labs .

2 .

develop a written plan describing information requirements for phmsa's new data system .

such a data plan should include information requirements needed to meet phmsa's goals and address risks .

we provided a draft of this product to the department of transportation ( dot ) for comment .

in written comments , reproduced in appendix i , dot concurred with our recommendations .

dot stated that it continues to dedicate resources to improve the safety , oversight , and system efficiency of the explosives classification approval program and described several recent actions taken to enhance its oversight .

in addition , dot provided a technical comment that we incorporated as appropriate .

we are sending copies of this report to the appropriate congressional committees , the secretary of the department of transportation , and other interested parties .

in addition , the report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-2834 or flemings@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix ii .

in addition to the contact named above , alwynne wilbur ( assistant director ) , tim bober , tara carter , david hooper , emily larson , saraann moessbauer , malika rice , amy rosewarne , and kelsey sagawa made key contributions to this report .

