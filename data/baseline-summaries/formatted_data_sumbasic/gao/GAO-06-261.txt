in 1992 , the united states began a unilateral moratorium on the testing of nuclear weapons .

prior to the moratorium , underground nuclear testing was a critical component of the evaluation and certification of the performance of a nuclear weapon .

confidence in the continued performance of stockpiled weapons relied heavily on the expert judgment of weapon designers who had significant experience with successful nuclear tests .

in addition , the training of new weapon designers depended on continued nuclear testing .

in 1993 , the department of energy ( doe ) , at the direction of the president and the congress , established the stockpile stewardship program to ensure the preservation of the united states' core intellectual and technical competencies in nuclear weapons without testing .

the national nuclear security administration ( nnsa ) , a separately organized agency within doe , is now responsible for carrying out the stockpile stewardship program , which includes activities associated with the research , design , development , simulation , modeling , and nonnuclear testing of nuclear weapons .

the three nuclear weapons design laboratories — lawrence livermore national laboratory ( llnl ) in california , los alamos national laboratory ( lanl ) in new mexico , and sandia national laboratories ( snl ) in california and new mexico — use the results of these activities to annually assess the safety and reliability of the nation's nuclear weapons stockpile and to certify to the president that the resumption of underground nuclear weapons testing is not needed .

when the moratorium began in 1992 , doe ( and subsequently nnsa ) faced several challenges in fulfilling its new mission of stockpile stewardship .

for example , since both expected and unexpected changes occur as the nuclear stockpile ages , nnsa has become more concerned with gaining a detailed understanding of how such changes might affect the safety and reliability of stockpiled weapons .

however , unlike the rest of a nuclear weapon , the nuclear explosive package — which contains the primary and the secondary — cannot be tested simply by evaluating individual components .

specifically , because the operation of the nuclear explosive package is highly integrated , nonlinear , occurs during a very short period of time , and reaches extreme temperatures and pressures , there are portions of the nuclear explosive package that cannot be tested outside of a nuclear explosion .

in addition , although the united states conducted about 1,000 nuclear weapons tests prior to the moratorium , only a few tests were designed to collect data on uncertainties associated with a particular part of the nuclear explosive package .

as a result , much of the scientific basis for the examination of an exploding nuclear weapon must be extrapolated from other phenomena .

finally , since nuclear testing is no longer available to train new weapons designers , nnsa and the weapons laboratories are faced with the need to develop a rigorous , transparent , and explainable approach to all aspects of the weapon design process , including the assessment and certification of the performance of nuclear weapons .

to address these challenges , in 1999 , doe established 18 programs — which it referred to as “campaigns” — six of which were intended to develop the scientific knowledge , tools , and methods required to provide confidence in the assessment and certification of the safety and reliability of the nuclear stockpile in the absence of nuclear testing .

these scientific campaigns include the ( 1 ) primary assessment technologies ( primary ) , ( 2 ) secondary assessment technologies ( secondary ) , ( 3 ) advanced simulation and computing ( asc ) , ( 4 ) advanced radiography , ( 5 ) dynamic materials properties , and ( 6 ) inertial confinement fusion and high yield ( icf ) campaigns .

in particular , the primary and secondary campaigns are designed to analyze and understand the different scientific phenomena that occur in the primary and secondary stages of a nuclear weapon during detonation .

as such , the primary and secondary campaigns are intended to set the requirements for the computer models and experimental data provided by the other campaigns that are needed to assess and certify the safety and reliability of nuclear weapons .

while the campaign structure brought increased organization to the scientific research conducted across the weapons complex , nnsa still lacked a coherent strategy for relating the scientific research conducted by the weapons laboratories to the needs of the nuclear stockpile and the stockpile stewardship program .

consequently , in 2001 , llnl and lanl began developing what is intended to be a common framework for a new methodology for assessing and certifying the safety and reliability of warheads in the nuclear stockpile in the absence of nuclear testing .

the stockpile stewardship program is now over 10 years old , nnsa's campaign structure is in its sixth year , and 4 years have passed since llnl and lanl began their effort to develop a new assessment and certification methodology .

as the weapons in the nuclear stockpile continue to age , and as more experienced weapon designers and other scientists and technicians retire , nnsa is faced with increased urgency in meeting the goals of the stockpile stewardship program .

furthermore , nnsa has recently created an effort , known as the reliable replacement warhead ( rrw ) program , to study a new approach to maintaining nuclear warheads over the long term .

the rrw program would redesign weapon components to be easier to manufacture , maintain , dismantle , and certify without nuclear testing , potentially allowing nnsa to transition to a smaller , more efficient weapons complex .

nnsa's ability to successfully manage these efforts will have a dramatic impact on the future of the u.s. nuclear stockpile and , ultimately , will affect the president's decision of whether a return to nuclear testing is required to maintain confidence in the safety and reliability of the stockpile .

in this context , you asked us to evaluate ( 1 ) the new methodology nnsa is developing for assessing and certifying the safety and reliability of the nuclear stockpile in the absence of nuclear testing and ( 2 ) nnsa's management of the implementation of this methodology .

to evaluate the new methodology nnsa is developing for assessing and certifying the safety and reliability of the nuclear stockpile in the absence of nuclear testing , we reviewed relevant policy and planning documents from nnsa and the three weapons laboratories , including implementation plans and program plans for the six scientific campaigns .

we focused our work principally on the primary and secondary campaigns because the primary and secondary are the key components of the nuclear explosive package and because the primary and secondary campaigns are intended to set the requirements for the experimental data and computer models needed to assess and certify the performance of nuclear weapons .

we also reviewed relevant reports , including those from nnsa's office of defense programs science council , the mitre corporation's jason panel , university of california review committees for lanl and llnl , and the strategic advisory group stockpile assessment team for u.s. strategic command .

in addition , we interviewed officials from nnsa headquarters and site offices , as well as contractors who operate nnsa sites .

our primary source of information was nnsa's office of defense programs .

we also met with officials at lanl , llnl , and snl .

finally , we interviewed nuclear weapons experts , senior scientists , and other relevant officials outside of nnsa and the laboratories , including members of nnsa's office of defense programs science council , the jason panel , university of california review committees for lanl and llnl , the strategic advisory group stockpile assessment team for u.s. strategic command , and the deputy assistant to the secretary of defense ( nuclear matters ) for the department of defense .

to evaluate nnsa's management of the implementation of its new methodology to assess and certify the safety and reliability of nuclear weapons in the absence of nuclear testing , we reviewed relevant nnsa policy , planning , and evaluation documents , including the office of defense program's program management manual , campaign program and implementation plans , contractor performance evaluation plans and reports , and internal reviews of nnsa management .

we also reviewed contractor planning and evaluation documents , including lanl , llnl , and snl performance evaluation plans and reports .

finally , we met with campaign managers and other officials at nnsa headquarters and site offices , lanl , llnl , and snl .

we performed our work between august 2004 and december 2005 in accordance with generally accepted government auditing standards .

most modern nuclear warheads contain a nuclear explosive package , which contains the primary and the secondary , and a set of nonnuclear components .

the nuclear detonation of the primary produces energy that drives the secondary , which produces further nuclear energy of a militarily significant yield .

the nonnuclear components control the use , arming , and firing of the warhead .

all nuclear weapons developed to date rely on nuclear fission to initiate their explosive release of energy .

most also rely on nuclear fusion to increase their total energy yield .

nuclear fission occurs when the nucleus of a heavy , unstable atom ( such as uranium - 235 ) is split into two lighter parts , which releases neutrons and produces large amounts of energy .

nuclear fusion occurs when the nuclei of two light atoms ( such as deuterium and tritium ) are joined , or fused , to form a heavier atom , with an accompanying release of neutrons and larger amounts of energy .

the u.s. nuclear stockpile consists of nine weapon types .

 ( see table 1. ) .

the lifetimes of the weapons currently in the stockpile have been extended well beyond the minimum life for which they were originally designed — generally about 20 years — increasing the average age of the stockpile and , for the first time , leaving nnsa with large numbers of weapons that are close to 30 years old .

established in 1993 , the stockpile stewardship program faces two main technical challenges: provide ( 1 ) a better scientific understanding of the basic phenomena associated with nuclear weapons and ( 2 ) an improved capability to predict the impact of aging and remanufactured components on the safety and reliability of nuclear weapons .

specifically , an exploding nuclear weapon creates the highest pressures , greatest temperatures , and most extreme densities ever made by man on earth , within some of the shortest times ever measured .

when combined , these variables exist nowhere else in nature .

while the united states conducted about 1,000 nuclear weapons tests prior to the moratorium , these tests were conducted mainly to look at broad indicators of weapon performance ( such as the yield of a weapon ) and were often not designed to collect data on specific properties of nuclear weapons physics .

after more than 60 years of developing nuclear weapons , while many of the physical processes are well understood and accurately modeled , the united states still does not possess a set of completely known and expressed laws and equations of nuclear weapons physics that link the physical event to first principles .

as nuclear weapons age , a number of physical changes can take place .

the effects of aging are not always gradual , and the potential for unexpected changes in materials causes significant concerns as to whether weapons will continue to function properly .

replacing aging components is , therefore , essential to ensure that the weapon will function as designed .

however , it may be difficult or impossible to ensure that all specifications for the manufacturing of new components are precisely met , especially since each weapon was essentially handmade .

in addition , some of the manufacturing process lines used for the original production have been disassembled .

in 1995 , the president established an annual assessment and reporting requirement designed to help ensure that nuclear weapons remain safe and reliable without underground testing .

as part of this requirement , the three weapons laboratories are required to issue a series of reports and letters that address the safety , reliability , performance , and military effectiveness of each weapon type in the stockpile .

the letters , submitted to the secretary of energy individually by the laboratory directors , summarize the results of the assessment reports and , among other things , express the directors' conclusions regarding whether an underground nuclear test is needed and the adequacy of various tools and methods currently in use to evaluate the stockpile .

to address these challenges , in 1999 doe developed a new three - part program structure for the stockpile stewardship program that included a series of campaigns , which doe defined as technically challenging , multiyear , multifunctional efforts to develop and maintain the critical capabilities needed to continue assessing the safety and reliability of the nuclear stockpile into the foreseeable future without underground testing .

doe originally created 18 campaigns that were designed to focus its efforts in science and computing , applied science and engineering , and production readiness .

six of these campaigns currently focus on the development and improvement of the scientific knowledge , tools , and methods required to provide confidence in the assessment and certification of the safety and reliability of the nuclear stockpile in the absence of nuclear testing .

these six campaigns are as follows: the primary and secondary campaigns were established to analyze and understand the different scientific phenomena that occur in the primary and secondary stages of a nuclear weapon during detonation .

as such , the primary and secondary campaigns are intended to support the development and implementation of the qmu methodology and to set the requirements for the computers , computer models , and experimental data needed to assess and certify the performance of nuclear weapons .

the asc campaign provides the leading - edge supercomputers and models that are used to simulate the detonation and performance of nuclear weapons .

two campaigns — advanced radiography and dynamic materials properties — provide data from laboratory experiments to support nuclear weapons theory and computational modeling .

for example , the advanced radiography campaign conducts experiments that measure how stockpile materials behave when exposed to explosively driven shocks .

one of the major facilities being built to support this campaign is the dual axis radiographic hydrodynamic test facility at lanl .

the icf campaign develops experimental capabilities and conducts experiments to examine phenomena at high temperature and pressure regimes that approach but do not equal those occurring in a nuclear weapon .

as a result , scientists currently have to extrapolate from the results of these experiments to understand similar phenomena in a nuclear weapon .

one of the major facilities being built as part of this campaign is the national ignition facility at llnl .

the other two program activities associated with the stockpile stewardship program are “directed stockpile work” and “readiness in technical base and facilities.” directed stockpile work includes the activities that directly support specific weapons in the stockpile , such as the stockpile life extension program , which employs a standardized approach for planning and carrying out nuclear weapons refurbishment activities to extend the operational lives of the weapons in the stockpile well beyond their original design lives .

the life extension for the w87 was completed in 2004 , and three other weapon systems — the b61 , w76 , and w80 — are currently undergoing life extensions .

each life extension program is specific to that weapon type , with different parts being replaced or refurbished for each weapon type .

readiness in technical base and facilities includes the physical infrastructure and operational readiness required to conduct campaign and directed stockpile work activities across the nuclear weapons complex .

the complex includes the three nuclear weapons design laboratories ( lanl , llnl , and snl ) , the nevada test site , and four production plants — the pantex plant in texas , the y - 12 plant in tennessee , a portion of the savannah river site in south carolina , and the kansas city plant in missouri .

from fiscal year 2001 through fiscal year 2005 , nnsa spent over $7 billion on the six scientific campaigns ( in inflation - adjusted dollars ) .

 ( see table 2. ) .

nnsa has requested almost $7 billion in funding for these campaigns over the next 5 years .

 ( see table 3. ) .

within nnsa , the office of defense programs is responsible for managing the campaigns and the stockpile stewardship program in general .

within this office , two organizations share responsibility for overall management of the scientific campaigns: the office of the assistant deputy administrator for research , development , and simulation and the office of the assistant deputy administrator for inertial confinement fusion and the national ignition facility project .

the first office oversees campaign activities associated with the primary and secondary campaigns — as well as the asc , advanced radiography , and dynamic materials properties campaigns — with a staff of about 13 people .

the second office oversees activities associated with the icf campaign with a single staff person .

actual campaign activities are conducted by scientists and other staff at the three weapons laboratories .

lanl and llnl conduct activities associated with the nuclear explosive package , while snl performs activities associated with the nonnuclear components that control the use , arming , and firing of the nuclear warhead .

nnsa has endorsed the use of a new common methodology , known as the quantification of margins and uncertainties , or qmu , for assessing and certifying the safety and reliability of the nuclear stockpile .

nnsa and laboratory officials told us that they have made progress in applying the principles of qmu to the certification and assessment of nuclear warheads in the stockpile .

however , qmu is still in its early stages of development , and important differences exist among the three laboratories in their application of qmu .

to date , nnsa has commissioned two technical reviews of the implementation of qmu at the weapons laboratories .

while strongly supporting qmu , the reviews found that the development and implementation of qmu was still in its early stages .

the reviews recommended that nnsa take steps to further define the technical details supporting the implementation of qmu and integrate the activities of the three weapons laboratories in implementing qmu .

however , nnsa and the weapons laboratories have not fully implemented these recommendations .

beyond the issues raised in the two reports , we also found differences in the understanding and application of qmu among the three laboratories .

when the primary and secondary campaigns were established in 1999 , they brought some organization and overall goals to the scientific research conducted across the weapons complex .

for example , as we noted in april 2005 , the primary campaign set an initial goal in the 2005 to 2010 time frame for certifying the performance of the primary of a nuclear weapon to within a stated yield level .

however , according to senior nnsa officials , nnsa still lacked a coherent strategy for relating the scientific work conducted by the weapons laboratories under the campaigns to the needs of the nuclear stockpile and the overall stockpile stewardship program .

this view was echoed by a nnsa advisory committee report , which stated in 2002 that the process used by the weapons laboratories to certify the safety and reliability of nuclear weapons was ill defined and unevenly applied , leading to major delays and inefficiencies in programs .

starting in 2001 , llnl and lanl began developing what is intended to be a common methodology for assessing and certifying the performance and safety of nuclear weapons in the absence of nuclear testing .

in 2003 , the associate directors for nuclear weapons at llnl and lanl published a white paper — entitled “national certification methodology for the nuclear weapon stockpile” — that described this new methodology , which they referred to as the quantification of margins and uncertainties or qmu .

according to the white paper , qmu is based on an adaptation of standard engineering practices and lends itself to the development of “rigorous , quantitative , and explicit criteria for judging the robustness of weapon system and component performance at a detailed level.” moreover , the quantitative results of this process would enable nnsa and the weapons laboratories to set priorities for their activities and thereby make rational decisions about allocating program resources to the nuclear stockpile .

the process envisaged in the white paper focuses on creating a “watch list” of factors that , in the judgment of nuclear weapons experts , are the most critical to the operation and performance of a nuclear weapon .

these factors include key operating characteristics and components of the nuclear weapon .

for each identified , critical factor leading to a nuclear explosion , nuclear weapons experts would define performance metrics .

these performance metrics would represent the experts' best judgment of what constitutes acceptable behavior — i.e. , the range of acceptable values for a critical function to successfully occur or for a critical component to function properly — as well as what constitutes unacceptable behavior or failure .

to use an analogy , consider the operation of a gasoline engine .

some of the events critical to the operation of the engine would include the opening and closing of valves , the firing of the spark plugs , and the ignition of the fuel in each cylinder .

relevant performance metrics for the ignition of fuel in a cylinder would include information on the condition of the spark plugs ( eg , whether they are corroded ) and the fuel / air mixture in the cylinder .

once nuclear experts have identified the relevant performance metrics for each critical factor , according to the 2003 white paper , the goal of qmu is to quantify these metrics .

specifically , the qmu methodology seeks to quantify ( 1 ) how close each critical factor is to the point at which it would fail to perform as designed ( i.e. , the performance margin or the margin to failure ) and ( 2 ) the uncertainty in calculating the margin .

according to the white paper , the weapons laboratories would be able to use their calculated values of margins and uncertainties as a way to assess their confidence in the performance of a nuclear weapon .

that is , the laboratories would establish a “confidence ratio” for each critical factor — they would divide their calculated value for the margin ( “m” ) by their calculations of the associated uncertainty ( “u” ) and arrive at a single number ( “m / u” ) .

according to the white paper , the weapons laboratories would only have confidence in the performance of a nuclear weapon if the margin “significantly” exceeds uncertainty for all critical issues .

however , the white paper did not define what the term “significantly” meant .

in a broad range of key planning and management documents that have followed the issuance of the white paper , nnsa and the weapons laboratories have endorsed the use of the qmu methodology as the principal tool for assessing and certifying the safety and reliability of the nuclear stockpile in the absence of nuclear testing .

for example , in its fiscal year 2006 implementation plan for the primary campaign , nnsa stated as a strategic objective that it needs to develop the capabilities and understanding necessary to apply qmu as the assessment and certification methodology for the nuclear explosive package .

in addition , in its fiscal year 2006 budget request , nnsa selected its progress toward the development and implementation of qmu as one of its major performance indicators .

finally , in the plans that nnsa uses to evaluate the performance of lanl and llnl , nnsa has established an overall objective for lanl and llnl to assess and certify the safety and reliability of nuclear weapons using a common qmu methodology .

officials at nnsa and the weapons laboratories have also stated that qmu will be vital to certifying any weapon redesigns , such as are envisioned by the rrw program .

for example , senior nnsa officials told us that the stockpile stewardship program will not be sustainable if it only involves the continued refurbishment in perpetuity of existing weapons in the current nuclear stockpile .

they stated that the accumulation of small changes over the extended lifetime of the current nuclear stockpile will result in increasing levels of uncertainty about its performance .

if nnsa moves forward with the rrw program , according to nnsa documents and officials , the future goal of the weapons program will be to use qmu to replace existing stockpile weapons with an rrw whose safety and reliability could be assured with the highest confidence , without nuclear testing , for as long as the united states requires nuclear forces .

according to nnsa and laboratory officials , the weapons laboratories have made progress in applying the principles of qmu to the certification of life extension programs and to the annual stockpile assessment process .

for example , llnl officials told us that they are applying qmu to the assessment of the w80 , which is currently undergoing a life extension .

they said that , in applying the qmu methodology , they tend to focus their efforts on identifying credible “failure modes,” which are based on observable problems , such as might be caused by the redesign of components in a nuclear weapon , changes to the manufacturing process for components , or the performance of a nuclear weapon under aged conditions .

they said that , for the w80 life extension program , they have developed a list of failure modes and quantified the margins and uncertainties associated with these failure modes .

based on their calculations , they said that they have increased their confidence in the performance of the w80 .

similarly , lanl officials told us that they are applying qmu to the w76 , which is also currently undergoing a life extension and is scheduled to finish its first production unit in 2007 .

they said that , in applying the qmu methodology , they tend to focus their efforts on defining “performance gates,” which are based on a number of critical points during the explosion of a nuclear weapon that separate the nuclear explosion into natural stages of operation .

the performance gates identify the characteristics that a nuclear weapon must have at a particular time during its operation to meet its performance requirements ( eg , to reach its expected yield ) .

lanl officials told us that they have developed a list of performance gates for the w76 life extension program and are beginning to quantify the margins and uncertainties associated with these performance gates .

despite this progress , we found that qmu is still in its early stages of development and that important differences exist among the weapons laboratories in their application of qmu .

to date , nnsa has commissioned two technical reviews of the implementation of qmu at the weapons laboratories .

the first review was conducted by nnsa's office of defense programs science council ( science council ) — which advises nnsa on scientific matters across a range of activities , including those associated with the scientific campaigns — and resulted in a march 2004 report .

the second review was conducted by the mitre corporation's jason panel and resulted in a february 2005 report .

both reports endorsed the use of qmu by the weapons laboratories and listed several potential benefits that qmu could bring to the nuclear weapons program .

for example , according to the science council report , qmu will serve an important role in training the next generation of nuclear weapon designers and will quantify and increase nnsa's confidence in the assessment and certification of the nuclear stockpile .

according to the jason report , qmu could become a useful management tool for directing investments in a given weapon system where they would be most effective in increasing confidence , as required by the life extension programs .

in addition , the jason report described how lanl and llnl officials had identified potential failure modes in several weapon systems and calculated the associated margins and uncertainties .

the report noted that , for most of these failure modes , the margin for success was large compared with the uncertainty in the performance .

however , according to both the science council and the jason reports , the development and implementation of qmu is still in its early stages .

for example , the jason report described qmu as highly promising but unfinished , incomplete and evolving , and in the early stages of development .

moreover , the chair of the jason panel on qmu told us in june 2005 that , during the course of his review , members of the jason panel found that qmu was not mature enough to assess its reliability or usefulness .

the reports also stated that the weapons laboratories have not fully developed or agreed upon the technical details supporting the implementation and application of qmu .

for example , the jason report stated that , in the course of its review , it became evident that there were a variety of differing and sometimes diverging reviews of what qmu really was and how it was working in practice .

as an example , the report stated that some of the scientists , designers , and engineers at lanl and llnl saw the role of expert judgment as an integral part of the qmu process , while others did not .

in discussions with the weapons laboratories about the two reports , lanl officials told us that they believed that the details of qmu as a formal methodology are still evolving , while llnl officials stated that qmu was “embryonic” and not fully developed .

while supporting qmu , the two reports noted that the weapons laboratories face challenges in successfully implementing a coherent and credible analytical method based on the qmu methodology .

for example , in its 2004 report , the science council stated that , in its view , the qmu methodology is based on the following core assumptions: computer simulations can accurately predict the behavior of a complex nuclear explosive system as a function of time .

it is sufficient for the assessment of the performance of a nuclear weapon to examine the simulation of the time evolution of a nuclear explosive system at a number of discrete time intervals and to determine whether the behavior of the system at each interval is within acceptable bounds .

the laboratories' determinations of acceptable behavior can be made quantitatively — that is , they will make a quantitative estimate of a system's margins and uncertainties .

given these quantitative measures of the margins and uncertainties , it is possible to calculate the probability ( or confidence level ) that the nuclear explosive system will perform as desired .

however , the science council's report noted that extraordinary degrees of complexity are involved in a rational implementation of qmu that are only beginning to be understood .

for example , in order for the qmu methodology to have validity , it must sufficiently identify all critical failure modes , critical events , and associated performance metrics .

however , as described earlier , the operation of an exploding nuclear weapon is highly integrated and nonlinear , occurs during a very short period of time , and reaches extreme temperatures and pressures .

in addition , the united states does not possess a set of completely known and expressed laws and equations of nuclear weapons physics .

given these complexities , it will be difficult to demonstrate the successful implementation of qmu , according to the report .

in addition , the science council stated that it was not presented with any evidence that there exists a method — even in principle — for calculating an overall probability that a nuclear explosive package will perform as designed from the set of quantitative margins and uncertainties at each time interval .

to address these and other issues , the two reports recommended that nnsa take steps to further define the technical details supporting the implementation of qmu and to integrate the activities of the three weapons laboratories in implementing qmu .

for example , the 2004 science council report recommended that nnsa direct the associate directors for nuclear weapons at lanl and llnl to undertake a major effort to define the details of qmu .

in particular , the report recommended that a trilaboratory team be charged with defining a common language for qmu and identifying the important performance gates , failure modes , and other criteria in the qmu approach .

the report stated that this agreed - upon “reference” set could then be used to support all analyses of stockpile issues .

in addition , the report recommended that nnsa consider establishing annual or semiannual workshops for the three weapons laboratories to improve the identification , study , and prioritization of potential failure modes and other factors that are critical to the operation and performance of nuclear weapons .

similarly , the 2005 jason panel report noted that the meaning and implications of qmu are currently unclear .

to rectify this problem , the report recommended that the associate directors for nuclear weapons at lanl and llnl write a new , and authoritative , paper defining qmu and submit it to nnsa .

furthermore , the report recommended that the laboratories establish a formal process to ( 1 ) identify all failure modes and performance gates associated with qmu , using the same methodology for all weapon systems , and ( 2 ) establish better relationships between the concepts of failure modes and performance gates for all weapon systems in the stockpile .

however , nnsa and laboratory officials have not fully implemented these recommendations , particularly the recommendations of the science council .

for example , while llnl and lanl officials are drafting a new “white paper” on qmu that attempts to clarify some fundamental tenets of the methodology , officials from snl are not involved in the drafting of this paper .

in addition , nnsa has not required the three weapons laboratories to hold regular meetings or workshops to improve the identification , prioritization , and integration of failure modes , performance gates , and other critical factors .

according to nnsa's assistant deputy administrator for research , development , and simulation , nnsa has not fully implemented the recommendations of the science council's report partly because the report was intended more to give nnsa a sense of the status of the implementation of qmu than it was to provide recommendations .

for example , the 2004 report states that the “friendly review,” as the report is referred to by nnsa , would not have budget implications and that the report's findings and recommendations would be reported only to the senior management of the weapons laboratories .

as a result , the assistant deputy administrator told us that he had referred the recommendations to the directors of the weapons laboratories and told them to implement the recommendations as they saw fit .

furthermore , llnl and lanl officials disagreed with some of the statements in the science council report and stressed that , in using qmu , they do not attempt to assign an overall probability that the nuclear explosive package will perform as desired .

that is , they do not attempt to add up calculations of margins and uncertainties for all the critical factors to arrive at a single estimate of margin and uncertainty , or a single confidence ratio , for the entire nuclear explosive package .

instead , they said that they focus on ensuring that the margin for each identified critical factor in the explosion of a nuclear weapon is greater than the uncertainty .

however , they said that , for a given critical factor , they do combine various calculations of individual uncertainties that contribute to the total amount of uncertainty for that factor .

in addition , in addressing comments in the jason report , llnl and lanl officials stressed that qmu has always relied , and will continue to rely heavily , on the judgment of nuclear weapons experts .

for example , llnl officials told us that since there is no single definition of what constitutes a threshold for failure , they use expert judgment to decide what to put on their list of failure modes .

they also said that the qmu methodology provides a way to make the entire annual assessment and certification process more transparent to peer review .

similarly , lanl officials said that they use expert judgment extensively in establishing performance metrics and threshold values for their performance gates .

they said that expert judgment will always be a part of the scientific process and a part of qmu .

beyond the issues raised in the two reports , we found that there are differences in the understanding and application of qmu among the three laboratories .

for example , the three laboratories do not agree about the application of qmu to areas outside of the nuclear explosive package .

specifically , llnl officials told us that the qmu methodology , as currently developed , only applies to the nuclear explosive package and not to the nonnuclear components that control the use , arming , and firing of the nuclear warhead .

according to llnl and lanl officials , snl scientists can run hundreds of experiments to test their components and , therefore , can use normal statistical analysis in certifying the performance of nonnuclear components .

as a result , according to llnl and lanl officials , snl does not have to cope with real uncertainty and does not “do” qmu .

furthermore , according to llnl officials , snl has chosen not to participate in the development of qmu with llnl and lanl .

however , snl officials told us that while some of the nonnuclear components are testable to a degree , snl is as challenged as the other two weapons laboratories in certifying the performance of their systems without actual testing .

for example , snl officials said that they simply do not have enough money to perform enough tests on all of their nonnuclear components to be able to rely completely on statistical analysis to meet their safety performance levels .

in addition , snl scientists are not able to test their components under the conditions of a nuclear explosion but are still required to certify the performance of the components under these conditions .

thus , snl officials told us that they had been using their own version of qmu for a long time .

snl officials told us that they define qmu as a way to make risk - informed decisions about the effect of variabilities and uncertainties on the performance of a nuclear weapon , including the nonnuclear components that control the use , arming , and firing of the nuclear warhead .

moreover , they said that this kind of risk - informed approach is not unique to the nuclear weapons laboratories and is used extensively in areas such as nuclear reactor safety .

however , they told us that they have been left out in the development of qmu by the two other weapons laboratories .

specifically , they said that while snl scientists have worked with other scientists at lanl and llnl at a “grass roots” level , there has only been limited cooperation and dialogue between upper - level management at the three laboratories concerning the development and implementation of qmu .

in addition , we found that while llnl and lanl both agree on the fundamental tenets of qmu at a high level , their application of the qmu methodology differs in some important respects .

for example , llnl and lanl officials told us that , at a detailed level , the two laboratories are pursuing different approaches to calculating and combining uncertainties .

for the w80 life extension program , llnl officials showed us how they combined calculations of individual uncertainties that contributed to the total uncertainty for a key failure mode of the primary — the amount of primary yield necessary to drive the secondary .

however , they said that the scientific support for their method for combining individual calculations of uncertainty was limited , and they stated that they are pursuing a variety of more sophisticated analyses to improve their current approach .

moreover , the two laboratories are taking a different approach to generating a confidence ratio for each critical factor , as described in the 2003 white paper on qmu .

for example , for the w80 life extension program , llnl officials showed us how they calculated a single confidence ratio for a key failure mode of the primary , based on their calculations of margin and uncertainty .

they said that the weapon systems for which they are responsible have a lot of margin built into them , and they feel comfortable generating this number .

in contrast , in discussions with lanl officials about the w76 life extension program , lanl officials told us that they prefer not to calculate a single confidence ratio for a performance gate , partly because they are concerned that their customers ( eg , the department of defense ) might think that the qmu methodology is more formal than it is currently .

in commenting on the differences between the two laboratories , nnsa officials stated that the two laboratories are pursuing complementary approaches , and that these differences are part of the rationale for a national policy decision to maintain two nuclear design laboratories .

in addition , they stated that the confidence in the correctness of scientific research is improved by achieving the same answer through multiple approaches .

llnl officials also made similar comments , stating that the nation will benefit from some amount of independence between the laboratories to assure that the best methodology for assessing the stockpile in the absence of nuclear testing is achieved .

nnsa relies on its primary and secondary campaigns to manage the development and implementation of qmu .

according to nnsa policies , campaign managers at nnsa headquarters are responsible for developing campaign plans and high - level milestones , overseeing the execution of these plans , and providing input to the evaluation of the performance of the weapons laboratories .

however , nnsa's management of these processes is deficient in four key areas .

first , the planning documents that nnsa has established for the primary and secondary campaigns do not adequately integrate the scientific research currently conducted that supports the development and implementation of qmu .

second , nnsa has not developed a clear , consistent set of milestones to guide the development and implementation of qmu .

third , nnsa has not established formal requirements for conducting annual , technical reviews of the implementation of qmu or for certifying the completion of qmu - related milestones .

finally , nnsa has not established adequate performance measures to determine the progress of the laboratories in developing and implementing qmu .

as part of its planning structure , nnsa requires the use of program and implementation plans to set requirements and manage resources for the campaigns and other programs associated with the stockpile stewardship program .

program plans are strategic in nature and identify the long - term goals , high - level milestones , and resources needed to support a particular program over a 7-year period , while implementation plans establish performance expectations for the program and each participating site for the current year of execution .

according to nnsa policies , program and implementation plans should flow from and interact with each other using a set of cascading goals and requirements .

nnsa has established a single program plan , which it calls the “science campaign program plan,” that encompasses the primary and the secondary campaigns , as well as two other campaigns — advanced radiography and dynamic materials properties .

nnsa has also established separate implementation plans for each of these campaigns , including the primary and secondary campaigns .

according to nnsa , it relies on these plans — and in particular the plans related to the primary and secondary campaigns — to manage the development and implementation of qmu , as well as to determine the requirements for the experimental data and computer modeling needed to analyze and understand the different scientific phenomena that occur in a nuclear weapon during detonation .

however , the current primary and secondary campaign plans do not contain a comprehensive , integrated list of the relevant scientific research being conducted across the weapons complex to support the development and implementation of qmu .

for example , according to the nnsa campaign manager for the primary campaign , he had to hold a workshop in 2005 with officials from the weapons laboratories in order to catalogue all of the scientific activities that are currently performed under the heading of “primary assessment” regardless of the nnsa funding source .

according to this official , the existing primary campaign implementation plan does not provide the integration across nnsa programs that is needed to achieve the goals of the primary campaign and to develop and implement qmu .

according to nnsa officials , the lack of integration has occurred in large part because a significant portion of the scientific research that is relevant to the primary and secondary campaigns is funded and carried out by different campaigns and other programs .

specifically , different nnsa campaign managers use different campaign planning documents to plan and oversee research and funding for activities that are directly relevant to the primary and secondary campaigns and the development and implementation of qmu .

for example , the asc campaign provides the supercomputing capability that the weapons laboratories use to simulate and predict the behavior of an exploding nuclear weapon .

moreover , the weapons laboratories rely on asc supercomputers to quantify their uncertainties with respect to the accuracy of these computer simulations — a key component in the implementation of qmu .

as a result , the asc campaign plans and funds activities that are critical to the development and implementation of qmu .

to address this problem , according to nnsa officials , nnsa is taking steps to establish better relationships among the campaign plans .

for example , nnsa is currently drafting a new plan — which it calls the primary assessment plan — in an attempt to better coordinate the activities covered under the separate program and implementation plans .

the draft plan outlines high - level research priorities , time lines , and proposed milestones necessary to support ( 1 ) nnsa's responsibilities for the current stockpile , ( 2 ) primary physics design for the development of an rrw , and ( 3 ) certification of an rrw in the 2012 time frame and a second rrw in the 2018 time frame .

according to nnsa officials , they expect to finalize this plan by the third quarter of fiscal year 2006 .

in addition , they expect to have a similar plan for the secondary campaign finalized by december 2006 and are considering combining both plans into a full - system assessment plan .

according to one nnsa official responsible for the primary and secondary campaigns , nnsa will revise the existing campaign program and implementation plans to be consistent with the primary assessment plan .

more fundamentally , some nuclear weapons experts have suggested that nnsa's planning structure should be reorganized to better reflect the use of qmu as nnsa's main strategy for assessing and certifying the performance of nuclear weapons .

for example , the chair of the llnl defense and nuclear technologies director's review committee — which conducts technical reviews of llnl's nuclear weapons activities for the university of california — told us that the current campaign structure has become a series of “stovepipes” that nnsa uses to manage stockpile stewardship .

he said that in order for nnsa to realize its long - term goals for implementing qmu , nnsa is going to have to reorganize itself around something that he called an “uncertainty spreadsheet” for each element of a weapon's performance ( eg , implosion of the primary , transfer of energy to the secondary , etc .

 ) , leading to the weapon's yield .

he said that the laboratories should develop a spreadsheet for each weapon in the stockpile that ( 1 ) identifies the major sources of uncertainty at each critical event in their assessment of the weapon's performance and ( 2 ) relates the laboratory's scientific activities and milestones to these identified sources of uncertainty .

he said that the development and use of these spreadsheets would essentially capture the intent of the scientific campaigns and make them unnecessary .

nnsa has established a number of milestones that relate to the development and implementation of qmu .

within the science campaign program plan , nnsa has established a series of high - level milestones , which it calls “level - 1” milestones .

according to nnsa policies , level - 1 milestones should be sufficient enough to allow strategic integration between sites involved in the campaigns and between programs in nnsa .

within the implementation plans for the primary and secondary campaigns , nnsa has established a number of lower - level milestones , which it calls “level - 2” milestones , which nnsa campaign managers use to track major activities for the current year of execution .

the level - 1 milestones related to qmu are shown in table 4 , and the level - 2 milestones related to qmu for the primary campaign are shown in table 5 .

according to nnsa officials , the level - 1 milestones in table 4 represent a two - stage path to systematically identify uncertainties and reduce them through analyzing past underground test results , developing new experimental capabilities , and performing new experiments to understand the relevant physical processes .

according to these level - 1 milestones , nnsa expects to complete the second stage or “cycle” of this process by fiscal year 2014 ( i.e. , milestone m20 ) , at which time nnsa will have sufficiently reduced major sources of uncertainties and will have confidence in its ability to predict the performance of nuclear weapons in the absence of nuclear testing .

however , we identified several problems with the nnsa milestones related to the development and implementation of qmu .

specifically , the level - 1 milestones in the science campaign program plan have the following problems: the milestones are not well - defined and never explicitly mention qmu .

according to nnsa officials responsible for overseeing the primary campaign , these milestones are too qualitative and too far in the future to enable nnsa to effectively plan for and oversee the implementation of qmu .

they described these milestones as “fuzzy” and said that they need to be better defined .

however , nnsa officials also stated that these milestones are not just for qmu but for the entire science campaign , of which qmu is only a part .

the milestones conflict with the performance measures shown in other important nnsa management documents .

specifically , while the science campaign program plan envisions a two - stage path to identify and reduce key uncertainties related to nuclear weapon operations using qmu by 2014 , the performance measures in nnsa's fiscal year 2006 budget request and in appendix a of the science campaign program plan call for the completion of qmu by 2010 .

the milestones have not been integrated with other qmu - related level - 1 milestones in other planning documents .

for example , the current asc campaign program plan contains a series of level - 1 milestones for completing the certification of several weapon systems — including the b61 , w80 , w76 , and w88 — with quantified margins and uncertainties by the end of fiscal year 2007 .

however , these milestones do not appear in and are not referenced by the science campaign program plan .

moreover , the asc campaign manager told us that , until recently , he was not aware of the existence of the level - 1 milestones for implementing qmu that are contained in the science campaign program plan .

in addition , we found that neither the science campaign program plan nor the primary campaign implementation plan describe how the level - 2 milestones on qmu in the primary campaign implementation plan are related to the level - 1 milestones on qmu in the science campaign program plan .

consequently , it is unclear how the achievement of specific level - 2 milestones — such as the development of probabilistic tools and methods to combine various sources of uncertainty for primary performance — will result in the achievement of level - 1 milestones for the implementation of qmu or how nnsa expects to certify several major nuclear weapon systems using qmu before the qmu methodology is fully developed and implemented .

nnsa , as well as laboratory officials , agreed that there are weaknesses with the current qmu milestones .

according to nnsa officials , when nnsa established the current tiered structure for campaign milestones in 2003 , the different tiers of milestones served different purposes and , therefore , were never well - integrated .

for example , nnsa officials said that the level - 1 milestones were originally created to reflect measures that were deemed to be important to senior nnsa officials , while level - 2 milestones were created to be used by nnsa campaign managers to perform more technical oversight of the weapons laboratories .

furthermore , according to nnsa officials , the current level - 2 milestones are only representative of campaign activities conducted by the weapons laboratories .

that is , the level - 2 milestones were never designed to cover the entire scope of work being conducted by the weapons laboratories and are , therefore , not comprehensive in scope .

to address these problems , according to nnsa officials , nnsa is taking steps to develop better milestones to track the implementation of the qmu methodology .

for example , in the draft primary assessment plan , nnsa has established 19 “high - level” milestones that cover the time period from fiscal year 2006 to fiscal year 2018 .

according to these draft milestones , by fiscal year 2010 , nnsa expects to “complete the experimental work and methodology development needed to demonstrate the ability of primary certification tools to support certification of existing stockpile system and rrw.” in addition , nnsa expects to certify a rrw in fiscal year 2012 and a second rrw in fiscal year 2018 .

according to nnsa policies , campaign managers are required to track the status of level - 1 and level - 2 milestones and provide routine , formal reports on the status of their programs .

for example , campaign managers are required to track , modify , and score the status of level - 1 and level - 2 milestones through the use of an internet - based application called the milestone reporting tool .

on a quarterly basis , campaign managers assign one of four possible scores for each milestone listed in the application: ( 1 ) “blue” for completed milestones , ( 2 ) “green” for milestones that are on track to be finished by the end of the fiscal year , ( 3 ) “yellow” for milestones that may not be completed by the end of the fiscal year , and ( 4 ) “red” for milestones that will not be completed by the end of the fiscal year .

at quarterly program review meetings , campaign managers brief senior - level nnsa officials on the status of major milestones , along with cost and expenditure data for their programs .

in addition , campaign managers are responsible for conducting technical reviews of the campaigns for which they are responsible , at least annually , to ensure that campaign activities are being executed properly and that campaign milestones are being completed .

however , nnsa campaign managers have not met all of the nnsa requirements needed to effectively oversee the primary and secondary campaigns .

for example , we found that the campaign managers for the primary and secondary campaigns have not established formal requirements for conducting annual , technical reviews of the implementation of qmu at the three weapons laboratories .

moreover , these officials have not established requirements for certifying the completion of level - 2 milestones that relate to qmu .

they could not provide us with documentation showing the specific activities or outcomes that they expected from the weapons laboratories in order to certify that the laboratories had completed the level - 2 milestones for qmu .

instead , they relied more on ad hoc reviews of campaign activities and level - 2 milestones as part of their oversight activities for their campaigns .

according to the primary campaign manager , the officials at the weapons laboratories are the principal managers of campaign activities .

as a result , he views his role as more of a “sponsor” for his program and , therefore , does not require any written reports or evidence from the laboratories to certify that they have completed specific milestones .

in contrast , we found that the asc campaign manager has established formal requirements for a variety of reoccurring technical reviews of activities associated with the asc campaign .

specifically , the asc campaign relies on semiannual reviews conducted by the asc predictive science committee — which provides an independent , technical review of the status of level - 2 milestones — as well as on annual “principal investigators” meetings that provide a technical review of every program element within the asc campaign .

the asc campaign manager told us that he relies on these technical reviews to oversee program activities because the quarterly program review meetings are not meant to help him manage his program but are really a way for senior - level nnsa officials to stay informed .

in addition , the asc campaign manager has established detailed , formal requirements for certifying the completion of level - 2 milestones for the asc campaign .

specifically , the fiscal year 2006 implementation plan for the asc campaign contains a detailed description of what nnsa expects from the completion of each level - 2 milestone , including a description of completion criteria , the method by which nnsa will certify the completion of the milestone , and an assessment of the risk level associated with the completion of the milestone .

the asc campaign manager told us that , when nnsa officials created the level - 2 milestones for the campaigns in 2003 , the milestones were really just “sentences” and lacked the detailed criteria that would enable nnsa managers to adequately track and document the completion of major milestones .

as a result , the asc campaign has made a major effort in recent years to develop detailed , formal requirements to support the completion of asc level - 2 milestones .

nnsa uses performance measurement data to inform resource decisions , improve the management and delivery of products and services , and justify budget requests .

according to nnsa requirements , performance measurement data should explain in clear , concise , meaningful , and measurable terms what program officials expect to accomplish for a specific funding level over a fixed period of time .

in addition , performance measurement data should include annual targets that describe specific outputs that can be measured , audited , and substantiated by the detailed technical milestones contained in documentation such as campaign implementation plans .

with respect to qmu , nnsa has established an overall annual performance target to measure the cumulative percentage of progress toward the development and implementation of the qmu methodology .

specifically , in its fiscal year 2006 budget request to the congress , nnsa stated that it expects to complete the development and implementation of qmu by 2010 as follows: 25 percent complete by the end of fiscal year 2005 , 40 percent complete by the end of fiscal year 2006 , 55 percent complete by the end of fiscal year 2007 , 70 percent complete by the end of fiscal year 2008 , 85 percent complete by the end of fiscal year 2009 , and 100 percent complete by the end of fiscal year 2010 .

according to nnsa , it had progressed 10 percent toward its target of completing qmu by the end of fiscal year 2004 .

however , nnsa officials could not document how they can measure progress toward the performance target for developing and implementing qmu .

moreover , nnsa officials could not explain how the 2010 overall performance target for the completion and implementation of qmu is related to the level - 1 milestones for qmu in the science campaign program plan , which describes a two - stage process to identify and reduce key uncertainties in nuclear weapon performance using qmu by 2014 .

according to one nnsa official responsible for overseeing the primary campaign , nnsa created this annual performance target because the office of management and budget requires agencies to express some of their annual performance targets in percentage terms .

however , this official said the actual percentages are not very meaningful , and he does not have any specific criteria for how to measure progress to justify the use of the percentages in the budget request .

nnsa has also established broad performance measures to evaluate the performance of lanl and llnl .

specifically , in its performance evaluation plans for lanl and llnl for fiscal year 2006 , nnsa has established the following three performance measures: use progress toward quantifying margins and uncertainty , and experience in application , to further refine and document the qmu methodology .

demonstrate application of a common assessment methodology ( i.e. , qmu ) in major warhead assessments and the certification of life extension program warheads .

complete the annual assessment of the safety , reliability , and performance of all warhead types in the stockpile , including reaching conclusions on whether nuclear testing is required to resolve any issues .

however , the plan that nnsa uses to evaluate the performance of snl does not contain any performance measures or targets specifically related to qmu , and the performance evaluation plans for lanl and llnl do not contain any annual targets that can be measured and linked to the specific performance measures related to qmu .

instead , the plans state that nnsa will rely on llnl and lanl officials to develop the relevant targets and related dates for each performance measure , as well as to correlate the level - 1 and level - 2 milestones with these measures .

when asked why these plans do not meet nnsa's own requirements , nnsa officials said that they have not included specific annual performance targets in the plans because to do so would make it harder for them to finalize the plans and adjust to changes in nnsa's budget .

however , they said that nnsa is planning on implementing more stringent plans that will include annual performance targets when the next contract for lanl and llnl is developed .

in addition , nnsa officials told us that they recognize the need to develop performance measures related to qmu for snl and anticipate implementing these changes in the fiscal year 2007 performance evaluation plan .

nnsa officials told us that they have used more specific measures , such as the completion of level - 2 milestones , in their assessment of the weapons laboratories' performance since fiscal year 2004 .

however , we also found problems with the way nnsa has assessed the performance of the weapons laboratories in implementing qmu .

for example , in nnsa's annual performance appraisal of lanl for fiscal year 2004 , nnsa states that lanl had completed 75 percent of the work required to develop “qmu logic” for the w76 life extension by the end of fiscal year 2004 .

however , nnsa officials could not document how they are able to measure progress toward the development and implementation of qmu logic for the w76 life extension .

again , an nnsa official responsible for overseeing the primary campaign told us that the actual percentages are not very meaningful , and that he did not have any specific criteria for how to measure progress to justify the use of the percentage in the appraisal .

in a recent report , we recognized the difficulties of developing useful results - oriented performance measures for programs such as those geared toward research and development programs .

for programs that can take years to observe program results , it can be difficult to identify performance measures that will provide information on the annual progress they are making toward achieving program results .

however , we also recognize that such efforts have the potential to provide important information to decision makers .

nnsa officials told us that they recognize the need for developing appropriate measures to ensure that adequate progress is being maintained toward achieving the goals and milestones of the campaigns .

however , according to nnsa , very few products of the scientific campaigns involve the repetition of specific operations whose costs can be monitored effectively as a measure of performance .

as a result , the best measure of progress for the scientific campaigns is through scientific review by qualified technical peers at appropriate points in the program .

however , nnsa has not established any performance measures or targets for implementing qmu that require periodic scientific peer reviews or define what is meant by “appropriate” points in the program .

faced with an aging nuclear stockpile , as well as an aging workforce , nnsa needs a methodologically rigorous , transparent , and explainable approach for how it will continue to assess and certify the safety and reliability of the nation's nuclear weapons stockpile , now and into the foreseeable future , without underground testing .

after over a decade of conducting stockpile stewardship , nnsa's selection of qmu as its methodology for assessment and certification represents a positive step toward a methodologically rigorous , transparent , and explainable approach that can be carried out by a new cadre of weapons designers .

however , important technical and management details must be resolved before nnsa can say with certainty that it has a sound and agreed upon approach .

first , nnsa must take steps to ensure that all three nuclear weapons laboratories — not just lanl and llnl — are in agreement about how qmu is to be defined and applied .

while we recognize that there will be methodological differences between lanl and llnl in the detailed application of qmu to specific weapon systems , we believe that it is fundamentally important that these differences be understood and , if need be , reconciled , to ensure that qmu achieves the goal of a common methodology with rigorous , quantitative , and explicit criteria , as envisioned by the original 2003 white paper on qmu .

more importantly , we believe that snl has an important role in the development and application of qmu to the entire warhead , and we find the continuing disagreement over the application of qmu to areas outside of the nuclear explosive package to be disconcerting .

there have been several recommendations calling for a new , technical paper defining qmu , as well as the establishment of regular forums to further develop the qmu methodology and reconcile any differences in approach .

we believe the nnsa needs to fully implement these recommendations .

second , nnsa has not made effective use of its current planning and program management structure to ensure that all of the research needed to support qmu is integrated and that scarce scientific resources are being used efficiently .

we believe that nnsa must establish an integrated management approach involving planning , oversight , and evaluation methods that are all clearly linked to the overall goal of the development and application of qmu .

in particular , we believe that nnsa needs clear , consistent , and realistic milestones and regular , technical reviews of the development of qmu in order to ensure sound progress .

finally , while we support the development of qmu and believe it must be effectively managed , we also believe it is important to recognize and acknowledge that the development and application of qmu , especially the complexities involved in analyzing and combining uncertainties related to potential failure modes and performance margins , represents a daunting research challenge that may not be achievable in the time constraints created by an aging nuclear stockpile .

to ensure that the weapons laboratories will have the proper tools in place to support the continued assessment of the existing stockpile or the certification of redesigned nuclear components under the rrw program , we recommend that the administrator of nnsa take the following two actions: require the three weapons laboratories to formally document an agreed upon , technical description of the qmu methodology that clearly recognizes and reconciles any methodological differences .

establish a formal requirement for periodic collaboration between the three weapons laboratories to increase their mutual understanding of the development and implementation of qmu .

to ensure that nnsa can more effectively manage the development and implementation of qmu , we recommend that the administrator of nnsa take the following three actions: develop an integrated plan for implementing qmu that contains ( 1 ) clear , consistent , and realistic milestones for the development and implementation of qmu across the weapons complex and ( 2 ) formal requirements for certifying the completion of these milestones .

establish a formal requirement for conducting annual , technical reviews of the scientific research conducted by the weapons laboratories that supports the development and implementation of qmu .

revise the performance evaluation plans for the three weapons laboratories so that they contain annual performance targets that can be measured and linked to specific milestones related to qmu .

we provided nnsa with a draft of this report for their review and comment .

overall , nnsa agreed that there was a need for an agreed - upon technical approach for implementing qmu and that nnsa needed to improve the management of qmu through clearer , long - term milestones and better integration across the program .

however , nnsa stated that qmu had already been effectively implemented and that we had not given nnsa sufficient credit for its success .

in addition , nnsa raised several issues about our conclusions and recommendations regarding their management of the qmu effort .

the complete text of nnsa's comments on our draft report is presented in appendix i. nnsa also made technical clarifications , which we incorporated in this report as appropriate .

with respect to whether qmu has already been effectively implemented , during the course of our work , lanl and llnl officials showed us examples of where they used the qmu methodology to examine specific issues associated with the stockpile .

at the same time , during our discussions with laboratory officials , as well as with the chairs of the jason panel on qmu , the office of defense programs science counsel , and the strategic advisory group stockpile assessment team of the u.s. strategic command , there was general agreement that the application of the qmu methodology was still in the early stages of development .

as nnsa pointed out in its letter commenting on our report , to implement qmu , the weapons laboratories need to make a number of improvements , including techniques for combining different kinds of uncertainties , as well as developing better models for a variety of complex processes that occur during a nuclear weapon explosion .

in addition , the successful implementation of qmu will continue to rely on the expert judgment and the successful completion of major scientific facilities such as the national ignition facility .

we have modified our report to more fully recognize that qmu is being used by the laboratories to address stockpile issues and to more completely characterize its current state of development .

at the same time , however , because qmu is still under development , we continue to believe that nnsa needs to make more effective use of its current planning and program management structure .

nnsa raised several specific concerns about our conclusions and recommendations .

first , nnsa disagreed with our conclusion and associated recommendations that nnsa take steps to ensure that all three nuclear weapons laboratories are in agreement about how qmu is to be defined and applied .

nnsa stated that we overemphasized the differences between lanl and llnl in implementing qmu and that , according to nnsa , lanl and llnl have a “common enough” agreement on qmu to go forward with its implementation .

moreover , nnsa stated that our recommendations blur very clear distinctions between snl and the two nuclear design labs .

according to nnsa , qmu is applied to issues regarding the nuclear explosive package , which is the mission of lanl and llnl .

while we believe that some of the technical differences between the laboratories remain significant , we have revised our report to more accurately reflect the nature of the differences between lanl and llnl .

with respect to snl , we would again point out that snl officials are still required to certify the performance of nuclear weapon components under the conditions of a nuclear explosion and , thus , use similar elements of the qmu methodology .

therefore , we continue to believe that all three laboratories , as well as nnsa , would benefit from efforts to more formally document the qmu methodology and regularly meet to increase their mutual understanding .

as evidence of the benefits of this approach , we would note that llnl and lanl are currently developing a revised “white paper” on qmu , and that in discussions with one of the two authors , he agreed that inclusion of snl in the development of the draft white paper could be beneficial .

second , nnsa made several comments with respect to our recommendation that nnsa develop an integrated plan for implementing qmu that contains clear , consistent , and realistic milestones .

for example , nnsa stated that they expect to demonstrate the success of the implementation of qmu and the scientific campaigns by the performance of a scientifically defensible qmu analysis for each required certification problem .

in addition , nnsa stated that the 2010 budget target and the 2014 milestone were developed for different purposes and measure progress at different times .

according to nnsa , the 2010 target describes developing qmu to the point that it can be applied to certification of a system ( eg , the w88 ) without underground testing , while the 2014 milestone is intended to be for the entire science campaign effort .

however , as we state in our report , and as acknowledged by nnsa officials responsible for the primary and secondary campaigns , there continue to be problems with the milestones that nnsa has established for implementing qmu .

among these problems is the fact that these milestones are not well - defined and conflict with other performance measures that nnsa has established for qmu .

moreover , in its comments on our report , nnsa agreed that better integration and connectivity of milestones between various program elements would improve the communications of the importance of program goals and improve the formality of coordination of program activities , “which is currently accomplished in an informal and less visible manner.” given this acknowledgment by nnsa , we continue to believe that an integrated plan for implementing qmu , rather than nnsa's current ad hoc approach , is warranted .

third , nnsa made several comments regarding our recommendation that nnsa establish a formal requirement for conducting annual , technical reviews of the scientific research conducted by the weapons laboratories that supports the development and implementation of qmu .

nnsa stated that it believes the ad hoc reviews it conducts , such as the jason review , provide sufficient information on scientific achievements , difficulties , and required redirection to manage these programs effectively .

as a result , nnsa stated that it has not selected a single review process to look at overall success in the implementation of qmu but expects to continue to rely on ad hoc reviews .

we agree that reviews , such as the jason review , are helpful , and we relied heavily on the jason review , as well as other reviews as part of our analysis .

however , as we point out in the report , the issue is that the campaign managers for the primary and secondary campaigns do not meet all of nnsa's own requirements for providing effective oversight , which include the establishment of formal requirements for conducting technical reviews of campaign activities .

therefore , we believe that nnsa needs to take steps to implement its own policies .

in addition , we believe that the asc campaign provides a good role model for how the primary and secondary campaigns should be managed .

finally , nnsa made several comments with respect to our recommendation for nnsa to revise the performance evaluation plans for the laboratories so that they contain annual performance targets that can be measured and linked to specific milestones related to qmu .

specifically , nnsa stated that the implementation of qmu is an area where it is difficult to establish a meaningful metric .

according to nnsa , since qmu is implicitly evaluated in every review of the components of the science campaign , nnsa does not believe it is necessary to formally state an annual qmu requirement .

however , as we point out in the report , the current performance evaluation plans for lanl and llnl do not meet nnsa's own requirements for the inclusion of annual performance targets that can be measured and linked to the specific performance measures related to qmu .

more fundamentally , since nnsa has placed such emphasis on the development and implementation of qmu in the years ahead , we continue to believe that nnsa needs to develop more meaningful criteria for assessing the laboratories' progress in developing and implementing qmu .

we are sending copies of this report to the administrator , nnsa ; the director of the office of management and budget ; and appropriate congressional committees .

we also will make copies available to others upon request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you or your staff have any questions about this report or need additional information , please contact me at ( 202 ) 512-3841 or aloisee@gao.gov .

contact points for our offices of congressional relations or public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix ii .

in addition to the individual named above , james noel , assistant director ; jason holliday ; keith rhodes ; peter ruedel ; and carol herrnstadt shulman made key contributions to this report .

