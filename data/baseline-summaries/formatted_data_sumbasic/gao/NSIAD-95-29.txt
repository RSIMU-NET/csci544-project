this report addresses your concerns that declining defense budgets are increasing the potential for a return to the days of “hollow forces” that prevailed during the 1970s .

more specifically , you asked that we conduct a review to determine ( 1 ) whether the definition and indicators of readiness adequately reflect the many complex components that contribute to overall military readiness and ( 2 ) whether there are current readiness indicators that can predict positive or negative changes in readiness .

during the past several years , service chiefs and commanders in chief ( cinc ) have expressed concerns about the effect on current and future readiness of ( 1 ) the level of current military operations , ( 2 ) contingency operations , ( 3 ) the shifting of funds to support these operations , and ( 4 ) personnel turbulence .

related to these concerns is a question about the ability of the department of defense's ( dod ) readiness reporting system to provide a comprehensive assessment of overall readiness .

dod's current system for reporting readiness to the joint chiefs of staff ( jcs ) is the status of resources and training system ( sorts ) .

this system measures the extent to which individual service units possess the required resources and are trained to undertake their wartime missions .

sorts was established to provide the current status of specific elements considered essential to readiness assessments , that is , personnel and equipment on hand , equipment condition , and the training of operating forces .

sorts' elements of measure , “c” ratings that range from c - 1 ( best ) to c - 4 ( worst ) , are probably the most frequently cited indicator of readiness in the military .

according to jcs and dod officials , the definition and measures of readiness that are currently available in sorts are no longer adequate in today's national security environment .

specifically , sorts does not ( 1 ) address all the factors that jcs considers critical , ( 2 ) provide a warning of impending decreases in readiness , and ( 3 ) provide data on joint readiness .

in addition , sorts includes subjective assessments of training proficiency .

figure 1 shows those elements reported under sorts and all the elements that jcs believes would make up a more comprehensive assessment .

information reported under sorts is a snapshot in time and does not predict impending changes .

units report readiness monthly or , for some units , upon a change of status .

these reports provide commanders and jcs with status information only for that point in time .

commanders have stated that in today's environment of force reductions and increasing commitments , there is a need for indicators that can predict readiness changes .

some elements of sorts are not based on objective data .

the c - rating for training , for example , is based on a commander's subjective assessment of the number of additional training days the unit needs to reach a c - 1 status .

this assessment may be based on any number of factors , including completion of required or scheduled training or personal observation .

in the past , we have found that army training assessments have not been reliable .

for example , in 1991 we reported that training readiness assessments of active army units may have been overstated .

we reported that the information provided to higher commands and jcs was of limited value because the assessments ( 1 ) were based on training conducted primarily at home stations rather than on results of more realistic exercises conducted at combat training centers and ( 2 ) may not have adequately considered the effect that the loss of key personnel had on proficiency .

likewise , in our reviews pertaining to the persian gulf war , we noted that readiness reports for army support forces and national guard combat forces were often inflated or unreliable .

for example , in a september 1991 report , we noted that when three army national guard combat brigades were mobilized for operation desert shield , their commanders were reporting readiness at the c - 2 and c - 3 levels , which meant that no more than 40 days of post - mobilization training would be needed for the brigades to be fully combat ready .

however , on the basis of their independent assessment of the brigades' proficiency , active army officials responsible for the brigades' post - mobilization training developed training plans calling for over three times the number of days that the readiness reports stated were needed .

finally , sorts does not provide data with which commanders can adequately assess joint readiness .

there is no clear definition of areas of joint readiness that incorporates all essential elements , such as individual service unit readiness , the deployability of forces , or en route and theater infrastructure support .

the need for joint readiness information was demonstrated by the persian gulf war and reaffirmed by contingency operations in somalia and bosnia .

officials at four joint commands told us that sorts , the primary source of readiness data , was inadequate for assessing joint readiness .

although the joint staff recently developed its first list of joint mission tasks , it has not developed the training conditions for conducting joint exercises and criteria for evaluating them .

it may be several years before jcs completes these efforts .

recognizing the limitations of sorts and the need for more reliable readiness information , dod and the services have initiated actions to improve readiness assessments .

in june 1994 the defense science board readiness task force , which is composed of retired general officers , issued its report to the secretary of defense on how to maintain readiness .

the task force identified major shortcomings in assessing joint readiness and noted that while the services have increased their commitment to joint and combined training since operation desert storm , such training requires greater emphasis .

the task force recommended improvements in the measurement of joint readiness , stating that “real readiness must be measured by a unit's ability to operate as part of a joint or combined task force.” more recently , dod created the senior readiness oversight council to evaluate and implement the recommendations of the readiness task force and to develop new ways to measure combat readiness .

the council , whose membership includes high - level military and civilian officials , is focusing on three main ways to improve readiness: ( 1 ) developing better analytical tools for determining the relationship of resources to readiness and predicting the potential impact of budget cuts on readiness , ( 2 ) developing analytical tools for measuring joint readiness , and ( 3 ) taking advantage of computer simulation to improve readiness , especially joint readiness .

the army implemented its readiness management system in june 1993 .

this system allows the army to project for 2 years the status of elements reported under sorts .

the system integrates the reported sorts data with other databases that contain future resource acquisition and distribution information .

the army can , for example , compare a unit's reported equipment shortages with planned acquisition and distribution schedules , and the system can then forecast when those shortages will be alleviated and the unit's readiness posture improved .

in september 1993 , the air force began to develop a computer model , called ultra , to forecast readiness .

ultra is intended to measure four major elements: ( 1 ) the ability to deploy the right forces in a timely manner to achieve national objectives ; ( 2 ) the ability to sustain operations ; ( 3 ) the personnel end strength , quality , and training of people ; and ( 4 ) the availability of facilities .

if successful , the system will allow the air force to estimate the effect that various levels of funding have on readiness .

the project is still under development , and the air force estimates it will be about 2 years before the system will provide credible , widely accepted forecasts .

to supplement data currently reported in sorts and facilitate readiness assessments at the unit level , the military commands in all four services independently monitor literally hundreds of additional indicators .

these indicators are generally not reported to higher command levels .

military commanders and outside defense experts agreed that many of the indicators are not only critical to a comprehensive readiness assessment at the unit level but also have some degree of predictive value regarding readiness changes within the services .

we compiled a list of over 650 indicators that 28 active and reserve service commands were monitoring in addition to sorts .

to further refine these indicators , we asked the commands to rate the indicators in three areas: ( 1 ) the importance of the indicator for assessing readiness , ( 2 ) the degree of value the indicator has as a predictor of readiness change , and ( 3 ) the quality of the information the indicator provides .

table 1 shows the readiness indicators that service officials told us were either critical or important to a more comprehensive assessment of readiness and that also have some predictive value .

the indicators that are shaded are those rated highest by at least one - half of the commands visited .

we asked the defense science board task force on readiness to examine the indicators presented in table 1 .

task force members agreed with the commands' ratings and said that the indicators are an excellent beginning for developing a more comprehensive readiness measurement system .

the task force suggested four additional indicators: ( 1 ) the use of simulators to improve individual and crew proficiency on weapon systems ; ( 2 ) the quality of recruits enlisted by the services ; ( 3 ) equipment readiness based on fully mission capable rates rather than on mission capable rates , which permit a weapon system to be reported as mission capable even though it cannot fully perform its mission ; and ( 4 ) the extent to which readiness - related information in dod is automated .

in commenting on a draft of this report dod pointed out that it is useful to know if a system having a multimission capability can perform parts of the mission , therefore , it believes that both fully mission capable and mission capable rates are useful indicators .

also , dod said that the extent to which readiness - related information is automated is not an indicator of readiness but that it might be helpful in obtaining an understanding of automation requirements .

we agree with dod's position on these two issues .

as table 1 shows , some indicators are supported more by commanders of one service than by the others .

for example , information on commitments and deployments ( training , item 15 ) and deployed equipment ( logistics , item 17 ) were assessed as critical by marine corps commanders because of the manner in which its forces and equipment are deployed .

they were not listed as critical by any of the commands from the other services .

by examining a group or series of indicators , one may gain a broader insight than is possible from a single indicator .

to illustrate , changes in the extent of borrowed manpower ( personnel , item 7 ) may be related to proficiency on weapon systems ( training , item 12 ) or crew turnover ( personnel , item 8 ) .

also , table 1 identifies indicators that because of restricted training time and opportunities are especially critical to the reserve components .

several of the indicators that commanders rated as critical to readiness assessments relate to major readiness concerns recently expressed by service chiefs and cincs .

for example , while in the midst of downsizing , u.s. military forces are being called upon for operational contingencies — delivering humanitarian aid in iraq , bosnia , rwanda , and somalia and enforcing “no - fly” zones in bosnia and iraq , to name just a few .

unusually high operating tempos required for these contingencies have exacerbated the turbulence inherent in a major downsizing of u.s. forces .

several senior service leaders have raised concerns about the impact of this situation on morale , retention , and the ability to maintain readiness for traditional warfighting missions .

among the indicators suggested by some of the command officials we interviewed were personnel tempo , a measure of the frequency and number of personnel deployed on assigned missions , and crew turnover , a measure of personnel turnover within weapon system crews .

similarly , the services report that they were required to shift funds from operations and maintenance appropriations to support contingency operations , and , according to officials of each of the services , some scheduled training exercises were canceled and others were postponed .

several commanders suggested readiness indicators related to operating tempo , funding levels , and individual / unit proficiency .

related to the feature of predictive capability is the ability to conduct trend analyses based on the most important indicators .

assuming that relevant data is available , the services can identify trends in the additional indicators over time .

however , no criteria are currently available to assess the meaning of a trend in terms of its impact on readiness .

during our visits to the military commands , we noted an unevenness in the availability of historical data , depending on the indicator being monitored .

also , the commands reported that there is unevenness in the quality of the data available for measurement .

while some indicators were rated high in importance , they were rated low in quality .

we recommend that the secretary of defense direct the under secretary of defense for personnel and readiness to develop a more comprehensive readiness measurement system to be used dod - wide .

we recommend that as part of this effort , the under secretary review the indicators we have identified as being critical to predicting readiness and select the specific indicators most relevant to a more comprehensive readiness assessment , develop criteria to evaluate the selected indicators and prescribe how often the indicators should be reported to supplement sorts data , and ensure that comparable data is maintained by all services to allow the development of trends in the selected indicators .

in written comments on a draft of our report , dod generally agreed with our findings and recommendation ( see app .

i ) .

the department said that it plans to address the issue of using readiness indicators not only to monitor force readiness but also to predict force readiness .

in response to our recommendation , dod said that it is developing a specification for a readiness prediction system and that it has already used the indicators presented in our report as input to that process .

dod did not agree with our assessment of the overall value of sorts information and the reliability of training ratings contained in sorts .

first , dod said that it did not agree that sorts information provided to higher commands and jcs is of limited value .

we agree that sorts provides valuable information on readiness .

nevertheless , the system does have several limitations .

the matters discussed in the report are not intended as criticisms of sorts but rather as examples of limitations that are inherent in the system .

for example , c - ratings represent a valuable snapshot of readiness in time but by design they do not address long - term readiness or signal impending changes in the status of resources .

second , dod said that it did not agree that sorts may not adequately consider the effect that the loss of key personnel has on proficiency .

dod may have misinterpreted our position on this issue .

although sorts recognizes the loss of key personnel , it does not always consider the impact of replacing key personnel with less experienced personnel .

lastly , dod cited a number of factors that it believes make it infeasible to base training readiness on the results of combat training center exercises .

this report does not propose that dod take this course of action .

reference to the fact that training readiness is based primarily on training conducted at home stations rather than on results of more realistic exercises conducted at combat training centers is intended only to illustrate how the reliability of sorts training information can be effected .

to assess the adequacy of the current definition and indicators of readiness , we examined military service and jcs regulations , reviewed the literature , and interviewed officials from 39 dod agencies , including active and reserve service commands , defense civilian agencies , unified commands , and the joint staff ( see app .

ii ) .

to identify indicators that are being monitored to supplement sorts data , we asked the 39 agencies to identify all the indicators they use to assess readiness and operational effectiveness .

after compiling and categorizing the indicators by type , that is , personnel , training , and logistics , we asked the commands to rate the indicators' significance , predictive value , and quality .

indicator significance was rated as either critical , important , or supplementary .

the commands' opinions of predictive value were provided on a five - point scale ranging from little or none to very great .

the quality of the indicator was rated on a three - point scale — low , medium , and high .

we asked the defense science board's task force on readiness to ( 1 ) review and comment on the indicators that the commands rated the highest in terms of their importance and predictive value and ( 2 ) identify additional indicators that , in their judgment , were also critical to a comprehensive readiness assessment .

we conducted our review from may 1993 to june 1994 in accordance with generally accepted government auditing standards .

as agreed with your office , unless you publicly announce this report's contents earlier , we plan no further distribution until 30 days from its issue date .

at that time , we will send copies to the chairmen of the senate and house committees on armed services and on appropriations ; the subcommittee on military readiness and defense infrastructure , senate armed services committee ; and the subcommittee on readiness , house armed services committee ; and to the secretaries of defense , the army , the navy , and the air force .

copies will also be made available to others on request .

please contact me at ( 202 ) 512-5140 if you or your staff have any questions concerning this report .

major contributors to this report are listed in appendix iii .

secretary of the army washington , d.c. 4th infantry division ( mechanized ) fort carson , colorado 18th airborne corps fort bragg , north carolina 24th infantry division fort stewart , georgia corps support command 18th airborne corps fort bragg , north carolina headquarters , forces command fort mcpherson , georgia headquarters , training and doctrine command fort monroe , virginia national guard bureau washington , d.c .

secretary of the navy washington , d.c .

secretary of the air force washington , d.c. 1st tactical fighter wing langley air force base , virginia 375th air wing scott air force base , illinois air combat command langley air force base , virginia air force reserve washington , d.c .

office of the inspector general washington , d.c .

office of the joint chiefs of staff washington , d.c .

ray s. carroll , jr. , evaluator - in - charge james e. lewis , evaluator ( data analyst ) james k. mahaffey , site senior robert c. mandigo , jr. , site senior jeffrey c. mcdowell , evaluator jeffrey l. overton , jr. , site senior susan j. schildkret , evaluator lester l. ward , site senior the first copy of each gao report and testimony is free .

additional copies are $2 each .

orders should be sent to the following address , accompanied by a check or money order made out to the superintendent of documents , when necessary .

orders for 100 or more copies to be mailed to a single address are discounted 25 percent .

u.s. general accounting office p.o .

box 6015 gaithersburg , md 20884-6015 room 1100 700 4th st. nw ( corner of 4th and g sts .

nw ) u.s. general accounting office washington , dc orders may also be placed by calling ( 202 ) 512-6000 or by using fax number ( 301 ) 258-4066 .

each day , gao issues a list of newly available reports and testimony .

to receive facsimile copies of the daily list or any list from the past 30 days , please call ( 301 ) 258-4097 using a touchtone phone .

a recorded menu will provide information on how to obtain these lists .

