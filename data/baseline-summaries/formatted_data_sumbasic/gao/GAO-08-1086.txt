securing the nation's borders from illegal entry of aliens and contraband continues to be a major challenge because much of the 6,000 miles of international borders with canada and mexico remains vulnerable to unlawful activities .

although the department of homeland security ( dhs ) apprehends hundreds of thousands of people entering the country illegally each year , many more unauthorized entrants go undetected .

as we have reported , previous attempts to acquire and deploy surveillance technologies along the nation's borders to assist in detecting and responding to illegal entries have not been successful .

specifically , the former immigration and naturalization service's integrated surveillance intelligence system , begun in the late 1990s , was difficult and expensive to maintain ; it provided limited command , control , and situational awareness capability ; and its component systems were not integrated .

in response , dhs established the america's shield initiative in 2004 , but this program was halted in 2005 because of the program's limited scope and the department's shift in strategy for achieving border security and interior enforcement goals .

in november 2005 , dhs launched the secure border initiative ( sbi ) , a multiyear , multibillion - dollar program to secure the nation's borders through enhanced use of surveillance technologies , increased staffing levels , improved infrastructure , and increased domestic enforcement of immigration laws .

one component of sbi , known as sbinet , is focused on the acquisition and deployment of surveillance and communications technologies .

this program is managed by the sbinet system program office within u.s. customs and border protection ( cbp ) .

because of the size and complexity of sbinet , and the problems experienced by its predecessors , you asked us to determine whether dhs ( 1 ) has defined the scope and timing of planned sbinet capabilities and how these capabilities will be developed and deployed , ( 2 ) is effectively defining and managing sbinet requirements , and ( 3 ) is effectively managing sbinet testing .

to accomplish our objectives , we reviewed key program documentation , including guidance , plans , and requirements and testing documentation .

in cases where such documentation was not available , we interviewed program officials about the development of capabilities and the management of requirements and testing .

we then compared this information to relevant federal system acquisition guidance .

we also analyzed a random probability sample of system requirements and observed operations of the initial sbinet project .

we conducted this performance audit from august 2007 to september 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are included in appendix i .

cbp's sbi program is to leverage technology , tactical infrastructure , and people to allow cbp agents to gain control of the nation's borders .

within sbi , sbinet is the program for acquiring , developing , integrating , and deploying an appropriate mix of ( 1 ) surveillance technologies , such as cameras , radars , and sensors , and ( 2 ) command , control , communications , and intelligence ( c3i ) technologies .

the initial focus of sbinet has been on addressing the requirements of cbp's office of border patrol , which is responsible for securing the borders between the established ports of entry .

the longer - term sbinet systems solution also is to address requirements of cbp's two other major components — the office of field operations , which controls vehicle and pedestrian traffic at the ports of entry , and the office of air and marine operations , which operates helicopters , fixed - wing aircraft , and marine vessels used in securing the borders .

figure 1 provides a high - level , operational concept of the long - term sbinet systems solution .

surveillance technologies are to include a variety of sensor systems that improve cbp's ability to detect , identify , classify , and track items of interest along the borders .

unattended ground sensors are to be used to detect heat and vibrations associated with foot traffic and metal associated with vehicles .

radars mounted on fixed and mobile towers are to detect movement , and cameras on fixed and mobile towers are to be used to identify , classify , and track items of interest detected by the ground sensors and the radars .

aerial assets are also to be used to provide video and infrared imaging to enhance tracking of targets .

the c3i technologies are to include software and hardware to produce a common operating picture ( cop ) — a uniform presentation of activities within specific areas along the border .

the sensors , radars , and cameras are to gather information along the border , and the system is to transmit this information to the cop terminals located in command centers and agent vehicles and assemble this information to provide cbp agents with border situational awareness .

more specifically , the cop technology is to allow agents to ( 1 ) view data from radars and sensors that detect and track movement in the border areas , ( 2 ) control cameras to help identify and classify illegal entries , ( 3 ) correlate entries with the positions of nearby agents , and ( 4 ) enhance tactical decision making regarding the appropriate response to apprehend an entry , if necessary .

initially , cop information is to be distributed to terminals in command centers .

we observed that these terminals look like a standard computer workstation with multiple screens .

from this workstation , an operator is to be able to view an area of interest in several different ways .

for example , the operator is to see different types of maps , satellite images , and camera footage on the multiple screens .

the operator is also to be able to move the cameras to track images on the screen .

according to program officials , eventually , when the radars detect potential items of interest , the system is to automatically move the cameras so the operator does not always need to initiate the search in the area .

we observed that cop data are also available on laptop computers , known as mobile data terminals , mounted in select agent vehicles in the field .

these terminals are to enable field agents to see information similar to that seen by command center operators .

eventually , the cop technology is to be capable of providing distributed surveillance and tactical decision - support information to other dhs agencies and stakeholders external to dhs , such as local law enforcement .

figure 2 shows examples of cop technology in a command station and an agent vehicle .

the first sbinet capabilities were deployed under a pilot or prototype effort known as “project 28.” project 28 is currently operating along 28 miles of the southwest border in the tucson sector of arizona .

project 28 was accepted by the government for deployment 8 months behind schedule ( in february 2008 ) ; this delay occurred because the contractor - delivered system did not perform as intended .

as we have previously reported , reasons for project 28 performance shortfalls and delays include the following: system requirements were not adequately defined , and users were not involved in developing the requirements .

system integration testing was not adequately performed .

contractor oversight was limited .

project scope and complexity were underestimated .

to manage sbinet , dhs established a program office within cbp .

the program office is led by a program manager and deputy program managers for program operations and mission operations .

the program manager is responsible for the execution of the program , including developing , producing , deploying , and sustaining the system to meet the users' needs .

among other things , this includes developing and analyzing requirements and system alternatives , managing system design and development , evaluating the system's operational effectiveness , and managing program risk .

a system life cycle management approach typically consists of a series of phases , milestone reviews , and related processes to guide the acquisition , development , deployment , and operation and maintenance of a system .

among other things , the phases , reviews , and processes cover such important life cycle activities as requirements development and management , design , software development , and testing .

based on available program documentation , augmented by program official briefings and statements , key aspects of the sbinet system life cycle management approach are described below .

in general , sbinet surveillance systems are to be acquired through the purchase of commercially available products , while the cop systems involve development of new , customized systems and software .

together , both categories are to form a deployable increment of the sbinet capabilities , which the program office refers to as a “block.” each block is to include a release or version of the cop .

sbinet documentation shows that the program office is acquiring the blocks incrementally using a “spiral” approach , under which an initial system capability is to be delivered based on a defined subset of the system's total requirements .

this approach is intended to allow cbp agents access to new technological tools sooner rather than later for both operational use and feedback on needed enhancements or changes .

subsequent spirals or iterations of system capability are to be delivered based on feedback and unmet requirements , as well as the availability of new technologies .

figure 3 illustrates conceptually how the different capabilities are to come together to form a block and how future blocks are to introduce more capabilities .

the approach used to design and develop sbinet system capabilities for each block includes such key activities as requirements development , system design , system acquisition and development , and testing .

the approach , as explained by program officials and depicted in part in various documents , also includes various reviews , or decision points , to help ensure that these activities are being done properly and that the system meets user needs and requirements .

these reviews are to be used in developing both the overall sbinet block 1 capability and the cop software .

table 1 provides a high - level description of the major reviews that are to be performed in designing and developing the system prior to deployment to the field and in the order that they occur .

before a set of capabilities ( i.e. , block ) is deployed to a specific area or sector of the border , activities such as site selection , surveys , and environmental impact assessments are conducted to determine the area's unique environmental requirements .

the border area that receives a given block , or set of system capabilities , is referred to as a “project.” each project is to have a given block configured to its unique environmental requirements , referred to as a project “laydown.” the deployment approach is to include such key activities as requirements development , system design , project laydown , integration , testing , and installation .

the deployment approach is also to entail various reviews , or decision points , to help ensure that these activities are being done properly and that the system meets user needs and requirements .

table 2 provides a high - level description of the major reviews that are to be part of project laydown in the order that they occur .

among the key processes provided for in the sbinet system life cycle management approach are processes for developing and managing requirements and for managing testing activities .

with respect to requirements development and management , sbinet requirements are to consist of a hierarchy of six types of requirements , with the high - level operational requirements at the top .

these high - level requirements are to be decomposed into lower - level , more detailed system , component , design , software , and project requirements .

having a decomposed hierarchy of requirements is a characteristic of complex information technology ( it ) projects .

the various types of sbinet requirements are described in table 3 .

figure 4 shows how each of these requirements relate to or are derived from the other requirements .

with respect to test management , sbinet testing consists of a sequence of tests that are intended to verify first that individual system parts meet specified requirements , and then verify that these combined parts perform as intended as an integrated and operational system .

such an incremental approach to testing is a characteristic of complex it system acquisition and development efforts .

through such an approach , the source of defects can be isolated more easily and sooner , before they are more difficult and expensive to address .

table 4 summarizes these tests .

important aspects of sbinet remain ambiguous and in a continued state of flux , making it unclear and uncertain what technology capabilities will be delivered , when and where they will be delivered , and how they will be delivered .

for example , the scope and timing of planned sbinet deployments and capabilities have continued to change since the program began and , even now , remain unclear .

further , the approach that is being used to define , develop , acquire , test , and deploy sbinet is similarly unclear and has continued to change .

according to sbinet officials , schedule changes are due largely to an immature system design , and the lack of a stable development approach is due to insufficient staff and turnover .

the absence of clarity and stability in these key aspects of sbinet introduces considerable program risks , hampers dhs's ability to measure program progress , and impairs the ability of the congress to oversee the program and hold dhs accountable for program results .

one key aspect of successfully managing large it programs , like sbinet , is establishing program commitments , including what capabilities are to be deployed and when and where they are to be deployed .

only when such commitments are clearly established can program progress be measured and can responsible parties be held accountable .

the scope and timing of planned sbinet deployments and capabilities that are to be delivered have not been clearly established , but rather have continued to change since the program began .

specifically , as of december 2006 , the sbinet system program office planned to deploy an “initial” set of capabilities along the entire southwest border by late 2008 and planned to deploy a “full” set of operational capabilities along the southern and northern borders ( a total of about 6,000 miles ) by late 2009 .

as of march 2007 , the program office had modified its plans , deciding instead to deploy the initial set of capabilities along the southwest border by the end of fiscal year 2007 ( almost a year earlier than originally planned ) and delayed the deployment of the final set of capabilities for the southern and northern borders until 2011 .

in march 2008 , the program office again modified its deployment plans , this time significantly reducing the area to which sbinet capabilities are to be deployed .

at this time , dhs planned to complete deployments to three out of nine sectors along the southwest border — specifically , to tucson sector by 2009 , yuma sector by 2010 , and el paso sector by 2011 .

according to program officials , other than the dates for the tucson , yuma , and el paso sectors , no other deployment dates have been established for the remainder of the southwest or northern borders .

 ( figure 5 shows the changes in the planned deployment areas. ) .

the figure also shows the two sites within the tucson sector , tucson 1 and ajo 1 , at which an initial block 1 capability is to be deployed .

together , these two deployments cover 53 miles of the 1,989-mile - long southern border .

according to the march 2008 sbi expenditure plan and agency documentation as of june 2008 , these two sites were to have been operational by the end of 2008 .

however , as of late july 2008 , program officials reported that the deployment schedule for these two sites has again been modified , and they will not be operational until “sometime” in 2009 .

according to program officials , the slippage in the deployment schedule is due to the need to complete environmental impact assessment documentation for these locations .

the slippages in the dates for the first two tucson deployments , according to a program official , will , in turn , delay subsequent tucson deployments , although revised dates for these subsequent deployments have not been set .

just as the scope and timing of planned deployments have not been clear and have changed over time , the specific capabilities that are to be deployed have been unclear .

for example , in april 2008 , program officials stated that they would not know which of the sbinet requirements would be met by block 1 until the critical design review , which at that time was scheduled for june 2008 .

at that time , program officials stated that the capabilities to be delivered would be driven by the functionality of the cop .

in june , the review was held , but according to available documentation , the government did not consider the design put forth by the contractor to be mature .

as a result , the system design was not accepted in june as planned .

among the design limitations found was a lack of evidence that the system requirements were used as the basis for the tucson 1 and ajo 1 design , lack of linkage between the performance of surveillance components and the system requirements , and incomplete definition of system interfaces .

as of late july 2008 , these issues were unresolved , and thus the design still had not been accepted .

in addition , in late july 2008 , agency officials stated that the capabilities to be delivered will be driven by the functionality of the surveillance components , not the cop .

in addition , the design does not provide key capabilities that are in requirements documents and were anticipated to be part of the block 1 deployments to tucson 1 and ajo 1 .

for example , the first deployments of block 1 will not have the mobile data terminals in border patrol vehicles , even though ( 1 ) such terminals are part of project 28 capabilities and ( 2 ) workshops were held with the users in february 2008 and june 2008 to specifically define the requirements for these terminals for inclusion in block 1 .

according to program officials , these terminals will not be part of block 1 because the wireless communications infrastructure needed to support these terminals will not be available in time for the tucson 1 deployment .

rather , they expect the wireless infrastructure to be ready “sometime” in 2009 and said that they will include the mobile data terminals in block 1 deployments when the infrastructure is ready .

without the mobile data terminals , agents will not be able to obtain key information from their vehicles , such as maps of activity in a specific area , incident reports , and the location of other agents in the area .

instead , the agents will have to use radios to communicate with the sector headquarters to obtain this information .

in addition , program officials told us that a number of other requirements cannot be included in the block 1 version of the cop , referred to as version 0.5 , due to cost and schedule issues .

however , we have yet to receive a list of these requirements .

according to program officials , they hope to upgrade the cop in 2009 to include these requirements .

without clearly establishing program commitments , such as capabilities to be deployed and when and where they are to be deployed , program progress cannot be measured and responsible parties cannot be held accountable .

another key aspect of successfully managing large programs like sbinet is having a schedule that defines the sequence and timing of key activities and events and is realistic , achievable , and minimizes program risks .

however , the program office does not yet have an approved integrated master schedule to guide the execution of sbinet , and according to program officials , such a schedule has not been in place since late 2007 .

in the absence of an approved integrated master schedule , program officials stated in mid - august 2008 that they have managed the program largely using task - order - specific baselined schedules , and have been working to create a more integrated approach .

a program official also stated that they have recently developed an integrated master schedule but that this schedule is already out of date and undergoing revision .

for example , the deployment of the sbinet system to the tucson sector will not be completed in 2009 as planned .

to understand where the program is relative to established commitments , we analyzed schedule - related information obtained from other available program documents , briefings , and interviews .

in short , our analysis shows a schedule in which key activities and events are subject to constant change , as depicted in the following two figures .

figure 6 shows the changes to the schedule of planned and held reviews and anticipated deployment dates , and figure 7 shows the changes to the schedule of testing activities .

in may 2008 , program officials stated that the schedule changes were due largely to the fact that the contractor had not yet provided a satisfactory system - level design .

they also noted that the contractor's workforce has experienced considerable turnover , including three different program managers and three different lead system engineers .

they also stated that the system program office has experienced attrition , including turnover in the sbinet program manager position .

without stability and certainty in the program's schedule , program cost and schedule risks increase , and meaningful measurement and oversight of program status and progress cannot occur , in turn , limiting accountability for results .

system quality and performance are in large part governed by the processes followed in developing and acquiring the system .

to the extent that a system's life cycle management approach and related development and acquisition processes are well - defined , the chances of delivering promised system capabilities and benefits on time and within budget are increased .

to be well - defined , the approach and processes should be fully documented , so that they can be understood and properly implemented by those responsible for doing so .

the life cycle management approach and processes being used by the sbinet system program office to manage the definition , design , development , testing , and deployment of system capabilities has not been fully and clearly documented .

rather , what is defined in various program documents is limited and not fully consistent across these documents .

moreover , in discussions with agency officials to clarify our understanding of these processes , new terms and processes have been routinely introduced , indicating that the processes are continuing to evolve .

agency officials acknowledge that they are still learning about and improving their processes .

without a clearly documented and universally understood life cycle management approach and supporting processes , the program is at increased risk of not meeting expectations .

key program documentation that is to be used to guide acquisition and development activities , including testing and deployment activities , is incomplete , even though sbinet acquisition and development are already under way .

for example , officials have stated that they are using the draft systems engineering plan , dated february 2008 , to guide the design , development , and deployment of system capabilities , and the draft test and evaluation master plan , dated may 2008 , to guide the testing process — but both of these documents are lacking sufficient information to clearly guide system activities , as the following examples explain: the systems engineering plan includes a diagram of the engineering process ; however , the steps of the process and the gate reviews are not defined or described in the text of the document .

for example , this document does not contain sufficient information to understand what occurs at key reviews , such as the preliminary design review , the critical design review , and the test readiness review .

the test and evaluation master plan describes in more detail some of the reviews that are not described in the systems engineering plan , but the reviews included are not consistent between the documents .

for example , the test and evaluation master plan includes a system development and demonstration review that is not listed in the systems engineering plan .

in addition , it is not clear from the test and evaluation master plan how the reviews fit into the overall engineering process .

statements by program officials responsible for system development and testing activities , as well as briefing materials and diagrams that these officials provided , did not add sufficient clarity to describe a well - defined life cycle management approach .

moreover , these descriptions were not always consistent with what was contained in the documentation , as the following examples demonstrate: component testing is not described in the test and evaluation master plan in a manner consistent with how officials described this testing .

specifically , while the plan states that components will be tested against the corresponding component requirements to ensure all component performance can be verified , program officials stated that not all components will undergo component testing .

instead , they said that testing is not required if component vendors submit a certificate of compliance for certain specifications .

functional qualification testing was described to us by program officials in july 2008 as a type of testing to be performed during software development activities .

however , this type of testing is not defined in available program documentation , and it was not included in any versions of the documentation associated with the life cycle management approach and related engineering processes .

certain reviews specified in documentation of the life cycle management process are not sufficiently defined .

for example , the systems engineering plan shows a production readiness review as part of the system - level process and an operational readiness review as part of the project - level process .

however , program officials stated that these reviews are not completely relevant to sbinet because they are targeted for informational systems rather than tactical support systems , such as sbinet .

according to the officials , they are in the process of determining how to apply the reviews to sbinet .

for example , in july 2008 , officials reported that they may move the production readiness review from the system - level set of activities , as shown in the systems engineering plan , to the project - level set of activities .

program officials also stated that they are working to better define these reviews in the systems engineering plan .

program officials told us that the sbinet life cycle management approach and related engineering processes are understood by both government and contractor staff through the combination of the draft systems engineering plan and government - contractor interactions during design meetings .

nevertheless , they acknowledged that the approach and processes are not well documented , citing a lack of sufficient staff to both document the processes and oversee the system's design , development , testing , and deployment .

they also told us that they are adding new people to the project with different acquisition backgrounds , and that they are still learning about , evolving , and improving the approach and processes .

according to these officials , a revised and updated systems engineering plan should be finalized by september 2008 .

the lack of definition and stability in the approach and related processes being used to define , design , develop , acquire , test , and deploy sbinet introduce considerable risk that both the program officials and contractor staff will not understand what needs to be done when , and thus that the program will not consistently employ disciplined and rigorous methods .

without the use of such methods , the risk of delivering a system that does not meet operational needs and does not perform as intend is increased .

moreover , without a well - defined approach and processes , it is difficult to gauge progress and thus promote performance and accountability for results .

well - defined and managed requirements are a cornerstone of effective system development and acquisition .

according to recognized guidance , documenting and implementing a disciplined process for developing and managing requirements can help reduce the risks of developing a system that does not meet user needs , cannot be adequately tested , and does not perform or function as intended .

such a process includes , among other things , eliciting user needs and involving users in the development process ; ensuring that requirements are complete , feasible , verifiable , and approved by all stakeholders ; documenting and approving the requirements to establish a baseline for subsequent development and change control ; and ensuring that requirements are traceable both back to operational requirements and forward to detailed system requirements and test cases .

to the program office's credit , it recently developed guidance for developing and managing requirements that is consistent with recognized leading practices .

for example , the program's guidance states that a requirements baseline should be established and that requirements are to be traceable both back to higher - level requirements and forward to verification methods .

however , this guidance was not finalized until february 2008 and thus was not used in performing a number of key requirements - related activities .

in the absence of well - defined guidance , the program's efforts to implement leading practices for developing and managing requirements have been mixed .

for example , while the program has elicited user needs as part of its efforts to develop high - level operational requirements , it has not baselined all requirements .

further , it has not ensured that the operational requirements were , for example , verifiable , and it has not made certain that all of the different levels of requirements are aligned to one another .

as a result , the risk of sbinet not meeting mission needs and performing as intended is increased , as are the chances of expensive and time - consuming system rework .

the sbinet program office has developed guidance for developing and managing requirements that is generally consistent with recognized leading practices .

according to these practices , effectively developing and managing requirements includes , among other things , eliciting users' needs early in the development process and involving them throughout the process ; ensuring that requirements are complete , feasible , verifiable , and approved by all stakeholders ; documenting and approving the requirements to establish a baseline for subsequent development and change control ; and ensuring that requirements are traceable both back to operational requirements and forward to detailed system requirements and test cases .

in february 2008 , the program office approved its sbinet requirements development and management plan .

according to the plan , its purpose is to describe a comprehensive approach to developing and managing requirements for the sbinet program .

our analysis of this plan shows that it is consistent with leading practices .

for example , the plan states that users should provide input to the requirements definition process through early and ongoing participation in integrated product teams , user conferences , and other requirements - gathering and verifying activities ; requirements developers are to ensure that requirements are complete , unambiguous , achievable , verifiable , and not redundant ; a requirements baseline should be created to provide a common understanding of the system to be built and to prevent deviations to the requirements from entering during design , development , or testing ; and bidirectional traceability both back to higher - level requirements and forward to detailed test methods should be established and maintained and that the requirements management team is responsible for maintaining the requirements management database and holding the contractor responsible for any traceability issues .

moreover , the plan defines procedures and steps for accomplishing each of these goals .

for example , the procedure for requirements development outlines the purpose of the procedure , who is involved , what documentation is necessary to begin the procedure , and what outputs are expected at the end .

the procedure then describes 16 steps necessary to achieve the requirements development activities .

a separate procedure is provided for requirements verification and validation .

however , the plan was not approved until after key sbinet requirements documents were written and baselined .

specifically , user operational requirements were approved and baselined in march 2007 ; system requirements were first baselined in march 2007 ; and several component requirements were baselined in june , august , and october 2007 .

as a result , the plan was not used to guide these requirements development and management efforts .

as noted above , one of the leading practices associated with effective requirements development and management is engaging system users early and continuously .

in doing so , the chances of defining , designing , and delivering a system that meets their needs and performs as intended are increased .

in developing the operational requirements , the system program office involved sbinet users in a manner that is consistent with leading practices and that reflects lessons learned from project 28 .

specifically , it conducted requirements - gathering workshops from october 2006 through april 2007 to ascertain the needs of border patrol agents .

in addition , it established work groups in september 2007 to inform the next revision of the operational requirements by soliciting input from both the office of air and marine operations and the office of field operations .

further , to develop the cop technology for sbinet , the program office is following a software development methodology that allows end users to be directly involved in software development activities and , thereby , permits software solutions to be tailored to users' needs .

through such efforts to identify and elicit user needs in developing high - level requirements , the chances of developing a system that will meet user needs are increased .

the creation of a requirements baseline is important for providing a stable basis for system design , development , and testing .

such a baseline establishes a set of requirements that have been formally reviewed and agreed on and thus serve as the basis for further development or delivery .

until requirements are baselined , they remain unclear and subject to considerable and uncontrolled change , which in turn makes system design , development , testing , and deployment efforts equally uncertain .

according to sbinet program officials , the sbinet requirements development and management plan , and leading practices , requirements should be baselined before key system design activities begin , since the requirements are intended to inform , guide , and constrain the system's design .

for sbinet , while many of the requirements have been baselined , two types have not yet been baselined .

according to the system program office , the operational requirements and the system requirements were approved and baselined in march 2007 .

in addition , various system component requirements were baselined in june , august , and october of 2007 .

however , the program had not baselined its cop software requirements as of july 2008 , although according to program officials , the cop has been designed and is under development .

further , it has yet to baseline its project - level requirements , which define the requirements for the system configuration to be deployed to a specific geographical area , such as tucson 1 .

with respect to the cop , requirements for the software and hardware had not been baselined as of the end of july 2008 , despite the fact that a combined preliminary design review and critical design review for the cop was held in february 2008 and a critical design review for the system as whole was held in june 2008 .

according to agency officials and the sbinet requirements development and management plan , requirements should be baselined before the critical design review .

regardless , program officials state that the contractor has developed several “builds” ( i.e. , versions ) of the cop , which are currently being tested .

according to program officials , the requirements were not complete because certain interface requirements had not yet been completely identified and defined .

without baselined requirements , the basis of the system design and the degree to which it satisfies requirements are unclear .

moreover , the risk of the design not aligning to requirements is increased .

according to the results of the critical design review , this risk was realized .

specifically , the system program office notified the contractor that there was no evidence linking the performance of surveillance components to the system requirements , that the review could not be completed until the interface requirements had been finalized , and that a mature design had not been presented at the review .

with respect to project - level ( i.e. , geographic area ) deployment requirements , baselined requirements do not yet exist .

specifically , requirements for the tucson sector , which includes tucson 1 and ajo 1 , have yet to be baselined .

according to the sbinet requirements development and management plan , requirements should be baselined before the project requirements review , and a new requirements baseline should be created following the subsequent deployment design review .

however , project - level requirements were not baselined at a project requirements review held for the tucson sector project in march 2007 or at a deployment design review in june 2007 .

officials stated that this is because the plan was not approved until february 2008 and thus was not in effect .

however , since the plan became effective , deployment design reviews were held for tucson 1 and ajo 1 in april 2008 and may 2008 , respectively , but the project - level requirements were not baselined .

despite the absence of baselined requirements , the system program office has proceeded with development , integration , and testing activities for the block 1 capabilities to be delivered to tucson 1 and ajo l. as a result , it faces an increased risk of deploying systems that do not align well with requirements and thus may require subsequent rework .

the lack of project requirements has already had an effect on testing activities .

specifically , the draft system integration test plan notes that , without project requirements , testing will have to be guided by a combination of other documents , including engineering development requirements , related component requirements , and architectural design documents .

as stated above , one of the leading practices for developing and managing requirements — which is reflected in the program office's own plan — is that requirements should be sufficiently analyzed to ensure that the requirements are , among other things , complete , unambiguous , and verifiable .

however , an independent review of sbinet operational requirements reported numerous problems .

specifically , a review of the sbinet program commissioned by the office of the deputy secretary of homeland security found that several requirements were unaffordable and unverifiable .

examples of these requirements include the following: allow for complete coverage of the specified area or zone to be surveilled .

maximize intended deterrence and minimize countermeasure effectiveness .

function with high reliability under reasonably foreseeable circumstances .

reliably provide the appropriate power and bandwidth at the least cost that will support the demand .

in april 2008 , a program official stated that the operational requirements document is currently being rewritten to address concerns raised by this review .

for example , we were told that certain system performance requirements are being revised in response to a finding that the requirements are insufficient to ensure delivery of a properly functioning system .

program officials stated that they expect to finalize the revised operational requirements document in october 2008 .

however , given the number and types of problems associated with the operational requirements — which are the program's most basic customer requirements and form the basis for all lower - level requirements — it is unclear how the system , component , and software requirements can be viewed as verifiable , testable , or affordable .

until these problems are addressed , the risk of building and deploying a system that does not meet mission needs and customer expectations is increased , which in turn increases the chances of expensive and time - consuming system rework .

as noted above , one of the leading practices associated with developing and managing requirements is maintaining bidirectional traceability from high - level operational requirements through detailed low - level requirements to test cases .

the sbinet requirements development and management plan recognizes the importance of traceability , stating that a traceability relationship should exist among the various levels of requirements .

for example , it states that operational requirements should trace to the system requirements , which in turn should trace to component requirements .

further , it states that component requirements should trace to design requirements and to a verification method .

in addition , the sbinet system program office established detailed guidance for populating and maintaining the requirements database for maintaining linkages among the various levels of requirements and test verification methods .

to provide for requirements traceability , the prime contractor established such a requirements management database .

however , the reliability of the requirements in this database is questionable , and the sbinet system program office has not effectively overseen the contractor's management of requirements through this database .

specifically , we attempted to trace requirements in the version of this database that the program office received in march 2008 and were unable to trace large percentages of component requirements to either higher - level or lower - level requirements .

for example , an estimated 76 percent ( with a 95 percent degree of confidence of being between 64 and 86 percent ) of the component requirements that we randomly sampled could not be traced to the system requirements and then to the operational requirements .

in addition , an estimated 20 percent ( with a 95 percent degree of confidence of being between 11 and 33 percent ) of the component requirements in our sample failed to trace to a verification method .

see table 5 for the failure rates for each of our tracing analyses , along with the related confidence intervals .

while program officials could not explain the reason for this lack of traceability in most cases , they did attribute the 100-percent failure in tracing component requirements to the design requirements to the absence of any design requirements in the program office's copy of the database .

a contributing factor to the program office's inability to explain why requirements were not traceable is its limited oversight of the contractor's efforts to manage requirements through this database .

according to program officials , the contractor created the sbinet requirements management database in december 2006 , but the program office did not receive a copy of the database until march 2008 , despite requests for it beginning in fall 2007 .

in early may 2008 , the chief engineer told us the contractor had been reluctant to provide the database because it viewed the database's maturity level as low .

moreover , the program office's direct access to the database had not been established because of security issues , according to this official .

following our efforts to trace requirements , the program office obtained direct access to the contractor's database and initiated efforts with the contractor to resolve the traceability gaps .

however , program officials told us that they are still not certain that the database currently contains all of the system requirements or even the reduced block 1 system requirements .

as a result , they did not rely on it during the recent critical design review to verify requirements traceability .

instead , they said that manual tracking methods were used .

without ensuring that requirements are fully traceable , the program office does not have a sufficient basis for knowing that the scope of the contractor's design , development , and testing efforts will produce a system solution that meets operational needs and performs as intended .

as a result , the risk of expensive and time - consuming system rework is increased .

to be effectively managed , testing should be planned and conducted in a structured and disciplined fashion .

this includes , among other things , having an overarching test plan or strategy as a basis for managing system testing , developing well - defined and approved plans for executing testing activities , and testing individual system components to ensure that they satisfy defined requirements prior to integrating them into the overall system .

the sbinet system program office is not effectively managing its testing activities .

specifically , it has not tested individual system components prior to integrating these components with other components and the cop software .

in addition , although the program's draft test and evaluation master plan is currently being used as the program's overall strategy to manage sbinet testing , the plan is incomplete and unclear with respect to several test management functions .

as a result , the chances of sbinet testing being effectively performed are reduced , which in turn increases the risk that the delivered and deployed system will not meet operational needs and not perform as intended .

to be effectively managed , relevant federal guidance states that testing should , among other things , be governed by a well - defined and approved plan , and it should be executed in accordance with this plan .

further , integration testing should be preceded by tests of system components ( whether acquired or developed ) that are to be integrated to form the overall system .

once the components are tested to ensure that they satisfy defined requirements , the integrated system can be tested to verify that it performs as required .

for sbinet , this has not occurred .

as a result , the risk of the system not meeting operational needs and not performing as intended is increased , which in turn is likely to introduce the need for expensive and time - consuming system rework .

the sbinet systems program office reports that it began block 1 system integration testing in june 2008 .

however , it still does not have an approved system integration test plan .

specifically , the system's critical design review , which was held in june 2008 , found numerous problems with the contractor's plan for system integration testing , and as a result , the program office did not accept the plan .

examples of problems were that the test plan refers to other test plans that the contractor had yet to deliver and the plan identified system components that were not expected to be part of block 1 .

as a result , the program office decided that the system integration plan needed to be revised to document and describe the full set of actual testing activities that were to occur , including identifying where and to what level the different phases of integration tests would occur .

notwithstanding these problems and the absence of an approved plan , the contractor began integration testing in june 2008 .

according to program officials , this was necessary to meet the tight time frames in the schedule .

however , without an accepted system integration test plan in place , testing cannot be effectively managed .

for example , the adequacy of the test scope cannot be assured , and the progress in completing test activities cannot be measured .

as a result , there is an increased risk that the delivered system will not meet operational needs and will not perform as intended and that expensive and time - consuming system rework will be required .

moreover , the sbinet draft test and evaluation master plan describes system integration testing as first testing individual components to verify that the smallest defined module of a system works as intended ( i.e. , meets functional and performance requirements ) .

this allows defects with individual components to be identified and corrected before they are integrated with other system components .

once the components are tested , their respective hardware and software interfaces are to be tested before subsystems are tested in combination with the cop software .

such an incremental approach to testing permits system defects to be found and addressed before system components are integrated and component problems become more expensive and time - consuming to correct .

however , the sbinet system program office has not performed individual component testing as part of integration testing .

as of july 2008 , agency officials reported that component - level tests had not been completed and were not scheduled to occur .

instead , officials stated that block 1 components were evaluated based on what they described as “informal tests” ( i.e. , contractor observations of cameras and radar suites in operation at a national guard facility in the tucson sector ) and stated that the contractors' self - certification that the components meet functional and performance requirements was acceptable .

however , this approach is not consistent with the test and evaluation master plan .

moreover , program officials acknowledged that this approach did not verify if the individual components in fact met requirements .

nevertheless , they said that they have recently modified their definition of component testing to allow the contractor's certification to be used .

in our view , relying solely on contractor certification is not a sufficient substitute for component testing — as defined in the test and evaluation master plan — because it increases the risk that components , and thus the entire system , will not perform as intended .

this risk is already starting to be realized .

specifically , the results of the block 1 critical design review performed in early in june 2008 show that design documents did not link components' performance to the system requirements .

to ensure that system testing is effectively performed , federal guidance provides for having an overarching test plan or strategy to use as a basis for managing system testing .

among other things , this test management plan should define the schedule of high - level test activities in sufficient detail to allow for more detailed test planning and execution to occur and to ensure that test progress can be tracked and results can be reported and addressed .

the plan should also define the roles and responsibilities of the various groups responsible for different levels of testing and include a description of how the test organization manages and oversees these groups in their activities .

the sbinet test and evaluation master plan , which documents the program's test strategy and is being used to manage system testing , has yet to be approved by the sbinet acting program manager .

as of july 2008 , program officials told us that they did not expect the draft plan to be approved until august 2008 , even though testing activities began in june 2008 .

moreover , the draft test and evaluation master plan is not complete .

for example , it does not contain an accurate and up - to - date test schedule with milestones and completion dates for all levels of test activities .

rather , the schedule information included in the plan has been overtaken by events , to the point that program officials stated that many of the dates on the schedules have changed or are not accurate .

moreover , they described attempting to have an accurate schedule of testing activities and events as “futile” because the program's schedule is constantly changing .

as a another example , the draft test and evaluation master plan does not identify any metrics for measuring testing progress for any type of testing to be performed .

according to federal guidance , an accurate schedule is necessary to inform planning for and sequencing of each type of testing to be performed , including ensuring that test resources are available when needed and that predecessor test events occur before successor events begin .

this guidance also states that without performance metrics , it is difficult to understand where test activities stand and what they show in a manner that can inform program decision making .

as another example , the draft test and evaluation master plan does not clearly define the roles and responsibilities of various entities that are involved in system testing .

specifically , the plan identifies seven entities , but it only provides vague descriptions of their respective roles and responsibilities that are not meaningful enough to effectively guide their efforts .

for example , the plan identifies two entities that are to be involved in operational testing: the dhs science and technology test and evaluation office and the u.s. army test and evaluation command .

according to the plan , the dhs office is to function as the operational test authority and will be responsible for initial planning of “dedicated initial” and “follow - on” operational testing and evaluation , and the army group is to conduct operational testing .

with no further clarification , it is not clear what is expected of each of these entities , including how they are to interact .

table 6 lists each of the identified entities and provides their respective roles and responsibilities copied from the draft plan .

besides being vague , the descriptions of roles and responsibilities are also incomplete and not consistent with other program documents .

for example , according to the draft plan , the test and evaluation division of the sbinet system program office is responsible only for developing test and evaluation reports .

however , according to the draft sbinet systems engineering plan , this entity is to act as a subject matter expert for the oversight or conduct of various testing activities .

beyond this lack of clearly defined roles and responsibilities , there are other problems with the groups assigned testing roles .

first , some of the entities identified in the draft plan are not yet operational and thus are unavailable to participate and perform their assigned roles and responsibilities .

according to program officials , the independent verification and validation agent has not been selected , the integrated project teams have not been chartered , and dhs is still in the process of establishing the dhs science and technology test and evaluation office .

second , although cbp has an interagency agreement with the u.s. army test and evaluation command for operational testing , no such agreement exists with the sbinet program specifically for block 1 testing .

finally , neither the draft test and evaluation master plan nor the draft systems engineering plan clearly defines the program office's role and responsibilities for managing and overseeing each of the other six test entities and their respective test activities .

the sbinet system program office is responsible and accountable for ensuring that the system is successfully deployed and operates as intended .

given the criticality of testing in ensuring a successful program , this means that the program office must ensure that each of these entities executes its assigned roles effectively .

however , the draft test and evaluation master plan does not recognize this role and its associated responsibilities .

further , while the draft systems engineering plan states that the program office is responsible for engaging external test agents , it provides no further description of the program office's roles and responsibilities .

without clearly defined roles and responsibilities for all entities involved in sbinet testing , the risk of test activities not being effectively and efficiently performed increases .

as a result , the chances are increased that the deployed system will not meet operational requirements and perform as intended .

ultimately , this could lead to expensive and time - consuming system rework .

a fundamental aspect of successfully implementing a large program like sbinet is establishing program commitments , including what capabilities will be delivered and when and where they will be delivered .

only through establishing such commitments and by adequately defining the approach and processes to be used in delivering these commitments , can dhs effectively position itself for measuring progress , ensuring accountability for results , and delivering a system solution with its promised capabilities and benefits on time and within budget constraints .

for sbinet , this has not occurred to the extent that it needs to for the program to have a meaningful chance of succeeding .

in particular , commitments to the timing and scope of system capabilities remain unclear and continue to change , with the program committing to far fewer capabilities than originally envisioned .

further , how the sbinet system solution is to be delivered has been equally unclear and inadequately defined .

moreover , while the program office has defined key practices for developing and managing requirements , these practices were developed after several key requirements activities were performed .

in addition , efforts performed to date to test whether the system meets requirements and functions as intended have been limited .

collectively , these limitations are significant in that they increase the risk that the delivered system solution will not meet user needs and operational requirements and will not perform as intended .

these consequences , in turn , increase the chances that the system will require expensive and time - consuming rework .

in light of these circumstances and risks surrounding sbinet , it is important for the program office to reassess its approach to and plans for the program — including its associated exposure to cost , schedule , and performance risks — and to disclose these risks and alternative courses of action for addressing them to dhs and congressional decision makers .

it is also important for the program to correct the weaknesses discussed in this report surrounding the program's unclear and constantly changing commitments and its life cycle management approach and processes , including the processes and efforts performed to date relating to requirements development and management and testing .

while doing so will not guarantee a successful program , it will minimize the program's exposure to risk and thus decrease the likelihood that it will fall short of expectations .

for sbinet , living up to expectations is important because the program is a large , complex , and integral component of dhs's border security and immigration control strategy .

to improve dhs's efforts to acquire and implement sbinet we are making eight recommendations .

to permit meaningful measurement and oversight of and accountability for the program , we recommend that the secretary of homeland security direct the cbp commissioner to ensure that ( 1 ) the risks associated with planned sbinet acquisition , development , testing , and deployment activities are immediately assessed and ( 2 ) the results , including proposed alternative courses of action for mitigating the risks , are provided to the commissioner and dhs's senior leadership , as well as to the department's congressional authorization and appropriation committees .

we further recommend that the secretary of homeland security direct the cbp commissioner to have the acting sbinet program manager take the following additional actions: establish and baseline the specific program commitments , including the specific system functional and performance capabilities that are to be deployed to the tucson , yuma , and el paso sectors , and establish when these capabilities are to be deployed and are to be operational .

finalize and approve an integrated master schedule that reflects the timing and sequencing of the work needed to achieve these commitments .

revise and approve versions of the sbinet life cycle management approach , including the draft systems engineering plan and draft test and evaluation management plan , and in doing so , ensure that these revised and approved versions are consistent with one another , reflect program officials' recently described changes to the engineering and testing approaches , and reflect relevant federal guidance and associated leading practices .

ensure that the revised and approved life cycle management approach is fully implemented .

implement key requirements development and management practices to include ( 1 ) baselining requirements before system design and development efforts begin ; ( 2 ) analyzing requirements prior to being baselined to ensure that they are complete , achievable , and verifiable ; and ( 3 ) tracing requirements to higher - level requirements , lower - level requirements , and test cases .

implement key test management practices to include ( 1 ) developing and documenting test plans prior to the start of testing ; ( 2 ) conducting appropriate component level testing prior to integrating system components ; and ( 3 ) approving a test management strategy that , at a minimum , includes a relevant testing schedule , establishes accountability for testing activities by clearly defining testing roles and responsibilities , and includes sufficient detail to allow for testing and oversight activities to be clearly understood and communicated to test stakeholders .

in written comments on a draft of this report , signed by the director , departmental gao / office of inspector general liaison and reprinted in appendix ii , the department stated that it agrees with seven of our eight recommendations , and partially disagrees with one aspect of the remaining recommendation .

the department also stated that our report is factually sound and that it is working to address our recommendations and resolve the management and operational challenges identified in the report as expeditiously as possible .

in this regard , it described actions recently completed , underway , and planned that it said addresses our recommendations .

it also provided technical comments that we have incorporated in the report , as appropriate .

regarding our recommendation to implement key test management practices , including conducting appropriate component - level testing prior to integrating system components , dhs commented that its current test strategy provides for the appropriate degree of technical confidence for commercially available products , as evidenced by either certificates of conformance from the original equipment manufacturer , test documentation from independent government laboratories , or the prime contractor's component / integration level testing .

we support dhs's current test strategy , as it is consistent with our recommendation .

specifically , it expands on the department's prior strategy for component testing , which was limited to manufacturer self - certification of component conformance and informal observations of system components , by adding the use of independent government laboratories to test the components .

we would emphasize , however , that regardless of the method used , it is important that confidence be gained in components prior to integrating them , which our recommendation recognizes .

as our report states , component - level testing was not performed for block 1 components prior to initiating integration testing .

federal guidance and the sbinet program office's own test and evaluation master plan recognize the need to first test individual components to verify that the system modules work as intended ( i.e. , meet functional and performance requirements ) before conducting integration testing .

by adopting such a hierarchical approach to testing , the source of any system defects can be discovered and isolated sooner rather than later , thus helping to avoid the potential for expensive and time - consuming system rework .

we are sending copies of this report to the chairmen and ranking members of the senate and house appropriations committees and other senate and house committees and subcommittees that have authorization and oversight responsibilities for homeland security .

we will also send copies to the secretary of homeland security , the commissioner of u.s. customs and border protection , and the director of the office of management and budget .

in addition , this report will be available at no cost on the gao web site at http: / / www.gao.gov .

should your offices have any questions on matters discussed in this report , please contact me at ( 202 ) 512-3439 or at hiter@gao.gov .

contact points for our office of congressional relations and public affairs may be found on the last page of this report .

key contributors to this report are listed in appendix iii .

our objectives were to determine whether the department of homeland security ( dhs ) ( 1 ) has defined the scope and timing of planned sbinet capabilities and how these capabilities will be developed and deployed , ( 2 ) is effectively defining and managing sbinet requirements , and ( 3 ) is effectively managing sbinet testing .

to determine the extent to which dhs has defined the scope and timing of planned sbinet capabilities and how these capabilities will be developed and deployed , we reviewed program documentation , such as the draft systems engineering plan , the systems engineering management plan , the operational requirements document , the mission engineering process , the draft test and evaluation master plan , and the 2008 sbi expenditure plan to understand the sbinet engineering process and the scope and timing of planned deployments .

we also interviewed sbinet officials and contractors to gain clarity beyond what was included in the program documentation and to obtain schedule information in the absence of an integrated master schedule for the program .

to determine if dhs is effectively defining and managing sbinet requirements , we reviewed relevant documentation , such as the requirements development and management plan , the requirements management plan , the configuration and data management plan , the operational requirements document , system of systems a - level specification , b - 2 specifications , and vendor item control drawings , and compared them to industry best practices to determine the extent to which the program has effectively managed the systems requirements and maintained traceability backwards to high - level operational requirements and system requirements , and forward to system design and verification methods .

to assess reliability of the requirements data , we reviewed quality and access controls of the requirements database .

we then randomly selected 59 requirements from a sample of 1,666 component requirements and traced them backwards to the system requirements and then to the operational requirements and forward to design requirements and verification methods .

because we followed a probability procedure based on random selection , we are 95 percent confident that each of the confidence intervals in this report will include the true values in the study population .

we used statistical methods appropriate for audit compliance testing to estimate 95 percent confidence intervals for the traceability of requirements in our sample .

in addition , we interviewed program and contractor officials involved in requirements management to understand their roles and responsibilities .

we also visited a contractor development facility in huntsville , alabama , to understand the contractor's role in requirements management and development and the use of its requirements management tool , known as the dynamic object - oriented requirements system ( doors ) .

in addition , we attended a demonstration of sbinet rapid application development / joint application design to understand how the users are involved in developing requirements .

to determine if dhs is effectively managing sbinet testing , we reviewed relevant documentation , such as the sbinet test and evaluation master plan , the systems integration test plan , the quality assurance surveillance plan , the requirements verification plan , the characterization test plan , and the prime mission product design , and compared them to relevant federal guidance to determine the extent to which the program has effectively managed its testing activities .

we also interviewed sbinet officials to gain clarity beyond what was included in the program documentation and to obtain schedule information in the absence of a formal testing schedule .

in addition , we visited a contractor facility in huntsville , alabama , to better understand the contractor's role in testing activities and to observe the test lab and how testing is performed .

in addition , we visited the tucson sector border patrol headquarters in tucson , arizona , to see the technology that was deployed as a prototype to understand the scope of the technology , how the border patrol agents use the technology , and future plans .

to assess data reliability , we reviewed related program documentation to substantiate data provided in interviews with knowledgeable agency officials , where available .

for the information contained in the dhs independent study on sbinet , we interviewed the individuals responsible for conducting the review to understand their methodology , and determined that the information derived from this study was sufficiently reliable for the purposes of this report .

we have made appropriate attribution indicating the data's sources .

we performed our work at the u.s. customs and border protection headquarters and contractor facilities in the washington , d.c. , metropolitan area ; the tucson sector border patrol headquarters in tucson , arizona ; and a contractor facility in huntsville , alabama .

we conducted this performance audit from august 2007 to september 2008 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , deborah davis ( assistant director ) , carl barden , neil doherty , lee mccracken , jamelyn payan , karl seifert , sushmita srikanth , karen talley , and merry woo made key contributions to this report .

