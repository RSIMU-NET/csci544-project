i am pleased to be here today to discuss the highlights and recommendations of our selected reports that focused on key aspects of the federal government's acquisition and management of information technology ( it ) investments .

as reported to the office of management and budget ( omb ) , federal agencies plan to spend at least $82 billion on it in fiscal year 2014 .

given the scale of such planned outlays and the criticality of many of these systems to the health , economy , and security of the nation , it is important that omb and federal agencies provide appropriate oversight and transparency into these programs and avoid duplicative investments , whenever possible , to ensure the most efficient use of resources .

as we have previously reported , federal it projects too frequently incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

during the past several years , we have issued multiple reports and testimonies on federal initiatives to acquire and improve the management of it investments .

in those reports , we made numerous recommendations to federal agencies and omb to further enhance the management and oversight of it programs .

as part of its response to our prior work , omb deployed a public website in 2009 , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 700 major federal it investments .

in addition , omb has initiated other significant efforts following the creation of the dashboard .

for example , omb began leading reviews — known as techstat accountability sessions ( techstats ) — of selected it investments to increase accountability and improve performance ; launched an initiative to reduce the number of federal data centers ( the federal data center consolidation initiative ( fdcci ) ) ; and initiated portfoliostat , which requires agencies to conduct annual reviews of their it investments and make decisions on eliminating duplication .

you asked us to testify on the results and recommendations from our selected reports that focused on key aspects of the federal government's management of it investments .

accordingly , my testimony specifically discusses our past work on omb's it dashboard , techstat reviews , it acquisition best practices , fdcci , and portfoliostat , as well as failed and challenged it projects .

my testimony also discusses ongoing follow - up work on our prior recommendations from these reports .

all work on which this testimony is based was performed in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

information technology should enable government to better serve the american people .

however , according to omb , despite spending more than $600 billion on it over the past decade , the federal government has achieved little of the productivity improvements that private industry has realized from it.too often , federal it projects run over budget , behind schedule , or fail to deliver promised functionality .

in combating this problem , proper oversight is critical .

both omb and federal agencies have key roles and responsibilities for overseeing it investment management .

omb is responsible for working with agencies to ensure investments are appropriately planned and justified .

additionally , each year , omb and federal agencies work together to determine how much the government plans to spend on it projects and how these funds are to be allocated .

to assist agencies in managing their it investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cio ) to advise and assist agency heads in carrying out this responsibility .

many of these investments are critical to our nation .

for example , they include systems to secure our nation , control aircraft , and process tax returns .

however , the federal government has spent billions of dollars on failed and poorly performing it investments , as the following examples illustrate: in december 2012 , the department of defense ( dod ) canceled the air force's expeditionary combat support system after having spent more than a billion dollars and missing multiple milestones , including failure to achieve deployment within 5 years of obligating funds .

the system was to provide the air force with a single , integrated logistics system that was to control and account for about $36 billion of inventory .

we issued several reports on this system and found that , among other things , the program was not fully following best practices for developing reliable schedules and cost estimates .

among other things , we had recommended that dod ensure that any future system deficiencies identified through independent assessments are resolved or mitigated prior to further deployment of the system .

in january 2011 , the department of homeland security ended the secure border initiative network ( sbinet ) program after obligating more than $1 billion to the program because it did not meet cost - effectiveness and viability standards .

since 2007 , we had identified a range of issues and made several recommendations to improve this program .

for example , in may 2010 we reported that the final acceptance of the first two deployments had slipped from november 2009 to september 2010 and from march 2010 to november 2010 , and that the cost - effectiveness of the system had not been justified .

as a result , we recommended that the department ( 1 ) limit near - term investment in the first incremental block of the program , ( 2 ) economically justify any longer - term investment in it , and ( 3 ) improve key program management disciplines .

this work contributed to the department's decision to cancel the program .

in february 2010 , a task force led by the president's office of science and technology policy decided to disband the national polar - orbiting operational environmental satellite system ( npoess ) , a weather satellite program managed by three different agencies , after having spent 16 years and almost $5 billion on the program .

we issued a series of reports on the npoess program that highlighted the technical challenges , cost growth , and tri - agency management challenges facing the program .

for example , in june 2009 we reported that the program's approved cost and schedule baselines were not achievable , and that costs could grow by approximately $1 billion over the then - current $13.95 billion estimate .

we further noted that schedules for the launch of a demonstration satellite and the first two operational satellites were expected to be delayed , increasing the risk of a gap in satellite continuity .

however , after the program's cancellation , the agencies were directed to undertake separate acquisitions .

appendix i provides further details on federal it projects that have failed or faced significant challenges .

in addition to these projects , the it dashboard identifies other at - risk investments .

specifically , as of july 2013 , according to the it dashboard , 154 of the federal government's approximately 700 major it investments — totaling about $10.4 billion — were in need of management attention ( rated to indicate the need for attention or to indicate significant concerns ) .

 ( see fig .

1. ) .

in addition to poorly performing investments , federal it spending is hampered by inefficient operations and duplication , as the following examples illustrate: federal data centers .

in march 2011 , we reported that the increasing demand for it had led to a dramatic rise in the number of federal data centers , with many housing similar types of equipment and providing similar processing and storage capabilities .

as federal agencies have modernized their operations , put more of their services online , and increased their information security profiles , they have demanded more computing power and data storage resources .

according to omb , the number of federal data centers grew from 432 in 1998 to more than 2,000 in 2010 .

the growth in the number of federal data centers , many offering similar services and resources , has resulted in overlap and duplication among the centers .

in addition , according to omb , in august 2009 the average utilization rate for servers ranged from 5 percent to 15 percent .

in contrast , omb's 2012 data center consolidation plan guidance states that the target for server utilization is 60 to 70 percent .

duplicative it investments .

in september 2011 , we reported that limitations in omb's guidance hindered efforts to identify it duplication .

specifically , although omb's guidance to federal agencies on how to categorize it investments allowed for analysis of investments with similar functions , it did not go far enough to allow identification of potentially duplicative investments .

specifically , since the fiscal year 2004 budget cycle , omb had required agencies to categorize their it investments according to primary function and subfunction .

in their fiscal year 2011 submissions , agencies reported the greatest number of it investments in information and technology management ( 1,536 investments ) , followed by supply chain management ( 777 investments ) , and human resource management ( 622 investments ) .

similarly , planned expenditures on investments were greatest in information and technology management , at about $35.5 billion .

figure 2 depicts , by primary function , the total number of investments within the 26 federal agencies that report to the it dashboard , as of july 2011 .

however , we noted that categorizing it investments according to primary function and subfunction limited omb's ability to identify potentially duplicative investments both within and across agencies because similar investments may be organized under different functions .

accordingly , we recommended that omb revise guidance to federal agencies on categorizing it investments to ensure that the categorizations are clear and that it allows agencies to choose secondary categories , where applicable .

omb generally agreed with this recommendation and has since taken action to implement it .

specifically , omb updated its policy to enable agencies to select one primary category and up to four secondary categories for each it investment .

we also reported that results of omb initiatives to identify potentially duplicative investments were mixed and that several federal agencies did not routinely assess their entire it portfolios to identify and remove or consolidate duplicative systems .

specifically , we said that most of omb's recent initiatives had not yet demonstrated results , and several agencies did not routinely assess legacy systems to determine if they are duplicative .

as a result , we recommended that omb require federal agencies to report the steps they take to ensure that their it investments are not duplicative as part of their annual budget and it investment submissions .

omb generally agreed with this recommendation and has since taken action to implement it .

specifically , in march 2012 , omb issued a memorandum to federal agencies regarding implementing portfoliostat reviews .

as previously mentioned , these reviews are intended to assist in ending the investment in duplicative it investments .

in addition , as part of this effort , omb is requiring agencies to document their cost savings and cost avoidance due to consolidation beginning in their fiscal year 2014 budget submissions .

omb has implemented a series of initiatives to improve the oversight of underperforming investments , more effectively manage it , and address duplicative investments .

these efforts include the following: it dashboard .

given the importance of transparency , oversight , and management of the government's it investments , in june 2009 omb established a public website , referred to as the it dashboard , that provides detailed information on approximately 700 major it investments at 27 federal agencies , including ratings of their performance against cost and schedule targets .

the public dissemination of this information is intended to allow omb ; other oversight bodies , including congress ; and the general public to hold agencies accountable for results and performance .

techstat reviews .

in january 2010 , the federal cio began leading techstats sessions — face - to - face meetings to terminate or turnaround it investments that are failing or are not producing results .

these meetings involve omb and agency leadership and are intended to increase accountability and transparency and improve performance .

subsequently , omb empowered agency cios to hold their own techstat sessions within their respective agencies .

according to the former federal cio , the efforts of omb and federal agencies to improve management and oversight of it investments have resulted in almost $4 billion in savings .

fdcci .

concerned about the growing number of federal data centers , in february 2010 the federal cio established fdcci .

this initiative's four high - level goals are to promote the use of “green it” by reducing the overall energy and real estate footprint of government data centers ; reduce the cost of data center hardware , software , and operations ; increase the overall it security posture of the government ; and shift it investments to more efficient computing platforms and technologies .

omb believes that this initiative has the potential to provide about $3 billion in savings by the end of 2015 .

portfoliostat .

in order to eliminate duplication , move to shared services , and improve portfolio management processes , in march 2012 omb launched the portfoliostat initiative .

specifically , portfoliostat requires agencies to conduct an annual agency - wide it portfolio review to , among other things , reduce commodity it spending and demonstrate how their it investments align with the agency's mission and business functions .

portfoliostat is designed to assist agencies in assessing the current maturity of their it investment management process , making decisions on eliminating duplicative investments , and moving to shared solutions in order to maximize the return on it investments across the portfolio .

omb believes that the portfoliostat effort has the potential to save the government $2.5 billion over the next 3 years by , for example , consolidating duplicative systems .

over the past several years , we have highlighted omb and agency efforts to improve the transparency into and oversight of underperforming federal it investments , more effectively manage it , and address duplicative investments .

notably , we issued a series of reports on: the it dashboard ; omb and agency efforts to address troubled projects through techstat reviews ; critical factors underlying successful acquisitions ; and omb and agency efforts to improve the management of it through federal data center consolidation efforts , as well as address duplication through portfoliostat .

omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , and by improving the accuracy of investment ratings .

however , we found weaknesses with the accuracy and reliability of cost and schedule data , and we recommended steps that omb should take to improve these data .

our july 2010 report found that the cost and schedule ratings on omb's dashboard were not always accurate for the investments we reviewed , because these ratings did not take into consideration current performance .

as a result , the ratings were based on outdated information .

we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting .

omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and completed activities .

similarly , in march 2011 , omb had initiated several efforts to increase the dashboard's value as an oversight tool , and had used its data to improve federal it management .

however , agency practices and the dashboard's calculations contributed to inaccuracies in the reported investment performance data .

these included , for instance , missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data .

as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance .

most agencies generally concurred with our recommendations ; omb agreed with our recommendation for improving ratings for schedule variance .

it disagreed with our recommendation to improve how it reflects current performance in cost and schedule ratings , but more recently made changes to dashboard calculations to address this while also noting challenges in comprehensively evaluating cost and schedule data for these investments .

our subsequent report in 2011 noted that that the accuracy of investment cost and schedule ratings had improved since our july 2010 report because omb had refined the dashboard's cost and schedule calculations .

most of the ratings for the eight investments we reviewed were accurate , although more could be done to inform oversight and decision making by emphasizing recent performance in the ratings .

we recommended that the general services administration comply with omb's guidance for updating its ratings when new information becomes available ( including when investments are rebaselined ) and the agency concurred .

since we previously recommended that omb improve how it rates investments , we did not make any further recommendations .

more recently , in october 2012 we found that opportunities existed to improve transparency and oversight of investment risk at our selected agencies .

specifically , cios at six federal agencies consistently rated the majority of their it investments as low risk .

these agencies rated no more than 12 percent of their investments as high or moderately high risk , and two agencies ( dod and the national science foundation ) rated no investments at these risk levels .

over time , about 47 percent of the agencies' dashboard investments received the same rating in every rating period .

for ratings that changed , the department of homeland security and office of personnel management reported more investments with reduced risk when initial ratings were compared with those in march 2012 ; the other four agencies reported more investments with increased risk .

in the past , omb reported trends for risky it investments needing management attention as part of the president's annual budget submission , but discontinued this reporting in fiscal year 2010 .

accordingly , we recommended omb analyze agencies' investment risk over time as reflected in the dashboard's cio ratings and present its analysis with the president's annual budget submission , with which omb concurred .

further , agencies generally followed omb's instructions for assigning cio ratings , which included considering stakeholder input , updating ratings when new data become available , and applying omb's six evaluation factors .

dod's ratings were unique in reflecting additional considerations , such as the likelihood of omb review , and consequently dod did not rate any of its investments as high risk .

however , in selected cases , these ratings did not appropriately reflect significant cost , schedule , and performance issues reported by gao and others .

although three dod investments experienced significant performance problems and were part of a gao high - risk area ( business systems modernization ) , they were all rated low risk or moderately low risk by the dod cio .

for example , in early 2012 , we reported that air force's defense enterprise accounting and management system faced a 2-year deployment delay and an estimated cost increase of about $500 million from an original life - cycle cost estimate of $1.1 billion ( an increase of approximately 45 percent ) , and that assessments by dod users had identified operational problems with the system , such as data accuracy issues , an inability to generate auditable financial reports , and the need for manual workarounds .

in july 2012 , the dod inspector general reported that the system's schedule delays were likely to diminish the cost savings it was to provide , and would jeopardize the department's goals for attaining an auditable financial statement .

dod's cio rated the defense enterprise accounting and management system low risk or moderately low risk from july 2009 through march 2012 .

moreover , dod did not apply its own risk management guidance to the ratings , which reduces their value for investment management and oversight .

therefore , we recommended that dod ensure that its cio ratings reflect available investment performance assessments and its risk management guidance .

dod concurred with our recommendation .

regarding techstat reviews , we reported that omb and selected agencies had held multiple techstats , but additional omb oversight was needed to ensure that these meetings were having the appropriate impact on underperforming projects and that resulting cost savings were valid .

specifically , we reported that as of april 2013 , omb reported conducting 79 techstats , which focused on 55 investments at 23 federal agencies .

further , 4 selected agencies — the departments of agriculture , commerce , health and human services , and homeland security — conducted 37 techstats covering 28 investments .

about 70 percent of the omb - led and 76 percent of agency - led techstats on major investments were considered medium to high risk at the time of the techstat .

however , the number of at - risk techstats held to date was relatively small compared to the current number of medium - and high - risk it investments .

specifically , the omb - led techstats represented roughly 18.5 percent of the investments across the government that had a medium - or high - risk cio rating .

for the 4 selected agencies , the number of techstats represented about 33 percent of the investments that have a medium - or high - risk cio rating .

we concluded that until omb and agencies develop plans to address these weaknesses , the investments would likely remain at risk .

in addition , we reported that omb and selected agencies had tracked and reported positive results from techstats , with most resulting in improved governance .

agencies also reported projects with accelerated delivery , reduced scope , or termination .

we also found that omb reported in 2011 that federal agencies achieved almost $4 billion in life - cycle cost savings as a result of techstat sessions .

however , we were unable to validate omb's reported results because omb did not provide artifacts showing that it ensured the results were valid .

from our selected agencies , three investments had cost implications .

agencies provided supporting documentation for about $22.2 million in cost savings and avoidances .

we concluded that until omb obtains and shares information on the methods used to validate reported results , it would be difficult for the results to be independently validated and for omb to provide assurance to congress and the public that techstats were achieving their intended impact .

we therefore recommended that omb validate the resulting cost savings from techstats that it reports to congress and require agencies to conduct techstats for each it investment rated with a moderately high - or high - risk cio rating on the it dashboard .

we also made recommendations to the selected agencies to strengthen their techstat processes .

omb and the department of commerce officials generally agreed with our recommendations .

the department of agriculture partially agreed with our assessment ; neither it nor the department of health and human services commented on the recommendations .

subsequent to the launch of the dashboard and the techstat reviews , and to help the federal agencies address the well - documented acquisition challenges they face , we identified seven successful investment acquisitions and nine common factors critical to their success in 2011 .

specifically , we reported that department officials identified seven successful investment acquisitions , in that they best achieved their respective cost , schedule , scope , and performance goals .

in addition , common factors critical to the success of three or more of the seven investments were: ( 1 ) program officials were actively engaged with stakeholders ; ( 2 ) program staff had the necessary knowledge and skills ; ( 3 ) senior department and agency executives supported the programs ; ( 4 ) end users and stakeholders were involved in the development of requirements ; ( 5 ) end users participated in testing of system functionality prior to formal end user acceptance testing ; ( 6 ) government and contractor staff were stable and consistent ; ( 7 ) program staff prioritized requirements ; ( 8 ) program officials maintained regular communication with the prime contractor ; and ( 9 ) programs received sufficient funding .

further , officials from all seven investments cited active engagement with program stakeholders as a critical factor to the success of those investments .

these critical factors support omb's objective of improving the management of large - scale it acquisitions across the federal government , and wide dissemination of these factors could complement omb's efforts .

in an effort to consolidate the growing number of federal data centers , in 2010 , omb launched the fdcci .

as part of this initiative , agencies developed plans to consolidate data centers ; however , these plans were incomplete and did not include best practices .

in addition , although agencies had made progress on their data center closures , omb had not determined initiative - wide cost savings , and oversight of the initiative was not being performed in all key areas .

finally , as part of ongoing follow - up work , we determined that agencies closed additional data centers , but that the number of federal data centers was significantly higher than previously estimated by omb .

in july 2011 , we issued a report on the status of fdcci and found that only 1 of the 24 agencies had submitted a complete inventory and no agency had submitted complete plans .

further , omb had not required agencies to document the steps they had taken , if any , to verify the inventory data .

we concluded that until these inventories and plans were complete , agencies would not be able to implement their consolidation activities and realize expected cost savings .

moreover , without an understanding of the validity of agencies' consolidation data , omb could not be assured that agencies were providing a sound baseline for estimating consolidation savings and measuring progress against those goals .

accordingly , we made several recommendations to omb , including that the federal cio require that agencies , when updating their data center inventory , state what actions were taken to verify the information in the inventory and to identify any associated limitations on the data , and to complete the missing elements in their inventories and consolidation plans .

omb generally agreed with our report and has since taken actions to address our recommendations .

for example , in july 2011 , omb required agency cios to submit a letter that identified steps taken to verify their data center inventory information and attest to the completeness of their consolidation plan .

in addition , in march 2012 , omb required that all agencies , by the end of the fourth quarter of every fiscal year , complete all elements missing from their consolidation plans .

additionally , in july 2012 , we updated our review of fdcci's status and found that , while agencies' 2011 inventories and plans had improved as compared to their 2010 submissions , only 3 agencies had submitted a complete inventory and only 1 agency had submitted a complete consolidation plan .

in addition , we noted that 3 agencies had submitted their inventory using an outdated format , in part , because omb had not publicly posted its revised guidance .

notwithstanding these weaknesses , we found that 19 agencies reported anticipating about $2.4 billion in cost savings between 2011 and 2015 .

we also reported that none of five selected agencies had a master program schedule or cost - benefit analysis that was fully consistent with best practices .

to assist agencies with their data center consolidation efforts , omb had sponsored the development of a fdcci total cost of ownership model that was intended to help agencies refine their estimated costs for consolidation ; however , agencies were not required to use the cost model as part of their cost estimating efforts .

accordingly , we reiterated our prior recommendation that agencies complete missing plan and inventory elements and made new recommendations to omb to publically post guidance updates on the fdcci website and to require agencies to use its cost model .

omb generally agreed with our recommendations and has since taken steps to address them .

more specifically , omb posted its 2012 guidance for updating data center inventories and plans , as well as guidance for reporting consolidation progress , to the fdcci public website .

further , the website has been updated to provide prior guidance documents and omb memoranda .

in addition , omb's 2012 consolidation plan guidance required agencies to use the cost model as they developed their 2014 budget request .

more recently , we reported and testified that the 24 fdcci agencies made progress towards omb's goal to close 40 percent , or 1,253 of the 3,133 total federal data centers , by the end of 2015 , but omb had not measured agencies' progress against its other goal of $3 billion in cost savings by the end of 2015 .

agencies closed 420 data centers by the end of december 2012 and had plans to close an additional 548 to reach 968 by december 2015 — 285 closures short of omb's goal .

omb had not determined agencies' progress against its cost savings goal because , according to omb staff , the agency has not determined a consistent and repeatable method for tracking cost savings .

we reported that this lack of information makes it uncertain whether the $3 billion in savings is achievable by the end of 2015 .

we concluded that until omb tracks and reports on performance measures such as cost savings , it will be limited in its ability to oversee agencies' progress against key goals .

further , we reported that , pursuant to omb direction , three organizations — the data center consolidation task force , the general services administration program management office , and omb — are responsible for federal data center consolidation oversight activities .

we found that while most activities were being performed , there were still several weaknesses in oversight .

for example , while the general services administration's program management office had collected agencies' quarterly data center closure updates and made the information publically available on an electronic dashboard for tracking consolidation progress , it had not fully performed other oversight activities , such as conducting analyses of agencies' inventories and plans .

in addition , while omb had implemented several initiatives to track agencies' consolidation progress , such as establishing requirements for agencies to update their plans and inventories yearly and to report quarterly on their consolidation progress , the agency had not approved the plans on the basis of their completeness or reported on progress against its goal of $3 billion in cost savings .

the weaknesses in oversight of the data center consolidation initiative were due , in part , to omb not ensuring that assigned responsibilities are being executed .

we concluded that improved oversight could better position omb to assess progress against its cost savings goal and minimize agencies' risk of not realizing expected cost savings .

we therefore recommended that omb's federal cio track and report on key performance measures , extend the time frame for achieving planned cost savings , and improve the execution of important oversight responsibilities .

omb agreed with two of our recommendations and stated that it plans to evaluate the remaining recommendation related to extending the time frame .

finally , as part of ongoing follow - up work , we reported that agencies had closed an additional 64 data centers compared to the total number of reported closures through the end of december 2012 .

more specifically , as of may 2013 , agencies had reported closing 484 data centers by the end of april 2013 , and were planning to close an additional 571 data centers — for a total of 1,055 — by september 2014 .

however , we also found that the number of federal data centers had grown significantly since omb had reported in december 2011 that there were approximately 3,133 data centers .

specifically , as of july 2013 , 22 of the 24 fdcci agencies had collectively reported 6,836 data centers in their inventories , which is approximately 3,700 data centers more than omb's previous estimate from december 2011 .

to address duplicative it investments , omb launched portfoliostat in march 2012 , which is designed to assist agencies in assessing the current maturity of their it portfolio management process , making decisions on eliminating duplication , and moving to shared services in order to maximize the return on it investments across the portfolio .

we recently reported and testified on omb's portfoliostat initiative , including that , in march 2013 , omb issued a memorandum documenting additional guidance to help strengthen the initiative .

in its memorandum , omb noted that the results from portfoliostat so far had been significant — including that agencies had identified and committed to nearly 100 opportunities to consolidate or eliminate commodity it investments and also described plans to strengthen the initiative by integrating portfoliostat and fdcci , streamlining agency reporting requirements , and establishing guidance for conducting portfoliostat sessions in fiscal year 2013 .

for example , to improve the outcomes of portfoliostat and to advance agency it portfolio management , omb's memorandum consolidated previously collected it plans , reports , and data calls into three primary collection channels — an information resources management strategic plan , an enterprise road map , and an integrated data collection channel .

agencies' draft versions of their strategic plans and enterprise road maps were due to omb in may 2013 , as well as their first integrated data collections .

the integrated data collections are to be updated quarterly beginning in august 2013 and the strategic plans and road maps are to be updated after congress receives the president's budget for fiscal year 2015 .

however , our april 2013 report noted that key data - center - related performance metrics of the combined initiative were not yet fully defined .

for example , omb's march 2013 memorandum stated that , to more effectively measure the efficiency of an agency's data center assets , agencies would also be measured by the extent to which their data centers are optimized for total cost of ownership by incorporating metrics for data center energy , facility , labor , and storage , among other things .

although omb had indicated which performance measures it planned to use going forward , it had not documented the specific metrics for agencies to report against .

omb's march 2013 memorandum indicates that these would be developed by the data center consolidation task force , but did not provide a time frame for when this will be completed .

further , our report noted that omb's integration of fdcci with portfoliostat also included a modification to the previous data center consolidation goal of closing approximately 40 percent of the total number of agency data centers .

specifically , omb stated an agency's data center population will now be placed into one of two categories — core and non - core data centers — but for which the memorandum did not provide specific definitions .

omb further stated that its new goal is to close 40 percent of non - core data centers but , as noted , the definitions of core and non - core data centers were not provided .

therefore , the total number of data centers to be closed under omb's revised goal could not be determined .

we also reported that , although omb had previously stated that portfoliostat was expected to result in savings of approximately $2.5 billion through 2015 , its march 2013 memorandum did not establish a new cost savings goal that reflected the integration of fdcci .

instead , omb stated that all cost savings goals previously associated with fdcci would be integrated into broader agency efforts to reshape their it portfolios , but did not provide a revised savings estimate .

we concluded that the lack of a new cost savings goal would limit omb's ability to determine whether or not the new combined initiative is on course toward achieving its planned objectives .

as a result , we recommended that omb track and annually report on key data center consolidation performance measures , such as the size of data centers being closed and cost savings to date .

omb agreed with our recommendation .

we have ongoing work looking at omb's portfoliostat initiative , including determining whether agencies completed key required portfoliostat actions , evaluating selected agencies' plans for making portfolio improvements and achieving associated cost savings , and describing omb's plans to improve the portfoliostat process .

in summary , omb's and agencies' recent efforts have resulted in greater transparency and oversight of federal spending , but continued leadership and attention are necessary to build on the progress that has been made .

for example , federal agencies need to continue to improve the accuracy of information on the dashboard to provide greater transparency and even more attention to the billions of dollars invested in troubled projects .

further , additional techstat reviews are needed to focus management attention on additional troubled projects and establish clear action items to turn the projects around or terminate them .

in addition , the expanded use of the common factors critical to the successful management of large - scale it acquisitions should result in the more effective delivery of mission - critical systems .

the federal government can also build on the momentum of progress on agencies' data center closures as the federal data center consolidation effort is integrated with portfoliostat .

omb recently released additional guidance that expanded this important initiative's scope and reported that significant progress had been made to date , including more than 100 opportunities to consolidate or eliminate commodity it investments .

moving forward , it will be important for omb to be transparent on agencies' progress against key performance metrics , such as data center consolidation cost savings , in order to ensure that the portfoliostat initiative is meeting its established objectives .

overall , the implementation of gao recommendations can help further reduce wasteful spending on poorly managed , unnecessary , and duplicative investments .

chairman mica , ranking member connolly , and members of the subcommittee , this completes my prepared statement .

i would be pleased to respond to any questions that you may have at this time .

if you or your staffs have any questions about this testimony , please contact me at ( 202 ) 512-9286 or at pownerd@gao.gov .

individuals who made key contributions to this testimony are dave hinchman ( assistant director ) , justin booth , rebecca eyler , lee a. mccracken , jonathan ticehurst , and kevin walsh .

the federal government continues to spend billions of dollars on troubled it investments .

this appendix identifies examples of major it investments that have failed or faced significant challenges .

in this regard , we focused on it projects that were considered major development or acquisition efforts , based on their size or mission criticality .

we considered a project to have failed if it was terminated by the agency after substantial investment and without delivering significant planned capabilities .

a project was considered to be challenged if we had identified significant issues in its performance or management and made recommendations for improvement .

to identify these projects , we reviewed our work published since january 1 , 2003 , that addressed the performance or management of federal agency it projects .

we did not include a project on our lists if it was not the subject of a gao report .

since 2003 , a number of major it projects at federal agencies have failed .

specifically , agencies canceled the following investments after spending millions of dollars: department of defense's ( dod ) expeditionary combat support system ( ecss ) dod's defense integrated military human resources system ( dimhrs ) department of homeland security's ( dhs ) computer - assisted passenger prescreening system ( capps ii ) dhs's electronically managing enterprise resources for government effectiveness and efficiency ( emerge2 ) dhs's next generation homeland security information network ( hsin next gen ) dhs's secure border initiative network ( sbinet ) federal bureau of investigation's ( fbi ) virtual case file ( vcf ) general services administration's ( gsa ) e - authentication program national archives and records administration's ( nara ) electronic records archive ( era ) national polar - orbiting operational environmental satellite system ( npoess ) office of personnel management's ( opm ) retirement systems veterans affairs' ( va ) scheduling replacement project va's core financial and logistics system ( corefls ) va's financial and logistics integrated technology enterprise ( flite ) program va's health information systems and technology architecture — foundations modernization ( vista - fm ) these projects were canceled after going over budget , missing schedule milestones , or failing to deliver intended capabilities .

in many of these cases , we had identified issues in the management of these programs and made recommendations for improvement .

these issues were often related to a lack of disciplined and effective management , such as project planning , requirements definition , and program oversight and governance .

the following provides additional information on these failed it investments .

dod's expeditionary combat support system ( ecss ) in december 2012 , dod canceled ecss after having spent more than a billion dollars and missing multiple milestones , including failure to achieve deployment within 5 years of obligating funds .

the system was to provide the air force with a single , integrated logistics system that was to control and account for about $36 billion of inventory .

we issued several reports on this system and found that , among other things , the program was not fully following best practices for developing reliable schedules and cost estimates .

among other things , we had recommended that dod ensure that any future system deficiencies identified through independent assessments be resolved or mitigated prior to further deployment of ecss .

dod's defense integrated military human resources system ( dimhrs ) in february 2010 , dimhrs was canceled after 10 years of development and approximately $850 million spent , due , in part , to a lack of strategic alignment , governance , and requirements management , as well as the overall size and scope of the effort .

the system was intended to provide a joint , integrated , standardized personnel and pay system for all military personnel .

in 2008 , we had reported that army officials had concerns about the extent to which army requirements were being incorporated into dimhrs and that dod had not established a clear , well - defined process for maintaining effective communications to better prepare the army to deploy dimhrs .

we had recommended that dod develop a clearly defined process for effectively communicating the differences between dimhrs's capabilities and the army's requirements .

however , subsequent to the cancellation decision , each military service is now responsible for developing its own integrated personnel and pay system .

dhs's computer - assisted passenger prescreening system ( capps ii ) in august 2004 , dhs canceled its capps ii program — a transportation security administration ( tsa ) initiative to develop a system to identify passengers requiring additional security attention — due to a variety of delays and challenges .

in february 2004 , we reported that , according to program officials , approximately $41.5 million had been allocated for the system's acquisition to date and that the program faced a number of implementation challenges .

specifically , key activities in the development of capps ii had been delayed , and tsa had not completed important system planning activities .

in addition , tsa had not completely addressed seven of the eight issues identified by congress as key areas of interest related to the development , operation , and public acceptance of capps ii .

we also identified other challenges , including developing the international cooperation needed to obtain passenger data , managing the possible expansion of the program's mission beyond its original purpose , and ensuring that identity theft could not be used to negate the security benefits of the system .

we recommended , among other things , that dhs develop project plans , including schedules and estimated costs , to guide development ; establish a plan for completing critical security activities ; and develop a process by which passengers can get erroneous information corrected .

capps ii was eventually replaced by the secure flight system , which in turn was completely revamped and replaced by a new version of secure flight that finally became operational .

dhs's electronically managing enterprise resources for government effectiveness and efficiency ( emerge ) dhs canceled its emerge project , including approximately $18 million in contractor costs .

as we reported in june 2007 , dhs officials stated that several of the work products developed for emerge however , we found that key work products were of limited value .

specifically , the concept of operations did not contain an adequate description of the legacy systems and a clear articulation of the vision that should guide the department's improvement efforts , and key requirements developed for the project were unclear and incomplete .

we recommended , among other things , that , going forward , dhs employ best practices in defining its financial management system strategy , such as developing a comprehensive concept of operations document , standardizing business processes , and using disciplined processes to minimize project risk .

dhs's next generation homeland security information network ( hsin next gen ) in october 2010 , dhs terminated the acquisition of its hsin next gen system , which was to be the follow - on to its original hsin system , the department's primary it system for sharing terrorism - related information .

the department cited , among other things , continuing cost , schedule , and performance shortfalls and the lack of key it management controls and capabilities that are essential to mitigating such shortfalls and ensuring successful system delivery .

in 2008 we reported that dhs had yet to implement the full set of controls essential to effectively manage the acquisition of hsin next gen. we recommended that dhs strengthen its acquisition management controls before it started to implement the system .

the termination of hsin next gen resulted in a cost reduction of $128,969,000 .

dhs's secure border initiative network ( sbinet ) in january 2011 , the secretary of homeland security ended the sbinet program after obligating more than $1 billion to the program because it did not meet cost - effectiveness and viability standards .

since 2007 , we had identified a range of issues and made several recommendations to improve this program .

for example , in may 2010 we reported that the final acceptance of the first two deployments had slipped from november 2009 to september 2010 and from march 2010 to november 2010 , and that the cost - effectiveness of the system had not been justified .

we concluded that dhs had not demonstrated that the considerable time and money being invested to acquire and deploy the program was a wise and prudent use of limited resources .

as a result , we recommended that the department ( 1 ) limit near - term investment in the first incremental block of the program , ( 2 ) economically justify any longer - term investment in it , and ( 3 ) improve key program management disciplines .

this work contributed to the department's decision to cancel the program .

fbi's virtual case file ( vcf ) in march 2005 , the fbi discontinued the vcf component of its trilogy project after investing 3 years and $170 million .

the fbi terminated the project after trilogy's overall projected costs grew from $380 million to $537 million , the program fell behind schedule , and pilot testing showed that completion of vcf was infeasible and cost prohibitive .

among reasons we and others cited for vcf's failure were poorly defined system requirements , ineffective requirements change control , limited contractor oversight , and human capital shortfalls due to , for example , a lack of continuity in certain management positions and a lack of trained staff for key program positions .

gsa's e - authentication program in october 2003 , it was reported that gsa had terminated plans to develop an “e - authentication gateway,” which was to provide a consolidated electronic authentication service to support the e - government initiatives sponsored by omb .

we had reported 1 month earlier in september 2003 that , according to agency officials , 13 agencies had provided a total of $13.5 million to gsa for the gateway as of august 18 , 2003 , with another $3 million expected from another agency by the end of fiscal year 2003 .

however , gsa had achieved few of its project objectives and extended the milestone for completing a fully operational system .

we noted that the modest progress that had been achieved to date called into question the likelihood that the project could successfully field an operational gateway , even within the revised schedule .

further , the project faced a number of challenges , including developing procedures and guidance defining the specific technologies to support different authentication requirements , agreeing upon technical standards to provide a basis for ensuring interoperability , and taking full measures to ensure that the gateway system was adequately secured and that privacy information adequately protected .

nara's electronic records archive ( era ) in july 2010 , omb directed nara to halt development of its era system at the end of fiscal year 2011 ( a year earlier than planned ) .

omb cited concerns about the system's cost , schedule , and performance and directed nara to better define system functionality and improve strategic planning .

through fiscal year 2010 , nara had spent about $375 million on the system .

we issued several reports and made recommendations to improve this system , noting , among other things , that nara's plans for era lacked sufficient detail to clearly show what functions had been delivered or were to be included in future increments and at what cost ; the agency had been inconsistent in its use of earned value management to monitor the project's progress ; and nara lacked a contingency plan for era to ensure system continuity in the event that normal operations were disrupted .

these findings and recommendations contributed to the decision to halt the system .

national polar - orbiting operational environmental satellite system ( npoess ) in february 2010 , a presidential task force decided to disband npoess after having spent 16 years and almost $5 billion on the program .

npoess was a tri - agency weather satellite program managed by the national oceanic and atmospheric administration ( noaa ) , dod , and the national aeronautics and space administration ( nasa ) .

we issued a series of reports on the npoess program that highlighted the technical challenges , cost growth , and tri - agency management challenges facing the program .

for example , in june 2009 we reported that the program's approved cost and schedule baselines were not achievable , and that costs could grow by approximately $1 billion over the then - current $13.95 billion estimate .

we further noted that schedules for the launch of a demonstration satellite and the first two operational satellites were expected to be delayed , increasing the risk of a gap in satellite continuity .

we had also found that the npoess program's tri - agency executive council was ineffective , and we made recommendations aimed at improving this executive - level oversight .

however , after the program's cancellation , the agencies were directed to undertake separate acquisitions .

opm's retirement systems modernization in february 2011 , opm canceled its retirement systems modernization program after several years of trying to improve the implementation of this investment .

this was the agency's third major effort over a more than 20-year period to automate the processing of federal employee retirement claims .

according to opm , it spent approximately $231 million on this investment .

we issued a series of reports on the agency's efforts to modernize its retirement system and found that the agency was hindered by weaknesses in several important management disciplines that are essential to successful it modernization efforts .

accordingly , we made recommendations in areas such as project management , organizational change management , testing , cost estimating , and earned value management .

in may 2008 , an opm official cited the issues that we identified as justification for issuing a stop work order to the system contractor , and the agency subsequently terminated the contract , which resulted in a cost reduction of $136.5 million between fiscal years 2009 and 2013 .

va's scheduling replacement project in september 2009 , va terminated its scheduling replacement project , after spending an estimated $127 million over 9 years .

the investment was to modernize its more than 25-year - old outpatient scheduling system , but the department had not yet implemented any of the planned system's capabilities .

va began a new initiative that it refers to as healthevet scheduling on october 1 , 2009 .

in may 2010 , we reported that va's efforts to successfully complete the scheduling replacement project were hindered by weaknesses in several key project management disciplines and a lack of effective oversight that , if not addressed , could undermine the department's second effort to replace its scheduling system .

as the department proceeded with future development , we recommended that it take actions to improve key processes , including acquisition management , system testing , and progress reporting , which are essential to the department's second outpatient scheduling system effort .

va's core financial and logistics system ( corefls ) va's first attempt to develop an integrated financial and asset management system , corefls , began in 1998 but was discontinued by the department in 2004 because the pilot system failed to support va's operations after the department reportedly spent more than $249 million on development .

the department conducted three independent assessments of the initiative that collectively identified 141 findings , which the department categorized into functional areas of responsibility such as acquisition management , organizational change management , program management , and systems engineering .

va aggregated these findings into a repository of lessons learned to inform future efforts .

however , we reported in september 2008 that the department had not taken corrective actions to address all the findings , and recommended that it do so .

subsequently , va officials provided documentation showing that all items in the repository had been addressed .

va's financial and logistics integrated technology enterprise ( flite ) program in october 2011 , va terminated its flite program , an effort to deliver an integrated financial and asset management system for the department .

begun in 2005 , the system was intended to be delivered by 2014 at a total estimated cost of $608.7 million but was canceled due to challenges in managing the program , including an unsuccessful pilot of the strategic asset management system .

the flite program was va's second effort to develop such a system .

we had reported in october 2009 that the department had not yet fully established capabilities needed to ensure that the program will be successfully implemented and recommended that it take steps to improve program management .

va's health information systems and technology architecture — foundations modernization ( vista - fm ) in october 2010 , va terminated its vista - fm program , which was to address the need to transition the va electronic medical record system to a new architecture .

as we reported in october 2009 , the program , which had been estimated to cost $1.9 billion , had significant weaknesses in its earned value management processes , and we estimated that the program would likely overrun its budget at completion by about $350.2 million .

as a result of our recommendations and an internal department evaluation of the program , multiple components of the program were suspended , before the program was finally terminated in 2010 .

in addition to canceling many failed projects , the government has continued to invest in challenged it projects .

the following are selected examples of such investments that have faced significant challenges: department of commerce / census bureau's 2010 decennial census department of commerce / noaa's geostationary operational environmental satellite - r ( goes - r ) series dhs's rescue 21 program dhs's united states visitor and immigrant status indicator technology ( us - visit ) program dod's armed forces health longitudinal technology application ( ahlta ) dod's defense enterprise accounting and management system ( deams ) dod's global combat support system - army ( gcss - army ) dod's global combat support system – marine corps ( gcss - mc ) dod's navy enterprise resource planning system ( navy erp ) dod's navy next generation enterprise network ( ngen ) dod / department of the treasury's navy cash program dod / va's integrated electronic health record ( iehr ) dod / va's federal health care center ( fhcc ) food and drug administration's ( fda ) mission accomplishments and regulatory compliance services ( marcs ) internal revenue service's ( irs ) customer account data engine ( cade ) nasa's james webb space telescope ( jwst ) u.s. department of agriculture's ( usda ) modernize and innovate the delivery of agricultural systems ( midas ) program we have identified challenges facing these projects , many due to ineffective or undisciplined management , and have made recommendations for improvement .

the following provides additional information on these challenged it investments .

department of commerce / census bureau's decennial census at a cost of about $13 billion , the 2010 decennial census was the costliest in history .

this was due , in part , to cost overruns and major performance problems with key it systems .

for example , the census bureau's field data collection automation ( fdca ) program , originally estimated to cost $596 million , was intended to use handheld mobile devices to support field data collection for the census , including in - person follow - up with those who did not return their census questionnaires ( nonresponse follow - up ) .

however , we testified in march 2008 that the program was experiencing significant problems , including schedule delays and cost increases from changes in requirements .

we had previously reported that the fdca project office had not implemented the full set of acquisition management capabilities that were needed to effectively manage the program and that the changes to requirements had been a contributing factor to both schedule delays and cost increases experienced by the fdca program .

in april 2008 , due to problems identified during testing and cost overruns and schedule slippages , the secretary of commerce announced a redesign of the 2010 census , resulting in a $205 million increase in life - cycle costs .

also in april 2008 , the census bureau decided not to use the handheld devices for nonresponse follow - up , but did continue to use the devices for other decennial census operations .

dropping the use of handhelds for nonresponse follow - up and replacing them with a paper - based system increased the cost of the census by up to $3 billion .

although the bureau worked aggressively to improve the paper - based system that replaced the handheld computers , we reported in december 2010 that the paper - based system also experienced significant issues when it was put in operation .

for example , performance problems with the it system used to manage the nonresponse follow - up process led to processing backlogs , which hindered the bureau's ability to fully implement quality assurance procedures as planned .

we recommended , accordingly , that the bureau incorporate best practices in its it acquisition management policy .

in september 2012 , we reported that the census bureau still needed to implement key it management practices to select , control , and evaluate its it investments and effectively manage system development , as well as key practices for effective workforce planning , and recommended it take eight actions to do so .

until such steps are taken , the bureau faces the risk that the same kind of it management and implementation challenges that faced the 2010 decennial census will impact the 2020 census .

department of commerce / noaa's geostationary operational environmental satellite - r ( goes - r ) series the department of commerce's noaa , with the aid of nasa , is to procure the next generation of geostationary operational environmental satellites — a series of four satellites intended to replace existing weather satellites that will likely reach the end of their useful lives in about 2015 .

this new series is considered critical to the united states' ability to maintain the continuity of data required for weather forecasting .

noaa estimates that the goes - r series will cost $10.9 billion through 2036 ; the launch of the first satellite is planned for october 2015 .

in september 2010 , we reported that , since 2006 , the launch dates of the first two satellites in the series have been delayed by about 3 years , which could lead to a gap in coverage if the existing weather satellites fail prematurely .

we also found that noaa had not established adequate continuity plans in the event of a satellite failure with no backup available and that the agency had not adequately involved users at federal agencies that rely on goes data in developing and prioritizing requirements .

we recommended that noaa address weaknesses in its continuity plans and improve its processes for involving other federal agencies .

subsequently , noaa established a continuity plan for goes - r and developed a communications plan for involving agencies that depend on goes data .

in june 2012 we reported that technical problems with instruments and spacecraft , among others , had delayed key reviews and led to increased complexity for the development of goes - r .

while the program reported having $1.2 billion in reserve to manage future delays and cost growth , significant development remained , and we concluded that the program may not be able to ensure that it has adequate resources to cover ongoing challenges and unexpected problems .

in addition , we found that the program's schedule contained deficiencies , and it had not fully implemented risk management best practices .

we recommended that noaa assess and report reserves needed over the life of the program and address the issues with its schedules and risk management .

noaa reported that it would take steps to implement these recommendations .

dhs's rescue 21 program dhs's rescue 21 program is intended to modernize the u.s. coast guard's maritime search and rescue communications capability .

since 2003 , we have reported on significant weaknesses in the oversight and management of the program , including continued cost growth and schedule delays .

for example , in may 2006 , we found that the estimated total acquisition cost for rescue 21 had increased from $250 million in 1999 to $710.5 million in 2005 , and the time line for achieving full operating capability had been delayed from 2006 until 2011 .

we recommended that executive - level management oversee rescue 21's progress toward cost and schedule milestones and manage risks ; establish milestones to complete an integrated baseline review ; and develop revised cost and schedule estimates .

more recently , in july 2012 , we reported that the rescue 21 program's life - cycle cost estimate had grown to approximately $2.7 billion — an increase of approximately $2.4 billion since 1999 — and that completion was delayed to 2017 .

program officials stated that increases in the cost estimate were due , in part , to additional schedule delays and more realistic estimates of future costs for ongoing system technology refreshment .

however , we noted that the program's cost estimate did not exhibit all qualities of a reliable cost estimate and recommended that dhs direct responsible officials to update future life - cycle cost estimates using cost - estimating practices that address the weaknesses we identified .

dhs's united states visitor and immigrant status indicator technology ( us - visit ) program dhs's us - visit program is charged with developing a biometric verification capability for non - u.s. citizens entering and leaving the country .

from fiscal year 2002 to fiscal year 2012 , dhs's us - visit program was appropriated over $3.5 billion , and the program has successfully developed a massive biometric database and deployed an entry capability .

however , the program had not developed an exit capability and last conducted an exit pilot in 2009 .

we have reported on issues associated with key aspects of us - visit program management , such as risk management and the reliability of cost estimates and program schedules , as well as the lack of a completed strategic plan and the results of the 2009 exit pilot .

we recommended that dhs review the program's approach to risk management and earned value management , and that dhs develop a plan and integrated schedule for a comprehensive exit capability .

we also recommended that the program incorporate additional sources of information when making future decisions about an exit capability .

however , there are no known current plans to develop an exit process , and the president's fiscal year 2013 budget request called for eliminating a standalone us - visit program office and incorporating the program's mission into customs and border protection and immigration and customs enforcement .

we have ongoing recommendation follow - up work regarding this issue .

dod's armed forces health longitudinal technology application ( ahlta ) dod has obligated approximately $2 billion over 13 years to acquire an electronic health record system — referred to as its ahlta initiative .

in october 2010 , we reported that dod had delivered various capabilities for outpatient care and dental care documentation , but it scaled back other capabilities it had originally planned to deliver , such as replacement of legacy systems and inpatient care management .

in addition , users continued to experience significant problems with the performance ( speed , usability , and availability ) of the portions of the system that have been deployed .

these problems were due in part to weaknesses in key acquisition management and planning processes .

dod initiated efforts to improve system performance and enhance functionality and plans to continue its efforts to stabilize the ahlta system through 2015 , as a "bridge" to the new electronic health record system it intends to acquire .

according to dod , the planned new electronic health record system — known as the ehr way ahead — is to be a comprehensive , real - time health record for service members and their families and beneficiaries .

to help better manage these efforts , we recommended that dod take six actions to help ensure that it has disciplined and effective processes in place to manage the acquisition of further electronic health record system capabilities .

dod's defense enterprise accounting and management system ( deams ) the air force's deams is the agency's target accounting system designed to provide accurate , reliable , and timely financial information .

in march 2012 , we reported that deams faced a 2-year deployment delay and an estimated cost increase of about $500 million from its original life - cycle cost estimate of $1.1 billion , an increase of approximately 45 percent .

further , in february 2012 , we reported that assessments by dod users had identified operational problems with the system , such as data accuracy issues , an inability to generate auditable financial reports , and the need for manual workarounds .

we recommended that dod take actions to ensure the correction of system problems prior to further system deployment , including user training .

in july 2012 , the dod inspector general reported that the deams schedule delays were likely to diminish its intended cost savings and would jeopardize the department's goals for attaining an auditable financial statement .

dod's global combat support system - army ( gcss - army ) gcss - army is intended to improve the army's supply chain management capabilities and provide accurate equipment readiness status reports , among other things .

in march 2012 , we reported that gcss - army was experiencing a cost overrun of approximately $300 million on an original life - cycle cost estimate of $3.9 billion ( an increase of approximately 8 percent ) and a deployment delay of approximately 2 years .

among other things , we recommended that dod ensure any future system deficiencies identified through independent assessments are resolved or mitigated prior to further deployment of the system .

in addition , because the dod cio rated gcss - army as low or moderately low risk from july 2009 through march 2012 on omb's federal it dashboard , we recommended that the department's cio reassess the department's considerations for assigning risk levels to investments on the dashboard , to include external assessments of performance and risk .

dod's global combat support system – marine corps ( gcss - mc ) gcss - mc is intended to provide the deployed warfighter enhanced capabilities in the areas of warehousing , distribution , logistical planning , depot maintenance , and improved asset visibility .

in july 2008 , we reported that not effectively implementing key it management controls , such as economically justifying investment in the system , had in part contributed to a 3-year schedule slippage and a cost overrun of about $193 million on the first phase of the program and would likely contribute to future delays and overruns if not corrected .

accordingly , we made recommendations to address the identified deficiencies , such as cost and schedule estimating , risk management , and system quality measurement weaknesses .

in october 2010 , we reported that gcss - mc faced a 3- year deployment delay on phase 1 .

we also reported in march 2012 that gcss - mc faced an estimated cost increase of about $970 million from its original life - cycle cost estimate of $126 million .

as of december 2011 , the life - cycle cost estimate was estimated to be $1.1 billion , and a revised full deployment date was being considered .

dod's navy enterprise resource planning system ( navy erp ) navy erp is intended to standardize the acquisition , financial , program management , maintenance , procurement , plant and wholesale supply , and workforce management capabilities of the navy .

in march 2012 , we reported that navy erp faced a 2-year deployment delay and an estimated cost increase of about $1 billion from its original life - cycle cost estimate of $1.87 billion .

this estimate was later revised in august 2004 , december 2006 , and again in september 2007 to $2.4 billion .

in october 2010 , we reported that these slippages occurred , in part , because of problems experienced in data conversion and adopting new business procedures associated with implementing the navy erp .

moreover , in september 2008 , we reported that not effectively implementing key it management controls , such as earned value management , had contributed to the more than 2-year schedule delay and almost $600 million cost overrun on the program since it began , and would likely contribute to future delays and overruns if not corrected .

accordingly , we made recommendations to address the identified deficiencies , such as improving cost and schedule estimating , earned value management , and risk management weaknesses .

dod's navy next generation enterprise network ( ngen ) dod's ngen is to replace the navy marine corps intranet program and include capabilities such as secure transport of voice and data , data storage , and e - mail , to be incrementally acquired through multiple providers .

the program , which is expected to cost about $38 billion through fiscal year 2024 , had weaknesses in its acquisition approach .

specifically , in march 2011 , we reported that the program was not well positioned to meet its cost and schedule estimates .

for example , the department had not sufficiently analyzed alternative acquisition approaches and did not have a reliable schedule for executing ngen .

we recommended dod limit further investment until it conducted an interim review to reconsider the selected acquisition approach and addresses its investment management issues .

in september 2012 , we reported that while dod had revised its acquisition approach , it was still suffering from management issues related to measuring cost effectiveness , completing milestones on schedule , and mitigating risk .

dod / department of the treasury's navy cash program initiated in 2001 , navy cash is a joint department of the navy and department of the treasury financial management service program to create a cashless environment on ships using smart card technology .

it was estimated to cost about $320 million to fully deploy .

in 2008 , we reported that the system had not been assessed and defined in a way to ensure that it was not duplicative of programs in the air force and the army that use smart card technology , nor economically justified on the basis of reliable analyses of estimated costs and expected benefits over the program's life .

in addition , we reported that system requirements and system security had not been effectively managed .

program oversight and management officials acknowledged the weaknesses and cited turnover of staff in key positions and their primary focus on deploying navy cash as reasons for the state of some of the it management controls .

we concluded that after investing about 6 years and $132 million on navy cash and planning to invest an additional $60 million to further develop the program , the department had yet to demonstrate through verifiable analysis and evidence that the program , as currently defined , was justified .

accordingly , we recommended that investment of modernization funding in the program be limited until a basis for informed decision making was established , and that other program management weaknesses were corrected .

dod / va's integrated electronic health record ( iehr ) dod and va have been challenged over the last 15 years on a variety of initiatives to share data among the departments' health information systems .

in march 2011 , the secretaries of dod and va committed their two departments to developing a new common iehr , and in may 2012 announced their goal of implementing it across the departments by 2017 .

according to the departments , the decision to pursue iehr would enable dod and va to align resources and investments with common business needs and programs , resulting in a platform that would replace the two departments' electronic health record systems with a common system .

the departments estimated the life - cycle cost of this effort at about $25 billion .

however , as we noted in a recent testimony , the secretaries announced in february 2013 that instead of developing a new common integrated electronic health record system , the departments would focus on integrating health records from separate dod and va systems .

va has stated that it will continue to modernize its existing system , called vista , while pursuing the integration of health data , while in may 2013 , dod announced that it planned to purchase a commercial off - the - shelf product .

the secretaries offered several reasons for this new direction , including cutting costs , simplifying the problem of integrating dod and va health data , and meeting the needs of veterans and service members sooner rather than later .

nevertheless , the departments' recent change in the program's direction and history of challenges to improving their health information systems heighten concern about whether this latest initiative will be successful .

dod / va's federal health care center ( fhcc ) dod and va jointly undertook an it project to support the fhcc in north chicago , illinois , which is the first medical facility to serve both departments' patient populations while operating under a single line of authority .

as we reported in february 2011 , despite an investment of more than $122 million , none of the fhcc's required it capabilities had been implemented as planned when the center opened in october 2010 .

while system components to support single sign - on and single - patient registration became operational in december 2010 , a component to support medical consults was not expected to be completed until march 2013 , and the departments did not have a schedule for completion of the component to support pharmacy .

we recommended that the departments strengthen their joint it system planning efforts for the fhcc by developing plans that include scope definition , cost and schedule estimation , and project plan documentation and approval .

we also noted that the two departments face barriers in three key it management areas — strategic planning , enterprise architecture , and investment management — and recommended steps for improvement .

fda's mission accomplishments and regulatory compliance services ( marcs ) fda's marcs program is intended to automate workflow , help track and manage information about firm compliance with fda's regulations , and eliminate fda's existing stove - piped databases .

however , the program has been rebaselined five times since 2002 , the total estimated cost has grown from $221.4 million to $282.7 million , and the estimated completion date for initial development has slipped from september 2008 to october 2016 .

according to omb exhibit 53s from 2004 to 2013 , fda has spent approximately $160 million from fiscal year 2002 to fiscal year 2011 on marcs .

in march 2012 , we reported fda had not developed a comprehensive integrated master schedule for marcs that would allow the agency to effectively gauge progress .

we further reported that the agency was reevaluating the scope of the initiative and concluded that , until this assessment was complete , it was uncertain how or when much of the intended functionality and improvements associated with marcs would be delivered .

we recommended that fda , in completing the assessment of marcs , develop an integrated master schedule that identifies which legacy systems will be replaced and when ; identifies all current and future tasks to be completed ; and defines and incorporates information reflecting needed resources and critical dependencies .

we further recommended that the agency use this schedule to monitor the progress of marcs .

irs's customer account data engine ( cade ) in december 2011 , irs ended further development of cade , its effort to replace legacy systems for storing , managing , and accessing individual taxpayer accounts .

irs started developing this system in 2002 to replace the legacy individual master file processing system and house tax information for more than 40 million taxpayers while providing faster return processing and refunds .

in december 2009 , we reported that after over 5 years and $400 million , cade was only processing about 15 percent of the functionality originally planned for completion by 2012 .

in addition , each successive release of the system was expected to process more complex returns , but several technical challenges had not been addressed .

given this , irs estimated that full implementation of cade would not be achieved until at least 2018 or possibly as late as 2028 .

as a consequence , in 2011 irs decided to stop development of new cade functionality and rethink its strategy for modernizing individual taxpayer accounts to determine whether an alternative approach could deliver improvements sooner .

this led to the development of cade 2 , a new program for replacing the individual master file .

beginning in january 2012 , irs started using cade 2 to process returns daily and issue refunds faster for about 84 million taxpayers .

as of january 2013 , irs reported that there have been no significant problems with the system .

nasa's james webb space telescope ( jwst ) nasa's jwst project is designed to advance understanding of the origin of the universe .

since 2006 , we have identified a range of issues with the project .

for example , in july 2006 , we reported that the jwst project had experienced cost growth exceeding $1 billion — which increased its life - cycle cost estimate from $3.5 billion to $4.5 billion — and its launch date had slipped nearly 2 years to 2013 .

we also found that the program was not fully adhering to a knowledge - based acquisition approach , which ensures that resources match requirements in terms of knowledge , time , and money before program start .

accordingly , we recommended that the program apply such an approach .

in october 2009 , we reported that as of may 2009 , the jwst contractor exceeded its planned cost target by $224.7 million and had not completed $9.4 million in planned work .

a key driver in this cost overrun was greater - than - expected complexity in the work , which required additional resources .

we concurred with the contractor's estimate that it would overrun its budget — worth approximately $1.3 billion — by $448.5 million .

we also found that the program had not fully implemented practices for earned value management and recommended steps for improvement .

in 2011 , the project finalized a major replanning that resulted in further growth to the project's expected costs , as well as additional delays to its expected launch date .

on the basis of the replanning , nasa announced that the project would be rebaselined at approximately $8.8 billion — a 78 percent increase to the project's life - cycle cost compared to the previous baseline — and would launch in october 2018 — a delay of 52 months .

usda's modernize and innovate the delivery of agricultural systems ( midas ) program usda's midas program is intended to modernize the it systems supporting the farm service agency's 37 farm programs .

as we reported in july 2011 , the implementation cost estimate is approximately $305 million , with a life - cycle cost of approximately $473 million .

however , we found that the implementation cost estimate was uncertain because it had not been updated since 2007 , and the program schedule had not been updated to account for delays .

in addition , we reported that the program's management approach , while including many leading practices , could be strengthened .

finally , we found there was a lack of clarity and definition regarding the roles of executive - level governance bodies responsible for overseeing the program .

we recommended that usda update cost and schedule estimates , address management issues , and clarify the roles and coordination among governance bodies .

this is a work of the u.s. government and is not subject to copyright protection in the united states .

the published product may be reproduced and distributed in its entirety without further permission from gao .

however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately .

