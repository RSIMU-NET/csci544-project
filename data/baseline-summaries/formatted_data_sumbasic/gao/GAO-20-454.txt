to be exempt from federal income tax , an entity must be organized for a purpose specified in the internal revenue code — such as providing charity , enhancing social welfare , or furthering the interests of the organization's membership — and must operate in accordance with that purpose .

internal revenue service ( irs ) oversight can help ensure that exempt organizations abide by the purposes that justify their tax exemption .

this oversight can also help safeguard the public's confidence in the integrity of the charitable sector .

the tax exempt and government entities ( te / ge ) division within irs oversees exempt organizations by conducting examinations and other activities to ensure compliance with the internal revenue code .

examinations are reviews of the books and records of exempt organizations to determine whether they operated in accordance with their exempt purposes , and paid taxes they owed .

if te / ge finds noncompliance , it may impose excise taxes for certain violations , or — in appropriate circumstances — it may revoke an organization's tax - exempt status .

limited resources have prompted te / ge to try refining its examination selection methods to focus examinations on the organizations with the highest potential for noncompliance .

in a 2015 review , we found deficiencies in te / ge's methods for selecting tax - exempt organizations for examination .

however , we noted that in fiscal year 2012 , te / ge had started analyzing more data reported on the form 990 , return of organization exempt from income tax , to identify potential noncompliance and to select returns for examination .

we did not make recommendations on the use of data for examination selection .

you asked that we review te / ge's use of form 990 data for exempt organization compliance efforts .

this report assesses ( 1 ) the use of data to select tax exempt organization returns for examination ; and ( 2 ) the process te / ge has established to select returns for examination .

to assess te / ge's use of data to select exempt organization returns for examination , we analyzed te / ge data for examinations closed from fiscal years 2016 to 2019 .

the analyses included information on examination closures and changes made to returns because of examinations .

based on our testing of the data and review of documentation and interviews , we determined that the examination data were reliable for the purposes of assessing te / ge's selection processes .

we reviewed the examination selection process and outcomes from te / ge's form 990 examination selection model .

to assess the process that te / ge established to select returns for examination , we analyzed te / ge documentation relative to five key internal controls steps and four other controls that we selected from the standards for internal control in the federal government ( green book or gb ) .

internal control comprises the plans , methods , policies , and procedures used to fulfill an agency's objectives .

we reviewed the internal revenue manual ( irm ) , work plans , desk guides , procedures , examination selection processes , model documentation and other documents .

in addition , we analyzed a generalizable stratified random sample of data queries from three examination selection models to verify whether te / ge tests and follows its approval and documentation procedures for new data queries .

we interviewed officials in te / ge's compliance planning and classification ( cp&c ) office and irs's research , applied analytics and statistics ( raas ) division .

for details on our scope and methodology , see appendix i .

we conducted this performance audit from november 2018 to june 2020 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

irs form 990-series return or notice must be filed by most organizations exempt from income tax under internal revenue code section 501 ( a ) , and certain political organizations and nonexempt charitable trusts .

te / ge uses form 990 reporting for promoting compliance and enforcing federal tax law for tax - exempt organizations ( see appendix ii for a copy of the form 990 and a list of its schedules ) .

form 990 asks for information about an organization such as: employees , governance , and compensation ; revenue and expenses ; assets and liabilities ; employment tax compliance ; and specific organizational issues , such as lobbying by charities and private foundations .

te / ge redesigned the form 990 for the first time in nearly 30 years for tax year 2008 , and has made subsequent changes to the form ( see appendix iii for a summary of the changes ) .

for tax year 2017 , which is the most recent year of completed filing data , organizations filed 319,183 form 990s .

beyond the basic form 990 , other versions include: form 990 – ez , short form return of organization exempt from income tax .

this form reduces the filing burden on small tax - exempt organizations .

organizations with less than $200,000 in gross receipts and less than $500,000 in total assets may use it .

for tax year 2017 , 232,764 form 990-ez's were filed .

form 990 – n , electronic notice ( e - postcard ) for tax - exempt organizations not required to file forms 990 or 990 – ez .

most small organizations whose annual gross receipts are normally $50,000 or less may file form 990-n. for tax year 2017 , 652,280 form 990-n's were filed .

form 990 – pf , return of private foundation .

in addition to private foundations , nonexempt charitable trusts treated as private foundations are required to file form 990-pf .

for tax year 2017 , 113,658 form 990-pf's were filed .

certain larger organizations are required to electronically file their returns .

the taxpayer first act of 2019 requires all organizations to electronically file form 990's for tax years beginning after july 1 , 2021 .

te / ge can assess financial penalties for failing to file a required form 990 .

as an employer , or if an exempt organization generates unrelated business income , additional tax reporting requirements may apply , such as for employment tax or unrelated business income .

a 2017 te / ge reorganization created cp&c to provide a centralized approach to compliance planning , examination selection and assignment and planning and monitoring activities .

cp&c has three groups as follows: 1 .

issue identification and special review identifies and develops issues for examinations or compliance activities and certain criteria for examination selection .

2 .

classification and case assignment uses irs staff known as “classifiers” to review returns for examination under different examination sources ( see appendix iv ) .

classification is the process of determining whether a return should be selected for compliance activities , what issues should be the primary focus of the compliance activity , and the type of compliance activity that should be conducted .

3 .

planning and monitoring develops an annual work plan and monitors performance .

the work plan details the number of examination starts , closures and other measures .

it develops classification requests to ensure that enough returns are available to meet work plan goals .

te / ge's compliance governance board ( governance board ) oversees te / ge's compliance program , including cp&c operations such as approving priority issue areas — known as compliance strategies .

the governance board also reviews program goals , considers metrics and reporting , and reviews the performance of compliance strategies .

the governance board has five te / ge executives plus counsel who are voting members as well as three non - voting members .

the exempt organizations examinations group is responsible for compliance activities .

examinations have various outcomes for an organization .

the most severe outcome is revocation of tax exempt status .

taxes — such as employment or excise — may be assessed as a result of an examination .

in fiscal year 2019 , approximately $131 million in taxes were assessed .

te / ge conducts compliance contacts — non - examination correspondence such as compliance checks and soft letters — that are used to handle some compliance issues .

for example , compliance checks determine whether specific reporting or filing requirements have been met .

a “soft letter” notifies an organization of changes in tax - exempt law or potential compliance issues .

a response to these letters is not required .

te / ge also reviews tax - exempt hospitals for compliance with certain community benefit requirements .

in fiscal year 2019 , te / ge closed 1,470 compliance checks , sent 3,955 soft letters , and closed 750 hospital reviews .

compliance checks and hospital reviews can result in an examination while responses to soft letters may result in a compliance check .

te / ge identifies exempt organization returns for examination from many sources and categorizes examinations into three groups , known as portfolios: ( 1 ) data driven approaches , ( 2 ) referrals and other casework , and ( 3 ) compliance strategies .

all three rely on data , to some extent , to make decisions on selecting returns for examination .

this portfolio uses analytical models and queries based on quantitative criteria to identify potential examinations .

te / ge has three separate models that review exempt organization data from forms 990 , 990-ez , and 990-pf for compliance .

the models “score” returns for examination based on potential noncompliance .

the form 990 , 990-ez and 990-pf models have 354 unique queries .

for purposes of this report , a query reviews databases to identify responses on returns that may indicate noncompliance because they do not meet certain criteria or expected values , such as exceeding a dollar threshold .

exempt organizations examination staff developed many of the queries , based on information collected on the form 990 after it was redesigned for tax year 2008 , according to te / ge officials .

as queries were developed , staff tested and used them to identify certain potentially noncompliant populations and to identify returns that were flagged by multiple queries .

starting in fiscal year 2016 , te / ge began using queries in models .

the models use a scoring system that applies weights , or points , to each query result to generate a score — which for the form 990 model has ranged from zero to more than 50 — for a return .

the models also screen out returns that are approaching a statute of limitations date , if the organization is not active , or has a current or recent examination history .

since november 2017 , staff have been able to submit potential compliance issues for consideration through an online submission portal for governance board approval .

cp&c has the option of considering whether these ideas result in model changes , according to irs officials .

twice a year , each model is run using the latest data , and generates a model score sheet ( mss ) .

the mss is a ranked list of returns that score above a minimum threshold .

a classifier uses the ranking to identify returns for potential examination .

although the models screen for examination status and statute of limitations , a te / ge official said the classifier also checks whether the statute of limitations date is near and whether the organization recently had undergone an examination or compliance check , as well as whether the return was identified under another selection method .

this official explained that a classifier checks the criteria because conditions may have changed since the model's last run .

the classifier selects returns to fulfill a stocking plan , which identifies the number and type of returns to be examined to meet work plan requirements .

see figure 1 .

aside from the three models , te / ge also uses other methods and data to identify and develop compliance work .

the data driven approaches portfolio includes approaches that te / ge developed in partnership with irs's raas division .

the partnership began in 2016 and continues today , according to irs officials .

the portfolio also includes some of the queries that te / ge ran prior to fiscal year 2016 for examination selection .

some of these examinations remained open as of fiscal year 2019 .

although not all of the returns selected for examination in this portfolio rely on data for examination selection , we describe them all below .

referrals .

referrals are complaints about exempt organization noncompliance made by third parties , including the public and other irs offices or divisions .

post determination compliance .

sampling and queries are used to identify organizations that file form 1023-ez .

claims .

claims are requests for tax refunds , adjustments of tax paid , or credits not previously reported or allowed .

form 990 queries ( pre - model ) : these queries were run prior to fiscal year 2016 .

some of these examinations remained open as of fiscal year 2019 .

training .

te / ge uses these examinations , selected based on various methods , to teach examiners .

other projects .

te / ge initiated these examinations under older compliance projects , using a variety of selection methods .

the compliance strategies portfolio consists of compliance issues that originated from a compliance issue submission portal for te / ge staff .

the strategies are approved by the governance board , which results in adding the compliance strategy to the work plan .

in fiscal year 2019 , te / ge closed examinations under three compliance strategies , including private foundation loans , and for - profit entities that converted to 501 ( c ) ( 3 ) organizations .

returns are selected using sampling or other uses of data .

table 1 shows examinations closed for the three portfolios .

once an examination is underway , an examiner may expand it to include an organization's returns for other tax years or other types of returns such as employment tax returns .

irs refers to these additional examinations as “pick - ups,” each of which is counted as a separate examination .

examiners must obtain manager approval to expand an examination .

examiners are required to check that an organization filed all returns that are required .

if the examiner finds that a return was not filed — such as an employment tax return — and is unable to secure the return , he or she may prepare a “dummy” return called a substitute for return ( sfr ) .

the organization's activities , records , and documents may then be examined .

in 2017 , te / ge hired a contractor to assess aspects of the exempt organization process for examination selection , with a focus on the form 990 model .

in january 2018 , the contractor released a report on the development and operation of the models .

the contractor released a second report in july 2018 on the form 990 model performance .

the contractor found the model was not always identifying the “next best case” as te / ge intended because scores did not consistently predict certain measures of noncompliance .

across both reports , the contractor made 17 recommendations , which we discuss later in this report ( see appendix v ) .

as of march 2020 , te / ge implemented one recommendation on model update submissions and part of another on hiring assessments .

in september 2019 , te / ge initiated another study with the same contractor — with a planned release of the report in september 2020 — on developing alternatives to the form 990 model .

since the form 990 model was first run for fiscal year 2016 , the percentage of examinations closed that were identified by using data , such as through models or queries , has increased each year , as shown in figure 2 .

almost half of these examinations are from the models .

this increased reliance on using data in selecting returns for examination offers potential efficiencies .

for example , a potential efficiency from using data to find possible noncompliance could mean fewer steps for staff who classify returns .

ultimately , this could allow te / ge to shift staff from classifying returns to doing compliance activities such as examinations to confirm any actual noncompliance .

another potential efficiency would be selecting more examinations that find changes to the return .

to measure the outcomes of examinations , te / ge computes a “change rate,” or the percentage of closed examinations with a change to the return .

in general , a higher change rate indicates that more examinations found noncompliance .

examinations selected using data have a slightly better change rate than other selection sources ( 84 percent versus 82 percent ) for closures in fiscal years 2016 through 2019 .

similar to all examinations that used data , the change rate for examinations selected using data through the form 990 model ( 87 percent ) was higher than the change rate for other selection sources ( 82 percent ) in fiscal years 2016 through 2019 .

however , we found evidence that the changes identified in examinations did not clearly result from using the form 990 model's scoring system .

specifically: the model has not improved change rates compared to pre - model form 990 queries .

a higher model score is not associated with a higher change rate .

most examination changes credited to the model come from pick - up returns and sfr's that examiners identify rather than from primary returns identified by the model score .

the scoring generated by the form 990 model has not improved change rates compared with the form 990 queries that te / ge used prior to the model .

the change rates for both the form 990 model and the pre - model queries , for fiscal years 2016 through 2019 , was 87 percent .

similarly , for the last 2 fiscal years , the change rate for all form 990 models was roughly equivalent to the change rate for other selection sources of exempt organization examinations .

as shown in table 2 , the models had a slightly higher change rate in fiscal year 2018 , and a slightly lower change rate in 2019 , compared to the other sources .

form 990 model scores for returns do not consistently predict examination change rates based on our analysis of examination closures since the model's first run in 2016 through fiscal year 2019 ; the scores better predicted the rate at which returns were selected for examination .

see figure 3 .

the figure shows little relationship between model scores and change rates ; change rates remained relatively flat as model scores increase .

while change rates were slightly higher for the less than 1 percent of returns scoring 45 or above relative to lower - scoring returns , te / ge only examined 65 returns during fiscal years 2016 through 2019 that scored this high .

the overall correlation between model scores and change rates is - .02 .

a te / ge official said that it is not difficult to find a small issue on a return , which allows for a change regardless of score .

to attempt to measure the severity of an examination change , te / ge developed a weighted disposal score ( wds ) .

however , te / ge does not have documented criteria or justifications for how the weights were developed .

a te / ge official acknowledged that te / ge has not used wds because of questions about how consistently the weights have been developed .

if wds was to be used as a measure , te / ge would need to ensure the adequacy of the support for the related weights and scores .

according to te / ge's fiscal year 2020 program letter , the model relies on quantitative criteria , “which allows te / ge to allocate resources that focus on issues that have the greatest impact.” to the extent that a higher model score does not predict a higher change rate , the model is not selecting returns with the greatest impact .

further , taxes assessed per return also indicate that examinations are not having the greatest impact .

for fiscal years 2016 through 2019 , the examinations credited to the model averaged $2,460 in proposed tax assessments per return , compared with an average of $19,042 for the rest of the exempt organization examinations .

te / ge acknowledged that its scoring methods are limited because it does not utilize modern data practices .

it contracted for a study , to be completed in september 2020 , of alternative model architectures and scoring methods that incorporate best practices for using criteria and options for scoring returns .

as shown in table 3 , the form 990 model scoring did not account for most closed examinations and examination changes credited to the model during fiscal years 2016 through 2019 .

rather , examinations of “pick - up” returns and substitutes for returns ( sfrs ) accounted for most closed examinations and produced a higher change rate than examinations of primary returns scored by the model .

examiners find these other returns during examinations of returns identified by the model .

the higher change rates for pick - up and sfr returns compared to the primary returns identified by the model support te / ge's policy to examine all pick - up returns and sfrs that meet examination criteria .

however , this raises questions about how well the model identifies noncompliant returns .

given the lower change rate for the returns the model scored , the queries for noncompliance on the form 990 may not be effective .

while the model includes queries on noncompliance related to “pick - up” issues such as unfiled employment tax returns , the necessary data were not available to allow us to analyze how often these queries identified the primary return for potential noncompliance .

as discussed later , an analysis of queries could provide insight into the validity of the model .

internal control should be an integral part of an agency's operational processes and structure to help managers achieve their objectives on an ongoing basis .

when evaluating implementation , management determines if the control exists and is operational .

a deficiency in implementation exists when no such control is present or is not implemented correctly , preventing objectives from being met .

documentation is required to show the effective design , implementation , and operation of an internal control system .

the level and nature of documentation can vary based on the size of the agency and the complexity of its processes .

management exercises judgment in determining the extent of documentation that is needed .

te / ge has not fully implemented or documented internal controls for analyzing data for examination selection , meaning it cannot be assured that its selection decisions will produce the desired outcomes .

the internal controls range from two controls that te / ge adequately documented and implemented to seven others where te / ge did not .

the seven include five controls presented as sequential steps in using data for making selection decisions as well as two controls addressing timely documentation of internal revenue manual ( irm ) sections and risk management .

the first internal control te / ge implemented involved assessing staff competence .

to ensure competence in using data to make decisions , te / ge officials contracted with data specialists for modeling expertise to incorporate statistical and machine learning into examination selection .

bringing in this modeling expertise was an important step because exempt organization examinations staff , rather than statisticians or data analysts , initially developed the examination selection models , according to te / ge officials .

te / ge also provided documents on training and basic duties for staff when analyzing data .

what are internal controls and why do they matter ? .

one way federal agencies can improve accountability in achieving their missions is to implement an effective internal control system .

effective internal control comprises the plans , methods , policies , and procedures used to fulfill objectives on an ongoing basis .

it serves as the first line of defense in safeguarding assets and increases the likelihood that an agency will achieve its objectives while adapting to changing environments , demands , risks , and priorities .

effective internal control provides reasonable , not absolute , assurance that an organization will meet its objectives .

the second internal control involved communicating inside and outside of te / ge .

internally , te / ge staff could provide feedback through an online compliance issue submission portal in fiscal year 2018 .

submissions may become compliance strategies or model queries .

as for external communication , te / ge collaborated on data - related issues in an irs - wide group and with statistical specialists in the raas division .

for example , raas identified potential data sources for compliance issues and drew samples for certain compliance strategies to test rates of noncompliance .

in addition , to show how it communicates essential information with staff and outside parties , te / ge provided examples on disseminating guidance and examination accomplishments , including examination starts and closures .

te / ge did not fully implement and document internal control over the processes and data used to select returns for examination .

these processes cover five key steps for using data to decide which returns to select for examination ( see figure 4 ) .

effective internal controls would enable te / ge to show how feedback and lessons learned in step 5 can help it better determine how to create and use quality information ( step 3 ) and what decisions to make ( step 4 ) when pursuing the established objectives ( step 1 ) .

however , te / ge has not defined measurable objectives or undertaken regular evaluations to assess progress toward objectives .

although te / ge was able to describe its approach for accessing relevant and reliable data , processing those data into quality information and using the data to make decisions , it was not able to fully document how its control processes worked , as discussed below .

since its 2017 reorganization , te / ge has not established measurable objectives to select exempt organization returns for examination ( see figure 5 ) .

specifically , te / ge has not produced formal objectives that are aligned with its mission and the irs strategic plan , are expressed in quantitative terms , and are related to examination selection and program outcomes .

te / ge documents , including program letters and business performance reviews , refer to outcomes that could constitute objectives — such as improving the models and advancing data analytics to drive decisions about identifying and addressing existing and emerging areas of noncompliance — but they do not identify them as such .

te / ge officials acknowledged the need to establish measurable objectives .

they said their efforts are evolving and they need to improve analytical abilities to help assess the capacity for meeting objectives .

for example , one official said they are working to establish objectives at the onset of a compliance strategy .

without measurable and defined objectives , te / ge cannot effectively analyze how well it selects returns for examination and lacks a clear vision of what it is trying to achieve .

a lack of measurable objectives also hinders implementing other internal controls , such as evaluating performance or assessing risk , as discussed later .

the irm has procedures for processing form 990 data , which include controls over acceptance and transmission of the data ( see figure 6 ) .

te / ge provided data that showed error rates for electronically filed returns filed in 2019 were between 1 and 4 percent .

however , taxpayer or transcription error rates for paper returns filed in 2019 were between 19 and 32 percent of filed returns , depending on the version of the form 990 .

te / ge was not able to show that it regularly reviews and remediates such errors to ensure the reliability of forms 990 data .

however , under the taxpayer first act of 2019 , electronic filing of all forms 990 will be required for tax years starting july 2 , 2021 .

this change should remediate the known errors from paper - filed returns and increase data reliability .

we found several issues with te / ge's processing of queries in the form 990 , 990-ez and 990-pf models that affect the validity or reliability of the scores that the models generate to rank returns for examination selection ( see figure 7 ) .

as a result , te / ge cannot ensure that the model scores properly rank the returns for examination selection .

specifically , te / ge does not consistently assign point values for the queries used to generate the model scores and inform selection decisions .

we also found errors in te / ge's documentation of the queries , which lead to redundant queries , and inflated model scores .

finally , te / ge has no control procedures to ensure consistent testing of proposed queries .

inconsistent point values for queries raise concerns about model scores we estimate that for 24 percent of queries ( 83 queries ) from the models , te / ge staff did not assign point values for queries consistent with its definitions for the four categories ( see table 4 ) .

not implementing the defined point values puts the model scores at risk of inconsistent scoring and examination selection .

we found three types of queries involved with the inconsistent assignment of point values .

1 .

miscategorized queries were not assigned to the category that matches te / ge's definition .

these occurred because te / ge has not documented specific rules for query categorization .

as a result , we found an estimated 7.4 percent of queries ( 26 queries ) where te / ge staff overrode the category definitions when assigning points without documenting the reasons .

absent the reasons , te / ge cannot ensure consistent treatment of similar queries .

in our sample , these override decisions included assigning: three queries to the speculative category , which is worth five points , when the definitions supported the automatic category , which is worth 10 points .

te / ge officials said they did this to offset potentially confusing language in the return lines or instructions .

one query to the automatic category rather than the speculative category supported by the definitions .

te / ge officials said they used the higher point value category to increase the chance of selection so that certain form 990-pf attachments , which the queries do not cover , would be more likely to be considered for examination .

2 .

queries could fit into more than one category based on te / ge's definitions .

we estimate that 16 percent ( 55 ) of the queries could fit in more than one category .

of these , 18 in our sample could have been placed in the missing schedule / form category .

in addition , we found one query in our sample that te / ge labeled as having a duplicate but one query was assigned to the automatic category worth 10 points and the other was assigned to the inconsistencies category worth one point .

te / ge officials acknowledged that some queries could fit in more than one category .

when we asked why certain queries for missing schedules and forms were not categorized as such , these officials described a hierarchy of missing forms based on being subject to penalties and interest , such as employment tax returns , and their associated categories .

they did not document or consistently implement this hierarchy as queries identifying the same missing form sometimes were in different categories .

3 .

sliding scale queries whose point values differ from those stated in te / ge's model documentation .

we found nine queries with sliding scale point values that involved form 1099 information returns .

the sliding scales reduce point values based on the severity of the compliance issue , such as reducing the query point values if the organization filed a low number of information returns .

te / ge did not provide documentation about the rationale and associated definitions for these queries .

without documentation on the different treatment of these queries , te / ge is not transparent about the rationale for assigning points through a sliding scale to support its model scoring .

te / ge officials said they have not updated definitions and criteria for using the categories and sliding scales because of a decision to keep the model operating as is and to update documentation as time permits .

after our preliminary analyses , te / ge provided updated definitions for the four categories , and descriptions of the sliding scales that were used for queries .

however , these definitions and descriptions do not include any decision rules or criteria that document how to apply them .

further , the sliding scale descriptions do not offer definitions for words like “low,” when referring to the volume of information returns filed .

definitions that are incomplete and not always followed when assigning point values raise concerns about consistency and transparency in scoring returns for examination selection .

te / ge's assignments affect scores and whether a return is placed on the mss for examination consideration .

inconsistent or invalid assignment of point values may distort the potential for examination .

for example , of the nine miscategorized queries we analyzed in our sample , we determined that if their categorizations were corrected , hits on three of the queries would make a return eligible for the mss and hits on two others may make a return eligible , depending on the other queries the return hit .

changes to two queries would have made returns no longer eligible for the mss .

query documentation has errors that forestall valid analysis of queries we estimate that about 27 percent ( 96 queries ) of the queries in the models had errors in the documented descriptions .

query descriptions detail the logic and data used from specific forms and line numbers that the queries scan .

the errors we found include: references to older versions of the forms as well as omissions of form lines used in the query ; and query descriptions that did not match programming code .

to address these differences , te / ge proposed corrections to the query descriptions .

a te / ge official said re - visiting the query documentation is part of the contractor's 2020 study and that te / ge does not have a timeline for correcting the documentation .

in addition to errors , the descriptions also use inconsistent language , which prevents easy identification of queries by issue .

for example , to identify all queries related to excess benefit transactions , one must manually search different fields for terms such as “excess benefit,” “excessive benefit,” and “ebt” ( excess benefit transaction ) .

furthermore , te / ge's database fields only capture one issue per query .

since many queries involve multiple issues , these fields cannot be used to fully inventory the queries .

these errors and inconsistencies in the query descriptions occurred because te / ge has no procedures for regular reviews of queries as forms or laws change .

te / ge compliance governance board ( governance board ) members review query descriptions prior to implementation but do not review details of the queries in the context of the entire model .

further , te / ge procedures only require review of programming code before queries are sent to the governance board .

review of the code once it is integrated into the model program is optional , according to te / ge procedures .

the errors and inconsistent descriptions prevent te / ge from having a comprehensive and accurate inventory of queries within and across models .

without regular reviews , te / ge cannot be assured that its programming code is correct and that any analyses of the performance of queries or the models as a whole are valid .

when we asked about the lack of regular reviews of queries , te / ge officials said they plan to implement reviews but did not provide us with a plan or timeframes for doing so .

another effect of not having a comprehensive and accurate inventory is that te / ge cannot analyze query performance and identify queries that look for the same compliance issue to prevent redundancies and to ensure valid and consistent scoring .

as a result , we found queries that address the same or similar issues with the same criteria , inflating scores for returns and making selection for examination more likely .

our analysis of the july 2019 form 990 model run showed 90 pairs of queries , involving 78 unique queries that hit together at least 90 percent of the time .

by having two queries that rely on the same criteria , returns accumulate extra points for the same behavior .

for example , all 910 returns that hit an employment tax query also hit a query that shares some of the same criteria and thresholds .

as a result , these returns accumulated 10 points rather than five points , making them eligible for the mss .

aside from our sample , we found queries seeking certain organizations with political campaign activities and political expenditures that would total 15 points in the form 990 model .

queries identifying these same activities and expenditures would total 30 points in the form 990-ez model .

tege's contractor recommended in 2018 that te / ge eliminate “redundant” queries , which is similar to our finding .

te / ge officials said they do not believe the redundant queries are duplicates and they are awaiting the results of the contractor's study in 2020 before making changes .

until te / ge resolves the extent to which it has redundant queries , it cannot do a valid analysis of whether its queries identify the most noncompliant returns .

te / ge lacks procedures and criteria for testing proposed model queries te / ge has no procedures requiring the testing of proposed model queries .

even so , based on our sample of the new queries in the fiscal year 2018 form 990 model , te / ge would be able to provide evidence of tests for an estimated 94 percent of all new queries .

however , te / ge also does not have procedures for how to conduct testing or what data to use .

the testing that has been performed consisted of running the query on certain tax years of returns to count the number of returns flagged , according to a te / ge official .

te / ge does not run the queries on data from closed examinations to see whether the queries would identify known compliance issues that justify an examination .

interactions with existing queries are not tested .

when considering new queries , governance board members see the number of returns flagged by each query during testing , but have no criteria to determine whether a query flags an appropriate number of returns .

a te / ge official said te / ge does not believe it needs to document procedures for testing .

in the absence of procedures and standards , te / ge cannot ensure that testing of new queries is done consistently with appropriate data sources and research standards .

by only testing the number of returns that a query flags , te / ge cannot validate that proposed queries can effectively identify the noncompliance that would be worth examining .

using tested , validated and documented data is a critical step in ensuring that research is proper , reliable , and accomplished in accordance with expectations , according to the irm .

without testing queries on reliable data , and making adjustments based on criteria , te / ge risks implementing queries that do not produce reasonable numbers of hits that are worth pursing through examinations .

for examination sources that used data other than the models , we found that te / ge did not always document its processing of data into quality information .

we identified common “start - to - finish” segments to this processing of data , including: submitting a proposal and supporting data to find noncompliance ; reviewing the potential data sources and queries or thresholds to be used as examination selection criteria ; and recommending the proposed effort for approval through the appropriate executives .

on one hand , te / ge provided documentation of the required approvals for these segments in processing data for five compliance strategies .

these strategies included examining loans by private foundations and collecting information on organizations that exceed investment income limitations .

on the other hand , te / ge did not provide similar start - to - finish documentation on processing quality information from other examination sources that use data outside of the models ; examples include research projects under the data driven approaches portfolio and projects that use queries under the referrals and other casework portfolio ( see table 1 ) .

over several discussions , te / ge did not explain why it did not fully document such projects .

by not fully documenting how it processes data into quality information , and by not linking such processes to measurable objectives , te / ge cannot ensure that it is analyzing quality information in selecting examinations .

for the compliance strategies , te / ge showed evidence of using the quality information to decide which returns to select for examination , such as for governance board decisions .

however , te / ge did not provide documentation on how it made selection decisions using data for other projects that use queries ( see figure 8 ) .

in addition , te / ge did not use quality information to decide how frequently to run the model .

te / ge decided to run the form 990 models twice per year without analyzing the effects .

moreover , we found that the time between runs is inconsistent .

since the form 990 model's first run , the time between runs ranged from 84 days to 251 days .

since returns are ranked on the mss , eliminations result in the classification staff selecting lower scoring returns .

the average score for examined returns was 27.1 for the list that was used for 84 days compared to 23.2 for the list used for 251 days .

to the extent that te / ge ensures that its model scores are as reliable and valid as possible , analyzing data could help te / ge identify the frequency of model runs that maximizes the use of model scores to guide decisions on examination selection .

for example , analyzing form 990 filing patterns could help identify the optimal timing of model runs , allowing for adequate time remaining under the statute of limitations .

te / ge does not regularly evaluate its models and other selection processes that use data .

in particular , model scores for all returns are not retained or are inconsistent from year to year which limits the ability to conduct evaluations .

furthermore , te / ge does not evaluate reasons why some selected returns are not examined , which could help improve selection methods ( figure 9 ) .

te / ge has no system for regularly evaluating examination selection decisions te / ge has not regularly evaluated its examination selection decisions that rely on data to improve its selection methods .

while te / ge commissioned the contractor evaluations of its form 990 model , it has no documented process for continued evaluations of the model or any evaluations of other sources , such as research projects , that rely on data to select returns for examination .

for its compliance strategies , not enough examinations have closed under the strategies to warrant evaluations yet , according to a te / ge official .

data limitations have challenged evaluation efforts , according to a te / ge official .

to address this , te / ge started capturing more detailed data on examination outcomes ; however no evaluations of outcomes have resulted .

the officials noted that they have been spending more time reporting and monitoring compared to analyzing and evaluating , which they said needs to occur more often .

without evaluation , te / ge cannot ensure that its use of data to select returns is working as intended .

in addition to not evaluating selection decisions and their outcomes , te / ge has also not addressed the form 990 model deficiencies the contractor previously identified .

in its 2018 reports , the contractor made 17 recommendations ( see appendix v for the status of each recommendation ) .

a te / ge official said it had not acted on many recommendations because all examination selection strategies are being evaluated with the transition to the compliance planning and classification ( cp&c ) office .

te / ge initiated another study in 2019 with the same contractor to address its 2018 recommendations among other tasks .

as of march 2020 , te / ge implemented one recommendation and part of another , deferred action on nine recommendations until after the contractor finishes the new study , deferred action on three due to other reasons , and did not clearly provide a status for two .

in addition , te / ge will likely not implement the other recommendation .

according to contract documentation , the study will explore architectures and alternative designs to the model , propose up to three compliance actions other than examinations , and recommend measures to monitor the actions' effectiveness .

te / ge expects a final report by september 2020 .

to the extent that te / ge has not implemented the contractor's recommendations from 2018 , the related deficiencies identified in the form 990 model will have persisted for more than 2 years by the time the contractor issues its 2020 report .

unless te / ge documents its consideration and action of the recommendations , the value of the contractor's work is diminished and possible improvements may be overlooked .

te / ge has not retained complete data to allow for full evaluation of its models until recently , te / ge did not retain model scores for each return and query performance data that would be useful for evaluation .

the january 2018 contractor report recommended that te / ge save model data .

for its july 2018 report , the contractor had to recreate historical scoring data for its evaluation .

te / ge officials said they increased storage space and saved the fiscal year 2018 data .

when we asked for these data , te / ge officials said that each time they run a model , they overwrite the old data .

the officials said they did not have space on their server to save all of the data .

instead , te / ge had been saving the mss's for each run .

however , the msss have only limited value for evaluating the model and queries .

specifically , the mss for each model run contains score information for only about 20,000 returns ( out of about 300,000 scored ) that have a certain minimum score and hit queries in certain categories .

further , the mss does not contain data on model queries that are flagged .

in september 2019 , te / ge officials said the research , applied analytics and statistics division provided temporary server storage space to save model data through september 2020 while the contractor assesses te / ge's models .

starting with the july 2019 model run , te / ge is saving score and query performance data for all filed returns .

in january 2020 , te / ge officials told us they developed a way to save data on query hits for all returns run through the model .

however , te / ge has not provided documentation to show exactly what data will be saved over the long term for all filed returns run through the model .

without complete historical data on model scores and query hits , te / ge cannot assess the full performance of its models .

such data would facilitate an analysis of the queries , and whether they identified returns with changes or related pick - up returns .

historical data on examination outcomes lack consistency , which complicates evaluation te / ge does not analyze consistent multi - year data on examination outcomes , which would facilitate evaluation of its use of data in selecting returns for examination .

te / ge officials said they use historical data — such as change rates — to determine the success of an examination source .

te / ge provided historical data on examination starts , closures , and pick - up returns covering 2 years but did not provide data beyond that and change rates were not always included .

further , te / ge has used different methods to organize and report examination outcomes over the years .

these differences in reporting outcomes affect te / ge's data in the following ways: starting with fiscal year 2018 , data on exempt organizations examinations include federal , state and local employment tax examinations .

prior to 2018 , te / ge reported these employment tax examination data separately .

after te / ges reorganization in 2017 , it grouped examinations into portfolios and changed the portfolio definitions during 2018 .

as of march 2020 , te / ge has not produced a consistent method of summarizing of historical data .

te / ge officials acknowledged data limitations , and said they are working to implement recommendations from a 2019 study to improve capturing examination data .

te / ge officials said the staff member analyzing data has been doing so for many years , allowing them to reconcile the data .

however , this poses a risk that other irs or oversight entities cannot reconcile the data .

according to internal control standards , agencies should establish effective methods for retaining organizational knowledge and mitigate the risk of having that knowledge limited to a few personnel , as well as contingency plans to respond to sudden personnel changes .

te / ge's inconsistent data limit its ability to conduct evaluations .

these inconsistent data also prevent te / ge from establishing baselines or targets for examination outcomes such as change rates to help measure the success of its selection methods .

in fiscal year 2019 , te / ge did not examine about 20 percent of the exempt organization returns that had been selected for examination .

although this rate of non - examined returns has improved in recent years , te / ge has not analyzed data to explore why the rate has improved and how to reduce it further .

our analysis showed that almost 30 percent of these returns were not examined because they were too close to the statute of limitations date .

te / ge officials did not have a reason why the returns were sent to the field for examination if the statute date was so close .

te / ge officials said they do not regularly analyze reasons for non - examined returns .

they said they have analyzed only the number of non - examined returns by manager and area .

in addition , te / ge officials said they implemented new guidance in fiscal year 2019 for staff who make decisions to not examine returns , which is intended to improve the information they have on these decisions .

as of fiscal year 2019 , te / ge began tracking certain non - examined returns by project code but has not committed to analyzing the data .

non - examined returns are not an efficient use of resources , as the time spent reviewing and rejecting these returns — even if minimal — reduces the time staff have for conducting examinations .

routinely analyzing reasons for non - examined returns , as well as related data , could help te / ge identify actions to reduce the number of returns that are sent to the field but are then declined for examination by a manager or examiner .

te / ge did not annually update procedures on examination selection and databases in certain irm sections since the may 2017 reorganization .

the internal revenue manual ( irm ) states that procedures in irm sections must be annually reviewed and updated as needed .

te / ge released updated irm sections for two of the three groups in cp&c .

it released a section on the issue identification and research in september 2018 , and one on the classification and case assignment procedures in september 2019 .

however , these sections do not cover the steps the model classifier takes when reviewing returns from the mss .

as of december 2019 , no irm section has been released on the planning and monitoring group .

as such , te / ge staff does not always have official information on roles and responsibilities for new entities and processes created since may 2017 .

for certain updated or new irm sections , te / ge did not release interim guidance while those sections awaited approval .

irs requires issuance of interim guidance to address deviations from the irm , even if temporary .

instead of developing interim guidance , te / ge officials stated that , in the wake of the reorganization , they decided to use desk guides , such as for the irm section on classification and case assignment processes .

however , te / ge did not update its desk guides on processes until more than 2 years after the reorganization .

furthermore , the desk guides do not cover the specific duties of the model classifier , or the steps for classification of returns identified for compliance strategies .

irm guidance states that management must develop and maintain documentation on data systems ; collection and analysis ; and responsibilities for data collection , input and analysis .

timely documentation of new procedures and responsibilities improves the accuracy and reliability of irm content .

according to the irm , when the irm and related guidance are not current , te / ge increases the risk that staff follow incorrect procedures , use guidance that is not transparent to the public , administer tax laws inconsistently , and misinform taxpayers .

good federal government practice requires risk management , without which , te / ge could undercut its use of data to enhance decisions on examination selection .

although the use of data in examination selection has the potential to improve efficiencies in classifying and examining returns to identify noncompliance , any new endeavor carries risks .

te / ge did not identify any te / ge - specific risks that could undercut its success in using data to select exempt organization returns for examination .

as of december 2019 , the te / ge risk register identified 12 risks , ranging from aging technology and infrastructure to employee engagement and morale .

one risk — data access and analytics — involved using data in general decision making at the irs level rather than te / ge decisions about examination selection or its related models .

te / ge officials said they are analyzing and responding to this risk under the irs - wide risk management process .

te / ge did not document why it did not identify any te / ge - specific risks in using data for examination selection .

our report discusses a number of deficiencies that could be potential risks to te / ge using data in selecting returns for examination .

for example , te / ge lacks program objectives that would be necessary to identify and assess risks .

we also found weaknesses in how te / ge processes and analyzes data to inform examination selection and how it evaluates selection decisions .

further , the irm states that te / ge's compliance governance board ( governance board ) should consider risks in its decisions and we saw that risks were considered in documents proposing examination selection criteria to the governance board .

we did not find evidence that te / ge's risk management process recorded these risks for analysis and any response if needed .

after we shared our concerns about the lack of identified risks , te / ge officials noted that te / ge participates in mitigation steps as identified by the irs risk office .

te / ge officials also mentioned cp&c representation in an irs pilot program designed to explore ways to better select employment tax cases .

while such actions could be a component of a risk management strategy , it is incomplete and it is unclear how this initiative would help te / ge identify , analyze , and mitigate risk .

not identifying and managing risks identified in this report leaves te / ge open to errors and examination selection decisions that are potentially not transparent or not fair .

as such , without objectives and a consistent and documented process for identifying and managing risks , te / ge cannot effectively address risks that may hamper its efforts to use data to enhance its compliance work .

increasingly constrained resources underscore the importance of te / ge's efforts to efficiently identify and examine exempt organization returns that have the highest noncompliance potential .

te / ge has developed ways to use data to aid in examination selection .

however , opportunities exist to strengthen internal controls to help ensure that data used are reliable , decision rules are clear and documented , and objectives are identified and being achieved .

te / ge should take several steps to improve the reliability and validity of the models .

these steps include improving documentation of decision rules and criteria for scoring ; regularly reviewing model documentation and programing ; testing new queries and their interaction with existing queries ; retaining model and query data ; and periodically evaluating the performance of selection methods .

in absence of regular evaluation of its examination selection decisions , te / ge misses opportunities for improving its selection processes .

deficiencies that te / ge's contractor already identified provide an opening for improving its models .

without consistent historical data , te / ge will be limited in assessing progress and making improvements .

a review of the reasons why certain returns selected for examination are not examined is an example of an evaluation that could help inform process improvements .

ensuring that all procedures are current and accurate would reduce the potential for employees following incorrect procedures and administering tax laws inconsistently .

te / ge's lack of identified risks from using data in examination selection precludes te / ge from analyzing and responding to those risks .

by taking actions to further strengthen these internal controls , te / ge could enhance its efforts to identify and examine the most noncompliant exempt organizations and enhance irs's oversight of tax exempt organizations and help maintain the integrity of the charitable sector and the larger exempt community .

we are making the following 13 recommendations to irs: the commissioner of internal revenue should document measurable objectives for using data in selecting exempt organization returns for examination .

 ( recommendation 1 ) the commissioner of internal revenue should document and consistently use clear criteria and decision rules on assigning point values to queries , using categories and sliding scales .

 ( recommendation 2 ) the commissioner of internal revenue should require a regular review of query descriptions and programming to ensure their accuracy and minimize queries that flag the same or similar compliance issue .

 ( recommendation 3 ) the commissioner of internal revenue should develop procedures and criteria to test new queries prior to implementation in the models .

 ( recommendation 4 ) the commissioner of internal revenue should more fully document how te / ge processes data and uses data to make examination selection decisions for sources outside of the model such as research projects and other projects that use queries .

 ( recommendation 5 ) the commissioner of internal revenue should conduct an analysis to identify the optimal interval between model runs .

 ( recommendation 6 ) the commissioner of internal revenue should establish a process for regularly evaluating selection decisions and related outcomes for the models and other processes that use data to select returns for examinations .

 ( recommendation 7 ) the commissioner of internal revenue should document consideration or action on recommendations from its 2018 and 2020 contractor assessments .

 ( recommendation 8 ) the commissioner of internal revenue should document how score and query data for all returns in the models will continue to be saved over the long term .

 ( recommendation 9 ) the commissioner of internal revenue should ensure that historical data on examination outcomes are consistently defined and used when doing analysis of examination outcomes .

 ( recommendation 10 ) the commissioner of internal revenue should routinely analyze the reasons for not examining selected returns and identify any necessary actions to address the reasons .

 ( recommendation 11 ) the commissioner of internal revenue should annually review and update procedures as needed in relevant irm sections on examination selection and issue interim guidance until the affected irm sections are updated .

 ( recommendation 12 ) the commissioner of internal revenue should document why te / ge has not identified any risks in its risk register for using data to select exempt organization returns for examination .

if risks are subsequently identified , te / ge should document how it plans to analyze and address them .

 ( recommendation 13 ) .

we provided a draft of this report to irs for review and comment .

irs provided written comments , which are reproduced in appendix vi and summarized below .

of our 13 recommendations , irs agreed with 12 and disagreed with one .

irs also provided technical comments , which we incorporated as appropriate .

irs disagreed with our recommendation on ensuring that historical data on examination outcomes are consistently defined ( recommendation 10 ) , pointing out that its raw data are consistently defined in its information systems .

our concern , however , is with how the outcome data are reported and analyzed , which inhibits understanding of outcome trends over time .

in response to irs comments , we added language to the final recommendation to more clearly focus on the consistency of the outcome data used and analyzed over the years .

in addition , although irs agreed with our recommendation to more fully document how te / ge processes and uses data to make examination selection decisions outside of the model ( recommendation 5 ) , irs said that it would provide documentation on a project ( other than compliance strategies ) that is approved by the governance board .

while we look forward to such documentation , we are primarily interested in irs documenting a system for how it processes and uses data to select returns for examinations for projects outside of the model , regardless of governance board approval .

as discussed in the report , irs has such a system for projects in its compliance strategies portfolio , which could provide a framework to follow .

similarly , irs agreed to analyze return due dates of the filing populations commonly associated with the examinations ( recommendation 6 ) .

we will be interested to see how that analysis helps irs to determine the optimal interval between model runs , which is the focus on our recommendation .

we are sending copies to the appropriate congressional committees , the secretary of the treasury , the commissioner of internal revenue , and other interested parties .

in addition , this report is available at no charge on the gao website at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-9110 or mctiguej@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made key contributions to this report are listed in appendix vii .

this report assesses ( 1 ) the use of data to select tax - exempt organization returns for examination ; and ( 2 ) the process the tax exempt and government entities ( te / ge ) division has established to select returns for examination .

to assess the use of data to select tax - exempt organization returns for examination , we reviewed data from the internal revenue service's ( irs ) returns inventory and classification system ( rics ) for fiscal years 2016 to 2019 .

table 5 defines the variables and measures we analyzed .

we analyzed aggregated data at the project code level , and we grouped project codes by examination source ( for example , examinations from referrals occurred under several project codes ) .

based on our testing of the data and review of documentation and interviews , we determined that the data were reliable for purposes of assessing te / ge's selection processes .

we analyzed outcomes from the form 990 , return of organization exempt from income tax model .

we used rics data and model score sheets ( mss ) , for examinations closed from october 1 , 2015 through september 30 , 2019 .

each model run generates an mss , which is a ranked list of form 990s that hit certain types of queries and have a minimum score .

we matched form 990 scores from the mss with selection information and examination outcomes in rics for examinations closed under all project codes , though the data presented in objective one is specific to examinations started under the form 990 project code .

we used source codes — which indicate whether an examination was a pick - up , substitute for return or primary return — to analyze what types of examinations produced the highest change rates under the form 990 model project code .

to inform this work , we reviewed recent te / ge contractor assessments of exempt organization examination selection and the form 990 model .

to assess the process that te / ge has established to select returns for examination , we reviewed internal controls steps in standards for internal control in the federal government ( green book ) .

given te / ge's emphasis on using data in examination selection , we identified five internal control steps related to analyzing data to select returns for examination to address our objectives .

we selected four other internal controls because they constitute practices common to all five steps in the selection process .

these are presented in table 6 .

define objectives in measurable terms so performance in achieving objectives can be assessed .

 ( green book ( gb ) 6.04 ) obtain relevant data from reliable internal and external sources in a timely manner based on identified information requirements .

 ( gb 13.04 ) process the obtained data into quality information that supports the internal control system ( i.e. , using data in decision making ) ; use quality information to achieve the entity's objectives ; and document policies on the responsibilities for data collection , input , and analysis .

 ( gb 13.05 , 13.01 , and 12.02 ) use the quality information to make informed decisions in achieving key objectives .

 ( gb 13.05 ) evaluate performance ( outcomes ) for key objectives and take actions to remediate deficiencies .

 ( gb 13.05 , 16.03 , and 17.06 ) develops , maintains , and updates in a timely fashion documentation on the responsibilities for data collection , input and analysis for using data in decision making .

 ( gb 12.02 and 12.05 and irm ) defines risk tolerances in specific and measurable terms , considers internal and external factors to identify risks , analyzes risks to estimate significance , and designs specific actions for response .

 ( gb 6.09 , 7.04 , 7.05 , and 7.09 ) ensures that personnel possess competence to meet responsibilities as well as understand the importance of effective data analysis in decision making .

 ( gb 4.04 ) communicates necessary information to enable personnel to perform key roles for analyzing data in decision making and with external parties .

 ( gb 14.03 and 15.20 ) to identify criteria specific to irs , we reviewed the internal revenue manual ( irm ) , which provides standards and guidance similar to the criteria we identified .

we shared the green book and irm criteria with te / ge , as well as our expectations of the documentation that would show adherence to these criteria .

our assessment focused on examination sources developed after the 2017 reorganization and sources that rely on data for selection ( such as models and projects that use queries ) .

examination sources that did not rely on data , such as claims , were not assessed .

we reviewed the referrals classification process to consider how data might be used to enhance it .

we analyzed te / ge documents such as program letters , business performance reviews , desk guides , memorandums , work plans , performance data , contractor reports and training documents .

in addition , we assessed documents — such as meeting minutes and research results — showing the development and approval of data queries and projects used in examination selection .

we reviewed the msss for the form 990 model , and procedures for the form 990 model , the form 990-ez , short form return of organization exempt from income tax , and form 990-pf , return of private foundation , models .

we selected a generalizable stratified random sample of 114 of the 354 unique queries in the three models ( see table 7 ) .

because we followed a probability procedure based on random selections , our sample is only one of a large number of samples that we might have drawn .

since each sample could have provided different estimates , we express our confidence in the precision of our particular sample's results as a 95 percent confidence interval ( eg , the margin of error is + / - 10 percentage points ) .

this is the interval that would contain the actual population value for 95 percent of the samples we could have drawn .

our sample is designed to control the margin of error of attribute estimates within the overall scope query sample as well as the combined form 990 query sample ( a combination of strata 2 and 4 plus certainty selections ) .

the sample was designed as follows .

there is one certainty stratum for form 990-pf queries where we selected a 100 percent sample ( i.e .

a census ) , and this stratum does not have a margin of error .

we selected these queries with certainty because of the smaller population size in this stratum .

for remaining strata , we selected the necessary sample size to achieve an overall 95 percent confidence interval for attribute ( percentage ) estimates with a margin of error of about + / - 10 percentage points under proportionate allocation .

in addition , the sample size was increased in strata 2 and 4 ( combining form 990 model queries ) to achieve the necessary sample size for a 95 percent confidence interval with a margin of error of about + / - 10 percentage points within this group .

for the sampled queries , we compared their category and descriptions as provided in the model documentation , with te / ge's definitions of the categories to assess whether the query was categorized appropriately .

we also compared the query descriptions with the forms to assess whether the referenced lines were relevant to the query .

additionally , we reviewed the model programming code to check for errors and consistency with the query descriptions .

for query categorizations that did not match te / ge's definitions or queries that appeared to have errors in the descriptions or programming , we asked te / ge to review and explain its decisions .

to identify potentially redundant queries , we analyzed output from the july 2019 form 990 model run , the only one available at the time of our analysis .

within our sample , we reviewed 36 of the 104 newly added queries in the fiscal year 2018 model .

specifically , we reviewed approval documentation and meeting minutes to test whether two levels of management and the compliance governance board approved new queries , consistent with te / ge procedures .

we also reviewed evidence that te / ge tested each query prior to its approval for inclusion in the models .

we held two telephone focus groups with the nine classifiers who review exempt organizations referrals .

we asked questions about the data and resources they use to classify referrals , how they convey their results , and how they are provided feedback .

we interviewed officials from the compliance planning and classification office and irs's research , applied analytics and statistics division who worked on several compliance research initiatives .

we met regularly with te / ge to share ongoing assessments .

we conducted this performance audit from november 2018 to june 2020 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the figure below shows the text of the 2019 version of form 990 , return of organization exempt from income tax .

a list of schedules for the form 990 is provided in table 8 following the form .

the remaining pages of the form 990 are available at irs's website , accessed march 23 , 2020: https: / / www.irs.gov / pub / irs - pdf / f990.pdf .

most exempt organizations are required to file an annual form to report their activities , structure , revenue and expenses , and other items .

the organization's classification under the internal revenue code and its gross receipts and total assets , determines which form must be filed .

most organizations file one of the following: form 990 , return of organization exempt from income tax ; form 990-ez , short form , return of organization exempt from form 990-pf , return of private foundation or section 4947 ( a ) ( 1 ) trust treated as private foundation .

the internal revenue service ( irs ) last redesigned the form 990 series for tax year 2008 .

the redesign added 14 schedules to the existing two , and reflected changes in the tax - exempt sector and tax law .

some changes from the redesign were phased in and implemented for tax year 2008 and 2009 filings .

we summarized changes as found in the “what's new” section of the form instructions for each of the three form 990 types and for each year .

we grouped the changes into two categories as defined by: new or revised question ( s ) : the addition of new lines , check boxes , narratives or schedules .

this includes changes to accommodate new laws or reporting requirements , such as new reporting thresholds or standards .

instructions and format: new descriptions or details in the instructions , such as specifying examples or how to provide certain information to irs .

this also includes changes that affect order of lines or schedules , but not the content .

for the form 990 changes since the redesign , irs made 56 changes to the form or its instructions for tax years 2009 through 2019 ( see table 9 below ) .

these changes include three to the 2018 form implementing new excise taxes on net investment income of certain colleges and universities and on certain tax - exempt organization executive compensation .

aside from new electronic filing requirements for tax years beginning july 2 , 2019 , the 2019 form did not have any changes .

in addition to the 56 changes , irs made 95 clarifications to existing lines or instructions , or revisions to definitions from tax years 2009 through 2018 .

these clarifications provide more specific definitions or other details .

further , several of the schedules had additions .

for example , the patient protection and affordable care act led to additional reporting on schedule h , hospitals , to fulfill requirements that hospitals report on each of their facilities and conduct a community health needs assessment every 3 years .

most of the form 990-ez's 27 changes occurred in tax years 2009 through 2012 , of which 12 were for 2011 and several of them focused on compensation reporting .

irs also made 27 clarifications for 2009-2013 .

public law 115-97 did not affect form 990-ez .

there were no changes to the 2019 form .

see table 10 .

for the form 990-pf , irs made the fewest changes compared to forms 990 and 990-ez , with only 11 changes and four clarifications for tax years 2009 through 2019 .

the form 990-pf had three changes prompted in 2018 by public law 115-97 .

electronic filing requirements apply to form 990-pf for tax years starting july 2 , 2019 , but there were no other changes for the 2019 form .

see table 11 .

appendix iv describes the general examination selection process for exempt organization returns , and specific classification steps that apply to certain returns .

the annual work plan is the foundation for identifying and assigning returns for examination .

the compliance planning and classification ( cp&c ) office follows various steps to identify returns to fulfill the work plan , which end in the assignment of returns for potential examinations to field work groups .

the intended process is in figure 11 and discussed below .

annual work plan .

cp&c's planning and monitoring group develops the annual work plan .

the work plan provides estimates of examination starts and closures .

it also has estimates for the number of hours to be spent per return examination and the number of days to complete an examination .

planning and monitoring develops estimates at the project code level , which corresponds to a specific examination source or project such as the form 990 model .

the tax exempt and government entities' ( te / ge ) compliance governance board approves the work plan .

te / ge provides a summary of the work plan in its annual program letter .

stocking report .

the planning and monitoring group uses the work plan to issue “stocking” reports to guide classifiers on types of returns to identify for potential examination .

planning and monitoring considers available examiners , and progress in meeting work plan numbers .

the report lists the number of returns needed by grade , project code , and classification source .

classification .

classifiers review stocking plans to identify returns for potential examination .

classifiers are to eliminate returns for consideration if the ( 1 ) return is approaching its statute of limitation date , ( 2 ) organization has been examined in the last 3 years , or ( 3 ) organization is under a compliance check .

establishing the return and initial case building .

if classifiers identify examination potential , they establish returns in the audit information management system and reporting compliance case management system ( rccms ) .

the returns are sent for initial case building — developing paperwork to initiate the examination — according to a te / ge official .

virtual shelf .

established returns and the initial case material are sent to the virtual shelf , which is an electronic inventory of returns that may be assigned for examination .

certain referrals , claims , compliance strategies , and other returns are prioritized , according to a te / ge official .

returns remain on the shelf until assigned for examination or otherwise closed due to statute of limitations , according to a te / ge official .

examination assignment .

functional assignment coordinators pull returns from the virtual shelf to fulfill field group work requests .

returns on the virtual shelf that matched a work order undergo additional case building before delivery to field examination groups .

monitoring .

planning and monitoring staff regularly review reports that compare work plan goals with current work , and run algorithms to forecast upcoming work .

these reviews are intended to ensure that sufficient work is available for assignment , excess work is not created , and returns approaching statute of limitations are identified .

the monitoring informs new stocking reports .

classification steps vary depending on how a return was identified for potential examination .

for returns identified with queries or models , classifiers check a limited set of criteria once a return is identified .

for returns identified through other sources , such as referrals , the classifier also reviews facts and circumstances about potential noncompliance in returns .

we focus here on examination sources that rely on data — such as models or queries — and referrals .

referrals are complaints of exempt organization noncompliance made by third parties , including the public and other parts of the internal revenue service .

we describe referrals classification because it is one of the top sources of exempt organizations examinations .

the models are run to identify returns with potential noncompliance and lists them on a model score sheet ( mss ) .

the mss is a ranked list of returns by scores from the model .

according to a te / ge official , the classifier: works down the list , starting with the highest scores , to fill stocking checks whether the return was also identified for a compliance strategy ; and .

eliminates returns based on the statute of limitations and recent examination activity .

for some projects in the compliance strategies portfolio , a query is run or returns are sampled to identify a population meeting indicators of potential noncompliance .

then , the classifier uses the stocking report to select returns with certain geographic or case grade criteria and eliminates returns based on statute of limitations , recent examination status , and resolving non - filing issues , according to a te / ge official .

te / ge classifiers do a triage to review and eliminate referrals that are not relevant to tax administration or do not have substantiated information .

the triage classifier sorts referrals and reviews the following: organization status ( for example , already revoked or terminated ) ; examination history of the organization ; and evidence of substantial inurement or private benefit , non - exempt activities , or material employment tax or unrelated business income that would result in a significant tax assessment .

referrals that pass triage are either sent to classification or , if they deal with political issues , are sent to a committee of three te / ge managers , who vote on a selection decision .

for all referrals , the classifier researches the referral .

research sources include websites , external databases , and irs taxpayer account databases .

the classifier may look at the organization's website , information about officers , or prior examination history .

referrals with examination potential are either assigned immediately or placed on the virtual shelf .

referrals that must be immediately assigned include those with strong indicators of fraud , illegal or illicit activities ( including terrorism ) , or referrals from whistleblowers , or certain other irs divisions .

other referrals are labeled as high , medium or lower priority , based on potential for revocation or significant tax assessments .

the tax exempt and government entities division ( te / ge ) hired a contractor to review the effectiveness of its form 990 examination selection model .

the contractor prepared two reports .

the first , delivered in january 2018 , makes recommendations on the model process , the computing environment , and performance measures .

the second , delivered in july 2018 , makes recommendations to more effectively and efficiently identify returns for examination , such as through the model .

within the two reports , the contractor made 17 recommendations .

table 12 lists the recommendations and the status of each .

in september 2019 , te / ge initiated another study , anticipated to be completed in september 2020 .

the study focuses on developing alternatives to enhance the models .

the study will explore architectures and alternative designs for the model and propose alternative compliance actions to examinations and recommend measures to monitor their effectiveness .

in addition to the contact named above , tom short ( assistant director ) , lindsay swenson ( analyst - in - charge ) , ann czapiewski , george guttman , amalia konstas , krista loose , alan rozzi , cynthia saunders , andrew j. stephens , and sonya vartivarian made key contributions to this report .

