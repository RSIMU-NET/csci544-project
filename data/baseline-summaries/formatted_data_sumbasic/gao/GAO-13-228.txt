how federal agencies manage toward better performance has a significant effect on many of the american public's most pressing concerns — ranging from unemployment to food safety to national security .

however , our previous work has shown that many federal agencies have struggled to adopt effective performance management practices .

specifically , our work over the past 15 years has indicated that using performance data for decision making can lead to better results , but in several surveys we have done , less than half of federal managers reported using performance data for decision making to a great or very great extent .

in looking for solutions to this long - standing problem , congress identified an effective management tool that has been widely adopted by local and state governments — data - driven performance review meetings , often referred to as “stat” meetings .

congress took steps to improve federal performance management with the passage of the government performance and results act modernization act of 2010 ( gprama ) , which included a specific provision for quarterly performance reviews , modeled after those at the local and state level .

as part of our mandate to review the implementation of gprama , this report ( 1 ) identifies practices that can promote successful data - driven performance reviews at the federal level and examines how these reviews are being implemented at selected agencies and across the government , and ( 2 ) examines the impact of quarterly data - driven performance reviews on selected agencies' progress toward high priority and other performance goals .

this report is the second in a series that examines how agencies are implementing various gprama requirements .

to address the first objective , we identified practices that can promote successful data - driven reviews at the federal level by conducting a review of relevant academic and policy literature , including our previous reports .

we refined these practices with additional information obtained from practitioners at the local , state , and federal level who shared their experiences and lessons learned .

we also compared these practices with recent gprama related guidance in the office of management and budget's ( omb ) circular no .

a - 11 and found them broadly consistent .

we observed two data - driven review meetings at the department of the treasury ( treasury ) , which was one of the agencies selected to address our reporting objectives .

we also examined how these reviews are being implemented at agencies across the government by conducting a survey of performance improvement officers ( pio ) in the 24 agencies covered by the chief financial officers act of 1990 and subject to gprama's requirements .

we received responses from all 24 pios — a 100 percent response rate .

to address both objectives , we selected three agencies to examine implementation of gprama - mandated quarterly performance reviews in greater depth — department of energy ( doe ) , small business administration ( sba ) , and treasury .

we selected these three agencies because they have been performing data - driven reviews for at least one year and , together , use a mix of government tools — such as direct service , regulations , grants , loans , and tax expenditures — to achieve their performance goals , among other reasons .

at each selected agency , we focused on two agency priority goals ( apg ) to examine how quarterly performance reviews affected the agency components responsible for achieving performance outcomes .

because the scope of our review was to examine data - driven performance reviews as a leadership strategy , we did not evaluate whether these goals were appropriate indicators of agency performance , sufficiently ambitious , or met other dimensions of quality .

we also reviewed memorandums , internal briefings , and other materials agencies used to prepare for the reviews , as well as documents used during the reviews and follow - up materials .

we conducted interviews with officials at omb , the performance improvement council ( pic ) , and officials involved in each agency's performance review process .

we asked to observe at least one review meeting at each agency .

treasury allowed us to observe two review meetings — one focused on the bureau of the fiscal service and one on the internal revenue service ( irs ) .

doe and sba did not allow us to observe their meetings , citing concerns that our presence could inhibit open discussion .

during the interviews , we asked officials to identify any challenges to effective implementation they faced as the process evolved or any lessons they learned .

we also asked officials to identify examples of any impacts on performance that they attributed to the reviews .

we conducted our work from april 2012 to february 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the model for data - driven performance reviews was established in the early 1990s by new york city police department ( nypd ) leadership as a strategy to reduce crime .

dubbed “compstat,” the nypd's weekly reviews grew from the premise that use of crime data could enable leadership to make better - informed , more effective decisions .

compstat followed four key tenets: 1 .

accurate and timely intelligence .

2 .

effective tactics .

3 .

rapid deployment .

4 .

relentless follow - up and assessment .

as the law enforcement community observed nypd's improved accountability and bottom - line results in crime reduction , many police departments nationwide began to replicate compstat in their own organizations .

the compstat model was subsequently adopted by many cities , municipalities , and some states — most notably washington and maryland — as a general performance management tool .

many of these efforts were patterned on the same four tenets as compstat .

nearly two decades later , the obama administration began to encourage the use of data - driven review meetings as a performance management tool through several memorandums issued by omb in 2010 and 2011 , and a june 2011 executive order .

during this timeframe , gprama introduced the concept of data - driven performance reviews at the federal level with a provision that federal agencies conduct quarterly performance reviews on progress toward their apgs .

specifically , agencies are required to assess how relevant programs and activities contribute to achieving apgs ; categorize goals by their risk of not being achieved ; and for those at risk , identify strategies to improve performance .

gprama also specified that the reviews must occur on at least a quarterly basis and involve key leadership and other relevant parties both within and outside the agency .

gprama required agencies to begin conducting quarterly performance reviews by june 2011 .

some agencies began conducting data - driven reviews earlier , in response to the executive order , omb guidance , or other performance management efforts .

several efforts aid agencies in implementing the gprama - required quarterly performance reviews .

the pic established a working group on internal agency performance reviews .

the working group meets monthly to share leading practices and discuss strategies for improving performance .

participation in the working group is voluntary and according to omb , pios or designees from 21 agencies across the federal government are currently represented .

in addition , omb supported performance review implementation by issuing new guidance in omb circular a - 11 during august 2012 .

doe , sba , and treasury each had experience with data - driven performance reviews before the june 2011 gprama implementation deadline .

officials at both sba and treasury said that when new leadership came in , they brought interest in data - driven decision making , along with key staff that were experienced at data - driven review or data analysis .

doe officials said that their experience with the american recovery and reinvestment act of 2009 ( recovery act ) work led them to begin data - driven performance reviews along with the gprama requirements .

 doe built on its recovery act - related performance reviews with the establishment of its quarterly performance reviews , called business quarterly reviews , in 2011 .

according to doe officials , the deputy secretary leads the review meetings with participation by under secretaries , and each review covers the department's 8 priority goals , 15 mission - related key goals , and 8 management and operations key goals .

the associate deputy secretary , who is responsible for agency - wide management and operations goals , also participates in the reviews .

the review meeting follows a structured format , with the first half focusing on performance goals and the second half focusing on fiscal issues , such as budget execution .

 sba began conducting reviews , called quarterly performance reviews , during the third quarter of 2009 .

sba started by holding separate meetings by program area but changed the review format in early 2010 to include all program offices at each review meeting .

according to sba officials , the deputy administrator leads sba's quarterly performance reviews with support from the chief operating officer ( coo ) and the pio .

the pio also assembles the data and disseminates follow - up action items .

review meetings also include senior officials from functional management areas ( eg , information technology , human capital , procurement , etc. ) .

in addition to discussing agency priority goals , sba reviews cover the rest of the agency's performance goals and objectives , as time permits .

 treasury started conducting department - level performance reviews , called stats , in 2010 .

treasury's reviews , led by the deputy secretary , are performed on an agency component - by - component basis .

reviews also involve senior officials responsible for functional management areas such as information technology , financial management , and procurement , as well as policy officials .

the stat process is managed by treasury's office of the deputy assistant secretary for management and budget , which reports to the pio .

the stats cover treasury's two priority goals along with a range of other department programmatic and operational goals and priority projects .

our analysis confirmed that to be successful , data - driven reviews should be used as a leadership strategy to drive performance improvement .

agency leadership must be directly and visibly engaged in the review process and invest the time necessary to understand and interpret the data being discussed during the meetings .

moreover , gprama requires an agency head and deputy head to conduct quarterly priority progress reviews which fosters ownership and helps ensure that participants take the reviews seriously and that decisions and commitments can be made .

omb circular no .

a - 11 also emphasizes the importance of leadership involvement in quarterly performance reviews , allowing the coo , the agency head , or both to conduct the review .

our survey of pios indicated that agency leadership , with the exception of agency heads , actively participated in these reviews .

this was consistent with what we found at doe , sba , and treasury .

the sidebar at left shows the survey results on the participation of key positions .

at treasury , where we observed two stat meetings , the deputy secretary used the stat meetings to challenge participants to stretch toward ambitious performance goals and to provide possible solutions for any issues that were discussed .

by holding separate reviews for all of treasury's bureaus and key offices , the deputy secretary committed a significant amount of time to these reviews .

other treasury officials — at both the department headquarters and bureau levels — were appreciative of the amount of time the deputy secretary devoted to the reviews .

the deputy secretary's high - level position allowed him to speak with clear authority in the department and garner the attention of component agency leaders on performance issues .

for example , during one review meeting that we observed , the deputy secretary challenged treasury's bureau leaders to develop new strategies for the department to collect delinquent debt payments owed to the federal government .

we did not observe quarterly performance meetings at doe or sba .

however , officials we interviewed said that top leadership was actively driving performance discussions .

at doe , the deputy secretary led the quarterly review sessions and guided performance discussions with the department's under secretaries on progress made toward achieving their respective performance goals .

doe's deputy secretary noted that it is important that under secretaries take ownership of the performance review process to ensure the reviews are useful .

at sba , the deputy administrator led the reviews with support from the coo and pio .

according to officials , sba's leadership asks probing questions of program heads concerning sba's performance goals .

officials from doe , sba , and treasury expressed concern about maintaining the continuity of leadership engagement in performance review meetings and pointed to ways to help ensure that the review process continues with transitions to new agency leadership .

for example , officials at doe pointed out that they have a responsibility to remind new leadership of the legislative requirement for performance reviews as well as omb's guidance on these reviews .

in addition , sba officials noted that having a mix of career and political leadership involved in the reviews can facilitate continuity of the review practices since career officials generally span successive administrations .

at treasury , performance budgeting staff developed written standard operating procedures as a way to document the review process and pave the way for new leadership engagement at the department .

to encourage new leadership to take ownership of the performance reviews , several agency officials noted the importance of designing the reviews to fit the leadership style and preferences of the incoming leader .

for example , one official said that it is critical for the incoming leader of the review meetings to provide input into the presentation format and structure of the review based upon personal preferences .

at treasury , headquarters staff responsible for managing the performance reviews worked closely with the deputy secretary to develop a review template and meeting format that met his needs .

our analysis indicated that performance review meeting participants should include high - level leaders and managers with an agency - wide perspective as well as those with programmatic knowledge and responsibility for the specific performance issues likely to be raised .

in addition , participants should typically include those with agency - wide functional management responsibilities , such as information technology , budget , and human capital .

this enables the reviews to facilitate problem solving by breaking down information silos and providing managers from across the agency and other contributing organizations with a forum to communicate with each other and identify improvement strategies and agree upon specific next steps .

at the city and state level , data - driven performance reviews typically include senior management from multiple agencies .

in addition to the benefit of in - person meeting attendance , there is also value in having key players participate in other parts of the review process , such as the data review and analysis leading up to the meetings and the follow - up actions that arise from the meetings .

consistent with this practice , omb's guidance directs agencies to include , as appropriate , relevant personnel from within and outside the agency in the review meetings .

according to our survey , most pios — 21 of 24 — reported that their reviews included the participants needed to facilitate problem solving and identify performance improvements the majority of the time .

further , 19 pios reported that goal leaders had large involvement in their agency's performance reviews , and 15 pios reported that internal contributors to agency goals and functional management chiefs , such as the cfo , had large involvement in their agency's reviews .

at doe , sba , and treasury , officials said they found that including senior management responsible for specific mission program areas — as well as those with functional management responsibilities in areas such as budget , information technology , human capital , procurement , and legal counsel — enhanced performance improvement efforts at review meetings .

for example , both doe and treasury officials said discussions with budget officials concerning performance goals provided an opportunity for performance issues to be discussed in the context of budget issues where relevant .

however , differences in the scope of the reviews influenced which key players attended the review meetings .

for instance , while treasury's reviews focused primarily on a specific component organization , such as a bureau or policy office , representatives from multiple components attended the review meetings when the achievement of a performance goal crossed component lines , or when the components had other commonalities .

one treasury official noted that the bureau of engraving and printing and the u.s. mint were asked to sit in on each others' sessions , as they both work under the treasurer and have similar operations .

treasury officials indicated that one of the reasons why they chose to focus their performance reviews on individual component agencies — such as irs and the bureau of the fiscal service — was that their performance goals are generally aligned by component rather than shared by components .

in addition , according to a senior treasury official , focusing reviews on components allows time for more in - depth reviews .

treasury's reviews include officials with department - level responsibilities in functional management areas such as budget , procurement , and legal counsel who can contribute to problem solving or who may be called on to take follow - up actions .

in addition , treasury officials noted that component - by - component reviews enable treasury not only to have attendance from the bureau head , but also from more members of the bureau's leadership team , such as the bureau cfo and others .

officials said that if reviews were held with all of treasury's components at one time it would be impractical to gather all key members of bureau leadership because of the limited time which would be available for discussion on any particular topic .

treasury officials said that their practice of using the same information template for the stat review with each component enables the reviews to cover similar issues across components and identify actions that should be addressed collaboratively , even though all components do not participate in the review meetings at the same time .

conversely , at doe and sba , each review meeting included leadership participation from across all agency mission areas .

however , doe and sba were similar to treasury in that they included officials with agency - wide functional management responsibilities in the reviews .

officials noted that doe's reviews provided an opportunity for senior leadership to discuss the context around their performance goals and improve results with help from other areas of the department .

in addition , the presence of senior officials across doe's programs provided an opportunity to share leading practices in various areas across program lines .

at sba , officials noted that many senior career officials attending these reviews had experience managing multiple programs and their broader experiences helped them understand and identify relevant performance metrics across program areas .

in addition , officials mentioned that , in contrast to a large department with many discrete mission areas , sba's programs are focused on achieving a relatively narrow mission of supporting small businesses , which creates more opportunities for cross - program collaboration .

according to our survey results , pios did not see getting the right personnel included in the meetings as a challenge: only 2 of 24 pios reported a challenge in including those managers or staff needed to facilitate problem solving and identify improvement opportunities .

however , officials from doe , sba , and treasury did note challenges in getting the right mix and number of participants to most effectively facilitate performance improvement efforts at the meetings .

see figure 1 .

according to survey results , 16 of 24 pios indicated that there was little to no involvement in the reviews from external officials who contribute to the achievement of agency goals .

this is consistent with what we found at doe , sba , and treasury , as stakeholders from outside the agencies did not participate in their performance review meetings .

at treasury , officials said they addressed performance issues requiring external collaboration by conducting follow - up meetings with relevant external participants .

these meetings were scheduled as part of treasury's practice of following up on issues raised during the review sessions .

for instance , during a treasury review meeting we observed , the deputy secretary directed staff to arrange a meeting with omb to help address an obstacle to achieving a performance goal .

likewise , doe and sba officials said they undertook collaborative actions as a result of discussions in their quarterly performance reviews , but neither planned to invite external representatives to their meetings .

for example , sba's reviews led to multiple discussions with other federal agencies on whether some of their government contracts that were going to large businesses could go to small businesses instead .

officials we interviewed cited several concerns that may explain why , at present , agencies are generally not including external participants — from other federal agencies or other relevant organizations — in their reviews .

first , officials did not include external stakeholders because they wanted to keep discussions focused on internal problem - solving and were concerned that including external parties might inhibit open discussions on performance issues .

second , reviews at doe , sba , and treasury mainly focused on goals achieved through internal contributors at each agency or office and officials noted that it would not currently be an efficient use of time to include external parties , even though external issues are discussed .

one official said that the logistics of including high - ranking agency managers from other agencies could make it difficult to schedule review meetings on a timely basis .

another official who had experience managing data - driven performance reviews at different levels of government noted that city - and state - level reviews tend to be run by a mayor or governor with direct authority over the various agencies that participate in the reviews .

this official pointed out that circumstances are different at the federal level , where an agency head could invite but not require outside participation and would not have control over the information shared or whether follow - up action was carried out .

however , we have previously reported that agencies can collaborate more effectively across organizational lines when presented with a clear and compelling rationale to do so and when agency leaders demonstrate their commitment to working collaboratively .

agency goals that require the efforts of more than one agency could serve as such a compelling rationale — even in the absence of direct authority requiring such collaboration .

moreover , our prior work has shown that agencies which participated in various planning and decision - making forums together — such as interagency councils or planning bodies — reported that such interactions contributed to achieving their goals .

specifically , agencies reported that such participation opened lines of communication , fostered trust , and helped build relationships , which can in turn lead to more effective collaboration across agency lines .

despite the concerns that doe , sba , and treasury raised about including external participants in their reviews , our survey results indicate that some agencies are doing so: 4 of 24 pios reported moderate to large involvement of external officials who contribute to the achievement of agency goals .

in addition , omb officials provided an example of two agencies which have been successfully making use of quarterly performance reviews to collaborate on their apgs .

these officials told us that the departments of housing and urban development and veterans affairs — which both contribute to efforts to reduce veterans' homelessness — had conducted several stat meetings jointly .

according to omb officials , program staff members from both agencies regularly participate in hud stat meetings , where they jointly analyze performance data to understand trends , identify best practices , and prioritize the actions needed to achieve veteran homelessness goals .

officials reported that these collaborative meetings have contributed to better outcomes .

moreover , officials from both omb and the pic indicated that agencies have increasingly been observing others' review meetings as a means of learning about different practices with no apparent harm to the effectiveness of the meetings .

this suggests that the challenges , if any , to outside participation can be overcome .

while there are many approaches to managing performance to achieve goals that rely on multiple agencies , few are likely to provide the benefit of bringing together the leadership and all the key players to solve problems and motivate performance improvement .

moreover , when key players are excluded from quarterly performance reviews , agencies may be missing opportunities to have all the relevant parties participate in developing solutions to performance problems .

instead , agencies will need to rely on potentially duplicative parallel coordination mechanisms , which could result in less than optimal performance improvement strategies .

our analysis showed that quarterly performance reviews should be used to align an organization's resources , programs , and activities to ensure they are contributing to the achievement of agency goals .

to help ensure that reviews are focusing on the appropriate interim goals and measures , agencies can develop models that describe the logical relationship between an agency's inputs , activities , outputs , and outcomes .

these logical relationships , sometimes called logic models , should be periodically assessed to determine if outcomes are being achieved as expected , and should be revised if necessary .

our survey results indicated that pios do not find goal alignment to be a challenging aspect of implementing quarterly performance reviews , with 13 of 24 pios reporting that ensuring alignment between performance reviews and strategic goals and performance objectives was easy .

only one pio reported this as being a challenge .

moreover , 22 pios reported that the majority of their reviews are aligned with strategic goals and performance objectives .

consistent with our survey results , we found that doe , sba , and treasury had selected performance metrics , initiatives , and other areas of focus in their reviews that were linked to the accomplishment of apgs and other goals , such as strategic plan objectives and key operational goals .

officials from both doe and treasury described the processes they undertook to choose useful performance information to frame the performance review discussion .

for example , doe narrowed its list of more than 190 performance measures to provide leadership with a focused view of the department's key goals , while also providing sufficient depth of information to be meaningful .

treasury officials responsible for designing the department's quarterly performance reviews described extensive interactions with the deputy secretary and the bureaus to identify performance measures that could be used to promote discussion of performance issues and opportunities for improvement .

in addition , the three agencies described efforts to use logic models , project milestones , and other approaches to identify early information on how they were progressing toward long - term outcome goals , which officials said could be challenging to monitor .

for example , doe's office of energy efficiency and renewable energy created a template for program managers to develop logic models linking program inputs and outputs to longer - term performance objectives , such as achieving clean generation of 80 percent of the nation's electricity by 2035 .

doe identified outputs or intermediate outcomes that contribute to the clean generation of energy , such as reducing the cost of solar energy , which the department measures to indicate progress toward its 2035 goal .

according to officials , logic models helped program staff communicate a coherent story about how the program's key activities contribute to its goals .

at treasury , performance budgeting staff said they included the status of priority projects — such as irs plans to develop a streamlined , user - friendly website — in its quarterly performance review information ( see figure 2 ) .

officials explained that these priority projects were designated as such because treasury sees them as the critical path to achieving agency priority goals and other key longer - term outcomes , which could not always be tracked on a quarterly basis .

for example , the deputy secretary wanted to monitor the status of irs's website update because it is seen to be a lever toward the agency's priority goal to improve the voluntary tax compliance rate , which can only be measured with data that lags by approximately five years .

our analysis showed that because data - driven reviews are to foster improved performance , the focus of accountability should be on the responsible manager's role in addressing problems and bringing about positive change .

agency leaders should hold goal leaders and other responsible managers accountable for knowing the progress being made in achieving goals and , if progress is insufficient , understanding why and having a plan for improvement .

if data is insufficient for gauging progress , managers should be held accountable for improving the quality of the data so that it is sufficient for decision making .

managers should also be held accountable for identifying and replicating effective practices to improve performance .

in addition , the goals addressed in the reviews should be aligned with managers' and staff's individual performance goals to create a line of sight that reinforces the connection between strategic goals and day - to - day activities of managers and staff .

gprama introduced specific roles and responsibilities for agency heads , coos , pios , and goal leaders in conducting quarterly performance reviews .

for each apg , agency heads and coos , with support from the pio , must: review with the appropriate goal leader the progress achieved during the most recent quarter , overall trend data , and the likelihood of meeting the planned level of performance ;  assess whether relevant organizations , program activities , regulations , policies , and other activities are contributing as planned to agency priority goals ; categorize agency priority goals by risk of not achieving the planned level of performance ; and identify prospects and strategies for performance improvement , including any needed changes to agency program activities , regulations , policies , or other activities for agency priority goals at greatest risk of not meeting the planned level of performance .

across the government , a majority of pios reported that their agency's reviews met these gprama requirements , as indicated in figure 3 .

our survey results indicated that most pios — 21 of 24 — reported using the reviews to identify actionable opportunities for performance improvement at least half the time .

consistent with the survey results , doe , sba , and treasury reported that top agency leadership held officials accountable for identifying performance problems and opportunities for improvement .

for example , treasury officials said — and the sessions we observed confirmed — that their stat meetings focus on performance and are a vehicle for the deputy secretary to challenge bureau heads to ensure their bureaus continually improve .

for example , the deputy secretary reported that many of the bureaus had not reviewed their management metrics , such as internal controls , diversity issues and employee survey scores , for some time and the stat reviews are an opportunity to engage the bureaus on these issues .

at the stat reviews we attended , we observed the deputy secretary discussing such management metrics .

for example , he asked bureau leadership to explain why their scores on a government - wide employee satisfaction survey had dipped during the past year , questioned a decline in survey response rates , and discussed specific next steps that he could take to support plans for improvement .

at doe , officials said that their quarterly performance reviews focused on areas where they were not on track to meet performance goals and ways to address this .

one official at doe noted that the deputy secretary expected the under secretaries to be accountable for all of their goals at the meeting .

at sba , officials said the focus of the meetings is to fix problems and that it is important to be prepared because the administrator asks probing questions and managers are called out when progress is not being made and asked to explain what is being done to resolve the issues .

each agency also reported taking steps to ensure that managers' individual performance objectives are aligned with priority and other agency performance goals .

for example , sba's performance agreements incorporate performance objectives which cascade from the agency's priority goals and other performance goals and are aligned with its strategic plan .

samples of performance agreements we reviewed identified “performance elements linked to organizational goals.” for example , a district manager had a performance objective to hold outreach events to connect small businesses to contracting opportunities .

this objective clearly links to sba's apg to increase small business participation in government contracting .

our analysis indicated that the capacity to collect and analyze accurate , useful , and timely data is critical to successful data - driven reviews .

agencies should track both outputs and outcomes .

agencies should also look for opportunities to leverage data produced by other agency components or outside entities .

in addition , having the capacity to disaggregate data according to demographic , geographic , or other relevant characteristics can aid in highlighting significant variation , which can help meeting participants to pinpoint problems and identify solutions .

agencies also need to plan for the time and resources required to generate and communicate performance data in a timely manner .

easy access to relevant databases and systems - generated analysis , such as providing analysts with the ability to develop performance reports without relying on information technology staff , can streamline the data collection and analysis processes .

while having accurate , timely , and useful data available is critical to successful performance reviews , 16 of 24 pios reported that this was a challenge — more than any other practice we asked about .

however , all 16 of those pios also responded that accurate , timely , and useful data is available for their agency's reviews about half the time or more , which may indicate that some agencies have found ways to address this challenge .

our review of doe , sba , and treasury illustrates how agencies can overcome some challenges to data availability .

our analysis of quarterly performance review documents indicated that each agency was producing data - rich analyses that identified trends and potential performance issues .

however , agency officials described initial challenges in these areas and said that improving their capacities for data collection and analysis took time .

for example , sba's office of government contracting and business development collects data on the percentage of all federal agency contracts being awarded to small businesses .

the office is dependent on a general services administration database , the federal procurement database system - next generation ( fpds - ng ) , for its information .

sba officials were concerned about the quality of the data since each federal agency enters its own information .

to address this concern , sba officials said they provided agencies with individualized reports of potential anomalies in their small business contracting data .

this process allowed agencies to verify and correct if necessary the anomalies before sba published the annual small business procurement scorecard report .

for example , if an agency listed a contract in fpds - ng as a small business set - aside at the same time that the agency listed the contract as an open procurement competition , this would be flagged .

sba would then notify the responsible agency and give it an opportunity to correct the anomaly in fpds - ng .

in addition , sba noted instances where performance data lag behind the performance review cycle .

for example , the department of defense holds its procurement data back from fpds - ng for one quarter for national security purposes .

sba officials said that instead of waiting for the next quarter , they obtain preliminary information from the department of defense .

our analysis indicated that agencies need staff with the skills to assess performance data for coverage and quality and to identify key trends , areas of strong or weak performance , and possible causal factors .

in addition , those messages need to be effectively communicated to management and staff that will play a role in identifying and solving performance problems and making related decisions .

analysts and managers should carefully consider the type and amount of information that will be useful for performance reviews , as well as how to present the information to audiences with varying levels of technical or quantitative skills .

providing the right amount of easy - to - understand performance information can promote effective decision making during the quarterly performance reviews .

for example , focusing the presentation on the message the data tell about performance , using well - designed graphics , and grounding the data in relevant context are effective communication techniques .

our pio survey results included information on 15 specific competencies associated with performance improvement responsibilities .

for each of these competencies , the majority of pios reported the competencies were present among performance improvement staff to a large extent .

however , survey results were less positive about 2 competencies specifically related to analytic abilities .

of the 24 pios , 9 reported that performance measurement competencies and 10 reported that organizational performance analysis competencies were present among their performance improvement staff to a small or moderate extent .

pios at doe , sba , and treasury each described the teams they had assembled to support their performance improvement efforts .

for example , sba's deputy pio had performance analysts to support the quarterly performance reviews and many other performance management activities , such as the production of a weekly dashboard of key performance metrics .

however , sba officials acknowledged that some staff were less comfortable working with data and they perceived this as a skills gap that needed to be addressed .

these officials said they are addressing this through a combination of training and hiring .

for example , as part of its leadership training , sba began developing courses related to “decision support ; ” officials said the courses were designed to lead to competencies in spreadsheet development and analysis , presentation delivery , development of decision support datasets , and other analytic and presentation skills .

participants began training in late summer of 2012 with courses titled principles of analytics and analytic boot camp .

having staff with abilities to communicate analyses effectively is an important factor in successful performance reviews , and most pios — 22 of 24 — reported that data and relevant analyses are presented effectively to participants in their agency's reviews about half the time or more .

however , 11 of those 22 pios also reported that effective presentation of data and relevant analysis was challenging — the second largest challenge cited by the pios among the practices we asked about .

consistent with our survey results , officials we interviewed at sba and treasury described the challenges they faced in developing skills sets that bridge the gap between data analysis and effective communication .

at sba , the office of performance management developed internal training to help sba managers improve their ability to communicate the message that the data suggest .

one official recounted initial struggles to interpret data and then effectively communicate the key points relevant to performance improvement to those who were not analysts .

he noted that the sba administrator told senior management that a “data dump” was not helpful , which helped them to realize what was needed .

training was developed to move managers and staff beyond basic analytic skills , with a focus on structuring presentations effectively , using data to drive management decisions , and in general , “telling your story so you're drawing out insights , rather than just summarizing facts.” see figure 4 .

at treasury , performance budgeting staff developed a powerpoint presentation template that was distributed to each bureau to complete in advance of the stat meetings .

the template provided a uniform data collection tool that incorporated data presentation design principles , to guide the bureaus in effectively communicating their message to the deputy secretary .

for example , templates for line charts prompted bureaus to indicate whether the desired trend line direction was up or down , since this is not always immediately apparent to high - ranking reviewers who may not have the depth of background into the particular program or operation .

according to a treasury official , designing an effective presentation is as important as doing relevant , high - quality data analysis .

one official pointed out , “if no one reads or understands the analysis , it doesn't matter how good it was.” sba and treasury officials responsible for managing the reviews also described challenges in balancing presentation uniformity with the need to provide context that varies .

these officials noted that consistency was key to making performance information quick and easy to absorb , especially for leadership that has limited time to review such information .

however , sba noted that the down side of consistency is people's tendencies to tune out information that appears to be repetitive .

sba's pio and coo said they have to continually look for ways to keep the performance review meetings engaging to participants and that “meeting fatigue” can be a problem .

further , several bureau officials at treasury we interviewed said that while they understood the need for uniformity , the templates did not always provide them with enough flexibility to provide sufficient context for their performance information .

while bureau officials we interviewed said that the process and template had improved over time , some felt that in the early days of the stat reviews , they were so limited in their ability to “tell their story” to the deputy secretary that they did not think he was getting an accurate understanding of the issues .

however , our review of multiple stat documents indicated that there was a specific page in the template left open for issues the bureaus wanted to raise , and further , that some bureaus appended information to the template to provide additional context .

our analysis found that sufficient preparation for the performance review meeting is critical for a successful review .

key participants must be prepared to discuss agenda items related to their performance measures and progress toward goals as well as any other issues to be addressed .

the time allotted to prepare for reviews also provides a prompt for participants to continuously update their performance data , assess progress toward their performance goals , and develop a response to any performance issues identified .

also , data to be presented during the reviews must be fully vetted prior to the meeting so that participants can focus discussions on data trends and analysis rather than on whether the data itself is correct .

according to our survey results , 22 of 24 pios reported that review participants are adequately prepared for performance reviews more than half the time , and 20 reported that , overall , it is not challenging for participants to be prepared for the reviews .

nevertheless , several agency officials from doe , sba , and treasury said that there was a significant time investment in preparing information and coordinating among managers and analysts across headquarters and components or offices .

officials at treasury noted that the process of preparing for the reviews forced the department and its component agencies to closely examine performance data and make sure they could explain it to the deputy secretary , and said this process was a valuable part of the performance review .

as one bureau - level official explained , nobody wants to go before the deputy secretary with data that indicates a performance problem , unless they are able to explain the issue and show that they have already thought of strategies for improvement .

as a result , preparing for the reviews sometimes prompted participants to conduct additional analysis and have advance discussions on how to address performance problems .

at doe , sba , and treasury , we found several practices in place to prepare participants for review meetings .

officials at each agency stressed the importance of meeting preparation to ensure that the review sessions were productive .

for example , treasury employed a rigorous pre - meeting process which started with the performance budgeting staff developing a powerpoint template , in consultation with the deputy secretary , specifying the performance information to be provided by each component agency for its stat session .

treasury officials said that performance budgeting staff then met with component staff to discuss the new template and any changes .

the templates we reviewed were organized into several categories , such as priority projects , management metrics , and other issues , and were distributed in advance of the stat meeting and used as the meeting agenda .

treasury officials said there were typically several rounds of revisions to the powerpoint template prior to the review session , with management and analysts at the component level coordinating with their counterparts at the department .

officials said that one of the goals of developing the template was to ensure that all participants are fully prepared and able to engage in meaningful discussions about performance .

in particular , one official explained that a guiding principle is that none of the participants should ever be surprised by any of the topics to be discussed .

in advance of each review session , treasury's deputy secretary reviews the completed document along with an explanatory briefing memo prepared by performance budgeting staff , which provides relevant context for any issues , suggests lines of questioning , and highlights particular decisions to be made .

performance budgeting staff also review the completed stat template with component staff to discuss the contents and ensure that the component is aware of issues likely to be brought to the attention of the deputy secretary .

treasury's senior officials emphasized the importance of ensuring that data discussed at the meeting were sufficiently vetted during meeting preparation so as not to spend time during the sessions determining whether the information is correct .

several officials pointed to instances in early review meetings that used valuable meeting time on data issues because department - level and component staff disagreed on baseline data used during the reviews .

doe's pre - meeting practices included briefings with the deputy secretary and under secretaries and development of a business quarterly review binder that included descriptions of the program offices' performance and budget information , a list of attendees , and background notes , as well as a list of actions from the previous performance review .

our analysis showed that in - person meetings which are both frequent and regularly scheduled are a defining characteristic of data - driven reviews .

regularly scheduled meetings foster a culture of performance management and continuous improvement .

the frequency of the meeting schedule should depend on the urgency of the problems to be fixed , frequency with which the data are collected , and speed with which agency action can have an impact on these data .

gprama requires that starting no later than june 2011 , agencies must conduct meetings at least quarterly , but agencies may meet more frequently if it meets their needs .

according to our survey , all 24 of the pios reported that their agencies were conducting performance reviews at least quarterly , with 7 of those 24 reporting conducting reviews more frequently .

at doe , sba , and treasury , we found each agency had frequent , regularly scheduled performance review meetings .

treasury's deputy secretary met separately with each of the bureaus two or three times a year .

generally , one or two of the meetings with each treasury bureau focused on performance , and the other meetings focused on budget .

treasury also holds stat meetings on certain department - wide goals related to human capital , procurement , and strategic sourcing .

as a result , treasury conducted more than three dozen performance review meetings throughout the year , with more than 100 reviews conducted as of november 2012 .

doe and sba both met quarterly with representatives from across their entire departments .

our survey results indicated that while holding performance reviews as scheduled is generally occurring , it may present challenges at some agencies .

of 24 pios , 20 reported that their agency held the performance reviews as scheduled more than half the time , with 2 of these pios reporting scheduling as a challenge .

in addition , the 4 pios who reported that this practice was occurring about half of the time or less also reported that it was a challenge .

experiences at treasury illustrated how it can be challenging to schedule performance review meetings with high - ranking officials .

at one bureau , officials said that last - minute cancellations due to the deputy secretary's schedule caused inefficiencies since extensive meeting preparations , including performance data analyses , had to be redone each time .

officials from this bureau also commented that the performance review meetings sometimes coincided with times during which the bureau had heavy workloads .

the officials noted that scheduling all of the meetings at the beginning of the year would be helpful so that they could plan around them .

according to treasury officials , currently sessions are scheduled 4 to 8 weeks in advance .

sba found that it was best to schedule a standing date for the performance review meetings , which are held on the third tuesday after the end of every quarter , rather than try to coordinate with numerous senior officials' schedules every time .

by selecting this meeting time , sba was also able to leverage an existing weekly operations meeting , rather than scheduling a separate meeting for the quarterly performance review .

rigorous and sustained follow - up on issues identified during the meetings is also critical to ensure the success of the reviews as a performance improvement tool .

important follow - up activities include identifying the individual or office responsible for each action item as well as who will be monitoring the follow - up .

follow - up actions should be included as agenda items for subsequent reviews to hold responsible officials accountable for addressing the issues raised and communicating what was done .

according to our survey results , follow - up activities are generally occurring even though some pios reported that this practice was challenging .

in particular , 21 of 24 pios reported that follow - up activities occurred at their agencies more than half the time .

although 7 pios reported that follow - up activities were challenging , 5 of those 7 pios also reported that this practice was occurring more than half the time , which may indicate that some agencies have found ways to overcome these challenges .

doe , sba , and treasury each had a different approach to ensure that follow - up items were carried out as agreed to at the review meetings and to ensure responsible parties were held accountable .

at doe , post - meeting activities included asking program offices questions that were generated by their review and assigning an analyst from headquarters performance staff to work with mission program staff to prepare answers to these follow - up questions and have them ready for the next performance review meeting .

according to an sba official , follow - up action items were typically discussed at weekly operations meetings , which helped officials to integrate their action plans into these other performance discussions .

one example of an sba follow - up item was to develop new strategies related to increasing federal contracts with small businesses .

officials said they ranked agencies by the total dollar value of contracts they issued , and they targeted procurement representatives at the top seven purchasing agencies for their outreach efforts .

at treasury , its performance budgeting staff generated follow - up memorandums immediately after the review meetings , naming action items , responsible parties , and due dates .

the status of follow - up items from the previous review meetings was also incorporated into the materials for the next review meeting so the deputy secretary could see if there was any lagging action .

in addition , performance budgeting staff tracked overall performance on post - review follow - up to ensure that this part of the process was being managed effectively ( see figure 5 ) .

several officials from treasury and sba noted the importance of receiving feedback from meeting participants to help improve the review process .

for example , treasury conducted a formal feedback meeting with bureau heads after the first round of review meetings and learned that components wanted more data from the department's performance staff to help substantiate their program performance .

more recently , treasury developed a survey to obtain formal feedback from participants and other management and staff that contribute to the review process .

the survey included questions on the amount of time invested in preparation and follow - up and asked respondents to rate their satisfaction with various aspects of the review process , including the template design and the guidance provided by performance budget staff .

respondents were also asked to rate the importance of the various aspects of the review to their bureau or office , among other questions .

according to treasury officials who manage the stat reviews , they have made specific improvements based on the survey results , such as sending out templates earlier to give components more time to prepare .

sba also solicited feedback from meeting participants and made some changes as a result .

for example , an sba official said that the agency used to schedule its quarterly performance review meeting after all the data were available , which was 45 days after the end of each quarter .

however , the official said that participants wanted the meeting to be held when the majority of the data was available — only one of its offices' data lagged — to allow them to take any needed action on a timely basis .

as a result , the officials said sba started scheduling the quarterly review meeting on the third tuesday after the end of each quarter and conducting a separate meeting with the one sba office that receives its data after that date .

doe , sba , and treasury officials said their quarterly performance reviews allowed different functional management groups and program areas within each agency to share information and ideas for performance improvement .

officials said these reviews helped them to solve problems that were impeding progress toward performance goals or to develop new performance improvement strategies .

in some cases , officials were able to point to specific performance improvements that they attributed to the reviews .

doe officials said that quarterly reviews helped them take a more critical look at subordinate activities contributing to the achievement of their apgs .

for instance , they examined barriers to achieving their weatherization goals and found they could produce better results by targeting not only individual homes but also multiuse buildings for retrofitting .

therefore , they created a program to leverage resources from nonprofit and private sector organizations focused on large scale retrofit projects for buildings .

in another example , doe identified barriers to achieving its solar energy cost reduction apg related to slow local permitting processes for solar installations along with other local - level activities that contributed toward the goal .

to address these barriers , doe funded a new program called the rooftop solar challenge in which teams develop actions plans to standardize permit processes , update planning and zoning codes , improve standards for connecting solar power to the electric grid , and increase access to financing .

officials said they expected these new approaches to improve doe's performance in reducing solar energy costs .

according to officials , performance reviews at doe also facilitated information sharing across the agency that led to better results .

for example , officials said discussions at doe's quarterly performance reviews led offices to share effective procurement practices .

in this case , offices have been sharing best practices related to strategic sourcing to identify areas of cost reduction .

for example , a recent reorganization between environmental management and national nuclear security administration presented an opportunity for environmental management to leverage existing capabilities in strategic sourcing .

the performance reviews have allowed for additional doe - wide discussions on strategic sourcing lessons - learned and partnerships to potentially achieve greater efficiencies and cost savings .

another example provided by sba officials illustrated how using the reviews to increase visibility of the small business contracting goal at higher management levels led to the adoption of new performance improvement strategies .

after discussing contracting goal data at a quarterly review , officials said the administrator and deputy administrator decided to call department secretaries at those federal agencies with the most potential for awarding small business contracts to emphasize the importance of the goal .

officials said other strategies , such as providing more training for procurement center representatives , who , among other things , assist small businesses in obtaining federal contracts , came out of discussion at the reviews .

officials said they anticipated that the new strategies that came out of the reviews would lead to better performance toward the small business contracting goal .

sba officials also provided an example that illustrated how the reviews facilitated intra - agency collaboration that improved sba's bottom line .

officials said that during a quarterly performance review meeting , the head of the office of capital access described anticipated staffing shortfalls and the head of sba's office of disaster assistance noted that he expected to have staff members available during the slower season in disaster assistance work , which is cyclical in nature .

sba's administrator instructed the two offices to work together , and as a result , officials said they were able to reduce the office of capital access's labor costs by 20 to 30 percent compared to the cost of paying employees overtime or hiring temporary contractor labor .

doe , sba , and treasury officials said the quarterly performance reviews provided a venue for top leadership to directly communicate their priorities and the priorities of the administration , which led to performance improvements in these areas .

for example , at treasury , nearly all of the bureau - level officials we interviewed said the stat meetings were valuable because they allowed for a firsthand understanding of the deputy secretary's priorities for their bureaus .

bureau officials said that the deputy secretary used these meetings to challenge their performance targets and approaches to addressing performance problems and to identify new opportunities for improvement .

for example , the bureau of the public debt commissioner said that upon hearing about the deputy secretary's interest in one of his ideas for improving how the agency communicates its bond pricing approach to customers , he moved ahead with developing an actionable strategy which he believes will ultimately lead to better customer service .

treasury officials said the stat review process improved decision making by creating an environment where meaningful discussions on improving performance were held , citing performance improvements at the u.s. mint as an example ( see figure 6 ) .

citing another example , treasury officials said that the deputy secretary had made contracting with small business a department - wide priority .

although increasing small business participation in government contracting was a sba priority goal , treasury , like all federal agencies subject to the small business act , had its own target to meet to contribute to the goal .

officials attributed increases in the department's percentage of contracts with small businesses — treasury was the only agency to achieve all of its fiscal year 2011 sba small business prime contracting goals — to the deputy secretary's “relentless attention” at the stat meetings .

at every stat session with every bureau , the deputy secretary reviewed the individual bureau's performance against the small business target goals .

officials said that treasury's chief procurement officer was present at every stat meeting to facilitate goal achievement .

the financial management service ( fms ) commissioner cited another example of how the stat meetings provided a venue for the deputy secretary to communicate his priorities , which led to better performance .

fms has been pursuing several priority projects to modernize its payments , collections , and central accounting systems that serve federal agencies across the government .

for example , one project is to replace a paper process that many agencies use to accept vendor invoices with a central invoicing system .

fms' analysts estimated that a central invoicing system could save the federal government $400 to $500 million annually , as well as provide vendors with online access to the status of their payments .

fms piloted the new system with the bureau of engraving and printing .

the new system enabled the bureau of engraving and printing to pay their vendors more quickly than before , which resulted in the vendors having to provide “prompt pay” discounts .

the decision was made to move forward government - wide , but to start with treasury's own bureaus .

following a stat meeting in which the fms commissioner cited delays in adoption by other treasury bureaus , the deputy secretary made it clear that implementing the new system was a priority .

the fms official said that , while he believed the adoption of the new system would have happened eventually even without the stat meetings , he credited the stat meetings with speeding up the process by three to four years .

a small number of treasury bureau officials we interviewed had mixed views on whether the stat meetings had actually improved performance , with some pointing out that certain performance improvements may have occurred without the stat meetings .

for example , treasury leadership said they used the stat sessions to closely monitor the consolidation of bureau of the public debt and fms into a single bureau , the bureau of the fiscal service .

some officials thought they would have achieved the various consolidation milestones and goals without the stat reviews and one said that the reviews added another layer of reporting that was time consuming .

however , treasury department officials noted that component management may not be aware of how the deputy secretary uses the reviews to better understand performance issues , including , in some cases , to ensure that the department is providing necessary support to improve performance .

our review of doe , sba , and treasury — as well as our survey of 24 pios — indicated that data - driven quarterly performance reviews hold promise as an effective management tool at the federal level .

however , unlike city - and state - level data - driven reviews , which typically include representatives from multiple agencies , officials at doe , sba , and treasury viewed their quarterly performance review meetings as an internal management tool and therefore did not open the reviews to outside participation .

officials said they relied on other means of collaborating with outside agencies and other partners that contribute to achieving cross - cutting goals .

furthermore , the majority of pios we surveyed indicated that there was little to no involvement in the reviews from external officials who could contribute to achieving agency goals .

successful data - driven performance reviews , which require extensive preparation and significant leadership time , do not come without a cost , so it is critical that agencies implement their reviews in a way that maximizes their effectiveness .

as the implementation of the various gprama provisions continues , agencies may need to reevaluate the most effective way to engage outside contributors in the quarterly performance review process for apgs and other performance goals that depend on other organizations to achieve desired outcomes .

while there are many approaches to managing performance toward such goals , agency quarterly performance reviews could provide opportunities to bring together the leadership and all the key players needed to improve cross - agency and internal agency performance .

to better leverage agency quarterly performance reviews as a mechanism to manage performance toward agency priority and other agency - level performance goals , we are recommending that the director of the office of management and budget — working with the performance improvement council and other relevant groups — identify and share promising practices to help agencies extend their quarterly performance reviews to include , as relevant , representatives from outside organizations that contribute to achieving their agency performance goals .

we provided a draft of this report to doe , omb , sba , and treasury for review and comment .

each agency provided technical comments which we incorporated as appropriate .

omb staff generally concurred with the recommendation in our report .

doe and treasury provided written comments , which are reproduced in appendixes iv and v , agreeing with our conclusions .

we are sending copies of this report to the appropriate congressional committees and the secretaries of energy and treasury , the administrator of sba , and the director of omb .

in addition , the report is available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have any questions about this report , please contact me at ( 202 ) 512-6806 or mihmj@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vii .

as part of our mandate to review the implementation of the government performance and results act modernization act of 2010 ( gprama ) , our reporting objectives were to ( 1 ) identify practices that can promote successful data - driven reviews at the federal level and examine how these reviews are being implemented at selected agencies and across the government , and ( 2 ) examine the impact of quarterly data - driven performance reviews on selected agencies' progress toward high priority and other performance goals .

this report is the second in a series that examines how agencies are implementing various gprama requirements .

to identify practices that can promote successful data - driven reviews at the federal level , we conducted a review of relevant academic and policy literature , including our previous reports .

based on our literature review , we developed a broad set of practices associated with the major contributors to success for data - driven reviews .

we refined these practices with additional information obtained from practitioners at the local , state , and federal level who shared their experiences and lessons learned .

as part of this engagement , we also compared these practices with recent gprama - related guidance in the office of management and budget's ( omb ) circular no .

a - 11 and found them broadly consistent .

we observed two data - driven review meetings at the department of the treasury ( treasury ) , which was one of the agencies selected to address our reporting objectives .

to examine how these reviews are being implemented at agencies across the government , we surveyed the performance improvement officer ( pio ) at each of the 24 federal agencies subject to the chief financial officers ( cfo ) act .

we surveyed these agencies because gprama directs us to focus on them in our reporting on the act .

additionally , several provisions of the act apply specifically to these agencies , including that the performance improvement council ( pic ) include them as members .

we received responses from 24 pios — a 100 percent response rate .

the web - based survey was administered from october 18 , 2012 , to december 14 , 2012 .

respondents were sent an e - mail invitation to complete the survey on a gao web server using a unique username and password .

during the data collection period , we sent reminder e - mails and made phone calls to nonresponding agencies .

because this was not a sample survey , it has no sampling errors .

the practical difficulties of conducting any survey may also introduce nonsampling errors , such as difficulties interpreting a particular question , which can introduce unwanted variability into the survey results .

we took steps to minimize nonsampling errors by pretesting the questionnaire in person with pios and deputy pios at three different agencies .

we conducted pretests to make sure that the questions were clear and unbiased , the data and information were readily obtainable , and the questionnaire did not place an undue burden on respondents .

we made appropriate revisions to the content and format of the questionnaire after the pretests and independent review .

all data analysis programs used to generate survey results were independently verified for accuracy .

additionally , in reviewing the answers from agencies , we confirmed that pios had correctly bypassed inapplicable questions ( skip patterns ) .

based on our findings , we determined that the survey data are sufficiently reliable for the purposes of this engagement .

to evaluate how effectively selected agencies are applying the practices of data - driven reviews in their gprama - mandated quarterly performance reviews , and the effectiveness of these reviews toward achieving agency priority and other performance goals , we chose three agencies — department of energy ( doe ) , small business administration ( sba ) , and department of the treasury ( treasury ) .

because one of our goals was to describe challenges and lessons learned that will be useful to as many federal agencies as possible — as well as to the administration and to congress — we selected agencies for case study that could provide a new illustration of challenges or lessons learned , that is , agencies that were not the recent or current subject of gao or other public administration or public policy case studies .

we also looked for agencies that could provide broadly applicable case illustrations , based on the scope of their mission , organizational complexity , and the mix of government tools — such as direct service , regulations , grants , loans , and tax expenditures — used to achieve their performance goals .

in addition , we excluded agencies that were undergoing significant management changes — such as leadership turnover or consolidation review — that could prevent us from gaining a clear picture of performance management or could interfere with our ability to make practicable recommendations .

we also excluded agencies that had less than one year of experience in conducting data - driven reviews at the time that we started our field work .

at each selected agency , we focused on two agency priority goals ( apg ) to examine how quarterly performance reviews affected the agency components responsible for achieving performance outcomes .

because the scope of our review was to examine data - driven performance reviews as a leadership strategy , we did not evaluate whether these goals were appropriate indicators of agency performance , sufficiently ambitious , or met other dimensions of quality .

the following are the complete sets of the agencies' 2013 apgs .

an asterisk indicates the ones we focused on in our review:  save low - income families money and energy through weatherization  reduce the department's cold war legacy environmental footprint .

 reduce the cost of batteries for electric drive vehicles to help increase the market for plug - in - hybrids and all electric vehicles and thereby reduce petroleum use and greenhouse gas emissions .

 make solar energy as cheap as traditional sources of electricity .

*  reduce consumer energy use and costs for household appliances .

 prioritization of scientific facilities to ensure optimal benefit from federal investments .

 make significant progress toward securing the most vulnerable nuclear materials worldwide within four years .

 maintain the u.s. nuclear weapons stockpile and dismantle excess nuclear weapons to meet national nuclear security requirements as assigned by the president through the nuclear posture review .

 process business loans as efficiently as possible .

* increase small business participation in government contracting .

*  process disaster assistance applications efficiently .

 expand access to long term capital .

increase electronic transactions with the public to improve service , prevent fraud , and reduce costs .

* increase voluntary tax compliance .

* to address both of our objectives , we reviewed memorandums , internal briefings , and other materials agencies used to prepare for the reviews , as well as documents used during the reviews and follow - up materials .

we conducted interviews with officials at omb , the performance improvement council , and senior - level officials involved in each agency's performance review process to gain various perspectives on these reviews .

 doe: we met with the deputy secretary , who also serves as doe's chief operating officer ( coo ) , and other senior - level officials including but not limited to the pio , the deputy pio , and budget officials .

in addition , we met with two apg goal leaders: the deputy assistant secretary for energy efficiency and the deputy assistant secretary for renewable energy .

 sba: we met with senior - level officials , including but not limited to the sba's pio , deputy pio , and coo .

in addition , we met with sba's associate administrator / office of capital access , who serves as the goal leader for one of the agps we reviewed .

 treasury: we met with the deputy secretary , who also serves as treasury's coo , and other senior - level officials at the department level , including the pio , who also serves as the cfo , the deputy pio , and budget officials .

we also met with senior - level officials at treasury's bureau of the fiscal service , bureau of the public debt , financial management service , and the internal revenue service .

in addition , we met with the assistant fiscal secretary who serves as the goal leader for one of the apgs we reviewed .

we asked to observe at least one review meeting at each agency .

treasury allowed us to observe two of its quarterly performance review meetings — one focused on the bureau of the fiscal service and one on the internal revenue service .

doe and sba did not allow us to observe their meetings , citing concerns that our presence could inhibit open discussion .

during the interviews , we asked officials to identify any challenges to effective implementation they faced as the process evolved or any lessons they learned .

we also asked officials to identify examples of any impacts on performance that they attributed to the reviews .

we conducted our work from april 2012 to february 2013 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

to address several research objectives related to implementation of the government performance and results act modernization act of 2010 ( gprama ) we distributed a web - based survey to the performance improvement officer ( pio ) at each of the 24 agencies subject to the chief financial officers act .

we received responses from all 24 pios .

there were 45 survey questions ; tables 1 through 9 below show questions and responses for the nine questions that were directly applicable to the research objectives in this report .

we will publish the full survey results in a report in the spring of 2013 on the implementation of key management positions under gprama .

for more information about our methodology for designing and distributing the survey , see appendix i .

this appendix includes the print version of the text contained in interactive figure 1 .

several officials noted challenges in achieving the proper balance between having a small number of meeting participants , which can allow more air time for each participant , compared to a larger group which might not be a productive use of time for those attending .

picture: treasury deputy secretary and coo neal wolin ( top ) leads the department's quarterly performance reviews , with support from assistant secretary for management nani coloretti , who also serves as treasury's pio .

in addition , balancing depth of knowledge and numbers of staff attending these reviews is another consideration for finding the right meeting size .

one official pointed out that having a relatively small group can help participants feel more comfortable in revealing performance problems but that there is a risk of leaving out key players that need to be part of a performance solution .

officials from each agency described different approaches to finding the right meeting size and composition for effective performance reviews , with several officials acknowledging that it takes time to get it right .

table 10 contains the rollover information from interactive figure 1 about how the department of energy ( doe ) , the small business administration ( sba ) , and the department of the treasury ( treasury ) conduct their quarterly performance review meetings .

elizabeth curda and laura miller craig managed this assignment .

darnita akers , virginia chanley , don kiggins , and bob yetvin made contributions to all aspects of the report .

kim gianopoulos , kay kuhlman , judith kordahl , jill lacey , sarah m. mcgrath , tim minelli , kathleen padulchick , william b. shear , albert sim , and meredith trauner also provided assistance .

in addition , sabrina streagle provided legal support and donna miller developed the report's graphics .

behn , robert .

“collaborating for performance: or can there exist such a thing as collaborationstat ? ” international public management journal , vol .

13 , no .

4 ( 2010 ) : 429-470 .

behn , robert .

“the core drivers of citistat: it's not just about the meetings and the maps,” international public management journal , vol .

8 , no .

3 ( 2005 ) : 295-319 .

behn , robert .

“the varieties of citistat,” public administration review , vol .

66 , no .

3 ( 2006 ) : 332-340 .

behn , robert .

“what all mayors would like to know about baltimore's citistat performance strategy,” ibm center for the business of government managing for performance and results series ( washington , d.c.: 2007 ) .

campbell , mary .

“bringing performance excellence to the public sector: washington state's odyssey,” asq world conference on quality and improvement proceedings , vol .

59 ( 2005 ) : 279-288 .

esty , daniel , and reece rushing .

“the promise of data - driven policymaking,” issues in science and technology , vol .

23 , no .

4 ( summer 2007 ) : 67-72 .

fillichio , carl .

“getting ahead of the curve: baltimore and citistat,” public manager , vol .

34 , no .

2 ( summer 2005 ) : 51-53 .

godown , jeff .

“the compstat process: four principles for managing crime reduction,” the police chief , vol .

lxxvi , no .

8 ( august 2009 ) , accessed on september 5 , 2012 , http: / / www.policechiefmagazine.org / magazine / index.cfm ? fuseaction=displ ay&article_id=1859&issue_id=82009 .

griffith , john , and gadi dechter .

“performance reviews that work: four case studies of successful performance review systems in the federal government,” center for american progress ( washington , d.c.: february 2011 ) .

hatry , harry and elizabeth davies , “a guide to data - driven performance reviews,” ibm center for the business of government improving performance series ( washington , d.c.: 2011 ) .

henderson , lenneal .

“the baltimore citistat program: performance and accountability,” ibm endowment for the business of government managing for results series ( arlington , va: may 2003 ) .

kettl , donald .

“china looks west for performance management,” governing , august 2011 , http: / / www.governing.com / columns / potomac - chronicle / china - looks - west - performance - management.html .

kingsley , christopher .

“smart cities: performancestat at 15,” university of pennsylvania fels institute of government ( philadelphia , pa ; october 2010 ) .

malinowski , chris , and sasha page .

“top 10 performance measurement dos and don'ts,” government finance review , vol .

20 , no .

5 ( october 2004 ) : 28+ .

moynihan , donald , and stéphane lavertu .

“do performance reforms change how federal mangers manage ? ” the brookings institution issues in governance studies series , 52 ( washington , d.c.: october 2012 ) .

o'connell , paul , and daniel forrester .

“think differently: update your stats to unlock outcomes,” the public manager , vol .

38 , no .

4 ( winter 2009-2010 ) : 5-9 .

o'connell , paul .

“using performance data for accountability: the new york city police department's compstat model of police management,” the pricewaterhousecoopers endowment for the business of government managing for results series ( arlington , va: 2001 ) .

partnership for public service .

“from data to decisions: the power of analytics,” ibm center for the business of government ( washington , d.c.: november 2011 ) .

perez , teresita and reece rushing .

“the citistat model: how data - driven government can increase efficiency and effectiveness,” center for american progress ( washington , d.c.: april 2007 ) .

queensland audit office .

better practice guide performance reviews .

brisbane , australia: 2010 .

sanger , mary bryna .

“from measurement to management: breaking through the barriers to state and local performance,” public administration review , s1 ( december 2008 ) : s70-85 .

scofield , jennifer .

“asking why cuyahoga countystat,” government finance review , vol .

27 , no .

6 ( december 2011 ) : 40-43 .

serpas , ronal , and matthew morley .

“the next step in accountability - driven leadership: “compstating” the compstat data,” the police chief , vol .

lxxv , no .

5 ( may 2008 ) , accessed on september 5 , 2012 , http: / / www.policechiefmagazine.org / magazine / index.cfm ? fuseaction=displ ay_arch&article_id=1501&issue_id=52008 .

shane , jon .

“compstat process,” fbi law enforcement bulletin , vol .

73 , no .

4 ( april 2004 ) : 12-21 .

sharp , cathy , jocelyn jones , and alison smith .

“what do we measure and why ? .

an evaluation of the citistat model of performance management and its applicability to the scottish public sector,” scottish executive social research ( edinburgh , uk: 2006 ) .

solan , david .

“the epastat quarterly report and lessons learned,” public performance & management review , vol .

33 , no .

2 ( december 2009 ) : 222-240 .

steinberg , harold .

“using performance information to drive performance improvement,” association of government accountants corporate partner advisory groupresearch series: report no .

29 ( alexandria , va: 2011 ) .

useem , greg .

“moving from reporting performance information to using it,” government finance review , vol .

25 , no .

2 ( april 2009 ) : 47-50 .

weitzman , beth , diana silver , and caitlyn brazill .

“efforts to improve public policy and programs through data practice: experiences in 15 distressed american cities,” public administration review , vol .

66 , no .

3 ( may / june 2006 ) : 386-399 .

managing for results: key considerations for implementing interagency collaborative mechanisms .

gao - 12-1022 .

washington , d.c.: september 27 , 2012 strategic sourcing: improved and expanded use could save billions in annual procurement costs .

gao - 12-919 .

washington , d.c.: september 20 , 2012 solar energy: federal initiatives overlap but take measures to avoid duplication .

gao - 12-843 .

washington , d.c.: august 30 , 2012 .

entrepreneurial assistance: opportunities exist to improve programs' collaboration , data - tracking , and performance management .

gao - 12-819 .

washington , d.c.: august 23 , 2012 .

managing for results: gao's work related to the interim crosscutting priority goals under the gpra modernization act .

gao - 12-620r .

washington , d.c.: may 31 , 2012 .

2012 annual report: opportunities to reduce duplication , overlap and fragmentation , achieve savings , and enhance revenue .

gao - 12-342sp .

washington d.c: february 28 , 2012 .

department of energy: additional opportunities exist to streamline support functions at nnsa and office of science sites .

gao - 12-255 .

washington , d.c.: january 31 , 2012 .

small business administration: progress continues in addressing reforms to the disaster loan program .

gao - 12-253t .

washington , d.c.: november 30 , 2011 .

u.s .

coins: replacing the $1 note with a $1 coin would provide a financial benefit to the government .

gao - 11-281 .

washington , d.c.: march 4 , 2011 .

government performance: strategies for building a results - oriented and collaborative culture in the federal government .

gao - 09-1011t .

washington , d.c.: september 24 , 2009 .

government performance: lessons learned for the next administration on using performance information to improve results .

gao - 08-1026t .

washington , d.c.: july 24 , 2008 .

results oriented cultures: creating a clear linkage between individual performance and organizational success .

gao - 03-488 .

washington , d.c.: march 14 , 2003 .

