the shift to a global economy and rapid advances in technology underscore the importance of preparing our current and future workforce for high - demand careers with 21st century skills , such as those that emphasize problem solving and teamwork .

in the 2006-2007 program year , more than 15 million high school and college students nationwide participated in career and technical education ( cte ) programs , which are designed to provide students with the academic and career and technical skills to help them succeed in the workforce .

as authorized by the carl d. perkins career and technical education act of 2006 ( perkins iv ) , congress provided states with $1.2 billion in fiscal year 2008 to support career and technical education in high schools and to support programs in postsecondary institutions , such as community colleges .

the u.s. department of education ( education ) estimates that approximately 5 percent of all funds that states use for cte programs are federal funds , with state and local funding generally covering the remainder .

the american recovery and reinvestment act of 2009 provides additional funds that states can use to help support their cte programs .

federal funds for cte programs are likely to take on increasing importance as states continue to confront mounting fiscal pressures that may lead them to propose cuts to secondary and postsecondary education spending used to support career and technical education .

perkins iv aims to prepare students for current or emerging high - skill , high - wage , or high - demand jobs by emphasizing rigorous student academic and technical skill achievement , increased accountability for student outcomes , and enhanced coordination between secondary and postsecondary career and technical education .

it also seeks to increase state and local flexibility in providing career and technical education by involving multiple groups such as students , parents , and local administrators in planning and administration , and by allowing states flexibility in the design of their accountability systems .

to increase accountability for student outcomes , perkins iv established student performance measures at the secondary and postsecondary levels for state agencies , such as state educational agencies or state college and university systems , as well as for local recipients of funds , such as school districts .

key performance measures include student attainment of academic content standards and student academic achievement standards , as adopted by the state in accordance with the requirements of title i of the elementary and secondary education act .

overall , perkins iv reflects a shift from an emphasis on vocational education — once considered by some to be an occupationally specific track for students with lower academic skills — to an emphasis on preparing students for entry into high - demand occupations .

education provides technical assistance and guidance to states regarding their data collection and student definitions and measurement approaches .

states report annually to education on their progress in meeting their performance targets for the measures .

in light of a governmentwide focus on performance and accountability , you asked us to examine ( 1 ) how states have implemented the perkins iv performance measures , and what , if any , challenges they have faced in implementing the measures ; ( 2 ) to what extent education has ensured that states are implementing the new performance measures and supported states in their efforts ; and ( 3 ) what education knows about the effectiveness of cte programs .

to answer our three research questions , we collected data through multiple methods .

first , to gather state - level information on perkins iv implementation , we collected information through two web - based surveys of state cte directors , at the secondary and postsecondary levels , in the 50 states and the district of columbia .

the surveys obtained information on the types of data states collect for the student performance measures and challenges they face ; technical assistance , guidance , and monitoring states receive from education ; and how states evaluate their cte programs .

we administered the surveys between january and april 2009 and received responses from all 50 states and the district of columbia .

while we did not fully validate specific information that state officials reported through our surveys , we reviewed the information to determine that their responses were complete and reasonable and found the information to be sufficiently reliable for the purposes of this report .

this report does not contain all the results from the surveys .

the surveys and a more complete tabulation of the results can be viewed online at gao - 09-737sp .

in addition to our surveys , we collected information from site visits to california , minnesota , and washington state .

these states represent variation across characteristics such as the type of state agency eligible to receive perkins funds ; amount of perkins iv funds received in fiscal year 2008 ; and type of approach used to assess how students attain technical skills , a key program outcome .

we interviewed secondary and postsecondary officials at the state level and officials from local recipients of perkins funds — that is , school districts and postsecondary institutions — that varied by number of cte students served , amount of perkins funding received , and geographic location ( urban versus rural ) .

we also reviewed relevant federal legislation and agency guidance and interviewed education officials to obtain information on how states have implemented the performance measures , how education has monitored and supported states in their efforts to implement the performance measures , and what education knows about how states are evaluating their local cte programs .

to analyze how states are evaluating cte programs , we reviewed state perkins plans and annual reports submitted to education from the 50 states and the district of columbia .

see appendix i for detailed information on our surveys and site visits .

we conducted this performance audit from august 2008 to july 2009 in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

the principal source of federal funding for cte , perkins iv authorizes federal grant funds for the enhancement of cte for secondary and postsecondary students .

in fiscal year 2008 , congress appropriated $1.2 billion for the improvement of local cte programs .

education's office of vocational and adult education allocates the funds to states , which retain up to 15 percent of the funds for administration and state leadership of cte programs , before passing at least 85 percent of the funds on to local recipients of funds , such as local school districts and community colleges .

states determine the percentage of funds that will be allocated to the secondary and postsecondary levels .

the majority of funds allocated to the secondary level are passed on to local recipients based on the school district's share of students from families below the poverty level for the preceding fiscal year .

postsecondary funds are primarily allocated based on the institution's share of pell grant recipients .

perkins iv established six student performance measures at the secondary level and five performance measures at the postsecondary level .

these measures represent a range of student outcomes , such as attainment of technical skills and placement in employment or further education following the completion of cte programs .

in addition , the measures include the nontraditional participation and completion of students from an underrepresented gender in programs with significant gender disparities ( such as women participating in auto repair ) , among others ( see tables 1 and 2 for a description of the perkins iv performance measures ) .

to ease states' transition to the new provisions in perkins iv , education permitted states to submit a 1-year transition plan that covered only the first program year of perkins iv implementation , 2007-2008 .

accordingly , states were required only to implement and report performance on two secondary performance measures for the 2007-2008 program year: academic attainment and student graduation rates .

these two measures are based on the same academic attainment and student graduation rate measures required by title i of the elementary and secondary education act .

beginning in the 2008-2009 program year , states are required to report on student outcomes for all of the performance measures .

states will report these outcomes to education in december 2009 .

perkins iv requires states to negotiate specific performance targets with education and to annually report their performance to education .

it also requires local recipients to negotiate performance targets with the states and to annually report to the state their progress toward meeting these targets .

perkins iv established additional accountability requirements for states and local recipients , including actions to address states that do not meet all of their performance targets .

under perkins iv , if a state does not meet at least 90 percent of its targets for one or more of the performance measures , it is required to develop and implement a program improvement plan that describes how it will address its failing performance targets .

prior to perkins iv , states were only required to develop and implement a program improvement plan if they failed to meet their targets in all of their performance measures , not just one measure .

states can also face financial sanctions .

for example , education can withhold all or a portion of funds if a state does not implement a program improvement plan , show improvement in meeting its failing performance measure , or meet the target for the same performance measure for 3 consecutive years .

local recipients that do not meet at least 90 percent of their performance targets have the same program improvement requirements as the state and face similar sanctions from the state .

in the event of financial sanctions , education is required to use the withheld funds to provide technical assistance to the state for improving its performance on the measures and the state is to use funds withheld from local recipients to provide cte services and activities to students .

in order to implement the performance measurement requirements of perkins iv , states must define which students will be included in the measures and collect data for each of the performance measures at the secondary and postsecondary levels .

for example , states define the minimum requirements , such as a certain number of cte credits that a student would need to obtain in order to be identified as a student concentrating in cte .

education has taken a range of actions to help states with these activities .

for example , in january 2007 , education began issuing nonregulatory guidance to states to help them develop their student definitions and data collection approaches for the performance measures .

education also issued guidance to states on the information states must include in their state perkins plans and in the annual reports that they submit to education .

in the state plans , states must detail how they intend to implement the performance measures , and in the annual reports states must describe their progress in meeting the negotiated performance targets .

in addition to implementing performance measures , states are required to evaluate programs , services , and activities supported with perkins funds and to report to education in their state plans how they intend to conduct these evaluations .

to meet this requirement , states describe the approaches , such as the use of state - developed standards , they will use to evaluate local cte programs .

in addition , education requires states to include a description of how they used perkins funds to evaluate their local cte programs in their annual reports .

a key feature of perkins iv — to enhance state and local flexibility in developing , implementing , and improving career and technical education — allows for considerable variation in how states implement some performance measures .

while perkins iv was designed to strengthen accountability for results at the state and local levels , it also allows states to establish their own accountability systems , including their own data collection methods for the performance measures .

of the 11 performance measures , the secondary and postsecondary levels have 3 measures in common: technical skill attainment , student placement , and participation in and completion of nontraditional programs ( see fig .

1 ) .

states may also include additional , state - developed performance measures in their accountability systems .

for example , washington state added three performance measures — earnings , employer satisfaction , and cte student satisfaction — to its accountability system .

consistent with perkins iv , education's guidance to states also allows for flexibility .

education issued nonregulatory guidance that proposed specific definitions that could be adopted by states to develop each of the secondary and postsecondary performance measures .

it also identified preferred approaches for collecting data for certain measures such as student technical skill attainment .

however , education noted that in accordance with perkins iv , states could propose other definitions and approaches to collect data for the required performance measures if they meet the requirements of the law .

we found through our surveys of state cte directors that states vary considerably in the extent to which they plan to follow education's guidance — specifically with regard to the technical skill attainment and secondary school completion measures .

as a result , education will collect student outcome data that vary across states for the same measures .

this can create challenges for education to aggregate student outcomes at the national level .

for example , a majority of states reported that they will use technical assessments — the approach recommended in education's guidance — to measure student attainment of skills at the secondary and postsecondary levels .

these include assessments leading to industry - ba certificates or state licenses .

however , a number of states will rely on other approaches to collect data for the performance measure , including grade poi nt average ( gpa ) , program completion , or other methods ( see table 3 ) .

officials in the states we visited provided a variety of reasons for their use of alternate methods to measure students' attainment of technical skills .

for example , postsecondary state officials in california said that a cte instructor's overall evaluation of a student's technical skill proficiency , in the form of a final grade , is a better measure of technical skill attainmentthan third - party technical assessments , and can more effectivel program improvement .

they questioned the value of technical assessments , in part because assessments often cannot keep pace with technology and changing cte program curricula , such as curricula for digital animation .

a washington state official told us that the state plans to use program completion to measure technical skills at the postsecondary level , noting that each postsecondary cte program incorporates industry - recognized standards into the curriculum .

he noted that a national system se it of third - party assessments may not be adequate or appropriate , becau would not necessarily incorporate the same standards .

local school officials in minnesota said that they will report on cte course complet view by for this measure .

because cte courses undergo curriculum re teachers as well as industry advisors , and align with relevant postsecondary programs in the area , school officials told us cours completion i attainment .

s sufficient to satisfy the definition of technical skill education's guidance also allows for considerable variation in the types of technical assessments states can use and when they can administer them .

most states at the secondary level reported in our survey that they plan to use industry - developed certificates or credentials most often administere at the end of a program , such as a certificate awarded for an automotive at the end of a program , such as a certificate awarded for an automotive technician .

at the postsecondary level , states plan to most often rely upon technician .

at the postsecondary level , states plan to most often rely upon the results of assessments for state lice the results of assessments for state licenses , such as state nursing licenses , to measure technical skills ( see fig .

2 ) .

nses , such as state nursing licenses , to measure technical skills ( see fig .

2 ) .

however , we found that while a majority of states plan to use assessment to report to education , the assessments are not currently in widespread use .

for example , more than half of states at the secondary and postsecondary levels reported that they plan to use these assessments to report on few to none of their state - approved cte programs in the 2008- s 2009 program year .

some states at the secondary level reported will use a combination of methods — including gpa o r program completion — to report on technical skill attainment .

we also found that states differ in whether they plan to report student dat on ged credentials , part of the secondary school completion measure .

thirty states reported through our survey that they do not plan to report ged data to education for the 2008-2009 program year , while 18 reported that they would .

about one - third of all states cited their ability to access accurate ged data as a great or very great challenge .

for example , state officials we interviewed said states face difficulty tracking the students that leave secondary education and return , sometimes several years lat to earn a ged credential .

an education official said that the agency is aware of the challenges and limitations states face in collecting ged data ed to provide technical assistance to states on and that the agency may ne ways to collect these data .

states reported in our surveys that they face the most difficulty in collecting student data for two of the performance measures: technical skill attainment and student placement ( see fig .

3 and fig .

4 ) .

thirty - eight states at the secondary level reported that they face great or very great challenges in collecting data on student technical skill attainment , while , similarly , 14 said they face challenges collecting data on student placement .

the results were similar at the postsecondary level: 39 states reported great or very great challenges with the technical skill attainment measure and 11 cited a similar level of difficulty with student placement .

states reported that the technical skill attainment measure at the secondary and postsecondary levels was most challenging to implement because of costs and the states' ability to collect accurate and complete student data .

specifically , states reported that the costs of state - developed assessments and third - party technical assessments — such as those for industry certifications — are high and often too expensive for many districts , institutions , or students .

several state cte directors commented in our surveys that their perkins funds are inadequate to pay for these assessments and additional funds would be necessary to cover the costs .

another cte director stated that economically disadvantaged students cannot afford the cost of assessments .

in addition to challenges due to cost , states are limited in their ability to access accurate and complete data .

for example , a state official said that washington state does not have data - sharing agreements with assessment providers to receive the results of student assessments .

as a result , the state will have to rely largely on students to self - report the results of their assessments , which raises concerns of data quality .

challenges such as these likely contribute to some states' use of other data — such as gpa or program completion — to collect and report information for this key student performance measure .

some states also reported difficulty collecting data on cte students after they leave the school system .

states at the secondary and postsecondary levels reported that their greatest challenge with the student placement measure is collecting data on students that are employed out of state .

as we previously reported , state wage records , such as unemployment insurance data , track employment - related outcomes only within a state , not across states .

a number of states commented in our surveys on challenges in tracking students because of the lack of data sharing across states .

we found that states face challenges in tracking students employed out of state regardless of the method they most commonly use to collect student placement data .

thirty - eight states at the secondary level will use student survey data from the state , school district , or a third party to track student placement and report to education , while 41 states at the postsecondary level will rely on state wage record data , despite potential gaps in student data ( see fig .

5 ) .

g. 5 ) .

states also cited other challenges in obtaining data on student placement for cte students .

at the secondary level , states reported that their next greatest challenge is linking secondary and postsecondary data systems in order to track students that pursue higher education after graduation .

to help overcome this challenge , minnesota — one of the states we visited — recently passed legislation to allow data sharing between the secondary and postsecondary levels .

our survey also found that states' next greatest challenge at the postsecondary level was collecting data on students who are self - employed after leaving postsecondary institutions .

community college officials in california said that while they rely on unemployment insurance wage record data , the data are incomplete and do not capture information on the self - employed , a group that is important for the measurement of cte outcomes at the postsecondary level .

states face similar challenges of cost and ability to access accurate data for the remaining performance measures .

for example , states at the secondary level commented on data challenges for the academic attainment and student graduation rate measures .

specifically , several states cited problems in obtaining data from separate student data systems containing academic and cte information .

this can be particularly challenging for states that are trying to match student data from different systems in order to track required cte student outcomes .

in addition , at the postsecondary level , states cited challenges in tracking student retention in postsecondary education or student transfer to a baccalaureate degree program .

in particular , accessing student data from out - of - state and private institutions and the high costs required to track these students were identified as the most challenging issues .

states most often reported that they will track these students through their state postsecondary data systems .

as we have previously reported , effective monitoring is a critical component of grant management .

the domestic working group's suggested grant practices state that financial and performance monitoring is important to ensure accountability and attainment of performance goals .

additionally , gao recently reported on the importance of using a risk - based strategy to monitor grants and noted that it is important to identify , prioritize , and manage potential at - risk grant recipients , given the large number of grants awarded by federal agencies .

education's approach to monitoring perkins is consistent with these suggested grant practices .

according to its perkins monitoring plan , education selects which states to monitor based on a combination of risk factors and monitors states in two ways: through on - site visits and off - site reviews of state plans , budgets , and annual reports for those states not visited in a given year .

to determine which states it will visit for on - site monitoring , education uses a combination of risk factors , such as grant award size , issues identified through reviews of state perkins plans , and time elapsed since education's last monitoring visit .

education officials told us that their goal is to visit each state at least once every 5 years and reported that they have conducted on - site monitoring visits to 28 states since 2006 .

education officials also told us that the same monitoring team performs both on - site and off - site reviews , which officials said helps to ensure continuity between the reviews .

furthermore , when conducting the off - site reviews , the monitoring team looks for trends in state data and for any problems with state data validity and reliability .

the team uses a checklist to match performance data to the data states report in their required annual reports .

according to education's inventory of open monitoring findings , as of may 2009 , 9 of the 28 open findings were related to accountability and states failing to submit complete or reliable data .

for example , in a february 2008 monitoring visit , education found that the monitored state's data system had design limitations that affected the state's ability to collect and assess data on career and technical education students .

specifically , education found that the various data systems across the local secondary and postsecondary levels did not share data with the state - level cte data system .

this data - sharing issue raised doubts about the validity and reliability of the state's perkins data .

education tracks the findings from each state's monitoring visit in a database and reviews the findings in an internal report that is updated monthly .

additionally , if a state has open findings , the state may be required to report corrective actions to education in the state's annual report .

officials told us that the amount of time it takes for a state to close out a finding depends upon the nature of the finding .

for example , a finding related to accountability may take up to a year to resolve because a state may have to undertake extensive actions to address the deficiency .

education officials reported that their monitoring process emphasizes program improvement rather than focusing solely on compliance issues and that they use monitoring findings to guide the technical assistance they provide to the states .

to evaluate its monitoring process , education sends a survey to the cte directors of states that were monitored that year and asks them to rate the format and content of education's perkins monitoring process .

for example , the survey asks states to report on whether they received sufficient notice that the site visit was going to take place , whether the monitoring team provided on - site technical assistance , and whether the state received a written report within a reasonable time frame following the visit .

we reviewed education's summaries of the state surveys and found that for 2004 and 2005 , the results of these surveys were generally positive .

for example , in a 2004 monitoring evaluation report , the 10 states that were surveyed all reported that they had received sufficient notice about the monitoring visit and that education staff provided on - site technical assistance .

according to our survey of secondary - level cte directors , about half of states have had a monitoring visit within the last 3 years , and almost all of the states whose monitoring visit resulted in findings said that education worked with them to ensure that the findings were addressed .

education provides states with guidance , technical assistance , and a variety of other resources and is taking actions to meet states' need for additional help .

since perkins iv was enacted , education has issued guidance to states on topics such as instructions for developing the state perkins plans and annual reports , as well as guidance related to the performance measures .

for example , education's guidance provides clarification to states on what information each state has to submit to education before it can receive its grant award for the next program year , such as any revisions a state wants to make to its definitions of student populations , measurement approaches , and proposed performance levels for each of the measures .

some of the guidance resulted from education's collaborative efforts with states .

for example , education's guidance to states on student definitions and measurement approaches incorporated the input given by state cte directors during national conference calls between states and education .

other guidance addresses questions raised by states during national perkins iv meetings , such as how a state should negotiate performance levels with its local recipients .

in addition to guidance , education offers states technical assistance from education staff — called regional accountability specialists — and through a private contractor .

education officials told us that each regional accountability specialist works with a specific group of states to negotiate state data collection approaches for the performance measures .

in addition , each specialist maintains regular contact with his or her states throughout the year and provides assistance on other issues , such as reporting requirements and program improvement plans .

in addition to the regional accountability specialists , education also provides states with technical assistance by using mpr associates , a private contractor .

mpr associates provides technical assistance that generally includes on - site visits and follow - up discussions to help states improve their cte programs and facilitate data collection for the performance measures .

for example , mpr associates met with one state to assist with developing population definitions and measurement approaches that aligned with education's guidance and helped another state with developing a plan for implementing secondary and postsecondary technical skill assessments .

after providing technical assistance to a state , mpr associates develops a summary report , which is then published on education's information - sharing web site , the peer collaborative resource network .

education also offers states a range of other resources , including data work groups and monthly conference calls .

see table 4 for a description of the various ways in which education provides assistance to states .

most states reported that the assistance provided by education has helped them implement the performance measures , but that more assistance in the area of technical skill attainment would be helpful .

in our survey , states responded positively about their regional accountability specialist and all of education's other forms of assistance , including the data quality institute and the next steps work group .

states also reported that more nonregulatory guidance and more individual technical assistance would improve their ability to implement the performance measures .

of the states that provided additional information on the areas in which they wanted assistance , 4 of 16 states at the secondary level and 9 of 20 states at the postsecondary level said that they wanted assistance on the technical skill attainment measure .

specifically , some of the states that provided additional information said they would like education to clarify its expectations for this measure , to provide states with a library of technical assessments , and to provide state - specific assistance with developing low - cost , effective technical assessments .

states also raised issues regarding the performance measures and their state's data collection challenges .

for example , one state reported that it was unsure how a state should report technical skill attainment as a single measure for over 400 distinct cte programs .

we found that education officials were aware of states' need for additional assistance and that education has taken some actions to address these needs , particularly in the area of technical assessments .

for example , through the next steps work group , education facilitated a technical skills attainment subgroup that is led by state officials and a national research organization .

the subgroup reviewed state perkins plans and annual reports for technical skill assessment strategies that states reported to education for consideration in upcoming guidance .

education also collaborated with mpr associates to conduct a study on the feasibility of a national technical assessment clearinghouse and test item bank .

the study , conducted with several cte research organizations and state - level consortia , proposed national clearinghouse models for technical assessments .

mpr associates concluded that clarifying ownership , such as who is responsible for the development and management of the system , and securing start - up funding were the two most likely impediments to creating such a system .

the report was presented to states at the october 2008 data quality institute seminar , and education officials reported that they are working with organizations such as the national association for state directors of career and technical education consortium and the council of chief state school officers to implement next steps .

in addition to helping states with the technical skill attainment measure , education also has taken actions to improve its information - sharing web site , the peer collaborative resource network .

specifically , a next steps work group subcommittee surveyed states for suggested ways to improve the web site and reported that states wanted to see the information on the site kept more current .

the subcommittee reported in december 2008 that education would use the survey results to develop a work plan to update the web site .

in may 2009 , education officials reported that they had implemented the work plan and were piloting the revamped site with selected state cte directors before the department finalizes and formally launches the site .

state performance measures are the primary source of data available to education for determining the effectiveness of cte programs , and education relies on student outcomes reported through these measures to gauge the success of states' programs .

while perkins iv requires states to evaluate their programs supported with perkins funds , it only requires states to report to education — through their state plans — how they intend to evaluate the effectiveness of their cte programs .

it does not require states to report on the findings of their evaluations and does not provide any specific guidance on how states should evaluate their programs .

because only 2 of 11 measures have been implemented and reported on thus far , education has little information to date on program outcomes .

in program year 2007-2008 , education required states to implement and report only the academic skill attainment and graduation rate measures .

states are required to provide education with outcome data for the remaining 9 secondary and postsecondary measures in december 2009 .

according to education's annual report for program year 2007-2008 , 43 states met their targets for the academic attainment in reading / language arts measure , 38 states met their targets for the academic attainment in mathematics measure , and 46 states met their targets for the graduation rate measure .

we analyzed the state plans of all 50 states and the district of columbia and found that , as required by perkins iv , states provide a description to education on how they are evaluating their cte programs .

the type of information that states provided varied .

for example , some states described the databases they use to capture key data and others explained how they use state - developed performance measures to evaluate their programs .

perkins iv does not require that states include information on what their evaluations may have found in terms of the success of a program .

in our surveys of state cte directors , nearly half of states ( 23 states at the secondary level and 21 states at the postsecondary level ) responded that they have conducted or sponsored a study , in the past 5 years , to examine the effectiveness of their cte programs .

in response to these survey results , we collected seven studies that states identified as evaluations of their program effectiveness .

we developed an instrument for evaluating these studies and determined the type of evaluation and methodology used by states in these studies .

we determined that four of the studies were outcome evaluations and the remaining three studies were not outcome , impact , or process evaluations .

for example , one state found in its outcome evaluation that high school graduates who completed a cte program of study entered postsecondary institutions directly after high school at the same rate as all graduates .

perkins iv provides states with considerable flexibility in how they implement the required performance measures and how they evaluate the effectiveness of their cte programs .

while this flexibility enables states to structure and evaluate their programs in ways that work best for them , it may hinder education's ability to gain a broader perspective on the success of state cte programs .

specifically , differences in how states collect data for some performance measures may challenge education's ability to aggregate student outcomes at a national level and compare student outcomes on a state - by - state basis .

further , education is limited in what it knows about the effectiveness of state cte programs , beyond what states report through the performance measures .

perkins only requires that states report on how they are evaluating their programs , and does not provide any guidance on how states should evaluate their programs or require that states report on the outcomes of their evaluations .

education is working with states to help them overcome challenges they face in collecting and reporting student outcomes , and over time , states may collect more consistent data for measures such as technical skill attainment .

as states become more adept at implementing the perkins performance measures , they will be better positioned to conduct more rigorous evaluations of their cte programs .

however this information may not be reported to education .

if policymakers are interested in obtaining information on state evaluations , they will need to weigh the benefits of education obtaining this information with the burden of additional reporting requirements .

we provided a draft of this report and the electronic supplement to the department of education for review and comment .

education provided technical comments on the report , which we incorporated as appropriate .

education had no comments on the electronic supplement .

we are sending copies of this report to appropriate congressional committees , the secretary of education , and other interested parties .

in addition , the report will be available at no charge on gao's web site at http: / / www.gao.gov .

if you or your staff have any questions about the report , please contact me at ( 202 ) 512-7215 or scottg@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff that made major contributions to this report are listed in appendix ii .

to obtain national - level information on states' implementation of perkins iv , we designed and administered two web - based surveys , at the secondary and postsecondary levels , to state directors of career and technical education ( cte ) in the 50 states and the district of columbia .

the surveys were conducted between january and april 2009 , with 100 percent of state cte directors responding to each survey .

the surveys included questions about the types of data states collect for the student performance measures and challenges they face ; the various kinds of technical assistance , guidance , and monitoring states received from education ; and how states evaluate their cte programs .

the surveys and a more complete tabulation of the results can be viewed at gao - 09-737sp .

because this was not a sample survey , there are no sampling errors .

however , the practical difficulties of conducting any survey may introduce nonsampling errors , such as variations in how respondents interpret questions and their willingness to offer accurate responses .

we took steps to minimize nonsampling errors , including pretesting draft survey instruments and using a web - based administration system .

specifically , during survey development , we pretested draft instruments with officials in minnesota , washington state , and vermont in december 2008 .

we also conducted expert reviews with officials from the national association of state directors of career and technical education consortium and mpr associates , who provided comments on the survey .

in the pretests and expert reviews , we were generally interested in the clarity of the questions and the flow and layout of the survey .

for example , we wanted to ensure that terms used in the surveys were clear and known to the respondents , categories provided in closed - ended questions were complete and exclusive , and the ordering of survey sections and the questions within each section were appropriate .

on the basis of the pretests and expert reviews , the web instruments underwent some revisions .

a second step we took to minimize nonsampling errors was using web - based surveys .

by allowing respondents to enter their responses directly into an electronic instrument , this method automatically created a record for each respondent in a data file and eliminated the need for and the errors associated with a manual data entry process .

when the survey data were analyzed , a second , independent analyst checked all computer programs to further minimize error .

while we did not fully validate all of the information that state officials reported through our surveys , we reviewed the survey responses overall to determine that they were complete and reasonable .

we also validated select pieces of information by corroborating the information with other sources .

for example , we compared select state responses with information submitted to education in state perkins plans .

on the basis of our checks , we believe our survey data are sufficiently reliable for the purposes of our work .

to better understand perkins iv implementation at the state and local levels , we conducted site visits to three states — california , minnesota , and washington state — between september 2008 and february 2009 .

in each state we spoke with secondary and postsecondary officials at the state level with cte and perkins responsibilities .

we also interviewed officials from local recipients of perkins funds — that is , school districts and postsecondary institutions .

through our interviews with state and local officials , we collected information on efforts to implement the perkins performance measures and uses of perkins funding , experiences with education's monitoring and technical assistance , and methods for cte program evaluation .

the states we selected represent variation across characteristics such as the type of state agency ( i.e. , state educational agencies or state college and university systems ) eligible to receive perkins funds , the amount of perkins iv funds received in fiscal year 2008 , and type of approach used to measure student attainment of technical skills .

the localities selected for site visits provided further variation in geographic location ( urban versus rural ) , number of cte students served , and amount of perkins funding received .

we conducted this performance audit from august 2008 to july 2009 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

in addition to the contact named above , elizabeth morrison ( assistant director ) , avani locke , robin nye , charlotte gamble , stephen steigleder , jessica orr , jean mcsween , christine san , and jessica botsford made key contributions to this report .

