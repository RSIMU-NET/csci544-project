billions of taxpayer dollars are spent on information technology ( it ) investments each year ; federal it spending has now risen to an estimated $79 billion for fiscal year 2011 .

during the past several years , we have issued several reports and testimonies and made numerous recommendations to the office of management and budget ( omb ) to improve the transparency , oversight , and management of the federal government's it investments .

in june 2009 , omb deployed a public website , known as the it dashboard , which provides detailed information on federal agencies' major it investments , including assessments of actual performance against cost and schedule targets ( referred to as ratings ) for approximately 800 major federal it investments .

the dashboard aims to improve the transparency and oversight of these investments .

this report responds to your request that we ( 1 ) examine the accuracy of the cost and schedule performance ratings on the dashboard for selected investments and ( 2 ) determine whether the data on the dashboard are used as a management tool to make improvements to it investments .

to address our first objective , we selected five agencies — the departments of agriculture ( usda ) , defense ( dod ) , energy ( doe ) , health and human services ( hhs ) , and justice ( doj ) — and ten investments to review .

to select these agencies and investments , we first identified ten agencies with large it budgets , and then identified the five largest investments at each of the ten agencies .

in narrowing the list to five agencies and ten total investments , we considered several factors to ensure there were two viable investments at each agency , such as selecting investments that were not part of our ongoing audit work and providing a balance of investment sizes .

we then collected and analyzed monthly investment performance reports from the ten investments .

we compared our analyses of each investment's performance to the ratings on the dashboard to determine if the information was consistent .

we also reviewed and analyzed omb's and the selected agencies' processes for populating and updating the dashboard .

additionally , we interviewed officials from omb and the agencies to obtain further information on their efforts to ensure the accuracy of the data on the dashboard .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on the documentation the agencies provided .

to address our second objective , we interviewed officials and analyzed documentation at the selected agencies to determine the extent to which they use the data on the dashboard to make management decisions .

we also attended one of omb's techstat sessions , which are reviews of selected it investments between omb and agencies .

we conducted this performance audit from january to july 2010 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

further details of our objectives , scope , and methodology are provided in appendix i .

each year , omb and federal agencies work together to determine how much the government plans to spend on it projects and how these funds are to be allocated .

planned federal it spending has now risen to an estimated $79.4 billion for fiscal year 2011 , a 1.2 percent increase from the 2010 level of $78.4 billion .

omb plays a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 , which requires omb to establish processes to analyze , track , and evaluate the risks and results of major capital investments in information systems made by federal agencies and report to congress on the net program performance benefits achieved as a result of these investments .

further , the act places responsibility for managing investments with the heads of agencies and establishes chief information officers ( cio ) to advise and assist agency heads in carrying out this responsibility .

another key law is the e - government act of 2002 , which requires omb to report annually to congress on the status of e - government .

in these reports , referred to as the implementation of the e - government act reports , omb is to describe the administration's use of e - government principles to improve government performance and the delivery of information and services to the public .

to help carry out its oversight role and assist the agencies in carrying out their responsibilities as assigned by the clinger - cohen act , omb developed a management watch list in 2003 and a high risk list in 2005 to focus executive attention and to ensure better planning and tracking of major it investments .

consistent with the clinger - cohen act , omb reported on the status of investments on the management watch list and high risk list in its annual budget documents .

over the past several years , we have reported and testified on omb's initiatives to highlight troubled projects , justify investments , and use project management tools .

we have made multiple recommendations to omb and federal agencies to improve these initiatives to further enhance the oversight and transparency of federal projects .

among other things , we recommended that omb develop a central list of projects and their deficiencies and analyze that list to develop governmentwide and agency assessments of the progress and risks of the investments , identifying opportunities for continued improvement .

in addition , in 2006 we also recommended that omb develop a single aggregate list of high - risk projects and their deficiencies and use that list to report to congress on progress made in correcting high - risk problems .

as a result , omb started publicly releasing aggregate data on its management watch list and disclosing the projects' deficiencies .

furthermore , omb issued governmentwide and agency assessments of the projects on the management watch list and identified risks and opportunities for improvement , including risk management and security .

table 1 provides a historical perspective of the number of projects on the management watch list and their associated budgets for the period of time during which omb updated the management watch list .

the table shows that while the number of projects and their associated budgets on the list generally decreased , the number of projects on the management watch list increased by 239 projects and $13 billion for fiscal year 2009 , and represented a significant percentage of the total budget .

more recently , to further improve the transparency into and oversight of agencies' it investments , and to address data quality issues , in june 2009 , omb publicly deployed a web site , known as the it dashboard , which replaced the management watch list and high risk list .

it displays information on federal agencies' cost , schedule , and performance information for the approximately 800 major federal it investments at 28 federal agencies .

according to omb , these data are intended to provide a near real - time perspective of the performance of these investments , as well as a historical perspective .

further , the public display of these data are intended to allow omb , other oversight bodies , and the general public to hold the government agencies accountable for results and progress .

the dashboard was initially deployed in june 2009 based on each agency's exhibit 53 and exhibit 300 submissions .

after the initial population of data , agency cios have been responsible for updating cost , schedule , and performance fields on a monthly basis , which is a major improvement from the quarterly reporting cycle omb previously used for the management watch list and high risk list .

for each major investment , the dashboard provides performance ratings on cost and schedule , a cio evaluation , and an overall rating which is based on the cost , schedule , and cio ratings .

the cost rating is determined by a formula that calculates the amount by which an investment's aggregated actual costs deviate from the aggregated planned costs .

table 2 displays the rating scale and associated category for cost variations .

an investment's schedule rating is calculated by determining the average days late or early .

table 3 displays the rating scale and associated category for schedule deviations .

each major investment on the dashboard also includes a rating determined by the agency cio , which is based on his or her evaluation of the performance of each investment .

the rating is expected to take into consideration the following criteria: risk management , requirements management , contractor oversight , historical performance , and human capital .

this rating is to be updated when new information becomes available that would impact the assessment of a given investment .

lastly , the dashboard calculates an overall rating for each major investment .

figure 1 identifies the dashboard's overall ratings scale .

this overall rating is an average of the cost , schedule , and cio ratings , with each representing one - third of the overall rating .

however , when the cio's rating is lower than both the cost and schedule ratings , the cio's rating will be the overall rating .

of the 792 major investments on the dashboard as of may 2010 , 540 ( 68 percent ) were green , 204 ( 26 percent ) were yellow , and 48 ( 6 percent ) were red .

earned value management is a technique that integrates the technical , cost , and schedule parameters of a development contract and measures progress against them .

during the planning phase , a performance measurement baseline is developed by assigning and scheduling budget resources for defined work .

as work is performed and measured against the baseline , the corresponding budget value is “earned.” using this earned value metric , cost and schedule variances , as well as cost and time to complete estimates , can be determined and analyzed .

without knowing the planned cost of completed work and work in progress ( i.e. , the earned value ) , it is difficult to determine a program's true status .

earned value allows for this key information , which provides an objective view of program status and is necessary for understanding the health of a program .

as a result , earned value management can alert program managers to potential problems sooner than using expenditures alone , thereby reducing the chance and magnitude of cost overruns and schedule slippages .

moreover , earned value management directly supports the institutionalization of key processes for acquiring and developing systems and the ability to effectively manage investments — areas that are often found to be inadequate on the basis of our assessments of major it investments .

in august 2005 , omb issued guidance that all major and high - risk development projects , among other things , develop comprehensive policies to ensure that their major it investments use earned value management to manage their investments .

cost and schedule performance ratings were not always accurate for the selected investments we reviewed .

a key reason for the inaccuracies is that the dashboard's cost and schedule ratings do not reflect current performance .

another issue with the ratings is that large inconsistencies exist in the number of milestones that agencies report on the dashboard .

the cost and schedule performance ratings of selected investments were not always accurate .

there were several instances of inaccurate cost ratings ; however , two investments experienced notable discrepancies while the other discrepancies were not as dramatic .

specifically , 5 of the 8 selected investments on the dashboard had inaccurate cost ratings: biosense , financial management modernization initiative , joint precision approach and landing system , law enforcement wireless communication , and unified financial management system .

for example , the dashboard rated the law enforcement wireless communication investment a 10 for cost ( less than 5 percent variance ) every month from july 2009 through january 2010 .

however , our analysis shows the investment's cost rating during december 2009 and january 2010 is equivalent to an 8 ( a variance of 10 percent to less than 15 percent ) .

accordingly , this investment's cost performance should have been rated a “yellow” instead of a “green,” meaning it needed attention .

further , the dashboard's cost rating for the financial management modernization initiative reported that this investment was “yellow,” while it should have been “green” for 7 months .

maneuver control system , sequoia platform , and risk management agency - 13 are the three investments that had accurate cost ratings .

figure 2 shows the comparison of selected investments' dashboard cost ratings to gao's ratings for the months of july 2009-january 2010 .

there were fewer instances of discrepancies with the schedule ratings ; however , these discrepancies were also notable .

specifically , of the 8 selected investments , the dashboard's schedule ratings were inaccurate for 2 investments: risk management agency - 13 and the unified financial management system .

the unified financial management system's last completed milestone was in may 2009 and the dashboard rating for the investment's schedule has been a 10 since july 2009 .

however , investment data we examined showed the schedule rating should have been a 5 ( greater than or equal to 30 days and less than 90 days behind schedule ) from september 2009 through december 2009 .

as a result , this investment's schedule performance should have been rated a “yellow” instead of a “green” for those months .

additionally , the dashboard's schedule rating for risk management agency - 13 reported that this investment was “red” for two months , while it should have been “green,” and “yellow” for four months , when it should have been “green.” biosense , financial management modernization initiative , joint precision approach and landing system , law enforcement wireless communication , maneuver control system , and sequoia platform are the 6 investments that had accurate schedule ratings .

figure 3 shows the comparison of selected investments' dashboard schedule ratings to gao's ratings for the months of july 2009-january 2010 .

in addition to determining that cost and schedule ratings are not always accurate , we found other data inaccuracies .

specifically , rebaseline information on the dashboard was not always accurate .

best practices and gao's cost estimating guide state that a rebaseline should occur when the current cost and schedule baseline does not adequately represent the amount of work to be completed , causing difficulty in monitoring progress of the program .

however , omb reports all major and minor corrections to planned information on the dashboard , includ typographical fixes , as a rebaseline .

more specifically , while the dashboard allows agencies to provide reasons for baseline changes , the current version of the dashboard , at a high level , identifies all changes to planned information as rebaselines .

for example , according to the dashboard , doj's law enforcement wireless communication investment has been rebaselined four times .

however , program officials stated that the program has only been rebaselined once .

similarly , the dashboard shows that the sequoia platform and integrated management navigation system investments at doe have both been rebaselined four times .

however , program officials stated that neither of these programs had actually been rebaselined .

rather , they stated that this number represents instances in which they made minor corrections to the data on the dashboard .

table 4 shows the selected investments whose program officials reported a lower number of rebaselines than what was reported on the dashboard .

a primary reason why the cost and schedule ratings were not always accurate is that the cost and schedule ratings do not take current performance into consideration for many investments on the dashboard , though it is intended to represent near real - time performance information on all major it investments .

specifically , as of april 2010 , the formula to calculate the cost ratings on the dashboard intentionally only factored in completed portions of the investments ( referred to as milestones ) to determine cost ratings .

as such , milestones that are currently under way are not taken into account .

table 5 identifies each selected investment's last completed milestone and the number of days that the dashboard's cost rating is out of date for each selected investment .

omb officials agreed that the ratings not factoring in current performance is an area needing improvement and said that they are planning on upgrading the dashboard application in july 2010 to include updated cost and schedule formulas that factor in the performance of ongoing milestones ; however , they have not yet made this change .

one step omb has taken toward collecting the information needed for the new formulas is that it now requires agencies to provide information on their investment milestones' planned and actual start dates .

in addition , omb officials stated that they plan to use a previously unused data field — percent complete .

these are key data points necessary to calculate the performance of ongoing milestones .

another issue with the ratings is that there were wide variations in the number of milestones agencies reported .

for example , doe's integrated management navigation system investment lists 314 milestones , whereas dod's joint precision approach and landing system investment lists 6 .

having too many milestones may mask recent performance problems because the performance of every milestone ( i.e. , historical and recently completed ) is equally averaged into the ratings .

specifically , investments that perform well during many previously completed milestones and then start performing poorly on a few recently completed milestones can maintain ratings that still reflect good performance .

a more appropriate approach could be to give additional weight to recently completed and ongoing milestones when calculating the ratings .

too many detailed milestones also defeat the purpose of an executive - level reporting tool .

conversely , having too few milestones can limit the amount of information available to track work and rate performance and allows agencies to potentially skew the performance ratings .

in commenting on a draft of this report , the federal cio stated that omb has a new version of the dashboard that implements updated cost and schedule calculations .

he stated that the new calculations greatly increase the weight of current activities .

as of july 1 , 2010 , this updated dashboard had not been released .

an omb analyst subsequently told us that the agency plans to release the new version in july 2010 .

additionally , omb officials have provided us with documentation of the new calculations and demonstrated the new version of the dashboard that will be released soon .

the federal cio also added that omb will consider additional changes to the ratings in the future .

table 6 demonstrates the large inconsistencies in the number of milestones reported for each selected investment .

in june 2009 , omb issued guidance that agencies are responsible for providing quality data and , at minimum , should provide milestones that consist of major segments of the investment , referred to as work breakdown structure level 2 , but prefers that agencies provide lower - level milestones within each segment ( work breakdown structure level 3 ) .

a work breakdown structure is the cornerstone of every program because it defines in detail the work necessary to accomplish a program's objectives .

standardizing a work breakdown structure is considered a best practice because it enables an organization to collect and share data among programs .

further , standardizing work breakdown structures allows data to be shared across organizations .

however , certain agencies are not following omb's guidance and list milestones that they consider to be at work breakdown structure level 1 , which are high - level milestones .

specifically , of the 5 agencies we reviewed , officials at dod , usda , and doe stated that they were reporting work breakdown structure level 1 milestones to the dashboard for each of their selected investments .

omb officials acknowledge that not all agencies are following their guidance , but stated that omb analysts are working with agencies to try to improve compliance .

furthermore , the guidance that omb has provided is not clear on the level of detail that it wants agencies to report in their milestones and has left it to the agencies to individually interpret their general guidance .

specifically , while omb states that agencies should report milestones that are , at a minimum , work breakdown structure level 2 , there is no commonly accepted definition among federal agencies on the level of detail that should comprise each of these levels .

omb officials acknowledged that they have not provided clear guidance , but recently stated that they have begun exploring ways to ensure more uniformity across agencies' reporting .

specifically , in commenting on a draft of this report , the federal cio stated that omb has recently chartered a working group comprised of representatives from several federal agencies , with the intention of developing clear guidance for standardizing and improving investment activity reporting .

omb and agencies acknowledge that additional improvements can be made beyond the cost and schedule ratings and have taken certain steps to try to improve the accuracy of the data .

for example , omb implemented an automated monthly data upload process and created a series of data validation rules that detect common data entry errors , such as investment milestone start dates that occur after completion dates .

in addition , four of the five agencies we reviewed indicated that they have processes in place aimed at improving the accuracy of the data .

for instance , hhs has established a process wherein an official has been assigned responsibility for ensuring the dashboard is accurately updated .

further , doj has developed an automated process to find missing data elements in the information to be uploaded on the dashboard .

despite these efforts , until omb upgrades the dashboard application to improve the accuracy of the cost and schedule ratings to include ongoing milestones , explains the outcome of these improvements in its next annual report to congress on the implementation of the e - government act ( which is a key mechanism for reporting on the implementation of the dashboard ) , provides clear and consistent guidance to agencies that standardizes milestone reporting , and ensures agencies comply with the new guidance , the dashboard's cost and schedule ratings will likely continue to experience data accuracy issues .

officials at three of the five agencies we reviewed — dod , doj , and hhs — stated that they are not using the dashboard to manage their investments , and the other two agencies , doe and usda , indicated that they are using the dashboard to manage their investments .

specifically , officials from the three agencies are not using the dashboard to manage their investments because they have other existing means to do so: dod officials indicated that they use the department's capital planning and investment control process to track it investment data — including cost and schedule .

doj uses an internal dashboard that the office of the cio developed that provides for more detailed management of investments than omb's dashboard .

hhs officials said they use a portfolio investment management tool , which they indicated provides greater insight into their investments .

officials from the other two agencies — doe and usda — noted that they are using the dashboard as a management tool to supplement their existing internal processes to manage their it investments .

doe officials stated that since their current process is based on a quarterly review cycle , the monthly reporting nature of the dashboard has allowed officials to gain more frequent insight into investment performance .

as a result , doe officials say that they are able to identify potential issues before these issues present problems for investments .

usda officials stated that they use the ratings on the dashboard to identify investments that appear to be problematic and hold meetings with the investments' program managers to discuss corrective actions .

additionally , in omb's fiscal year 2009 report to congress on the implementation of the e - government act of 2002 , 11 agencies reported on how the dashboard has increased their visibility and awareness of it investments .

for example , the department of veterans' affairs terminated 12 it projects , partly because of the increased visibility that the cio obtained from the dashboard .

omb indicated that it is using the dashboard to manage it investments .

specifically , omb analysts are using the dashboard's investment trend data to track changes and identify issues with investments' performance in a timely manner and are also using the dashboard to identify and drive investment data quality issues .

the federal cio stated that the dashboard has greatly improved oversight capabilities compared to previously used mechanisms .

he also stated that the dashboard has increased the accountability of agencies' cios and established much - needed visibility .

according to omb officials , the dashboard is one of the key sources of information that omb analysts use to identify investments that are experiencing performance problems and select them for a techstat session — a review of selected it investments between omb and agency leadership that is led by the federal cio .

omb has identified factors that may result in a techstat session , such as policy interests , dashboard data inconsistencies , recurring patterns of problems , and / or an omb analyst's concerns with an investment .

as of june 2010 , omb officials indicated that 27 techstat sessions have been held with federal agencies .

according to omb , this program enables the government to improve or terminate it investments that are experiencing performance problems .

omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard .

however , the cost and schedule ratings on the dashboard were not always accurate .

further , the rebaseline data were not always accurate .

the cost and schedule inaccuracies were due , in part , to calculations of ratings that did not factor in current performance .

additionally , there were large inconsistencies in the number of milestones that agencies report on the dashboard because omb has not fully defined the level of detail that federal agencies should use to populate the dashboard and several selected agencies decided not to follow omb's general guidance .

moreover , the performance of historical and recently completed milestones are equally averaged in the cost and schedule ratings , which is counter to omb's goal to report near real - time performance on the dashboard .

while the use of the dashboard as a management tool varies , omb has efforts under way to include the performance of ongoing milestones and its officials acknowledge that additional improvements are needed .

nevertheless , until omb explains in its next annual implementation of the e - government act report how the upgrade to the dashboard application has improved the accuracy of the cost and schedule ratings , and provides clear and consistent guidance that enables agencies to report standardized information on their milestones , the accuracy of the data on the dashboard may continue to be in question .

to better ensure that the it dashboard provides meaningful ratings and accurate investment data , we are recommending that the director of omb take the following two actions: include in its next annual implementation of the e - government act report the effect of planned formula changes on the accuracy of data ; and develop and issue clear guidance that standardizes milestone reporting on the dashboard .

in addition , we are recommending that the secretaries of the departments of agriculture , defense , and energy direct their chief information officers to ensure that they comply with omb's guidance on standardized milestone reporting , once it is available .

we received written comments on a draft of this report from the federal cio and doe's associate cio for it planning , architecture , and e - government .

letters from these agencies are reprinted in appendixes iii and iv .

in addition , we received technical comments via e - mail from a coordinator at hhs , which we incorporated where appropriate .

in addition , the deputy cio from usda , the principal director to the deputy assistant secretary of defense for resources from dod , and an audit liaison specialist from doj indicated via e - mail that they had reviewed the draft report and did not have any comments .

in omb's comments on our draft report , which contained four recommendations to the omb director , the federal cio stated that he agreed with two recommendations and disagreed with two because of actions omb has recently taken .

after reviewing these actions , we agreed that they addressed our concerns and will not make these two recommendations .

omb agreed with our recommendation that it include in its next annual implementation of the e - government act report how the planned formula changes have improved the accuracy of data .

omb agreed with our recommendation that it develop and issue clear guidance that standardizes milestone reporting on the dashboard .

additionally , the federal cio asked that we update the report to reflect that they have recently chartered a working group comprised of representatives from several federal agencies , with the intention of developing clear guidance for standardizing and improving investment activity reporting .

we have incorporated this additional information into the report .

in response to our draft recommendation that omb revise the it dashboard and its guidance so that only major changes to investments are considered to be rebaselines , omb provided us with its new guidance on managing it baselines , which was issued on june 28 , 2010 .

the guidance , among other things , describes when agencies should report baseline changes on the dashboard .

omb also provided documentation of the specific modifications that will be made in an upcoming release of the dashboard to improve the way baseline changes are displayed .

we agree that these recent changes address our recommendation .

as such , we updated the report to acknowledge and include this additional information , where appropriate .

regarding our recommendation that omb consider weighing recently completed and ongoing milestones more heavily than historical milestones in the cost and schedule ratings , the federal cio stated that omb has a new version of the dashboard that implements updated cost and schedule calculations .

he stated that the new calculations greatly increase the weight of current activities .

as previously stated , as of july 1 , 2010 , this updated dashboard had not been released .

an omb analyst subsequently told us that the agency plans to release the new version in july 2010 .

additionally , omb officials have provided us with documentation of the new calculations and demonstrated the new version of the dashboard that will be released soon .

the federal cio also added that omb will consider additional changes to the ratings in the future .

we agree that these recent changes address our recommendation .

as such , we updated the report to acknowledge and include this additional information , where appropriate .

additionally , omb will report on the effect of the upcoming changes to the calculations in its next annual implementation of the e - government act report .

omb also provided additional comments , which we address in appendix iii .

in doe's comments on our draft report , the associate cio for it planning , architecture , and e - government indicated that she agreed with our assessment of the implementation of the it dashboard across federal agencies and with the recommendations presented to omb .

additionally , in response to our recommendation that the cio of doe comply with omb guidance on milestone reporting once it is available , the associate cio stated that once omb releases the additional guidance , doe officials will work to ensure the appropriate level of detail is reported on the dashboard .

doe also provided an additional comment , which we address in appendix iv .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to interested congressional committees ; the director of the office of management and budget ; the secretaries of the departments of agriculture , defense , energy , health and human services , and justice ; and other interested parties .

in addition , the report will be available at no charge on our web site at http: / / www.gao.gov .

if you or your staffs have any questions on the matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix v .

our objectives were to ( 1 ) examine the accuracy of the cost and schedule performance ratings on the dashboard for selected investments and ( 2 ) determine whether the data on the dashboard are used as a management tool to make improvements to it investments .

to address both objectives , we selected five agencies and ten investments to review .

to select these agencies and investments , we first identified ten agencies with large it budgets as reported in the office of management and budget's ( omb ) fiscal year 2010 exhibit 53 .

we then identified the five largest investments at each of the ten agencies , according to the fiscal year 2010 budget , that were spending more than half of their budget on it development , modernization , and enhancement work , and were primarily carried out by contractors .

in narrowing the list to five agencies and ten total investments , we considered several factors to ensure there were two viable investments at each agency: the investment is not part of our ongoing audit work related to cost , schedule , and technical performance .

the investment is not part of a recent governmentwide earned value management review .

the investment has not been highlighted as an investment needing significant attention .

the collective list of investments creates a balance of investment sizes to include both larger and smaller investments .

the five agencies are: the departments of agriculture ( usda ) , defense ( dod ) , energy ( doe ) , health and human services ( hhs ) , and justice ( doj ) .

the ten investments are: usda's financial management modernization initiative and risk management agency - 13 program ; dod's joint precision approach and landing system and maneuver control system ; doe's integrated management navigation system and sequoia platform ; hhs's biosense program and electronic research administration system ; doj's law enforcement wireless communication and unified financial management system ( see appendix ii for descriptions of each investment ) .

to address the first objective , we evaluated earned value data of the selected investments to determine their cost and schedule performance and compared it to the ratings on the dashboard .

the investment earned value data was contained in contractor earned value management performance reports obtained from the programs .

to perform this analysis , we compared the investment's cumulative cost variance for each month from july 2009 through january 2010 to the cost variance reported on the dashboard for those months .

similarly , we calculated the number of months each investment was ahead or behind schedule over the same period on the dashboard .

we also assessed 13 months of investment data to analyze trends in cost and schedule performances .

to further assess the accuracy of the cost data , we compared it with other available supporting program documents , including monthly and quarterly investment program management reports ; electronically tested the data to identify obvious problems with completeness or accuracy ; and interviewed agency and program officials about the data and earned value management systems .

for the purposes of this report , we determined that the cost data at eight of the investments were sufficiently reliable to use for our assessment .

for the two remaining investments , we determined that based on their methods of earned value management , the data would not allow us to sufficiently assess and rate monthly investment performance .

we did not test the adequacy of the agency or contractor cost - accounting systems .

our evaluation of these cost data was based on the documentation the agency provided .

we also reviewed and analyzed omb's and the selected agencies' processes for populating and updating the dashboard .

additionally , we interviewed officials from omb and the selected agencies and reviewed omb guidance to obtain additional information on omb's and agencies' efforts to ensure the accuracy of the investment performance data and cost and schedule performance ratings on the dashboard .

we used the information provided by omb and agency officials to identify the factors contributing to inaccurate cost and schedule performance ratings on the dashboard .

moreover , we used this information to examine the accuracy of the rebaseline information on the dashboard , we interviewed agency and program officials about the number of rebaselines each investment has had , and compared these data with the rebaseline information listed on the dashboard .

to address our second objective , we analyzed related agency documentation to assess what policies or procedures they have implemented for using the data on the dashboard to make management decisions .

we also interviewed agency and program officials regarding the extent to which they use the data on the dashboard as a management tool .

additionally , we attended one of omb's techstat sessions , which are reviews of selected it investments between omb and agencies .

we conducted this performance audit from january to july 2010 at the selected agencies' offices in the washington , d.c. , metropolitan area .

our work was done in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

below are descriptions of each of the selected investments that are included in this review .

the financial management modernization initiative is usda's financial management system modernization program .

it is intended to be the central financial system for usda and is to consolidate the current financial management system environment from 19 legacy systems into one web - based system .

usda's risk management agency - 13 program is intended to support the reengineering of all business systems associated with the crop insurance program and provide a central financial system that will provide web - based tools and applications for accessing risk management agency data .

dod's joint precision approach and landing system investment is intended to provide a precision approach and landing capability for all dod ground and airborne systems .

it is intended to enable u.s. forces to safely land aircraft on any suitable surface worldwide ( land and sea ) , with ceiling and / or visibility the limiting factor .

dod's maneuver control system investment is intended to provide , among other things , the warfighter environment and collaborative and situational awareness tools used to support executive decision making , planning , rehearsal , and execution management .

this system is to be used throughout the army to provide a common view of critical information .

doe's integrated management navigation system consists of 5 major projects and is intended to standardize and integrate accounting , data warehouse , human resource , procurement , and budget processes throughout doe .

the integrated management navigation system incorporates enterprisewide projects from doe's office of the chief financial officer , office of human capital management , and office of management .

doe's sequoia platform is a supercomputer being developed for use by three weapons laboratories — los alamos , lawrence livermore , and sandia national laboratories — to contribute dramatically to the national security enterprise .

this supercomputer will also be used in maintaining the nuclear deterrence and areas of nonproliferation , nuclear counterterrorism , and support to the intelligence community .

hhs's biosense program is intended to improve the nation's capabilities for disease detection , monitoring , and near real - time health situational awareness by creating a system that uses data from existing health - related databases to identify patterns of disease symptoms prior to specific diagnoses .

hhs's electronic research administration program is the national institutes of health's system for conducting interactive electronic transactions for the receipt , review , monitoring , and administration of grant awards to biomedical investigators worldwide .

it is also intended to provide the technology capabilities for the agency to efficiently and effectively perform grants administration functions .

doj's law enforcement wireless communication system , also known as the integrated wireless network , is to support the replacement and modernization of failing radio systems and achieve communication standards at doj's law enforcement agencies .

this program is intended to provide all four law enforcement components with a shared unified radio network , which should eliminate redundant coverage and duplicative radio sites , while providing efficient and comparable coverage .

doj's unified financial management system is to improve the existing and future financial management and procurement operations across doj .

upon full implementation , the unified financial management system will replace five financial management systems and multiple procurement systems with an integrated commercial off - the - shelf solution .

this is to streamline and standardize business processes and procedures across the doj components .

table 7 provides additional details for each of the selected investments in our review .

the following is gao's response to the office of management and budget's ( omb ) additional comments .

1 .

we agree that the dashboard has increased transparency , accountability , and oversight ; therefore , we updated the report to discuss additional uses of the dashboard , such as the use of trend data , improved oversight capabilities , and enhancements to agencies' investment management processes .

we also updated the number of techstat sessions that have taken place .

2 .

while additional data quality issues need to be addressed in the dashboard , we agree that the dashboard is an improvement when compared to omb's previous oversight tools such as the management watch list and high risk list .

as such , we modified the report to highlight these improvements .

for example , we added to the report that the dashboard's monthly reporting cycle is a significant improvement in the quality of the data from the management watch list and high risk list , which were updated on a quarterly basis .

3 .

as stated in the report , we found that the ratings were not always accurate .

we based this characterization on the fact that there were several instances in which the ratings were inconsistent with the performance indicated in our analysis of the investments' earned value management ( evm ) reports and were notably different ( eg , ratings of “green” versus “yellow” ) .

we agree that evm data generally only covers the contracted development parts of the investments .

as such , as part of our methodology , we specifically selected investments where the majority of each investment was focused on development efforts ( versus operational ) and primarily carried out by contractors .

as such , we maintain that the comparison between the selected investments' dashboard ratings and the performance indicated in their evm reports is a fair assessment .

4 .

we acknowledge that the quality of evm reports can vary .

as such , we took steps to ensure that the evm reports we used were reliable enough to evaluate the ratings on the dashboard , and as omb's comments indicate , we discounted two of the ten selected investments after determining that their data was insufficient for our needs .

we do not state that omb should base their ratings solely on evm data .

5 .

we agree that the original cost and schedule calculations are performing as planned ( i.e. , are not defective ) and we further clarified this point in the report .

we also note that planned changes to the rating calculations will incorporate current performance .

however , these calculations , as originally planned and implemented , do not factor in the performance of ongoing milestones , which we and omb agree is an area for improvement .

6 .

we agree that the severity of the discrepancies were not always dramatic .

however , 4 of the 8 investments had notable discrepancies on either their cost or schedule ratings .

specifically , as demonstrated in the report , there were multiple instances in which the ratings were discrepant enough to change the color of the ratings .

the difference between a “green” rating ( i.e. , normal performance ) and a “yellow” rating ( i.e. , needs attention ) is the difference between whether an investment is flagged for needing attention or not , which we believe is an important point to highlight .

7 .

we agree that agencies have a responsibility to provide quality milestone data ; however , we maintain that omb's existing guidance on which milestones to report is too general for agencies to ensure they are reporting consistently .

omb acknowledges that this is an area for improvement and has established a working group to address this issue .

8 .

as previously discussed , on june 28 , 2010 , omb issued its new guidance on managing it baselines .

this guidance , among other things , describes when agencies should report baselines changes to the dashboard .

officials also provided information on the upcoming release of the dashboard — which is intended to be released in july 2010 — that will change the way baseline changes are displayed .

we agree that these recent changes address the issues we identified .

9 .

we acknowledge that the dashboard has made significant improvements to oversight and transparency , in comparison to omb's previous methods of overseeing it investments , and we have added additional information to the background of the report to highlight this point .

the following is gao's response to the department of energy's ( doe ) additional comment .

omb's guidance required agencies to provide data at one consistent work breakdown structure level , rather than a mix of multiple levels .

omb and others confirmed that agencies were able to transmit milestones at a single consistent level .

for this report , we observed agencies uploading at levels 1 through 4 and , thus , disagree that agencies were unable to transmit milestones lower than level 1 .

in addition to the contact name above , the following staff also made key contributions to this report: shannin o'neill , assistant director ; carol cha ; eric costello ; rebecca eyler ; emily longcore ; bradley roach ; and kevin walsh .

