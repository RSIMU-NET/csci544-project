a well - defined enterprise architecture provides a clear and comprehensive picture of the structure of any enterprise , whether it is an organization or a functional or mission area .

this structure is defined in models that describe ( in both business and technology terms ) how the entity operates today and how it intends to operate in the future ; it also includes a plan for transitioning to this future state .

such an architecture is an essential tool for leveraging information technology ( it ) in the transformation of business and mission operations .

as our experience with federal agencies has shown , attempting to modernize and evolve it environments without an enterprise architecture to guide and constrain investments often results in operations and systems that are duplicative , not well integrated , unnecessarily costly to maintain and interface , and ineffective in supporting mission goals .

a properly managed enterprise architecture helps to clarify and optimize the interdependencies and relationships among enterprise operations and their supporting it assets , so that agencies can base it investment decisions on an explicit and common understanding of both today's and tomorrow's environments .

the development , implementation , and maintenance of architectures are widely recognized as hallmarks of successful public and private organizations , and their use is required by the clinger - cohen act and the implementing guidance , issued by the office of management and budget ( omb ) .

further , the e - government act of 2002 assigns omb responsibility for overseeing enterprise architectures .

in light of the importance of these architectures , you requested that we review the state of their use in the federal government .

as agreed , our objectives were to determine ( 1 ) what progress federal agencies have made in effectively developing , implementing , and maintaining their enterprise architectures and ( 2 ) omb's actions to advance the state of enterprise architecture development and use across the federal government .

we also collected a variety of related information on agency enterprise architecture experiences and practices , which are described in appendixes i and ii of this report .

to accomplish these objectives , we surveyed federal agencies using a questionnaire that was based on our maturity framework for assessing and improving enterprise architecture management .

we then analyzed agency questionnaire responses and compared them with the results of a similar analysis that we conducted in 2001 .

to corroborate certain questionnaire responses , we requested that agencies provide us with supporting documentation .

for agency responses related to the content of their respective architectures , we relied on agency responses and did not independently assess the quality of agency enterprise architecture products .

further details of our objectives , scope , and methodology are in appendix iii .

the concept of an architecture to describe an enterprise first emerged in the mid - 1980s , and over the years , various frameworks for defining the content of enterprise architectures have been published .

our work in the early 1990s identified architectures as critical success factors in allowing organizations to effectively apply it to meet mission goals .

since then , we have worked with the congress , omb , and the federal chief information officers ( cio ) council to promote the importance of architectures and assist agencies in developing , maintaining , and using them .

in our reviews of selected agency it management practices and major systems modernization programs , we have continued to identify the lack of an architecture as a major management weakness , and we have made recommendations to address this important area .

in simple terms , an enterprise can be viewed as any purposeful activity , and an architecture can be characterized as the structure ( or structural description ) of any activity .

building on this , we can view enterprise architectures as systematically derived and captured structural descriptions — in useful models , diagrams , and narrative — of the mode of operation for a given enterprise , which can be either a single organization or a functional or mission area that transcends more than one organizational boundary ( eg , financial management , homeland security ) .

the architecture describes the enterprise's operations in both logical terms ( such as interrelated business processes and business rules , information needs and flows , and work locations and users ) and technical terms ( such as hardware , software , data , communications , and security attributes and performance standards ) .

it provides these perspectives both for the enterprise's current ( or “as - is” ) environment and for its target ( or “to - be” ) environment , as well as a transition plan for moving from the “as - is” to the “to - be” environment .

the importance of enterprise architectures is a basic tenet of it management , and their effective use is a recognized hallmark of successful public and private organizations .

for over a decade , we have promoted the use of architectures , recognizing them as a crucial means to a challenging goal: that is , agency operational structures that are optimally defined , in terms of both business and technology .

the alternative , as our work has shown , is perpetuation of the kinds of operational environments that saddle most agencies today , in which the lack of integration among business operations and the it resources that support them leads to systems that are duplicative , not well integrated , and unnecessarily costly to maintain and interface .

managed properly , an enterprise architecture can clarify and help optimize the interdependencies and relationships among an organization's business operations and the underlying it infrastructure and applications that support these operations .

employed in concert with other important it management controls ( such as portfolio - based capital planning and investment control practices ) , architectures can greatly increase the chances that organizations' operational and it environments will be configured so as to optimize mission performance .

enterprise architectures are integral to managing large - scale programs as well as initiatives that span several agencies , such as those currently being undertaken to support the electronic government ( e - government ) efforts led by omb .

during the mid - 1980s , john zachman , widely recognized as a leader in the field of enterprise architecture , identified the need to use a logical construction blueprint ( i.e. , an architecture ) for defining and controlling the integration of systems and their components .

accordingly , zachman developed a structure or framework for defining and capturing an architecture , which provides for six “windows” from which to view the enterprise .

zachman also proposed six abstractions or models associated with each of these perspectives .

zachman's framework provides a way to identify and describe an entity's existing and planned component parts , and the relationships between those parts , before the entity begins the costly and time - consuming efforts associated with developing or transforming itself .

since zachman introduced his framework , a number of frameworks have emerged within the federal government , beginning with the publication of the national institute of standards and technology ( nist ) framework in 1989 .

since that time , other federal entities have issued enterprise architecture frameworks , including the department of defense ( dod ) and the department of the treasury .

in september 1999 , the federal cio council published the federal enterprise architecture framework ( feaf ) , which was intended to provide federal agencies with a common construct for their architectures , thereby facilitating the coordination of common business processes , technology insertion , information flows , and system investments among federal agencies .

the feaf describes an approach , including models and definitions , for developing and documenting architecture descriptions for multiorganizational functional segments of the federal government .

more recently , omb established the federal enterprise architecture program management office to develop a federal enterprise architecture ( fea ) according to a collection of five “reference models,” which are intended to facilitate governmentwide improvement through cross - agency analysis and the identification of duplicative investments , gaps , and opportunities for collaboration , interoperability , and integration within and across government agencies .

the fea reference models are summarized in table 1 .

although these post - zachman frameworks differ in their nomenclatures and modeling approaches , each consistently provides for defining an enterprise's operations in both logical terms and technical terms , provides for defining these perspectives for the enterprise's current and target environments , and calls for a transition plan between the two .

several laws and regulations have established requirements and guidance , respectively , for agencies' management of architectures , beginning with the clinger - cohen act in 1996 , which directs the cios of major departments and agencies to develop , maintain , and facilitate the implementation of it architectures as a means of integrating agency goals and business processes with it .

omb circular a - 130 , which implements the clinger - cohen act , requires that agencies document and submit their initial enterprise architectures to omb and that agencies submit updates to omb when significant changes to their enterprise architectures occur .

the circular also directs the omb director to use various kinds of reviews to evaluate the adequacy and efficiency of each agency's compliance with the circular .

omb was given explicit responsibility for overseeing government enterprise architectures by the e - government act of 2002 , which established the office of electronic government within omb .

this act gives omb the responsibility for facilitating the development of enterprise architectures within and across agencies and supporting improvements in government operations through the use of it .

we began reviewing federal agencies' use of architectures in 1994 , initially focusing on those agencies that were pursuing major systems modernization programs that were high risk .

these included the national weather service systems modernization , the federal aviation administration air traffic control modernization , and the internal revenue service ( irs ) tax systems modernization .

generally , we reported that these agencies' enterprise architectures were incomplete , and we made recommendations that they develop and implement complete enterprise architectures to guide their modernization efforts .

since then , we have reviewed architecture management at other federal agencies , including the department of education , the former customs service , the former immigration and naturalization service , and the centers for medicare and medicaid services .

we have also reviewed the use of enterprise architectures for critical agency functional areas , such as the integration and sharing of terrorist watch lists across key federal departments , and the logistics management area within dod .

these reviews have continued to identify the absence of complete and enforced enterprise architectures , which in turn has led to agency business operations , systems , and data that are not integrated ( “stovepiped” ) , duplicative , and incompatible .

these conditions have either prevented agencies from sharing data or forced them to depend on expensive , custom - developed interface systems to do so .

in 2002 , we published version 1.0 of our enterprise architecture management maturity framework ( eammf ) to provide federal agencies with a common benchmarking tool for planning and measuring their enterprise architecture efforts , as well as to provide omb with a means for doing the same governmentwide .

this framework is an extension of a practical guide to federal enterprise architecture , version 1.0 , published by the cio council .

the framework arranges core elements from the practical guide into a matrix of five hierarchical stages and four critical success attributes ; that is , each core element appears at a particular stage of maturity , and it is also associated with one of the critical success attributes .

in april 2003 , we published version 1.1 of this framework , which reflects changes and additions that are based on comments we received on the initial version .

the eammf is made up of five stages of maturity , each of which includes an associated set of elements , along with all of the elements of the previous stages .

table 2 shows these stages , followed by the description of each as contained in version 1.0 of our framework .

stage 1: creating ea awareness .

agencies at this stage are characterized either by no plans to develop and use an enterprise architecture , or by plans and actions that do not yet demonstrate an awareness of the value of having and using one .

although stage 1 agencies may have initiated some enterprise architecture core elements , these agencies' efforts are ad hoc and unstructured , and they do not provide the management foundation that is necessary for successful enterprise architecture development .

stage 2: building the ea management foundation .

the focus at stage 2 is on assignment of roles and responsibilities and establishment of plans for developing enterprise architecture products .

specifically , a stage 2 agency has designated a chief architect and established and staffed a program office that is responsible for enterprise architecture development .

further , a steering committee or group that has responsibility for directing and overseeing the development has been established , and the membership of the steering committee includes business and it representatives .

at stage 2 , the agency either has plans for developing or has begun development of at least some of the necessary enterprise architecture products .

this stage also requires the agency to have selected both a framework that will be the basis for the nature and content of the specific products it plans to develop and an automated tool to help in the development .

stage 3: developing architecture products .

at stage 3 , an agency focuses on actual development of enterprise architecture products .

the agency has defined the scope of its enterprise architecture as encompassing the entire enterprise , whether an organization or a functional area , and it has a written and approved policy demonstrating institutional commitment .

although the products may not yet be complete , they are intended to describe the agency in terms of business , data , applications , and technology .

further , the products are to describe the current and future states and the sequencing plan for transitioning from current to future state .

as the architecture products are being developed , they are to be subject to configuration control .

stage 4: completing ea products .

an agency at stage 4 has complete and approved enterprise architecture products that it can use to help select and control its portfolio of it investments .

the complete products describe the organization in terms of business , data , applications , and technology .

also , the products are complete in that they describe the agency's current and future states and the transition plan for sequencing from the current state to the future state .

further , the agency's cio has approved the enterprise architecture , and the agency has a written policy requiring that it investments comply with the enterprise architecture .

stage 5: leveraging the ea to manage change .

at stage 5 , an agency is able to evolve the enterprise architecture products according to a written and approved policy for maintaining the architecture .

also at this stage , the steering committee , investment review board , or agency head approves the enterprise architecture .

finally , the agency has incorporated the enterprise architecture into its corporate decision making , and it has established and is using metrics to measure the effectiveness of its enterprise architecture .

in addition to the maturity stages , each core element is also associated with attributes that are critical to the successful performance of any management function ( see table 3 ) .

the critical success attributes are identical in versions 1.0 and 1.1 of our framework .

attribute 1: demonstrates commitment .

because the enterprise architecture is a corporate asset for systematically managing institutional change , the support and sponsorship of the head of the enterprise are essential to the success of the architecture effort .

an approved enterprise policy statement provides such support and sponsorship , promoting institutional “buy in” and encouraging resource commitment from participating components .

equally important in demonstrating commitment is vesting ownership of the architecture with an executive body that collectively owns the enterprise .

attribute 2: provides capability to meet commitment .

the success of the enterprise architecture effort depends largely on the organization's capacity to develop , maintain , and implement the enterprise architecture .

consistent with any large it project , these capabilities include providing adequate resources ( i.e. , people , processes , and technology ) ; defining clear roles and responsibilities ; and defining and implementing organizational structures and process management controls that promote accountability and effective project execution .

attribute 3: demonstrates satisfaction of commitment .

satisfaction of the organization's commitment to develop , maintain , and implement an enterprise architecture is demonstrated by the production of artifacts ( eg , the plans and products ) .

such artifacts demonstrate “follow through” — actual enterprise architecture production .

satisfaction of commitment is further demonstrated by senior leadership approval of enterprise architecture documents and artifacts ; such approval communicates institutional endorsement and ownership of the architecture and the change that it is intended to drive .

attribute 4: verifies satisfaction of commitment .

this attribute focuses on measuring and disclosing the extent to which efforts to develop , maintain , and implement the enterprise architecture have fulfilled stated goals or commitments .

measuring such performance allows for tracking progress that has been made toward stated goals , allows the appropriate actions to be taken when performance deviates significantly from goals , and creates incentives to influence both institutional and individual behaviors .

collectively , these attributes form the basis by which an organization can institutionalize management of any given function or program , such as enterprise architecture management .

within each stage , each critical success attribute includes between one and four core elements , which are descriptions of a practice or condition that is needed for effective enterprise architecture management .

on the basis of the implicit dependencies among the core elements , the eammf associates each element with one of five hierarchical management stages , referred to as maturity stages .

each stage reflects the collection of enterprise architecture management practices and conditions ( i.e. , core elements ) that are being undertaken by an enterprise at a given maturity level .

figure 1 is a summary of version 1.0 of the framework , showing the key elements associated with the stages and attributes previously described .

version 1.1 of this framework was released in april 2003 .

like the initial version , version 1.1 is based on the cio council guidance and augmented by our research experience in reviewing architecture programs .

changes and additions to the framework were also based on comments received on the initial version .

as a comparison between the two frameworks shows , a number of new elements have been added to version 1.1 .

figure 2 shows a summary of the new framework , version 1.1 .

the stages and attributes remain the same as with version 1.0 , although the descriptions of the stages are updated in version 1.1 to reflect the new elements in the framework , as follows: stage 1: creating ea awareness .

as with version 1.0 , at stage 1 , either an organization does not have plans to develop and use an architecture , or it has plans that do not demonstrate an awareness of the value of having and using an architecture .

although stage 1 agencies may have initiated some enterprise architecture activity , these agencies' efforts are ad hoc and unstructured , lack institutional leadership and direction , and do not provide the management foundation that is necessary for successful enterprise architecture development as defined in stage 2 .

stage 2: building the ea management foundation .

an organization at stage 2 recognizes that the enterprise architecture is a corporate asset by vesting accountability for it in an executive body that represents the entire enterprise .

at this stage , an organization assigns enterprise architecture management roles and responsibilities and establishes plans for developing enterprise architecture products and for measuring program progress and product quality .

an organization at this stage also commits the necessary resources for developing an architecture — people , processes , and tools .

specifically , a stage 2 organization has designated a chief architect and established and staffed a program office that is responsible for enterprise architecture development and maintenance .

further , it has established a committee or group that has responsibility for enterprise architecture governance ( i.e. , directing , overseeing , and approving architecture development and maintenance ) .

this committee or group membership has enterprisewide representation .

at stage 2 , the organization either has plans for developing or has started developing at least some enterprise architecture products , and it has fostered an enterprisewide awareness of the value of enterprise architecture and its intended use in managing its it investments .

the organization has also selected a framework and a methodology that will be the basis for developing the enterprise architecture products and has selected a tool for automating these activities .

stage 3: developing the ea .

an organization at stage 3 focuses on developing architecture products according to the selected framework , methodology , tool , and established management plans .

roles and responsibilities assigned in the previous stage are in place , and resources are being applied to develop actual enterprise architecture products .

at this stage , the scope of the architecture has been defined to encompass the entire enterprise , whether an organization or a functional area .

although the products may not be complete , they are intended to describe the organization in terms of business , performance , information / data , service / application , and technology ( including security explicitly in each ) , as provided for in the framework , methodology , tool , and management plans .

further , the products are to describe the current ( “as - is” ) and future ( “to - be” ) states and the plan for transitioning from the current to the future state ( the sequencing plan ) .

as the products are developed and evolve , they are subject to configuration management .

further , through the established enterprise architecture management foundation , the organization is tracking and measuring its progress against plans ; identifying and addressing variances , as appropriate ; and then reporting on its progress .

stage 4: completing the ea .

an organization at stage 4 has completed its enterprise architecture products , meaning that the products have been approved by the enterprise architecture steering committee ( established in stage 2 ) or an investment review board , and by the cio .

the completed products collectively describe the enterprise in terms of business , performance , information / data , service / application , and technology for both its current and future operating states , and the products include a sequencing plan for transitioning from the current to the future state .

further , an independent agent has assessed the quality ( i.e. , completeness and accuracy ) of the enterprise architecture products .

additionally , evolution of the approved products is governed by a written enterprise architecture maintenance policy that is approved by the head of the organization .

stage 5: leveraging the ea to manage change .

an organization at stage 5 has secured senior leadership approval of the enterprise architecture products and a written institutional policy stating that it investments must comply with the architecture , unless granted an explicit compliance waiver .

further , decision makers are using the architecture to identify and address ongoing and proposed it investments that are conflicting , overlapping , not strategically linked , or redundant .

as a result , stage 5 entities avoid unwarranted overlap across investments and ensure maximum systems interoperability , which in turn ensures the selection and funding of it investments with manageable risks and returns .

also , at stage 5 , the organization tracks and measures enterprise architecture benefits or return on investment , and adjustments are continuously made to both the enterprise architecture management process and the enterprise architecture products .

overall , version 1.1 is more demanding ( i.e. , sets a higher standard ) than version 1.0 because version 1.1 adds important content , clarifies existing content , and links the eammf framework to related it management guidance , such as our it investment management framework .

key differences in version 1.1 of the framework appear first in stage 2 and affect later stages either explicitly or implicitly .

that is , some planning elements associated with stage 2 now propagate explicitly through later stages as plans are executed and architecture products are developed , completed , and implemented .

for example: version 1.1 includes “performance” among the models that are needed to describe the “as - is” and “to - be” environments ; these models are introduced into the planning elements in stage 2 and built upon as plans are executed: that is , as architecture products are developed and completed in stages 3 and 4 , respectively .

version 1.1 explicitly recognizes the need to address security in the descriptions of the “as - is” and “to - be” environments ; this element is introduced in stage 2 and reiterated in stages 3 and 4 .

version 1.1 introduces the need to plan for metrics in stage 2 and to measure different aspects of enterprise architecture development , quality , and use in stages 3 , 4 , and 5 .

other differences introduced in version 1.1 affect later stages implicitly , since each stage includes all elements of previous stages .

for example , in stage 2 , an element has been added that recognizes the need for adequate resources ( people , processes , and technology ) .

this element appears in stage 2 explicitly , but it is included in later stages implicitly .

stage 4 now includes an element requiring that enterprise architecture products and management processes undergo independent verification and validation ; this element continues in stage 5 .

in addition , two core elements , both in stage 2 , have been altered from version 1.0 , as follows: enterprise architecture maintenance , in addition to development , is now included among the responsibilities of the program office .

the use of an enterprise architecture methodology is added to the use of a framework and automated tool in developing the architecture .

last , the sequence of two elements ( the policies on maintenance and on it investment compliance with the architecture ) is reversed in version 1.1 .

that is , maintenance policy is now associated with stage 4 and investment compliance with stage 5 .

this reordering reflects greater alignment of these elements with the definitions of their respective framework stages .

finally , several new elements were added to stage 5 that provide for maximizing the value and use of the enterprise architecture by keeping it current and using it to manage change ( including the existence of a process to formally manage enterprise architecture change , the enterprise architecture being an integral component of the it investment management process , the periodic updating of enterprise architecture products , and the compliance of it investments with the enterprise architecture ) .

these and the other changes are summarized in table 4 .

we first surveyed enterprise architecture management maturity across the federal government in 2001 , and we reported in february 2002 that about 52 percent of federal agencies reported having at least the management foundation that is needed to begin successfully developing , implementing , and maintaining an enterprise architecture , and that about 48 percent of agencies had not yet advanced to that basic stage of maturity .

at the other extreme , about 4 percent of federal agencies' enterprise architecture efforts had matured to the point that they could be considered effective , with one agency attaining the highest stage of maturity .

this overall state of affairs was consistent for the three agency types that we surveyed: cabinet - level departments ( eg , treasury ) ; department component agencies ( eg , irs , which is a component of treasury ) ; and independent agencies ( eg , social security administration ) .

we also reported that the state of architecture management across the federal government was attributable to four management challenges that agencies reported facing as they attempt to develop and use architectures .

these challenges were ( 1 ) overcoming limited executive understanding , ( 2 ) inadequate funding , ( 3 ) insufficient skilled staff , and ( 4 ) organizational parochialism .

additionally , we recognized omb's efforts to promote and oversee agencies' enterprise architecture efforts .

nevertheless , we determined that omb's leadership and oversight could be improved by , for example , using a more structured means of measuring agencies' progress and by addressing the above management challenges .

to this end , our february 2002 report provided omb with the necessary baseline data , improvement framework , and several recommendations .

omb generally agreed with our findings and conclusions in that report and stated that it would consider our recommendations .

our 2003 survey results indicate that while some individual agencies have made progress in improving their enterprise architecture management maturity , progress for the federal government as a whole has not occurred .

specifically , while about one - fourth of all agencies improved their enterprise architecture management maturity stage relative to version 1.0 of our framework , about one - fourth of all agencies decreased in maturity and about one - half of all agencies remained at the same stage .

furthermore , the more demanding standard established by our framework version 1.1 caused a decline in agency maturity levels , demonstrating that improvements are needed before agencies' enterprise architecture management practices can be considered effective .

the average maturity stage for the 96 responses included in our survey was 1.76 when measured against version 1.0 of our framework and 1.33 when compared with version 1.1 of our framework .

appendix iv provides a list of these individual agencies and their maturity stages .

overall , little substantial change was revealed in agencies' overall enterprise architecture maturity when their efforts were compared with version 1.0 of our framework .

of the 93 agencies included in both our 2001 and 2003 surveys , 22 agencies ( 24 percent ) increased their respective eammf maturity stages , 24 agencies ( 26 percent ) decreased their stages , and 47 agencies ( 51 percent ) remained the same .

 ( see fig .

3. ) .

at the department level , 4 departments increased their maturity stage , 4 decreased , and 6 stayed at the same stage .

the department of homeland security — which began operations as a department in march 2003 — debuted at stage 3 .

although progress for agencies in the aggregate continued to be limited , departments as a group made the most progress: the average maturity for the 14 departments that responded to both the 2001 and 2003 surveys increased from 1.93 to 2.00 against version 1.0 of the framework .

in contrast , component agencies showed a slight decline in maturity against version 1.0 .

specifically , of the component agencies that responded to both surveys , 9 increased their maturity stage , 15 decreased in maturity , and 31 stayed the same , with the average maturity stage decreasing from 1.69 to 1.62 .

for independent agencies that responded to both surveys , 9 increased their maturity stage , 5 decreased in maturity , and 10 stayed at the same stage .

on average , independent agencies showed an increase in maturity , from 1.75 to 1.96 against version 1.0 .

figure 4 summarizes the maturity status of departments , components , independent agencies , and all agencies , according to version 1.0 of our framework , and compares our 2001 and 2003 survey results .

most agencies that made progress from 2001 to 2003 moved from a lower maturity stage to stage 2 or 3 ( as shown in fig .

4 , most agencies were clustered in stages 1 and 2 , so this is not unexpected ) .

in particular , of the 22 agencies that increased their maturity stage , 6 increased from stage 1 to stage 2 , and 12 increased from stage 1 or 2 to stage 3 .

most agencies that regressed fell to stage 1 from stages 2 and 3 .

specifically , of the 24 agencies that decreased their maturity stage , 16 decreased to stage 1 from stage 2 or 3 .

figure 5 shows the number of agencies whose maturity levels improved and declined between 2001 and 2003 as measured against version 1.0 of our maturity framework .

agencies' progress since our first survey is similarly limited when we consider the total number of core elements satisfied .

the 93 agencies that responded to both the 2001 and 2003 surveys satisfied an average of about 11 of the 19 elements in version 1.0 in both 2001 and 2003 .

as a whole , the 93 agencies satisfied about 57 percent of all possible framework elements in 2001 and about 60 percent of all possible framework elements in 2003 .

from 2001 to 2003 , agencies showed improvements in satisfying certain core elements , but these improvements were offset by declines in agency satisfaction of other core elements .

examples of core elements where agency satisfaction significantly improved are as follows: “metrics exist for measuring ea benefits” ( about a 38 percent increase ) , “chief architect exists” ( about a 23 percent improvement ) , and “ea products are under configuration management” ( about an 18 percent increase ) .

examples of core elements where agency satisfaction significantly declined are as follows: “ea products describe ‘as - is' environment , ‘to - be' environment , and sequencing plan” ( about a 39 percent decrease ) ; “ea products describe enterprise's business — and the data , applications , and technology that support it” ( about a 36 percent decrease ) ; “either ea steering committee , investment review board , or agency head has approved ea” ( about a 25 percent decrease ) ; and “program office responsible for ea development exists” ( about a 23 percent decrease ) .

figures 6 to 10 show the number of agencies that satisfied the framework elements in each stage of version 1.0 in 2001 and in 2003 .

appendixes v , vi , and vii provide detailed tables showing each of the 93 agencies' status regarding the elements of the framework .

for the 22 agencies that advanced one or more maturity stages from 2001 to 2003 , fulfillment of no single core element resulted in these advancements .

that is , for the 22 agencies , increases in maturity stages are attributable to the fulfillment of 7 core elements spanning three stages of maturity .

table 5 shows those newly satisfied core elements that accounted for increases in maturity stage .

as with increases in agency maturity levels , no single core element accounted for the decreases in agency maturity between our 2001 and 2003 surveys .

however , as shown in table 6 , the stage 2 framework element requiring a program office was the most significant newly unsatisfied element for the 24 agencies that decreased maturity levels .

one factor accounting for decreases in maturity is improved accuracy in agencies' responses to our survey .

improved accuracy is a function of ( 1 ) improved agency familiarity with and understanding of enterprise architecture management and our framework since our last survey and ( 2 ) the requirement in our 2003 survey for documentation to support certain survey responses .

when compared with version 1.1 of our framework , the state of enterprise architecture management across the federal government is not mature .

in particular , about 21 percent of federal agencies ( 20 of 96 ) have the stage 2 management foundation that is needed to begin successfully developing , implementing , and maintaining an enterprise architecture , and about 79 percent of agencies ( 76 of 96 ) have not yet advanced to this basic stage of maturity .

one agency , the executive office of the president , provided responses placing it at a stage of enterprise architecture management maturity that can be considered mature and effective .

this overall state of federal government maturity is consistent for each of the three groups that make up the 96 agencies surveyed: departments , component agencies , and independent agencies .

figure 11 summarizes the maturity status of departments , component agencies , independent agencies , and all agencies according to version 1.1 of our framework .

no single core element that was added to our framework contributed significantly to this situation , but the “methodology” subelement of the stage 2 element “ea is being developed with a framework , methodology , and automated tool” was the most significant factor keeping agencies from achieving stage 2 .

specifically , the absence of a “methodology” kept 7 agencies from attaining stage 2 status .

nevertheless , certain core elements of version 1.1 of our framework were frequently not satisfied by agencies .

of the 31 core elements in version 1.1 , 17 were not satisfied by over 50 percent of agencies .

furthermore , 8 elements associated with maturity stages 4 and 5 were not satisfied by over 80 percent of agencies .

figures 12 to 16 show how departments , component agencies , and independent agencies were rated against each of the version 1.1 core elements .

although significant gaps existed across federal agencies in meeting the core elements of version 1.1 of the framework , at least 80 percent of agencies reported performing 8 core elements that were related to stages 2 and 3 of our framework .

the most often satisfied elements included the following stage 2 elements: “ea plans call for describing both the ‘as - is' and the ‘to - be' environments of the enterprise , as well as a sequencing plan for transitioning from the ‘as - is' to the ‘to - be'” ( about 94 percent ) ; “ea plans call for describing both the ‘as - is' and the ‘to - be' environments in terms of business , performance , information / data , application / service , and technology” ( about 90 percent ) ; and “ea plans call for business , performance , information / data , application / service , and technology descriptions to address security” ( about 86 percent ) .

the most often satisfied elements also included the stage 3 element: “ea products describe or will describe both the ‘as - is' and the ‘to - be' environments of the enterprise , as well as a sequencing plan for transitioning from the ‘as - is' to the ‘to - be'” ( about 88 percent ) .

in addition , although only one agency has achieved stage 5 , most agencies reported satisfying the stage 5 core elements requiring that it investments comply with their enterprise architecture ( about 80 percent ) and that enterprise architecture is an integral component of it investment management process ( about 69 percent ) .

furthermore , 96 percent of agencies in stages 1 through 4 are performing at least 1 core element above their current maturity stage , which means that agencies as a whole are , to varying degrees , performing above their assigned maturity stages .

specifically , of the 76 agencies at stage 1 , about 95 percent are performing at least 1 core element in a higher maturity stage .

about 35 percent of agencies need to satisfy only 1 additional core element to advance to at least the next maturity stage .

two of these agencies , commerce and the u.s. mint , could advance two stages by satisfying just 1 additional core element .

commerce , currently a stage 1 agency , could advance to stage 3 by satisfying the framework element “program office responsible for development and maintenance exists.” the mint , also currently a stage 1 agency , could advance to stage 3 by satisfying the framework element “adequate resources exist.” departments , component agencies , and independent agencies had varying degrees of success satisfying certain core elements within individual stages .

in general , departments had more success satisfying lower stage elements than did components and independent agencies .

in stage 2 , for example , while 69 percent of departments reported using a framework , methodology , and automated tool to develop their enterprise architecture , only 29 percent of components and 50 percent of independent agencies reported the same .

additionally , in stage 3 , while 81 percent of departments reported that progress against plans is measured and reported , only 25 percent of components and 25 percent of independent agencies reported the same .

one possible reason for this situation , which is discussed later in this report , is that omb's oversight of agency enterprise architecture efforts focuses on departments and major independent agencies — not on component agencies .

although , as a whole , departments satisfied more lower level framework elements than did component agencies and independent agencies , departments generally still need to satisfy several lower level framework elements to achieve a stage 3 maturity level .

on average , each department needs to satisfy 2 core elements to satisfy all stage 2 and 3 framework elements .

the maturity stage of a department generally was not indicative of the maturity of its component agencies .

for example , the departments of health and human services and transportation reached stage 2 , while their component agencies averaged stage 1 .

dod's global information grid ( gig ) architecture was at stage 3 and its business enterprise architecture was at stage 1 , while dod components averaged slightly over 1 .

conversely , the departments of commerce , justice , and the treasury were at stage 1 , with their component agencies averaging higher maturity levels .

component agencies of commerce showed a slightly higher maturity level than did component agencies of other departments .

although the average maturity level of the 56 department component agencies we surveyed was 1.23 , the five commerce component agencies showed an average maturity level of 1.80 , largely owing to the maturity levels for the bureau of the census ( stage 3 ) , the u.s. patent and trademark office ( stage 2 ) , and the national oceanic and atmospheric administration ( stage 2 ) .

the department of agriculture's maturity level ( stage 1 ) was the same as the average maturity level of its component agencies .

figure 16 summarizes the average maturity level for departments and their respective component agencies .

the results of our survey and analysis of survey responses against version 1.1 of our maturity framework show that the executive office of the president ( eop ) is the sole stage 5 agency .

however , 7 other agencies are close to becoming models of enterprise architecture management .

for example , the dod gig architecture and irs , both of which attained stage 3 of version 1.1 , need to satisfy only 3 more elements to become stage 5 agencies .

to achieve stage 5 , the gig architecture needs to satisfy the stage 4 element “ea products describe both the ‘as - is' and the ‘to - be' environments of the enterprise , as well as a sequencing plan for transitioning from the ‘as - is' to the ‘to - be' ” and the stage 5 elements “return on ea investment is measured and reported” and “organization head has approved current version of ea.” irs could become a stage 5 agency by satisfying the stage 4 elements “business , performance , information / data , application / service , and technology descriptions address security” and “ea products and management processes undergo independent verification and validation” and the stage 5 element “return on ea investment is measured and reported.” table 7 shows the agencies that need to satisfy 5 or fewer elements to achieve stage 5 under version 1.1 .

omb has taken a number of steps to promote , standardize , and improve enterprise architecture use across the government .

for example , omb now requires agencies to submit enterprise architectures for review .

it also leads various cio council initiatives to develop the fea , including associated models , and to facilitate cross - agency efforts and major initiatives such as e - government .

however , despite omb's actions , the same management challenges facing agencies 2 years ago have increased in prevalence , and agencies report mixed results from omb's efforts to address these challenges .

the persistence of these challenges can be attributed , at least in part , to the office not implementing our prior recommendations aimed at addressing them and improving its enterprise architecture oversight .

omb recognizes the importance of enterprise architectures and has supported their use since the passage of the clinger - cohen act of 1996 , with particular emphasis and attention in the last 2 years .

for example , in collaboration with others and us , omb issued guidance on the purpose and use of enterprise architectures shortly after passage of the act .

it has also incorporated enterprise architecture considerations into its oversight processes and issued guidance directing that agency it investments be based on agency it enterprise architectures .

more recently , it has launched efforts to promote the development and use of enterprise architectures through the budget process and various cio council initiatives .

as a means of promoting agencies' enterprise architecture use , omb has also included requirements for having and using enterprise architectures as part of the budget process , which began with the fiscal year 2002 budget cycle and , according to omb officials , has continued through the current cycle ( fiscal year 2005 ) .

more specifically: for the fiscal year 2002 budget cycle , omb required agency budget submissions to provide investment plans in several areas , including enterprise architectures .

for fiscal year 2003 , omb required departments and major agencies that are cio council members to address how it investment decision making incorporated architecture alignment and , for agencies that do not have architectures , to provide a plan for developing one .

omb also assessed the status of major department and agency architectures against the cio council's practical guide for federal enterprise architecture and reported the assessment results in the president's fiscal year 2003 budget .

however , this assessment covered only 23 of the 96 agencies included in this survey , and assessment results were not reported in a way to permit a clear understanding of the agencies' enterprise architecture management status or to facilitate year - to - year progress determinations .

for example , for the environmental protection agency ( epa ) , the assessment resulted in the following report: “epa has the fundamental elements of an ea documented.” as part of the fiscal year 2004 budget cycle , omb again assessed major department and agency architectures and reported the assessment results in the president's fiscal year 2004 budget .

however , the scope of the assessment again was not as comprehensive and meaningful as our survey results and covered only 22 of the 96 agencies included in this survey .

for example , for the department of agriculture , omb reported , “usda's ea is continuing to focus on the business , data , application , and technology layers of the ea .

usda is also working to integrate the ea efforts throughout the department.” also for the fiscal year 2004 cycle , the office evaluated major it investment business cases for consistency with agency architectures and with the fea business reference model .

omb has also worked through the cio council , which is co - chaired by omb's deputy director of management , to improve enterprise architecture management and use .

specifically , the cio council established the architecture and infrastructure committee to , for example , develop simpler and more consistent enterprise architecture terminology and facilitate cross - agency enterprise architecture efforts .

this committee has three subcommittees that , since being chartered in october 2002 , have organized , appointed leaders , established membership , and begun implementing plans .

the name and objective of each subcommittee are provided below .

the enterprise architecture governance subcommittee was established to provide policy guidance and advice and assistance in the definition , design , and implementation of enterprise architecture discipline and practice throughout the federal government .

it is expected to support the alignment of the fea with agency enterprise architectures and to serve as the core federal group providing advocacy for enterprise architecture integration of business and technology architectures across state , local , and international boundaries .

the emerging technology subcommittee was created to identify technologies with the potential to improve the value and quality of the fea .

the component subcommittee is expected to foster the identification , maturation , use , and reuse of component - based architectures and architectural components in the federal government .

omb's development of the fea is intended to facilitate governmentwide improvement through cross - agency analysis and the identification of duplicative investments , gaps , and opportunities for collaboration , interoperability , and integration within and across government agencies .

according to omb , the result will be a more citizen - centered , customer - focused government that maximizes technology investments to better achieve mission outcomes .

as previously mentioned , the fea will be composed of five reference models: business reference model .

the business reference model serves as the foundation for the fea .

it is intended to describe the federal government's businesses , independent of the agencies that perform them .

the model consists of four business areas: ( 1 ) services for citizens , ( 2 ) mode of delivery , ( 3 ) support delivery of services , and ( 4 ) management of government resources .

these four business areas are decomposed into 39 lines of business , which are made up of 153 subfunctions .

examples of lines of business under the services for citizens business area are homeland security , law enforcement , and economic development .

each of these lines of business includes a number of subfunctions .

for example , for the homeland security line of business , a subfunction is border and transportation security ; for law enforcement , a subfunction is citizen protection ; and for economic development , a subfunction is financial sector oversight .

version 1.0 of the model was released to agencies in july 2002 and was used in the fiscal year 2004 budget process .

according to omb , version 1.0 of the model revealed that many federal agencies were involved in each line of business , and that agencies' proposed fiscal year 2004 it investments offered multibillion - dollar consolidation opportunities .

in june 2003 , version 2.0 was released , which , according to omb , reflects changes to align the model with other governmentwide management frameworks ( eg , budget function codes ) and improvement initiatives ( eg , the president's budget performance integration initiative ) and addresses comments from agencies .

omb expects agencies to use the model , as part of their capital planning and investment control processes , to help identify opportunities to consolidate it investments across the federal government .

service component reference model .

the service component reference model is intended to identify and classify it service ( i.e. , application ) components that support federal agencies and promote the reuse of components across agencies .

the model is organized as a hierarchy , beginning with seven service domains , as shown in table 8 .

these service domains are decomposed into 29 service types , which are further broken down into 168 components .

for example , the customer services domain is made up of 3 service types: customer relationship management , customer preferences , and customer - initiated assistance .

components of the customer relationship management service type include call center management and customer analytics , components of the customer preferences service type include personalization and subscriptions , and components of the customer - initiated assistance service type include on - line help and on - line tutorials .

version 1.0 of the service component reference model was released in june 2003 .

the model is intended to help agencies and omb identify , among other things , agencies that are building or have already built similar service components that can be reused .

technical reference model .

the technical reference model is intended to describe the standards , specifications , and technologies that collectively support the secure delivery , exchange , and construction of service components .

the model is made up of the following four core service areas: service access and delivery: the collection of standards and specifications that support external access , exchange , and delivery of service components .

service platform and infrastructure: the delivery platforms and infrastructure that support the construction , maintenance , and availability of a service component or capabilities .

component framework: the underlying foundation , technologies , standards , and specifications by which service components are built , exchanged , and deployed .

service interface and integration: the collection of technologies , methodologies , standards , and specifications that govern how agencies will interface internally and externally with a service component .

each of these service areas is made up of service categories , which identify lower levels of technologies , standards , and specifications ; service standards , which define the standards and technologies that support the service category ; and the service specification , which details the standard specification or the provider of the specification .

for example , within the first core service area ( service access and delivery ) , an example of a service category is access channels , and service standards are web browsers and wireless personal digital assistants .

examples of service specifications for the web browser service standard are internet explorer and netscape navigator .

version 1.0 of the technical reference model was released in january 2003 , followed by version 1.1 , reflecting minor revisions that were based , in part , on agencies' reviews , in august 2003 .

the model is intended to help agencies in defining their target technical architectures .

performance reference model .

the performance reference model is intended to describe a set of performance measures for the federal government ( i.e. , outcome and output measures for each line of business and subfunction identified in the business reference model ) .

thus , the model is expected to support the measurement of cross - agency initiatives .

version 1.0 of the model was released in september 2003 .

data and information reference model .

the data and information reference model is intended to describe the type of data and information that support program and business line operations and the relationships among these types .

thus , the model is to help describe the types of interactions and information exchanges that occur between the government and its customers .

omb plans to release version 1.0 of the model in october 2003 .

for the fiscal year 2005 budget cycle , omb officials told us that they will use the fea performance , service component , and technical reference models to evaluate agencies' major it investments .

agency responses to our survey indicated high levels of understanding and support for omb's fea work .

for example , about 80 percent of agencies responded that they understand the goals and objectives of the fea ( about 8 percent did not ) and that they support those goals and objectives ( about 6 percent did not ) , and about 72 percent of agencies responded that their agency's architecture is traceable to the fea ( about 6 percent were not ) .

additionally , about 67 percent responded that they understand the approach to developing the fea ( about 13 percent did not ) , and about 63 percent stated that they support this approach ( about 10 percent did not ) .

about 61 percent of agencies responded that their enterprise architecture would change as a result of the fea ( about 8 percent would not ) .

 ( see table 9. ) .

despite omb's architecture - related activities , agencies continue to face the same management challenges that we identified 2 years ago — that is , obtaining top management support and commitment , overcoming parochialism , and having the requisite resources ( financial and human capital ) to get the job done .

moreover , the percentage of agencies identifying these management challenges has grown .

for example , getting top management to understand the purpose , content , and value of architectures was seen as a challenge by about 50 percent of agencies — up from 39 percent in our last survey .

as our framework recognizes , obtaining executive understanding and support is essential to having an effective enterprise architecture program .

without it , agencies will have increased difficulty in addressing other challenges , such as overcoming parochialism among organizational components and obtaining requisite resources ( funding and human capital ) .

our survey results bear this out — at the same time that the percentage of agencies identifying top management understanding and support as a challenge rose , the percentage of agencies identifying these other challenges almost all rose .

for example , the percentage that identified parochialism as a challenge grew from 39 to 47 percent .

also , while 50 percent of agencies continued to report funding as a significant challenge , the percentage of agencies that reported obtaining skilled staff as a challenge grew from 32 to 49 percent .

 ( see table 10. ) .

agencies also reported mixed levels of satisfaction with omb's efforts to address these management challenges .

specifically , just over half of agencies were satisfied with omb's efforts to foster top management understanding and to overcome agency component organization parochialism ( 58 and 53 percent , respectively ) .

moreover , fewer than half of agencies ( 40 percent ) were satisfied with omb's actions to address their enterprise architecture funding and staffing challenges .

 ( see table 11. ) .

our february 2002 report concluded that omb needed to advance the level of enterprise architecture management maturity by exercising improved oversight and identifying governmentwide solutions to common enterprise architecture management challenges facing agencies .

specifically , we recommended that the omb director , in collaboration with the federal cio council , use the maturity framework and agency baseline information provided in our february 2002 report as the basis for helping agencies to advance the state of their respective enterprise architecture development , implementation , and maintenance efforts , and for measuring agency progress .

we further recommended that in doing so , the director require each of the 116 agencies surveyed in our 2002 report to ( 1 ) submit to omb an annual update of the agency's satisfaction of each of the core elements contained in the maturity framework and ( 2 ) have this update verified by the agency's inspector general or comparable audit function before it is submitted to omb .

additionally , we recommended in our 2002 report that the omb director , in collaboration with the cio council , develop and implement a plan to address the governmentwide impediments to greater agency use of enterprise architectures .

we recommended that , at a minimum , this plan should include the two primary challenges identified in the 2002 report — that is , agency executive management understanding of enterprise architectures and the availability of enterprise architecture human capital expertise .

finally , we recommended that the director report annually to the senate committee on governmental affairs and the house committee on government reform on the results of omb's annual update of the state and progress of federal agencies' enterprise architecture efforts .

omb officials generally agreed with the findings and conclusions of our 2002 report and stated that they would consider using our framework .

however , after 18 months , office officials told us that they are still considering using our framework as a basis for evaluating agencies' progress in developing and implementing their architectures , but had not committed to doing so because they were still reviewing the options for evaluating agencies' progress in developing and implementing their enterprise architectures using our framework and other potential tools .

additionally , the office did not report any plans to address governmentwide impediments to greater agency use of architectures .

further , omb reported that it has and plans to continue to provide information to the congress on the state of agency enterprise architecture efforts and on progress in implementing the fea .

overall , the federal government's state of enterprise architecture management remains less than satisfactory , with little progress being made over the last 2 years .

as a result , most federal agencies continue to run the serious risk of investing in it solutions that will not overcome , but rather will perpetuate , long - standing incompatibilities and duplication within agency operational and systems environments .

omb has taken steps to promote the development and use of enterprise architectures ; however , these steps have yet to produce desired results .

it is thus important for omb to take additional actions , such as those that we have previously recommended and omb has yet to implement .

to do less risks continued exposure of agency it investments to the unnecessary risk of being duplicative , incompatible , and needlessly costly .

we reiterate the recommendations we made in our february 2002 report on the governmentwide status of enterprise architecture use , with the modification that omb use version 1.1 of our framework and the baseline data from our 2003 survey included in this report , rather than version 1.0 of our framework and our 2001 survey data .

additionally , we recommend that the omb director , in developing and implementing the plan we previously recommended to address governmentwide impediments to greater agency use of enterprise architectures , ensure that the plan provides for identifying agencies that have effectively overcome enterprise architecture management challenges and sharing those and other lessons learned and best practices .

also , we recommend that the director , in annually reporting to the senate committee on governmental affairs and the house committee on government reform , as we previously recommended , include in the report what steps have been taken to implement our recommendations , including reasons for not adopting our maturity framework .

in oral comments on a draft of this report , officials from omb's office of information and regulatory affairs and the federal enterprise architecture program management office stated that they generally agreed with our findings and recommendations .

they also stated that they agreed with the need for agency assessments using version 1.1 of our framework , and that these assessments should be independently verified .

they added that fully implementing our recommendations would require sustained management attention .

as agreed with your offices , unless you publicly announce its contents earlier , we plan no further distribution of this report until 30 days after the date of this letter .

at that time , we will send copies to interested congressional committees , the omb director , and agencies that participated in our survey .

we will also provide copies to others on request .

in addition , the report will be available at no charge on the gao web site at http: / / www.gao.gov .

if you have any questions concerning this information , please contact me at ( 202 ) 512-3439 or by e - mail at hiter@gao.gov .

key contributors to this report are listed in appendix ix .

in response to our 2003 survey , agencies reported additional information related to the implementation of their enterprise architectures .

this information includes architecture benefits and architecture tool , framework , methodology , and contractor experiences .

office of management and budget ( omb ) policy , chief information officer ( cio ) council guidance , and our research and reviews of agencies' management of information technology ( it ) have identified multiple benefits of effectively using enterprise architectures , including avoiding duplication between it systems , promoting integration of systems , reducing system - related costs , and optimizing agency mission performance .

agency responses to our 2001 survey affirmed these and offered additional benefits , such as lower system - related costs and benefits related to enhanced productivity and improved efficiency .

agencies responding to our 2003 survey reported similar benefits .

for example , benefits related to improved systems interoperability were cited by 53 percent of agencies , while improved organization and change management were cited by 51 percent of agencies .

also , enhanced productivity and lower system - related costs were cited by 41 percent and 39 percent , respectively .

table 12 shows the benefits that were most frequently identified by survey respondents .

one new benefit cited by 56 percent of agencies was the use of “enterprise licenses.” such licenses take advantage of the economies of scale associated with purchasing a large number of commercial product licenses .

an automated enterprise architecture tool serves as the repository of architecture artifacts , which are work products that are produced and used to capture and convey architectural information .

an agency's choice of tool should be based on a number of considerations , including agency needs and the size and complexity of the architecture .

agencies reported using various automated tools to develop and maintain their enterprise architectures .

the most commonly identified architecture products were microsoft office ( 72 agencies ) , system architect ( 31 agencies ) , the enterprise architecture management system ( 18 agencies ) , rational rose ( 17 agencies ) , metis ( 11 agencies ) , and framework ( 7 agencies ) .

forty - one agencies reported using “other” architecture products .

figure 17 indicates the proportion of agencies that indicated using each architecture tool .

agencies reported different levels of satisfaction with the enterprise architecture tools they are using .

as shown in table 13 , about 68 percent of agencies using system architect were satisfied , about 73 percent of agencies using metis were satisfied , and about 61 percent of agencies using microsoft's office suite were satisfied .

in contrast , about 17 percent of agencies using the ea management system were satisfied ( about 67 percent of agencies using ea management system responded that it was too early to comment on satisfaction levels ) , and about 41 and 43 percent of agencies using rational rose and framework , respectively , were satisfied .

with respect to agencies' dissatisfaction with their tools , about 3 percent of agencies using system architect were dissatisfied , and about 13 percent of agencies using microsoft's office suite were dissatisfied .

also , about 11 percent of agencies using the ea management system were dissatisfied , and about 12 and about 29 percent of agencies using rational rose and framework , respectively , were dissatisfied with those tools .

no agencies using metis were dissatisfied .

an enterprise architecture framework ( or model ) provides a formal structure for representing the architecture and serves as the basis for the nature and content of the specific products that the agency plans to develop , use , and maintain .

as such , a framework helps to ensure the consistent representation of information from across the organization and supports orderly capture and maintenance of architecture content .

agencies reported using various frameworks .

the most frequently cited frameworks in our survey responses were the federal enterprise architecture framework ( feaf ) ( 61 agencies ) , the federal enterprise architecture program management office ( feapmo ) reference models ( 56 agencies ) , and the zachman framework ( 36 agencies ) .

figure 18 indicates the proportion of agencies that indicated using each framework .

other frameworks used included the treasury enterprise architecture framework ( teaf ) ; the national institute of standards and technology framework ( nist framework ) ; the command , control , communications , computers , intelligence , surveillance , and reconnaissance ( c4isr ) framework ; and the department of defense architecture framework ( dodaf ) .

agencies reported different levels of satisfaction with the enterprise architecture frameworks they are using , as shown in table 14 .

the levels of satisfaction ranged from 81 percent , reported by agencies using the zachman framework , to 45 percent , reported by agencies using the nist framework .

as table 14 shows , few agencies reported being dissatisfied out of 209 responses .

an enterprise architecture methodology provides a common set of procedures for developing architecture products and , if implemented properly , helps to ensure consistency in the procedures used across the organization for developing and maintaining the enterprise architecture .

an organization's methodology or methodologies should govern how the architecture products will be developed , maintained , and validated .

methodologies need to be documented , understood , and consistently applied .

they should prescribe the standards , steps , tools , techniques , and measures to be used to provide reasonable assurance that expected product quality is attained .

less than half ( 41 percent ) of the federal agencies that we surveyed had selected a methodology .

about 55 percent ( 23 of 42 ) of the methodologies that agencies reported using were spewak's enterprise architecture planning methodology or a variation .

four of the remaining 19 methodologies were developed by meta group , and 2 were developed by gartner , inc. two agencies cited james martin's information strategy planning , and 2 agencies cited the department of commerce's enterprise architecture methodology .

the remaining 21 percent ( 9 of 42 ) were unique methodologies .

agencies reported heavy use of contractor support for developing their respective architectures .

most agencies ( 72 of 92 agencies that responded to this question — 78 percent ) stated that their architectures were developed in - house with contractor support .

ten agencies ( 11 percent ) reported that contractors developed their enterprise architectures .

ten agencies ( 11 percent ) reported that they developed their enterprise architectures in - house without any contractor support .

table 15 describes the level of contractor use , by agency type .

agency - reported data revealed a wide variance in the cost of developing , completing , and maintaining enterprise architectures .

agencies generally reported that their architecture development costs could be allocated to several categories , with the majority of costs attributable to agency and contractor personnel .

as we have previously reported , the scope and nature of the enterprise and the extent of enterprise transformation and modernization envisioned will dictate the depth and detail of the architecture to be developed and maintained .

restated , the architecture should be tailored to the individual enterprise and that enterprise's intended use of the architecture .

accordingly , the level of resources that an agency invests in its architecture is likely to vary .

agency responses to our survey showed this to be the case .

agencies that reported cost data reported $599 million being spent to date on the development of architectures , with individual agency development costs to date ranging from $5,000 to $248 million .

departments' architecture development costs varied more than component and independent agencies' costs , while component agencies reported spending the most to date , with independent agencies spending the least .

agencies reported estimated costs to complete architecture development ranging from $3,000 to $319 million , and annual estimated maintenance costs ranging from $1,000 to $36 million .

figures 19 through 27 depict the variability of cost data reported by departments , component agencies , and independent agencies .

of the $599 million reported in architecture development costs , agencies allocated $511 million to the following seven cost categories that we identified in our questionnaire: agency personnel , contractor personnel , tools , methodologies , independent validation and verification , training , and other .

for those agencies that reported and allocated costs , the majority of these costs were for agency and contractor personnel — $116.7 million ( 23 percent ) were attributed to agency personnel and $188.9 million ( 37 percent ) were attributed to contractor personnel .

about $193.3 million ( 38 percent ) were attributed to “other” costs , $7.1 million ( 1 percent ) to architecture tools , and $3.9 million ( eight - tenths of 1 percent ) to independent validation and verification contract personnel .

further , $1.0 million ( two - tenths of 1 percent ) of costs were attributed to methodologies and another $1.0 million ( two - tenths of 1 percent ) to training .

figure 28 shows the architecture development costs by category .

table 16 shows enterprise architecture development , completion , and maintenance costs for each agency that provided cost data .

our objectives were to determine ( 1 ) what progress federal agencies have made in effectively developing , implementing , and maintaining their enterprise architectures and ( 2 ) the actions of the office of management and budget ( omb ) to advance the state of enterprise architecture development and use across the federal government .

to address our objectives , we obtained and reviewed relevant guidance on enterprise architectures , such as omb circular a - 130 and guidance published by the federal chief information officers ( cio ) council , including the federal enterprise architecture framework version 1.1 and the practical guide .

we also researched our past reports and guidance on the management and use of enterprise architectures , including the results of our 2001 governmentwide enterprise architecture survey and our enterprise architecture management maturity framework .

next , we used the cio council's practical guide and our enterprise architecture management maturity framework to develop two data collection instruments — one for federal departments and one for agencies that are either components within a department or are independent ( see app .

viii ) .

we pretested our survey instruments at one federal department and one component agency .

to ensure consistency and comparability with our 2001 governmentwide enterprise architecture survey , we based our survey population on the same 116 agencies , with appropriate additions and deletions .

these agencies consisted of all cabinet - level departments , major component agencies within departments , and other independent agencies .

we modified our 2001 survey population to reflect the federal government's reorganization of march 1 , 2003 , in which the department of homeland security ( dhs ) and its directorates ( i.e. , component agencies ) became operational , resulting in the addition of 5 agencies .

at the same time , the establishment of dhs resulted in 4 agencies that were included in our 2001 survey being eliminated from our survey population because they were absorbed into dhs directorates .

we also eliminated the u.s. marine corps as a separate agency within our population so that the department of the navy , at its request , could provide a single response for the navy and the marine corps .

table 17 lists additions to and deletions from our 2001 survey population and provides explanations for each change .

for each of the 116 agencies , we identified the cio or comparable official and notified them of our work and distributed the appropriate survey instrument to designated officials via e - mail .

we also discussed the purpose and content of the survey instrument with agency officials when requested .

after receiving our survey , officials from dhs and the departments of the interior and veterans affairs told us that their respective architectures cover their component agencies and , thus , a single response would be provided .

 ( when departments opted to provide a departmental response inclusive of component agencies , our analysis pertains to the department as a whole .

conversely , when departments and their component agencies reported separately , our departmental analysis is exclusive of component agencies. ) .

additionally , officials from the department of agriculture's farm service agency , natural resources conservation service , and rural utilities service told us they would provide a response that reflects the service center modernization initiative , which encompasses those three component agencies .

we agreed with these proposed approaches .

both the department of defense's business enterprise architecture and agriculture's previously mentioned service center modernization initiative provided responses that were not solicited in our survey population , which we included in our analysis and in this report .

tables 18 and 19 show the consolidated , omitted , and additional responses that led to the difference between our survey population of 116 agencies and the 96 respondents included in this report , including an explanation for each adjustment .

the timing of the 96 responses varied , ranging from april 1 to july 9 , 2003 , and thus the determinations in this report regarding the state of enterprise architecture development and use and progress at specific agencies and groups of agencies are linked to particular points in time .

appendixes v , vi , and vii , which contain the results of our analysis of each agency's response to our survey , identify the date that each agency responded .

to verify the accuracy of agencies' responses to our survey regarding enterprise architecture management policies , organizations , and responsibilities , we required agencies to submit documentation or additional information for survey questions related to certain framework criteria .

specifically , we requested agencies to submit documentation or additional information for questions 6 to 11 , 18 , 20 to 24 , 26 , and 35 to 39 .

although our survey requested that agencies provide data about the status of various enterprise architecture products , we did not independently verify the data that agencies provided about the comprehensiveness or completeness of their architecture products .

additionally , we contacted agency officials when necessary to clarify their responses .

to determine the progress of federal agencies' enterprise architecture efforts , we analyzed agency survey responses using version 1.0 of our maturity framework and compared them with the results of our 2001 survey , which were also based on version 1.0 .

we also analyzed survey responses using version 1.1 of our maturity framework to establish a new baseline against which future progress can be measured .

when an agency's response and our subsequent analysis indicated that it did not meet a core element as defined in the framework , we assigned that agency to the next lowest stage of framework maturity ( i.e. , to achieve a given stage of maturity , an agency must meet all core elements at that stage ) .

for example , if an agency satisfied all stage 2 and stage 4 elements , but did not satisfy one stage 3 element , that agency is considered to be a stage 2 agency .

when determining agency maturity levels , we did not consider whether agency enterprise architecture plans or products included “performance” because explicitly including enterprise performance data is a relatively new concept , and there was a minimal amount of federal guidance related to enterprise performance data available to agencies at the time our surveys were distributed .

tables 20 to 23 show the relationship between the survey questions and the framework elements for version 1.0 of the framework , as well as identify where documentation was required to support answers .

tables 24 to 27 show the relationship between the survey questions and the framework elements for version 1.1 of the framework .

after compiling agency responses and determining agencies' respective maturity stages , we analyzed responses across different slices of our respondent population to determine patterns and issues .

finally , to determine omb's actions to oversee agency enterprise architecture management efforts , we analyzed relevant policy and budget guidance , obtained information about omb's roles in the cio council and efforts to develop and use the federal enterprise architecture ( fea ) ( including omb's use of the fea in the budget process ) , and interviewed omb officials about ongoing and planned management actions .

we also analyzed agency responses to survey questions regarding omb's enterprise architecture - related oversight and guidance .

we conducted our work in the washington , d.c. , metropolitan area , from september 2002 to november 2003 , in accordance with generally accepted government auditing standards .

the following table presents three assessments of the maturity stage of each listed organization on the basis of the following: ( 1 ) responses to our 2001 survey evaluated against version 1.0 of our framework , ( 2 ) responses to our 2003 survey evaluated against version 1.0 of our framework , and ( 3 ) responses to our 2003 survey evaluated against version 1.1 of our framework .

the department of agriculture provided its 2001 survey responses on july 9 , 2001 , and its 2003 responses on may 12 , 2003 .

the department of commerce provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 18 , 2003 .

the department of defense provided its 2001 survey responses on july 25 , 2001 , and its 2003 response for its global information grid on june 5 , 2003 .

the department of defense provided its 2003 response for its business enterprise architecture on may 30 , 2003 .

the department did not provide a similar response to our 2001 survey .

the department of education provided its 2001 survey responses on july 23 , 2001 , and its 2003 responses on april 28 , 2003 .

the department of energy provided its 2001 survey responses on june 28 , 2001 , and its 2003 responses on april 23 , 2003 .

the department of health and human services provided its 2001 survey responses on august 14 , 2001 , and its 2003 responses on may 12 , 2003 .

the department of homeland security was not involved in our 2001 survey because it was established on march 1 , 2003 .

it provided its 2003 responses on june 10 , 2003 .

the department of housing and urban development provided its 2001 survey responses on june 28 , 2001 , and its 2003 responses on april 21 , 2003 .

the department of the interior provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the department of justice provided its 2001 survey responses on july 10 , 2001 , and its 2003 responses on may 20 , 2003 .

the department of labor provided its 2001 survey responses on july 2 , 2001 , and its 2003 responses on april 17 , 2003 .

the department of state provided its 2001 survey responses on july 13 , 2001 , and its 2003 responses on may 12 , 2003 .

the department of transportation provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 24 , 2003 .

the department of the treasury provided its 2001 survey responses on june 28 , 2001 , and its 2003 responses on april 26 , 2003 .

the department of veterans affairs provided its 2001 survey responses on august 17 , 2001 , and its 2003 responses on april 21 , 2003 .

the agricultural marketing service provided its 2001 survey responses on july 9 , 2001 , and its 2003 responses on may 13 , 2003 .

the agricultural research service provided its 2001 survey responses on july 13 , 2001 , and its 2003 responses on april 18 , 2003 .

the animal and plant health inspection service provided its 2001 survey responses on june 26 , 2001 , and its 2003 responses on april 21 , 2003 .

the cooperative state research , education , and extension service provided its 2001 survey responses on july 9 , 2001 , and its 2003 responses on april 16 , 2003 .

the food and nutrition service provided its 2001 survey responses on july 17 , 2001 , and its 2003 responses on april 24 , 2003 .

the food safety and inspection service provided its 2001 survey responses on july 9 , 2001 , and its 2003 responses on june 10 , 2003 .

july 12 , 2001 , and its 2003 responses on may 5 , 2003 .

the forest service provided its 2001 survey responses on august 3 , 2001 , and its 2003 responses on april 21 , 2003 .

the risk management agency provided its 2001 survey responses on july 27 , 2001 , and its 2003 responses on may 6 , 2003 .

the service center modernization initiative provided its responses on may 16 , 2003 .

the bureau of the census provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the economic development administration provided its 2001 survey responses on july 10 , 2001 , and its 2003 responses on april 28 , 2003 .

the international trade administration provided its 2001 survey responses on june 26 , 2001 , and its 2003 responses on april 29 , 2003 .

the national oceanic and atmospheric administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the u.s. patent and trademark office provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the ballistic missile defense organization provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 10 , 2003 .

the defense advanced research projects agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on april 7 , 2003 .

the defense commissary agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 9 , 2003 .

the defense contract audit agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on may 30 , 2003 .

the defense contract management agency provided its 2001 survey responses on july 3 , 2001 , and its 2003 responses on may 30 , 2003 .

the defense information systems agency provided its 2001 survey responses on july 11 , 2001 , and its 2003 responses on june 10 , 2003 .

july 25 , 2001 , and its 2003 responses on june 20 , 2003 .

the defense logistics agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on may 22 , 2003 .

the defense security cooperation agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 19 , 2003 .

the defense security service provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 9 , 2003 .

the defense threat reduction agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on may 29 , 2003 .

july 27 , 2001 , and its 2003 responses on june 2 , 2003 .

the department of the army provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 2 , 2003 .

the department of the navy provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 9 , 2003 .

the national imagery and mapping agency provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on june 6 , 2003 .

the administration for children and families provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 12 , 2003 .

the agency for healthcare research and quality provided its 2001 survey responses on july 12 , 2001 , and its 2003 responses on may 12 , 2003 .

the centers for disease control and prevention provided its 2001 survey responses on july 23 , 2001 , and its 2003 responses on may 12 , 2003 .

the centers for medicare and medicaid services provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 12 , 2003 .

the food and drug administration provided its 2001 survey responses on july 13 , 2001 , and its 2003 responses on may 12 , 2003 .

the health resources and services administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 12 , 2003 .

the indian health service provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 12 , 2003 .

the program support center provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 12 , 2003 .

the bureau of alcohol , tobacco , firearms and explosives provided its 2001 survey responses on july 16 , 2001 , and its 2003 responses on april 21 , 2003 .

the drug enforcement administration provided its 2001 survey responses on july 18 , 2001 , and its 2003 responses on may 20 , 2003 .

the federal bureau of investigation provided its 2001 survey responses on july 18 , 2001 , and its 2003 responses on may 28 , 2003 .

the federal bureau of prisons provided its 2001 survey responses on july 18 , 2001 , and its 2003 responses on may 22 , 2003 .

the u.s .

marshals service provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on may 19 , 2003 .

the federal aviation administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 29 , 2003 .

the federal highway administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 1 , 2003 .

the federal motor carrier safety administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 24 , 2003 .

the federal railroad administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 24 , 2003 .

the federal transit administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 24 , 2003 .

the national highway traffic safety administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 24 , 2003 .

the bureau of engraving and printing provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the bureau of the public debt provided its 2001 survey responses on july 5 , 2001 , and its 2003 responses on april 21 , 2003 .

june 28 , 2001 , and its 2003 responses on april 16 , 2003 .

the financial management service provided its 2001 survey responses on june 28 , 2001 , and its 2003 responses on may 19 , 2003 .

the internal revenue service provided its 2001 survey responses on july 20 , 2001 , and its 2003 responses on april 21 , 2003 .

the office of thrift supervision provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on june 9 , 2003 .

the u.s. mint provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 21 , 2003 .

the agency for international development provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 22 , 2003 .

the central intelligence agency provided its 2001 survey responses on august 6 , 2001 , and its 2003 responses on may 30 , 2003 .

the corporation for national and community service provided its 2001 survey responses on july 20 , 2001 , and its 2003 responses on april 22 , 2003 .

the environmental protection agency provided its 2001 survey responses on june 28 , 2001 , and its 2003 responses on may 15 , 2003 .

the equal employment opportunity commission provided its 2001 survey responses on august 1 , 2001 , and its 2003 responses on may 2 , 2003 .

the executive office of the president provided its 2001 survey responses on october 1 , 2001 , and its 2003 responses on june 6 , 2003 .

the export - import bank provided its 2001 survey responses on september 20 , 2001 , and its 2003 responses on june 11 , 2003 .

the federal deposit insurance corporation provided its 2001 survey responses on july 20 , 2001 , and its 2003 responses on april 18 , 2003 .

the federal energy regulatory commission provided its 2001 survey responses on august 27 , 2001 , and its 2003 responses on may 12 , 2003 .

the federal reserve system provided its 2001 survey responses on august 23 , 2001 , and its 2003 responses on april 23 , 2003 .

the federal retirement thrift investment board provided its 2001 survey responses on july 20 , 2001 , and its 2003 responses on july 9 , 2003 .

the general services administration provided its 2001 survey responses on july 2 , 2001 , and its 2003 responses on april 23 , 2003 .

the national aeronautics and space administration provided its 2001 survey responses on july 25 , 2001 , and its 2003 responses on april 21 , 2003 .

the national credit union administration provided its 2001 survey responses on july 18 , 2001 , and its 2003 responses on april 10 , 2003 .

the national labor relations board provided its 2001 survey responses on august 9 , 2001 , and its 2003 responses on june 9 , 2003 .

the nuclear regulatory commission provided its 2001 survey responses on july 23 , 2001 , and its 2003 responses on april 21 , 2003 .

the office of personnel management provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 28 , 2003 .

the peace corps provided its 2001 survey responses on july 20 , 2001 , and its 2003 responses on may 15 , 2003 .

the railroad retirement board provided its 2001 survey responses on july 11 , 2001 , and its 2003 responses on april 18 , 2003 .

the securities and exchange commission provided its 2001 survey responses on july 19 , 2001 , and its 2003 responses on april 22 , 2003 .

the small business administration provided its 2001 survey responses on june 29 , 2001 , and its 2003 responses on april 22 , 2003 .

2001 , and its 2003 responses on april 21 , 2003 .

the social security administration provided its 2001 survey responses on july 3 , 2001 , and its 2003 responses on april 21 , 2003 .

the u.s .

postal service provided its 2001 survey responses on august 13 , 2001 , and its 2003 responses on april 21 , 2003 .

to assess agency enterprise architecture management maturity levels , we developed two similar surveys , one addressed to departments and the other to component and independent agencies .

these two surveys were largely identical , with the following differences: throughout , questions referred to “departments” in the department survey and to “agencies” in the agency survey .

two questions on the department survey ( questions 39 and 40 ) and three questions on the agency survey ( questions 39 to 41 ) were addressed specifically to departments and agencies , respectively .

the last five questions on the two surveys were numbered differently , since they followed the department - and agency - specific questions described above .

questions 41 to 45 on the department survey were numbered 42 to 46 on the agency survey .

 ( note , however , that these five questions were not used in the decision criteria described in app .

iii. ) .

the following reproduced survey combines the two surveys into one display by using the phrase “agency / department” in places where one or the other term had been used in the separate surveys .

it also displays both the two department questions and the three agency questions that were addressed specifically as described above .

we are also asking that you provide the name and government , gao is conducting a survey of federal telephone number of a contact for your departments' and agencies' enterprise architecture ( ea ) agency / department who can answer any questions we efforts to gauge progress towards meeting clinger - cohen may have about your survey responses .

act and omb requirements and to identify successes that can be shared with other federal agencies .

there are two versions of this survey .

one version is being sent to federal agencies and a different version is being sent to cabinet - level departments .

enterprise architectures are well defined and enforced blueprints ( i.e. , descriptions ) for operational and technological change .

such architectures provide a clear ( ) and comprehensive picture of an entity , whether it is an organization ( eg , federal department , agency , or bureau ) ( ) if you have any questions , please contact: environment ; and ( 3 ) a capital investment roadmap for transitioning from the current to the target environment ( i.e .

sequencing plan ) .

we are requesting departments and agencies to provide information from readily available data .

we are not asking that extensive analyses be performed in order to respond to these questions .

please complete this survey and return it to gao no later than april 21 , 2003 .

you may return your completed survey and any supporting materials by e - mail , fax , or federal express .

if you return your survey by e - mail , the address is: pettisb@gao.gov .

if you return your survey by fax , the fax number is: ( 202 ) 512-6450 - attn: scott pettis .

if you return your survey by federal express , the address is: scott pettis , senior it analyst , 441 g st. nw , rm .

4y12 , washington , dc 20548 .

1 .

which of the following best describes your agency / department's status with respect to enterprise architecture ? .

 ( check one box. ) .

1 .

we have developed an enterprise architecture skip to question 3 .

2 .

we do not have an enterprise architecture , but are in the process of developing one skip to question 3 .

3 .

we do not have an enterprise architecture , but plan to develop one skip to question 3 .

4 .

we do not plan to develop an enterprise architecture answer question 2 .

2 .

please explain why your agency / department does not plan to develop an enterprise architecture .

 ( enter your response in the box below. ) .

if you were directed to answer question 2 , you have completed the survey .

please return it as soon as possible .

thank you .

you should answer the following questions if your agency / department has an enterprise architecture , is in the process of developing one , or plans to develop one .

3 .

which of the following best describes the scope of your agency / department's completed , in - process , or planned enterprise architecture ( s ) .

 ( check all that apply and provide additional information if necessary. ) .

1 .

agency / department wide , organization based ( i.e. , all mission and business functions ) 2 .

agency / department wide , function based ( eg , financial management , logistics management , grant management , etc. ) .

3 .

non - agency / department wide organization based 4 .

non - agency / department wide function based if you checked box 3 or 4 above because your architecture is not agency / department wide , please list the organizations or functions covered by your enterprise architecture , and explain the basis for the defined scope .

 ( enter your response in the box below. ) .

4 .

does ( or will ) this particular enterprise architecture include the following ? .

 ( check one box for each row. ) .

 ( 1 ) ( 2 ) ( 3 ) .

performance measurement , information / data , services / applications , and technology descriptions of the agency / department .

a description of the agency / department's future or “to - be” environment , an explicit discussion of security in the “to be” business operations , performance measurement , information / data , services / applications , and technology descriptions of the agency / department .

a description of the sequencing plan for moving from the “as is” to the “to be” environment .

if you answered “no” to any of the items in question 4 , please explain why .

 ( enter your response in the box below. ) .

is your agency / department's enterprise architecture published ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes please provide a list naming each enterprise architecture product / artifact with a brief description of each product / artifact .

7 .

does your agency / department have a written and approved policy for the development , maintenance , and use of enterprise architecture ? .

 ( check one box for each row .

if policy is written but not approved , please check “no”. ) .

 ( 1 ) ( 2 ) .

maintenance of the enterprise architecture use of the enterprise architecture if you checked “yes” for development , maintenance , or use , please provide a copy of the written and approved 8 .

has your agency / department established committees or groups that represent the agency / department and have responsibility for the following ? .

 ( check one box for each row. ) .

 ( 1 ) ( 2 ) .

oversight of the enterprise architecture approval authority for the enterprise architecture other aspects of the enterprise architecture ( describe ) if yes , please provide a copy of the charter or comparable documentation .

9 .

has your agency / department established an official program office with responsibility for the following ? .

 ( check one box for each row. ) .

 ( 1 ) ( 2 ) .

maintenance of the enterprise architecture if yes , please provide a copy of the charter or comparable documentation .

10 .

does your agency / department have an individual designated as the chief architect ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes please provide this individual's name and phone number: ( ) does this individual report to the chief information officer ? .

2 .

no what position does the chief architect report to ? .

2 .

no skip to question 12 .

11 .

is your agency / department's chief architect responsible for each of the following ? .

 ( check one box for each row. ) .

 ( 1 ) ( 2 ) .

directing development of the enterprise architecture directing maintenance of the enterprise architecture please provide a position description or comparable document describing the chief architect's responsibilities .

12 .

please provide the costs of developing and maintaining your enterprise architecture by the following major cost elements: ( if you are in the process of developing your enterprise architecture , please enter data in all three columns. ) .

if any , to complete ( to date ) other ( describe ) $ .

please quantify your agency / department's requested and approved enterprise architecture resources .

personnel ( ftes ) if any gap exists between requested and approved resources for fiscal year 2001 , 2002 , or 2003 , please answer question 14 .

otherwise , proceed to question 15 .

14 .

how much of an impact , if any , has the gap between enterprise architecture resources requested and resources finally approved had on your agency / department's enterprise architecture program ? .

 ( check one and provide additional information if necessary. ) .

1 .

very adverse impact 2 .

somewhat adverse impact 3 .

moderate adverse impact 4 .

slight adverse impact 5 .

no adverse impact please provide any additional details about the impact of any gap noted above .

 ( enter your response in the box below. ) .

15 .

which of the following automated tools are being used for this enterprise architecture ? .

for each tool being used , how satisfied or dissatisfied are you with it ? .

 ( check yes or no in each row .

if yes , check additional box. ) .

if tool is being used , are you .

 .

 .

used ? .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) .

 ( eams ) framework by ptech inc. jcaps by logicon inc .

yes 16 .

which of the following model ( s ) or framework ( s ) ( i.e. , a formal structure for representing the enterprise architecture ) is your agency / department using to develop this enterprise architecture ? .

for each model or framework being used , how satisfied or dissatisfied are you with it ? .

 ( check yes or no in each row .

if yes , check additional box. ) .

if model or framework is being used , are you .

 .

 .

being used ? .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) .

 ( c4isr ) framework ( dodaf ) framework ( feaf ) ( nist ) framework ( teaf ) yes 17 .

which of the following best describes how your agency / department's enterprise architecture was or is being developed ? .

 ( check one box and provide additional information if necessary. ) .

1 .

developed in - house using contractor ( s ) support 2 .

developed in - house without any contractor ( s ) support 3 .

developed by contractor ( s ) please provide the contractor's name ( s ) : 18 .

is your agency / department using an enterprise architecture development methodology or methodologies ( i.e. , a common set of procedures , such as spewak's enterprise architecture planning methodology , for developing enterprise architecture products ) ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes provide the following information about the enterprise architecture methodology or methodologies your agency / department is using: 19 .

to what extent was or is your agency / department's “business” side involved in developing the enterprise architecture ? .

 ( check one. ) .

1 .

very great extent 4 .

some or little extent 5 .

no extent 20 .

was the current version ( i.e. , latest major release ) of your agency / department's enterprise architecture submitted and approved by the following entities: ( check one box in each row under submitted and approved .

if the enterprise architecture was submitted but not approved , please check “no” .

if no , indicate whether action is planned. ) .

if no , is action planned ? .

information officer ? .

committee ? .

approved by a committee or enterprise ? .

investment review board ? .

approved by the head of your agency / department ? .

approved by other official or committee ? .

please specify: submitted to omb ? .

please provide documentation for each approval indicated above .

21 .

do your agency / department's enterprise architecture products undergo independent verification and validation ( iv&v ) ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes if iv&v is contractor - provided , please provide a copy of the contractor's statement of work .

22 .

do your agency / department's enterprise architecture management processes undergo independent verification and validation ( iv&v ) ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes if iv&v is contractor - provided , please provide a copy of the contractor's statement of work .

2 .

no 23 .

does your agency / department periodically update its enterprise architecture products ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes if yes , please provide date of last update: 24 .

is your agency / department's enterprise architecture under configuration management ( i.e. , a process for establishing and maintaining the integrity of work products ) ? .

 ( check one box and provide additional information if necessary. ) .

1 .

yes if yes , please provide date of current version: 25 .

does a process exist for formally managing changes to your agency / department's enterprise architecture ? .

 ( check 26 .

does your agency / department have a written and approved policy that requires that it investments comply with the enterprise architecture ? .

 ( check one box and provide additional information if necessary .

if policy is written but not approved , please check “no”. ) .

1 .

yes please provide a copy of the written policy .

continue with question 27 2 .

no skip to question 28 27 .

does your agency / department permit waivers to its requirement that it investments comply with the enterprise architecture ? .

 ( check one. ) .

1 .

yes , only if the request provides a written justification 2 .

yes , a waiver can be granted based on an informal request 3 .

no , the agency / department does not provide for waivers to this policy 28 .

is your agency / department's enterprise architecture an integral component of your agency / department's it investment management process ? .

 ( check one. ) .

29 .

to what extent does your agency / department's it investments comply with the enterprise architecture ? .

 ( check one. ) .

1 .

very great extent 4 .

some or little extent 5 .

no extent 30 .

was your agency / department's decision to develop an enterprise architecture based on: 1 ) a business case that provided economic justification ( i.e. , benefits in excess of costs ) ; 2 ) the need to comply with the clinger - cohen act and / or omb requirements ; 3 ) the need to respond to the president's management agenda ; and / or , 4 ) some other factor ( s ) that was considered ? .

 ( check all that apply. ) .

1 .

a business case that anticipated a positive return 2 .

the need to comply with clinger - cohen and / or omb requirements 3 .

the need to respond to the president's management agenda 4 .

other factor ( s ) - please specify in the box below: 31 .

what benefits , if any , can be attributed to your agency / department's use of an enterprise architecture ? .

if the benefit can be attributed to the use of an enterprise architecture , to what extent , if at all , has the benefit been attained thus ( check yes or no in each row .

if yes , indicate extent benefit attained. ) .

architecture ? .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) .

yes no 32 .

to what extent , if at all , did the following challenges affect the development of your agency / department's enterprise architecture ? .

 ( check one box in each row. ) .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) .

from this point , the agency and department surveys differ .

bureaus ) enterprise architecture development , maintenance , or use ? .

 ( check one box and provide 1 .

yes continue with question 40 additional information if necessary. ) .

2 .

no skip to question 42 1 .

yes please provide a copy of the policy or guidance with your response .

department provided oversight of your enterprise architecture efforts ? .

 ( check one. ) .

 ( eg , oversight and approval processes ) ? .

a .

b. c .

by your department's chief information officer ? .

 ( check one. ) .

from this point , the questions are again the same for each survey , except for their numbering: on the department survey , each question number was one less than the numbering shown in the following ( the numbering shown corresponds to that on the agency survey ) .

note that none of the questions that follow were used in the decision criteria that determined the maturity stage assigned to any respondent ( see appendix iii for these criteria ) .

42 .

overall , how satisfied or dissatisfied is your agency / department with omb's direction and guidance to your agency / department regarding development , maintenance , and implementation of your enterprise architecture ? .

 ( question responses will be aggregated and not directly attributable to any agency / department. ) .

 ( check one and provide additional information if necessary. ) .

3 .

neither satisfied nor dissatisfied if you indicated that your agency / department is other than “very satisfied” or “satisfied,” please describe why and what improvements are needed .

43 .

how satisfied is your agency / department with omb's efforts to address the following enterprise architecture management challenges gao reported in its february 2002 report ( gao - 02-6 ) ? .

 ( question responses will be aggregated and not directly attributable to any agency / department. ) .

 ( check one box in each row and provide additional information if necessary. ) .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) .

if you indicated that your agency / department is other than “very satisfied” or “satisfied,” to any of the above , please describe why and what improvements are needed .

44 .

do you agree or disagree with the following statements as they apply to omb's federal enterprise architecture ( fea ) ? .

 ( question responses will be aggregated and not directly attributable to any agency / department. ) .

 ( check one box in each row. ) .

 ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) .

change as a result of the fea if you indicated other than “strongly agree” or “agree,” to any of the above , please describe why and what improvements are needed .

45 .

in your agency / department's opinion , what impact has the fea had ( or will the fea have ) on your agency / department's enterprise architecture ? .

 ( question responses will be aggregated and not directly attributable to any agency / department. ) .

 ( check one. ) .

1 .

very positive impact 2 .

generally positive impact 3 .

neither positive nor negative impact 4 .

generally negative impact 5 .

very negative impact 6 .

no basis to judge 46 .

please provide any additional comments on your agency / department's enterprise architecture program in the box thank you for your assistance .

please return your survey and any requested supporting materials to the e - mail address or fax number indicated on page 1 .

in addition to the person named above , barbara s. collier , william b. cook , neal j. doherty , michael holland , catherine m. hurley , stuart m. kaufman , scott pettis , and david b. shumate made key contributions to this report .

the general accounting office , the audit , evaluation and investigative arm of congress , exists to support congress in meeting its constitutional responsibilities and to help improve the performance and accountability of the federal government for the american people .

gao examines the use of public funds ; evaluates federal programs and policies ; and provides analyses , recommendations , and other assistance to help congress make informed oversight , policy , and funding decisions .

gao's commitment to good government is reflected in its core values of accountability , integrity , and reliability .

the fastest and easiest way to obtain copies of gao documents at no cost is through the internet .

gao's web site ( www.gao.gov ) contains abstracts and full - text files of current reports and testimony and an expanding archive of older products .

the web site features a search engine to help you locate documents using key words and phrases .

you can print these documents in their entirety , including charts and other graphics .

each day , gao issues a list of newly released reports , testimony , and correspondence .

gao posts this list , known as “today's reports,” on its web site daily .

the list contains links to the full - text document files .

to have gao e - mail this list to you every afternoon , go to www.gao.gov and select “subscribe to e - mail alerts” under the “order gao products” heading .

