information systems are critical to the health , economy , and security of the nation .

to support agency missions , the federal government plans to spend at least $82 billion on information technology ( it ) investments in fiscal year 2014 .

however , as we have previously reported , prior it expenditures too often have produced failed projects — that is , projects with multimillion dollar cost overruns and schedule delays measured in years , with questionable mission - related achievements .

many of these investments have been broadly scoped projects that aim to deliver their capabilities several years after initiation .

to help resolve these issues , congress and the office of management and budget ( omb ) have called for agencies to deliver investments in smaller parts , or increments , in order to reduce investment risk , deliver capabilities more quickly , and facilitate the adoption of emerging in 2010 , omb placed a renewed emphasis on incremental technologies .

it development by calling for major it investments to deliver functionality at least every 12 months .

subsequently , omb has made this guidance more stringent , and annual budget guidance now states that each project associated with major it investments is to deliver functionality every 6 months .

this report responds to your request that we review agencies' incremental it development .

specifically , we ( 1 ) assessed whether selected agencies have established policies for incremental it development , ( 2 ) determined whether selected agencies are using incremental development approaches to manage their it investments , and ( 3 ) identified the key factors that enabled and inhibited the selected agencies' abilities to effectively use incremental development approaches to manage their it investments .

in conducting our review , we selected five agencies and 89 investments to review .

we selected the five agencies with the largest it budgets for development , modernization , and enhancement on major it investmentsin fiscal years 2013 and 2014: the departments of defense ( defense ) , health and human services ( hhs ) , homeland security ( dhs ) , transportation ( transportation ) , and veterans affairs ( va ) .

we then selected the major it investments at these agencies that planned to spend more than 50 percent of the investments' fiscal year 2013 and 2014 budgets on development , modernization , and enhancement .

these 89 investments are identified in appendix ii .

omb , management of federal information resources , circular no .

a - 130 revised .

processes throughout an organization.agencies' incremental development policies to the components we identified .

we then compared the selected to address our second objective , we analyzed information obtained from data collection instruments describing how often the selected investments planned to deliver functionality .

specifically , we administered a data collection instrument to officials responsible for each of the selected investments that asked how often they planned to deliver functionality during fiscal years 2013 and 2014 .

using these data , we determined the extent to which the selected investments and their projects are meeting omb's guidance on incremental development .

to address our third objective , officials from the five selected agencies and investments identified the key factors that have both enabled and inhibited their efforts to deliver functionality for their major it investments , consistent with omb guidance .

we then determined the key factors that were commonly identified by the selected agencies and investments .

we conducted this performance audit from may 2013 to may 2014 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

details of our objectives , scope , and methodology are contained in appendix i .

if done correctly , investments in it have the potential to make organizations more efficient in fulfilling their missions .

for example , we recently reported that defense officials stated that an it system supporting military logistics has improved the organization's performance by providing real - time information about road conditions , construction , incidents , and weather to facilitate rapid deployment of military assets .

however , as we have previously reported , investments in federal it too frequently result in failed projects that incur cost overruns and schedule slippages while contributing little to mission - related outcomes .

for example: in january 2011 , the secretary of homeland security ended the secure border initiative network program after obligating more than $1 billion for the program because it did not meet cost - effectiveness and viability standards .

since 2007 , we have identified a range of issues and made several recommendations to improve this program .

for example , in may 2010 , we reported that the final acceptance of the first two deployments had slipped from november 2009 and march 2010 to september 2010 and november 2010 , respectively , and that the cost - effectiveness of the system had not been justified .

we concluded that dhs had not demonstrated that the considerable time and money being invested to acquire and deploy the program were a wise and prudent use of limited resources .

as a result , we recommended that the department ( 1 ) limit near - term investment in the program , ( 2 ) economically justify any longer - term investment in it , and ( 3 ) improve key program management disciplines .

this work contributed to the department's decision to cancel the program .

in february 2011 , the office of personnel management canceled its retirement systems modernization program after several years of trying to improve the implementation of this investment.the office of personnel management , it spent approximately $231 million on this investment .

we issued a series of reports on the according to agency's efforts to modernize its retirement system and found that the office of personnel management was hindered by weaknesses in several important management disciplines that are essential to successful it modernization efforts .

accordingly , we made recommendations in areas such as project management , organizational change management , testing , and cost estimating .

in may 2008 , an office of personnel management official cited the issues that we identified as justification for issuing a stop work order to the system contractor , and the agency subsequently terminated the contract .

in december 2012 , defense canceled the air force's expeditionary combat support system after having spent more than a billion dollars and missing multiple milestones , including failure to achieve deployment within 5 years of obligating funds .

we issued several reports on this system and found that , among other things , the program was not fully following best practices for developing reliable schedules and cost estimates .

agencies have reported that poor - performing projects have often used a “big bang” approach — that is , projects that are broadly scoped and aim to deliver functionality several years after initiation .

for example , in 2009 the defense science board reported that defense's acquisition process for it systems — which was rooted in the “waterfall” development model — was too long , ineffective , and did not accommodate the rapid evolution of it.the board reported that the average time to deliver an initial program capability for a major it system acquisition at defense was over 7 years .

also in 2009 , va's former chief information officer ( cio ) reported that many of its projects exceeded cost estimates by more than 50 percent and missed scheduled completion dates by more than a year .

that official concluded that va needed to make substantial changes to its acquisition process in order to eliminate project failures associated with the “big bang” approach .

one approach to reducing the risks from broadly scoped , multiyear projects is to divide investments into smaller parts — a technique long advocated by congress and omb .

can potentially by following this approach , agencies deliver capabilities to their users more rapidly , giving them more flexibility to respond to changing agency priorities ; increase the likelihood that each project will achieve its cost , schedule , and performance goals ; obtain additional feedback from users , increasing the probability that each successive increment and project will meet user needs ; more easily incorporate emerging technologies ; and terminate poorly performing investments with fewer sunk costs .

see clinger - cohen act of 1996 , pub .

l. no .

104-106 § 5202 , 110 stat .

186 , 690 ( 1996 ) , codified at 41 u.s.c .

§ 2308 ; see also 48 c.f.r .

§ 39.103 ( federal acquisition regulation ) ; omb , management of federal information resources , circular no .

a - 130 revised .

implement them .

more recently , in its 2010 it reform plan , omb called for it programs to deliver functionality at least every 12 months and complete initial deployment to end users no later than 18 months after the start of the program .

in 2011 , as part of its budget guidance , omb first recommended that projects associated with major it investments deliver functionality every 6 months .

omb's latest guidance now makes this mandatory ; specifically , in 2012 , omb began requiring that functionality be delivered at least every 6 months .

over the last three decades , congress has enacted several laws to assist agencies and the federal government in managing it investments .

for example , the paperwork reduction act of 1995 requires that omb develop and oversee policies , principles , standards , and guidelines for federal agency it functions , including periodic evaluations of major information systems .

in addition , to assist agencies in managing their investments , congress enacted the clinger - cohen act of 1996 .

among other things , the act requires agency heads to appoint cios and specifies many of their responsibilities .

with regard to it management , cios are responsible for implementing and enforcing applicable governmentwide and agency it management principles , standards , and guidelines ; assuming responsibility and accountability for it investments ; and monitoring the performance of it programs and advising the agency head whether to continue , modify , or terminate such programs .

additionally , with regard to incremental development , clinger - cohen calls for provisions in the federal acquisition regulation that encourage agencies to structure their it contracts such that the capabilities are delivered in smaller increments .

the federal acquisition regulation provisions are also to provide , to the maximum extent practicable , that the increment should be delivered within 18 months of the contract solicitation .

as set out in these laws , omb is to play a key role in helping federal agencies manage their investments by working with them to better plan , justify , and determine how much they need to spend on projects and how to manage approved projects .

within omb , the office of e - government and information technology , headed by the federal cio , directs the policy and strategic planning of federal it investments and is responsible for oversight of federal technology spending .

in carrying out its responsibilities , omb uses several data collection mechanisms to oversee federal it spending during the annual budget formulation process .

specifically , omb requires federal departments and agencies to provide information related to their it investments ( called exhibit 53s ) and capital asset plans and business cases ( called exhibit 300s ) .

exhibit 53 .

the purpose of the exhibit 53 is to identify all it investments — both major and nonmajor — and their associated costs within a federal organization .

information included in agency exhibit 53s is designed , in part , to help omb better understand agencies' spending on it investments .

exhibit 300 .

the purpose of the exhibit 300 is to provide a business case for each major it investment and to allow omb to monitor it investments once they are funded .

an it investment may include one or more projects that are to develop , modernize , enhance , or maintain a single it asset or group of it assets with related functionality .

agencies are required to provide information on each major investment's projects , including cost , schedule , and performance information .

for example , in order to measure compliance with its requirement that projects deliver functionality in 6-month cycles , omb requires agencies to break their projects into activities , and describe when the activities are to deliver functionality .

omb has implemented a series of initiatives to improve the oversight of underperforming investments and more effectively manage it .

these efforts include the following: it dashboard .

in june 2009 , to further improve the transparency into and oversight of agencies' it investments , omb publicly deployed the it dashboard .

as part of this effort , omb issued guidance directing federal agencies to report , via the dashboard , the performance of their it investments .

currently , the dashboard publicly displays information on the cost , schedule , and performance of over 700 major federal it investments at 26 federal agencies .

further , the public display of these data is intended to allow omb , other oversight bodies , and the general public to hold the government agencies accountable for results and progress .

techstat reviews .

in january 2010 , the federal cio began leading techstats sessions — face - to - face meetings to terminate or turn around it investments that are failing or are not producing results .

these meetings involve omb and agency leadership and are intended to increase accountability and transparency and improve performance .

for example , the federal cio testified in june 2013 that he holds techstat meetings on large investments that are not being acquired incrementally .

more recently , omb empowered agency cios to hold their own techstat sessions within their respective agencies .

in doing so , omb has called for agencies to use their techstat processes to identify investments that are not being acquired incrementally and undertake corrective actions .

it reform plan .

in december 2010 , omb released its 25-point plan to reform federal it .

this document established an ambitious plan for achieving operational efficiencies and effectively managing large - scale it programs .

in particular , as part of its effort to effectively manage it acquisitions , the plan calls for federal it programs to deploy functionality in release cycles no longer than 12 months , and ideally , less than 6 months .

the plan also identifies key actions that can help agencies implement this incremental development guidance , such as working with congress to develop it budget models that align with incremental development , and issuing contracting guidance and templates to support incremental development .

in april 2012 , we reported on omb's efforts to implement the actions called for in its it reform plan and found that it had partially completed work on two key action items relating to incremental development — issuing contracting guidance and templates to support incremental development and working with congress to create it budget models that align with incremental development .

with respect to the contracting guidance and templates , we found that , although omb worked with the it and acquisition community to develop guidance , it had not yet issued this guidance or the templates .

regarding the it budget models , we found that , although omb worked to promote ideas for it budget flexibility ( such as multiyear budgets or revolving funds ) with congressional committees , there has not yet been any new legislation to create budget models , and omb has not identified options to increase transparency for programs that would fall under these budgetary flexibilities .

we recommended that the director of omb ensure that all action items called for in the it reform plan are completed .

omb agreed with this recommendation .

omb has since issued contracting guidance for incremental development,office of e - government and information technology stated that activities to address the development of new it budget models are still ongoing .

but , as of january 2014 , a staff member from the omb additionally , in 2011 , we identified seven successful investment acquisitions and nine common factors critical to their success .

specifically , we reported that department officials identified seven successful investment acquisitions , in that they best achieved their respective cost , schedule , scope , and performance goals .

notably , all of these were smaller increments , phases , or releases of larger projects .

for example , the defense investment in our sample was the seventh increment of an ongoing investment ; the department of energy system was the first of two phases ; the dhs investment was rolled out to two locations prior to deployment to 37 additional locations ; and the transportation investment had been part of a prototype deployed to four airports .

in addition , common factors critical to the success of three or more of the seven investments were: ( 1 ) program officials were actively engaged with stakeholders , ( 2 ) program staff had the necessary knowledge and skills , ( 3 ) senior department and agency executives supported the programs , ( 4 ) end users and stakeholders were involved in the development of requirements , ( 5 ) end users participated in testing of system functionality prior to formal end user acceptance testing , ( 6 ) government and contractor staff were stable and consistent , ( 7 ) program staff prioritized requirements , ( 8 ) program officials maintained regular communication with the prime contractor , and ( 9 ) programs received sufficient funding .

these critical factors support omb's objective of improving the management of large - scale it acquisitions across the federal government , and wide dissemination of these factors could complement omb's efforts .

further , in 2012 , we identified 32 practices and approaches as effective for applying agile software development methods to it projects.officials from five agencies who had used agile methods on federal projects cited beneficial practices , such as obtaining stakeholder and customer feedback frequently , managing requirements , and ensuring staff had the proper knowledge and experience .

we also identified 14 challenges with adapting and applying agile in the federal environment , including agencies having difficulty with committing staff to projects , procurement practices that did not support agile projects , and compliance reviews that were difficult to execute within an iterative time frame .

we noted that the effective practices and approaches identified in the report , as well as input from others with broad agile experience , could help agencies in the initial stages of adopting agile .

since 2000 , omb circular a - 130 has required agencies to ( 1 ) develop policies that require their major investments to deliver functionality incrementally and ( 2 ) ensure that investments comply with their policies .

in addition , as part of its recent budget guidance , omb has defined how often investments must deliver functionality .

specifically , each project associated with major it investments is to deliver functionality at least further , through the president's budget , omb once every 6 months.has provided additional guidance on how incremental development is to be enforced by requiring agencies to use their techstat processes to identify investments that are not being acquired incrementally and undertake corrective actions .

although omb's guidance requires agencies to develop incremental development policies , it does not specify what those policies are to include .

absent this detail and in reviewing the previously mentioned guidance and leading practices on institutionalizing processes throughout an organization , include in their policies in order to effectively carry out omb's incremental development guidance: we identified three components that agencies should require that all projects associated with major it investments deliver functionality in cycles that are not more than 6-months long ; define functionality — that is , what the projects are to deliver at the end of a 6-month cycle ; and define a process for ensuring that major it investments and their projects deliver functionality every 6 months .

this should include identifying investments that are not being acquired incrementally through agency techstat processes and undertaking corrective actions .

although all five selected agencies developed policies that address incremental development , the majority of the agencies' policies did not fully address all three components .

specifically , only va fully addressed the three components ; defense partially addressed the majority of them ; and hhs , dhs , and transportation did not address the majority of components .

table 1 provides a detailed assessment of each agency's policies against the three key components of an incremental development policy .

in addition , a discussion of each policy component follows the table .

sei , cmmi - acq , version 1.3 ( november 2010 ) .

component .

●=fully met — the agency provided evidence that addressed the component .

◐=partially met — the agency provided evidence that addressed about half or a large portion of the ○=not met — the agency did not provide evidence that addressed the component or provided evidence that minimally addressed the component .

require delivery of functionality every 6 months .

only one of the five agencies — va — fully addressed this policy component by clearly requiring that its projects be completed in increments that must not exceed 6 months .

the other four agencies did not address this policy component .

three of these agencies — defense , hhs , and dhs — all developed policies that promote the use of incremental development , but these policies do not require functionality to be delivered every 6 months .

specifically , with regard to defense , although the department's acquisition framework calls for investments to use incremental development , its policy on it budget submissions encourages investments to deliver functionality every 12-18 months — not every 6 months .

according to officials of the defense office of the cio , 12-18 month incremental development is better aligned with the acquisition framework that many of its it acquisitions have used .

for hhs , although its policy requires incremental development , its policy recommends — but does not require — that all projects deliver functionality every 3-6 months .

according to an hhs office of the cio official , hhs does not require its projects to deliver functionality every 6 months because it wants to provide projects with flexibility .

for dhs , in june 2012 , the former dhs cio issued a draft policy encouraging it projects to move towards agile development approaches .

additionally , with respect to financial systems modernization programs , dhs's policy calls for providing financial capabilities to the customer in small increments of 6-12 months .

according to dhs officials representing the office of the cio , the department is currently developing a departmentwide policy on incremental development ; however , they said that the draft currently encourages investments to deliver the first release 18 months after program initiation and thereafter deploy functionality in cycles no longer than 12 months , but ideally less than 6 months .

lastly , transportation also did not address the component because , although transportation has a policy that calls for projects to deliver functionality every 6 months , officials from the office of the cio explained that this policy does not apply to the federal aviation administration ( faa ) .

these officials explained that how often faa projects deliver functionality depends on their size , scope , risk , visibility , and interdependencies with other programs .

define functionality .

only one of the five agencies — va — fully addressed this policy component .

va has a policy that defines what it means to deliver functionality — both in terms of what constitutes an increment and what should be delivered at the end of an increment .

for example , va defines an increment as the segment of the project that produces , in a cycle of 6 months or less , a deliverable that can be used by customers in an operational environment .

for the agency that partially addressed the component — defense — although it has defined functionality for purposes of its acquisition framework , it has not defined the functionality that its it budget submission policy encourages projects to deliver every 12-18 months .

the department stated that it is working with omb to define this term .

lastly , three agencies — hhs , dhs , and transportation — had not defined functionality in terms of what they expected projects to deliver at the end of a development cycle .

officials representing these agencies' respective office of the cio acknowledged that they have not defined functionality .

these officials told us that they would update their policies to define the term , but officials from hhs and transportation did not provide a time frame for doing so .

dhs officials stated that , although they did not have a definitive timeframe , they hoped to finalize the policy in 2014 .

until the agencies define this term , investments may create definitions that are inconsistent with the intent of omb's policy .

define a process for enforcing compliance .

only one of the five agencies — va — fully addressed this policy component by defining processes for ensuring that increments are structured to deliver functionality every 6 months or less and for reviewing projects that fall behind schedule .

in particular , va's policy requires the agency to hold a techstat session when any increment delivery date has been or will be missed .

two agencies partially addressed this component — defense and hhs — because , although they established processes for ensuring that it is acquired incrementally , these processes do not ( 1 ) require enforcement of incremental development within the specific time frames consistent with omb guidance ( 12-18 months for defense and 3-6 months for hhs ) or ( 2 ) include using techstat processes to identify investments that are not being acquired incrementally .

finally , two agencies — dhs and transportation — have not established processes for enforcing compliance with their incremental development policies .

officials from their respective office of the cio told us that they are updating their policies to address this issue .

transportation officials representing the office of the cio stated that it would update its policy later this year ; dhs office of the cio officials stated that , although they did not have a definitive timeframe , they hoped to finalize their policy in 2014 .

agencies cited several underlying reasons that contributed to these weaknesses: ( 1 ) they were not always aware of omb guidance , ( 2 ) they did not believe that the guidance was realistic , and ( 3 ) they said the guidance was not always clear .

regarding agency awareness of the guidance , since the 2010 it reform plan , omb has communicated changes to incremental development requirements , such as the change from 12 to 6 months , through budget guidance .

however , selected agency officials said they were not always aware of this guidance .

for example , dhs office of the cio officials told us that did not know about omb's requirement to deliver functionality every 6 months .

additionally , transportation officials representing the office of the cio were not aware that in 2012 omb had changed its guidance from recommending to requiring that projects deliver in 6-month cycles .

with respect to whether omb's guidance is realistic , officials from defense , hhs , dhs , and transportation explained that they do not want to require all of their projects to deliver functionality every 6 months because it may not be reasonable for all investments to do so .

defense , dhs , and transportation officials said that delivering every 12 months , as advocated in omb's it reform plan , is more reasonable.defense officials from the office of the cio , 12-18 month incremental development is better aligned with the acquisition framework that many of its it investments have used .

dhs and transportation office of the cio officials stated that , depending on program size , scope , complexity , budget , schedule , and expertise , it may be more reasonable to deliver functionality every 12 months .

as discussed later in this report , we agree that omb's requirement to deliver functionality every 6 months is unrealistic .

omb staff members from the office of e - government and information technology noted that it will take time for agencies to embrace delivering functionality every 6 months because of the perceived risk of adopting such approaches .

those staff members explained that agencies , such as defense , have been using the waterfall development method for many years , and they perceive less risk in continuing with that method than with changing to a method that produces functionality more rapidly .

lastly , two key components of omb's guidance are not clear .

first , in revising circular a - 130 in 2000 , omb did not identify the minimum requirements of what the agencies' policies are to include and did not specify when the policies are to be completed .

although omb issued later guidance on incremental development , it has not yet specified what agencies' incremental development policies are to include .

second , omb's guidance did not provide a complete definition of the functionality it expects to be delivered every 6 months .

according to staff from the office of e - government , omb intends for agencies to deliver functionality that can be tested by business users ; nevertheless , they noted that they left this definition out of their guidance so that agencies could develop a definition that would be flexible enough to meet their needs .

however , in the absence of further guidance from omb , agencies may continue to not define this term or may create definitions that are inconsistent with the intent of omb's policy .

for example , hhs officials from the office of the cio told us that the completion of requirements documents could meet omb's definition of delivering functionality .

additionally , an faa office of the cio official explained that some investments have classified the delivery of requirements documentation as functionality .

these two examples are not consistent with omb's intent since they do not deliver functionality that can be tested , but instead only plan to deliver project documentation .

until omb explicitly issues realistic and clear guidance and defense , hhs , dhs , and transportation address the identified weaknesses in their incremental development policies , it will be difficult to deliver project functionality more rapidly , measure how often projects are delivering functionality , and enforce compliance with the delivery time frames called for in their policies .

in its 2010 it reform plan , omb called for it programs to deliver functionality at least every 12 months .

subsequently , omb has made this requirement more stringent in that it now requires projects associated with major it investments to deliver functionality every 6 months .

the majority of the selected investments we reviewed did not plan to deliver functionality every 6 months .

specifically , only 23 of the selected 89 investments had one or more projects that , when taken collectively , planned to deliver functionality every 6 months .

to va's credit , all six of the department's selected investments planned to deliver functionality every 6 months .

the other agencies varied in the extent to which they met the standards established by omb's guidance .

table 2 shows how many of the selected investments at each agency planned on delivering functionality every 6 months during fiscal years 2013 and 2014 .

the variety of life - cycle cost estimates for these investments shows that incremental development can be applied to a wide variety of investment scopes .

specifically , of the 23 investments that planned to deliver functionality in 6-month cycles , 9 had cost estimates that were less than $250 million , 4 had estimates between $250 and $575 million , 5 had estimates between $575 million and $2 billion , and 5 had estimates greater than $2 billion .

twenty - seven of the 89 investments in our review ( 30 percent ) reported using an agile development methodology for one or more of their projects .

of those 27 investments , 14 ( 52 percent ) planned to deliver functionality every 6 months .

the other 13 investments using agile did not plan on delivering functionality as frequently as omb guidance requires .

we have previously found that agile projects typically produce working functionality every 1 to 8 weeks ; as such , it appears that these investments may not be properly implementing an agile development methodology .

agency officials cited three types of investments for which it may not always be practical or necessary to expect functionality to be delivered in 6-month cycles: ( 1 ) investments in life - cycle phases other than acquisition ( i.e. , planning and budgeting , management in - use , and disposition ) ; ( 2 ) investments intended to develop it infrastructure ; and ( 3 ) research and development investments .

life - cycle phases other than acquisition .

officials from defense , hhs , dhs , and transportation stated that it is not reasonable to expect investments to deliver functionality every 6 months when their investments' projects are not in the acquisition phase .

specifically , 24 investments did not have projects in the acquisition stage .

of those 24 investments , 22 did not plan to deliver functionality in 6-month cycles ( 10 from defense , 3 from hhs , 1 from dhs , and 8 from transportation ) , and 2 investments did plan to do so ( 2 from hhs ) .

for the 2 investments that planned to deliver functionality every 6 months , both had at least one project in the management in - use phase , meaning that at least one of the investments' projects were beyond the planning and development stages and were being used to support agency operations .

infrastructure investments .

officials from defense , dhs , and transportation explained that not all infrastructure investments can be expected to deliver functionality every 6 months .

specifically , 21 investments provide infrastructure , such as it security , office automation , and telecommunications .

for example , officials representing two of the dhs investments explained that , prior to deploying functionality , they need to acquire real estate , conduct environmental assessments , and perform construction work , such as digging trenches , burying cables , and building facilities .

of the 21 investments , 20 did not plan to deliver functionality every 6 months ( 17 from defense , 1 from hhs , and 2 from dhs ) ; however , 1 investment did plan to do so ( 1 from defense ) .

research and development investments .

officials from faa's office of the cio explained that faa's research and development investments are not intended to deliver functionality .

those officials stated that , before the agency approves a technology for further development , it performs research and development to ensure that the technology meets safety standards .

if those standards are met , faa creates a new investment aimed at deploying that technology .

consistent with this , none of faa's six research and development investments planned to deliver any functionality during fiscal years 2013 and 2014 .

these concerns have merit .

for example , with respect to investments in life - cycle phases other than acquisition , at the outset of a new investment , an agency may need more than 6 months to , among other things , define high - level requirements and find a contractor to help develop the system .

in addition , it may not be necessary for an investment to continue delivering new functionality when all planned functionality has been fully deployed .

regarding infrastructure investments , it may not be practical or cost - effective for an investment to refresh fully functioning hardware every 6 months .

additionally , it may not be feasible for an agency to build a physical facility ( eg , data center ) within 6 months .

further , for research and development investments , industry practices and our work on best practices support faa's efforts to thoroughly validate the feasibility and cost - effectiveness of new technology prior to making significant investments .

although omb requires all investments to deliver functionality every 6 months , an omb staff member from the office of e - government explained that not all investments will be able to meet this goal .

instead , that staff member said that about half of the federal government's major it investments will deliver functionality in 6 months or less , and the other half will have longer development cycles .

however , omb's guidance does not make this distinction .

as a result , agencies may be confused about whether omb's incremental development guidance applies to all investments .

if these three types of investments , which account for 40 of the selected 29 of the remaining 49 investments 89 investments , are not considered,did not plan to deliver functionality in 6-month cycles .

table 3 shows , after removing the three types of investments discussed above , how many of the selected investments at each agency planned to deliver functionality every 6 months during fiscal years 2013 and 2014 .

considering agencies' concerns about delivering functionality every 6 months for the three types of investments discussed above and omb's own expectations that many investments will not meet this goal , it is unclear whether this is the most appropriate governmentwide goal , and it raises the question of whether omb should consider a longer time frame , such as 12 months , as called for in omb's it reform plan .

however , even using the time frame of 12 months as the target , less than half of the selected investments planned to deliver functionality in 12-month cycles .

specifically , 41 of the 89 selected investments planned to deliver functionality every 12 months during fiscal years 2013 and 2014 and 48 did not .

most notably , the preponderance of defense and transportation investments ( 70 percent for defense and 65 percent for transportation ) did not plan to deliver functionality every 12 months .

table 4 shows how many of the selected investments at each agency planned on delivering functionality every 12 months during fiscal years 2013 and 2014 .

the previously discussed weaknesses in agency policies have permitted the inconsistent implementation of incremental development approaches .

omb staff members from the office of e - government acknowledged that inconsistent implementation of omb's guidance can be at least partially attributed to challenges in ensuring that agencies develop consistent definitions of and approaches to incremental development .

although omb has led the government's recent effort to improve incremental development , it has not completed a key commitment aimed at improving incremental development — namely , omb has rarely used the techstat process to turn around or cancel investments that are not using incremental development .

as previously mentioned , techstat sessions are intended to terminate or turn around it investments that are failing or are not producing results .

additionally , many failed it investments have not used incremental development approaches .

therefore , omb could use techstat sessions as a powerful tool to turn around investments that are not being acquired incrementally .

however , omb staff members from the office of e - government said that omb has only held one such techstat .

omb staff from the office of e - government explained that , in order to select investments in need of a techstat session , agency exhibit 300 data are reviewed for evidence of poor performance .

however , the usefulness of the exhibit 300 data for the purpose of identifying whether investments are using incremental approaches is limited .

for example , omb does not require agencies to explicitly identify whether their investments are using incremental approaches .

additionally , of the 89 selected investments , 34 had activities in their exhibit 300 submissions that were inaccurately classified as delivering functionality ( 9 from defense , 8 from hhs , 5 from dhs , 9 from transportation , and 3 from va ) .

for example , one defense investment indicated that awarding a contract constituted a delivery of functionality .

omb staff from the office of e - government acknowledged that the exhibit 300 data are not as helpful as possible in addressing incremental development .

consequently , for its techstat reviews , omb's insight into investments' use of incremental development approaches is limited .

officials from defense , hhs , dhs , and transportation attributed the problem to a lack of guidance from omb on what is to be delivered every 6 months .

nevertheless , officials from defense , dhs , and va stated that they would properly classify activities in future exhibit 300 submissions.without additional guidance from omb , agencies may continue to improperly classify activities .

additionally , as previously mentioned , four of the five selected agencies — defense , hhs , dhs , and transportation — have not updated their techstat policies to include identifying investments that are not being acquired incrementally .

without better implementation of incremental development approaches and identification of investments using incremental development , it expenditures will likely continue to produce disappointing results — including large cost overruns , long schedule delays , and questionable mission - related achievements .

further , without useful information , including whether investments are following an incremental approach , on projects associated with major it investments , omb does not have the necessary information to oversee the extent to which projects and investments are implementing its guidance .

multiple factors were identified by the five agencies as enabling and inhibiting incremental development during a 6-month period .

specifically , eight factors were identified by three or more of the five agencies in our review as enabling incremental development of it systems , and seven factors were identified by three or more agencies as inhibiting incremental development .

the enabling factor identified by all of five of the agencies was active engagement of program officials with stakeholders .

the inhibiting factors identified by all five agencies were ( 1 ) the lack of sufficient , timely funding ; ( 2 ) program characteristics that made rapid incremental development infeasible ; and ( 3 ) the lack of stable , prioritized requirements .

eight factors were identified by three or more of the five agencies in our review as contributing to the successful development of functionality in 6- month cycles .

all five of the agencies in our review cited active program engagement with stakeholders .

table 5 shows the distribution of the eight factors among the agencies , and examples of how the agencies implemented them are discussed following the table .

program officials actively engaged with stakeholders .

officials from all five agencies explained that active engagement with program stakeholders — individuals or groups with an interest in the success of the investment — was a factor that enabled the development of functionality in 6-month cycles .

for example , officials from one of the hhs investments that we reviewed stated that having strong communication between the business program , the it program , and contractors enabled projects to move forward on schedule in a cohesive manner with clear goals and objectives .

programs used an agile development methodology .

officials from four of the five agencies indicated that the use of an agile development methodology helped them to deliver functionality every 6 months .

for example , defense officials explained that the use of agile development processes allowed software to be broken into smaller releases that were easily achieved , tested , and fielded .

additionally , va officials stated that the agency has embraced agile development , which has helped the department deliver functionality more quickly .

those officials also noted that merely forcing investments to use 6- month waterfall iterations would not have resulted in the same success .

programs successfully prioritized , managed , and tested requirements .

officials from four of the five agencies identified implementation of requirements management practices — including requirements prioritization , management , and testing — as a factor that enabled the agencies to deliver functionality every 6 months .

for example , transportation officials explained that the primary factor that enabled incremental development has been obtaining clear requirements .

further , hhs officials stated that the use of a prioritized product backlogrequirements should be allocated to future releases .

has helped teams make decisions about which staff had the necessary skills and experience .

officials from four of the five agencies stated that consistent and stable program staff with experience and expertise allowed them to deliver functionality frequently .

for example , dhs officials representing one of the selected investments indicated that having skilled program managers has a large impact on the success of a program .

in addition , va officials reported that their ability to retain key personnel with both functional and technical expertise has enabled them to deliver high - quality functionality rapidly .

programs successfully implemented cost and schedule estimating best practices .

officials from four of the five agencies cited the implementation of cost and schedule estimating practices as enabling the frequent delivery of functionality .

for example , defense officials told us that development of realistic cost estimates and comprehensive program schedules helped programs to deliver functionality while meeting established performance goals .

further , transportation officials stated that improved guidance and training on cost estimating and scheduling helped them to deliver functionality in 6-month cycles .

officials used various contracting strategies to increase flexibility and improve contractor oversight .

officials from four of the five agencies cited the use of specific contracting strategies .

for example , dhs officials indicated that they worked with contractors to modify their contracts from the traditional waterfall approach to a structure that allows for agile development .

in addition , officials from va stated that the use of performance - based acquisitions assisted them in monitoring contractor progress towards achieving actual results against planned objectives .

staff used key technologies to accelerate development work .

officials from three of the five agencies indicated that the use of key technologies enabled them to deliver functionality more quickly .

for example , hhs officials explained that having an established cloud environment has enabled them to reduce deployment time for releases , while also providing the needed flexibility to meet their customers' changing needs .

additionally , dhs officials explained that projects , especially those using an agile development methodology , have saved time using automated testing tools .

programs successfully implemented risk management best practices .

officials from three agencies explained that the successful implementation of risk management practices helped them to deliver functionality more rapidly .

for example , officials from one of the selected va investments told us that the organization's risk management process allowed teams to quickly escalate risks to senior leadership so that they could be managed before their impacts were fully realized .

many of the factors that the five agencies cited are consistent with our 2011 work on factors critical to successful it acquisitions and our 2012 work on effective practices in implementing agile software development.in particular , our work in both areas discussed the importance of active engagement with program stakeholders , such as actively engaging with stakeholders to obtain customer feedback , and effectively managing requirements .

additionally , the eight commonly identified factors that enable the delivery of functionality in 6 months are consistent with omb's it reform plan .

in particular , as previously mentioned , one high - level objective of the plan — effectively managing large - scale it programs — aims to improve areas that impact the success rates of large it programs by , among other things , enabling programs to use incremental development approaches .

as part of this high - level objective , the plan addresses the importance of actively engaging with stakeholders , ensuring that program management professionals have proper skills and experience , and developing new contracting strategies that more effectively support incremental development .

seven factors were identified by three or more of the five agencies as inhibiting the development of it functionality every 6 months .

the factors most commonly cited include ( 1 ) programs did not receive sufficient funding or receiving funding later than needed , ( 2 ) program characteristics made rapid delivery of functionality infeasible or impractical , ( 3 ) programs did not have stable and prioritized requirements .

table 6 shows the distribution of the seven factors among the agencies , and examples of how the factors impacted the agencies are discussed following the table .

programs did not receive sufficient funding or received funding later than needed .

officials from all five departments cited insufficient funding , such as reductions caused by the fiscal year 2013 sequester , or receiving funding later than needed because of continuing resolutions .

for example , defense officials representing one of the investments we reviewed stated that furloughs brought on by the 2013 sequester significantly impacted program schedules , both in terms of lost work days and the inability to coordinate integration between software providers , resulting in overall inefficiencies .

in addition , several faa officials explained that the delivery of planned functionality was adversely affected by the uncertainty brought about by the 2013 sequester , and that future funding instability has impacted the agency's ability to plan when functionality is to be delivered .

program characteristics made rapid delivery of functionality infeasible or impractical .

officials from all five agencies indicated that some of their programs have certain characteristics — such as the deployment of new physical infrastructure , human health and safety concerns , or external dependencies — that make the delivery of functionality in 6- month time frames infeasible or impractical .

for example , dhs officials explained that their infrastructure projects cannot deliver functionality until all key activities ( eg , land acquisition , environmental assessments , site preparation , and construction ) have been completed and the new infrastructure has been fully deployed , tested , and accepted .

additionally , transportation officials reported that air traffic control systems require years of development and testing in order to ensure that the systems do not compromise airspace security .

programs did not have stable , prioritized requirements .

officials from all five agencies stated that not having complete requirements at the beginning of their investments' development or having changes made to requirements and their relative priorities during development was a factor that negatively affected their programs' delivery of incremental functionality in 6-month periods .

for example , hhs officials representing one of the selected investments explained that that final rules detailing eligibility for a medical program were not completed until late in the development cycle ; consequently , the program could not define the corresponding business requirements in time to meet the development schedule .

further , va officials stated that schedules are disrupted when stakeholders identify new program requirements that must be delivered before others .

development work was slowed by inefficient governance and oversight processes .

officials from three of the five agencies cited inefficient governance and oversight processes .

for example , dhs officials representing the office of the cio stated that the current dhs governance model was not conducive to frequent delivery of functionality .

to illustrate , those officials noted that it can take up to 2 months to schedule a meeting with dhs review boards prior to releasing functionality .

however , a dhs official from program accountability and risk management disagreed with this statement , explaining that dhs's acquisition review boards perform reviews very quickly , and that any delays in completing these reviews are attributable to investments being unprepared .

further , dhs office of the cio officials suggested that governance over programs using an agile development methodology should be performed at the lowest practicable level of the organization .

in addition , va officials explained that , although the periodic approvals required of various department groups are useful , these approvals can slow the testing and release processes and could use further streamlining .

development schedules were impeded by procurement delays .

officials from three of the five agencies explained that procurement delays — such as delays in getting contracts awarded or approving contract modifications — contributed to difficulties in delivering incremental functionality .

for example , dhs officials explained the process of planning for an acquisition , developing solicitations , selecting contractors , and , in some cases , addressing protests , are not conducive to delivering functionality in 6-month cycles .

program staff were overutilized or lacked the necessary skills and experience .

officials from three of the five agencies indicated that they did not have enough program staff with the expertise and experience necessary to deliver functionality every 6 months .

for example , officials representing one of the hhs investments stated that the loss of key personnel and the lack of staff with knowledge and skill in key disciplines , such as incremental development , have negatively impacted the agency's ability to deliver functionality .

incremental development was impeded by select technologies .

officials from three of the five agencies in our review explained that the software and tools they selected for their programs either introduced delays in development or were ultimately not usable by the program .

transportation officials representing one of the selected investments reported that their chosen technology added a new level of complexity to storage and data processing that required all development and testing to be completed before the implementation could occur .

as a result , those officials told us that all functionality had to be deployed in one release , instead of being spread out in stages .

many of the factors that the five agencies identified as inhibiting delivery of functionality in 6-month increments were consistent with our work on the challenges we identified in the application of agile software development methods.deploying software developed using an agile development method is that traditional governance and oversight activities were difficult to execute for example , we noted that one challenge in within an iterative time frame .

in particular , one agency official stated that completing the necessary reviews within the short , fixed time frame of an agile iteration was difficult because reviewers followed a slower waterfall schedule with reviews that could take months to perform after the completion of an iteration .

this caused delays for iterations that needed such reviews within the few weeks of the iteration .

we also noted that procurement practices sometimes delay agile development .

further , omb's it reform plan also identified some of the same problems that inhibit incremental development and described solutions to in particular , as previously mentioned , one item of the address them.plan was to improve funding of programs by working with congress to create it budget models that promote it budget flexibility ( such as multiyear budgets or revolving funds ) .

in addition , the plan addresses the importance of streamlining it governance by strengthening the quality and timing of oversight activities .

the factors identified in this report as enabling and inhibiting incremental development could help agencies address the challenges they face in incrementally acquiring it .

given the enormous size of the federal government's investment in it and the often disappointing results from it development efforts — many of which are attributable to the use of a “big bang” approach — finding ways to improve the quality and timeliness of agencies' investments is important .

congress , omb , and our work support the use of incremental development practices .

however , although the selected agencies have developed policies that address incremental development , most of the agencies' policies have significant weaknesses .

with the exception of va , these policies do not fully define functionality , and do not have a complete process for ensuring that the agencies' investments are developed incrementally , including the use of techstat sessions to enforce compliance with incremental development policies .

in the absence of such policies , agencies continue to run the risk of failing to deliver major investments in a cost - effective and efficient manner .

regarding implementation of incremental development , slightly more than one - fourth of selected investments planned to deliver functionality every 6 months — and less than one - half planned to do so every 12 months .

thus , delivering functionality every 6 months is not an appropriate requirement for all agencies given current performance .

requiring the delivery of functionality every 12 months , consistent with omb's it reform plan , would be an appropriate starting point and be a substantial improvement .

further , since there are three types of investments for which it may not always be practical or necessary to expect functionality to be delivered in 6-month cycles , this raises questions about whether omb's requirement for shorter delivery cycles should be applied to all investments or whether investments should be allowed the latitude to determine a more appropriate time frame .

the lack of progress in updating policies and implementing omb's guidance was enabled by weaknesses in omb's guidance .

in the absence of agency and omb use of techstat sessions to ensure compliance with incremental development policy , investments will continue to be at risk of not delivering promised capabilities on time and within budget .

until omb clearly and explicitly disseminates guidance with realistic goals and clear expectations , and agencies update their policies to reflect this guidance , agencies may not consistently adopt incremental development approaches , and it expenditures will continue to produce disappointing results — including sizable cost overruns and schedule slippages , and questionable progress in meeting mission goals and outcomes .

additionally , without useful information on how often projects are delivering functionality , it will be difficult for omb to identify investments that are not implementing its guidance .

further , dissemination of the factors identified in this report as enabling and inhibiting incremental development may help federal agencies address the challenges they face in acquiring it investments faster and more efficiently .

we recommend that the director of the office of management and budget direct the federal chief information officer to take the following two actions .

update , and clearly and explicitly issue incremental development guidance that addresses the following three components: requires projects associated with major it investments to deliver incremental functionality at least every 12 months , with the exception of the three types of investments identified in this report ; specifies how agencies are to define the project functionality that is to be delivered ; and requires agencies to define a process for enforcing compliance with incremental functionality delivery , such as the use of techstat sessions .

require agencies to clearly identify on exhibit 300 submissions whether , for each project , functionality will be delivered within the time frames called for by this incremental development guidance , and to provide justification for projects that do not plan to do so .

we further recommend that the secretaries of defense , health and human services , homeland security , and transportation take the following two actions: modify , finalize , and implement their agencies' policies governing incremental development to ensure that those policies comply with omb's guidance , once that guidance is made available ; and when updating their policies , consider the factors identified in this report as enabling and inhibiting incremental development .

we also recommend that the secretary of veterans affairs consider incorporating the factors identified in this report as enabling and inhibiting incremental development in the department's related policy .

we received comments on a draft of this report from omb and the five agencies in our review .

omb agreed with one recommendation and partially disagreed with the other ; defense generally concurred with the report ; hhs neither agreed nor disagreed with the report's recommendations ; dhs agreed with our recommendations ; transportation did not agree with the recommendations in that it did not believe the department should be dependent on omb first taking action ; and va generally agreed with the report's conclusions and concurred with our recommendation .

each agency's comments that we received are discussed in more detail below .

in comments provided via e - mail on april 15 , 2014 , staff in omb's office of general counsel , on behalf of omb , stated that the agency agreed with one of our recommendations and partially disagreed with the other .

specifically , omb agreed with our recommendation to require agencies to clearly identify on exhibit 300 submissions whether functionality will be delivered within the time frames called for by this incremental development guidance .

omb stated that it agreed with our recommendation to update and issue incremental development guidance , but did not agree that this guidance should require major it investments to deliver incremental functionality at least every 12 months .

omb explained that changing the requirement from 6 to 12 months would reduce the emphasis on incremental development that it has been advocating .

omb also noted that it believes requiring investments to deliver functionality every 6 months is an appropriate governmentwide goal and said that updating and clarifying its guidance on incremental development will make it easier for agencies to meet this target .

however , as we state in this report , slightly more than one - fourth of selected investments planned to deliver functionality every 6 months — and less than one - half planned to do so every 12 months .

additionally , there are three types of investments for which it may not always be practical or necessary to expect functionality to be delivered in 6-month cycles .

thus , we continue to believe that delivering functionality every 6 months is not an appropriate requirement for all agencies and that requiring the delivery of functionality every 12 months , consistent with omb's it reform plan , is a more appropriate starting point .

we therefore maintain that omb should require projects associated with major it investments to deliver functionality at least every 12 months .

in written comments , defense stated that it generally concurred with the report and outlined planned actions to address the recommendations .

however , defense explained that many of its it investments are consistent with the three types of investments for which it may not always be practical or necessary to expect functionality to be delivered in 6-month cycles or have other exceptional circumstances .

given these issues , defense stated that requiring the delivery of functionality every 12 months is not a useful management constraint .

however , as we state in our report , many failed projects have been broadly scoped in that they aim to deliver their capabilities several years after initiation .

additionally , the defense science board reported that defense's acquisition process for it systems was too long , was ineffective , and did not accommodate the rapid evolution of it .

in order to resolve these issues , we continue to believe that investments and their projects should deliver functionality more frequently .

in addition , the existence of many investments that are consistent with the three types of investments identified in this report for which it may not be practical to deliver functionality quickly does not excuse defense from the requirement of delivering functionality every 12 months .

consistent with our recommendation to omb , investments and projects that meet these three exceptions should be exempt from the requirement , but all other investments and projects should aim to meet this requirement .

further , we expect that defense's actions to implement our recommendation to consider the factors identified in this report that enable and inhibit incremental development will help its investments and projects deliver functionality more rapidly .

as such , we continue to believe that requiring the delivery of functionality every 12 months is an appropriate goal for all federal agencies , including defense .

defense's comments are reprinted in appendix iii .

the department also provided technical comments , which we have incorporated in the report as appropriate .

in comments provided via e - mail on april 15 , 2014 , an official from hhs's office of the assistant secretary for legislation , on behalf of hhs , stated that the department had no comments .

in written comments , dhs stated that it concurred with our recommendations and outlined planned actions to address the recommendations .

dhs's comments are reprinted in appendix iv .

the department also provided technical comments , which we have incorporated in the report as appropriate .

in comments provided via e - mail on april 15 , 2014 , transportation's deputy director of audit relations , on behalf of transportation , stated that the department would prefer to have specific recommendations and deliverables so that it can achieve success in closing them .

specifically , the department explained that relying on another agency to concur with one of our recommendations before transportation can take action leaves the department with the potential challenge of a recommendation that cannot be implemented .

however , as previously stated , omb agrees with our recommendation to update and issue incremental guidance , meaning that omb has committed to taking the actions necessary to enable transportation to begin addressing our recommendation .

additionally , our recommendation to consider the factors identified in this report as enabling and inhibiting incremental development when updating incremental development policies does not require another agency to take action before transportation can implement it .

accordingly , we continue to believe that our recommendations are warranted and can be implemented .

in written comments , va stated that it generally agreed with the report's conclusions , that it concurred with our recommendation to the department , and that it will review its existing policy for incremental development and consider incorporating the factors identified as enabling and inhibiting incremental development .

va's comments are reprinted in appendix v. the department also provided technical comments , which we have incorporated in the report as appropriate .

as agreed with your offices , unless you publicly announce the contents of this report earlier , we plan no further distribution until 30 days from the report date .

at that time , we will send copies of this report to interested congressional committees ; the director of the office of management and budget ; and the secretaries of the departments of defense , health and human services , homeland security , transportation , and veterans affairs .

in addition , the report will also be available at no charge on our website at http: / / www.gao.gov / .

if you or your staffs have any questions on matters discussed in this report , please contact me at ( 202 ) 512-9286 or pownerd@gao.gov .

contact points for our offices of congressional relations and public affairs may be found on the last page of this report .

gao staff who made major contributions to this report are listed in appendix vi .

our objectives for this review were to ( 1 ) assess whether selected agencies have established policies for incremental information technology ( it ) development , ( 2 ) determine whether selected agencies are using incremental development approaches to manage their it investments , and ( 3 ) identify the key factors that enabled and inhibited the selected agencies' abilities to effectively use incremental development approaches to manage their it investments .

in conducting our review , we selected five agencies and a total of 89 investments from those agencies .

to choose the agencies , we identified the five agencies with the largest it budgets for development , modernization , and enhancement on major it investments in fiscal years 2013 and 2014 as reported in the office of management and budget's ( omb ) fiscal year 2014 exhibit 53 .

those agencies are the departments of defense ( defense ) , health and human services ( hhs ) , homeland security ( dhs ) , transportation ( transportation ) , and veterans affairs ( va ) .

to choose the agencies' investments , we identified the 98 major it investments for which the selected agencies planned to spend more than 50 percent of the investments' fiscal year 2013 and 2014 budgets on development , modernization , and enhancement as reported in omb's fiscal year 2014 exhibit 53 .

we removed 9 investments because , after reporting their planned budgets for omb's fiscal year 2014 exhibit 53 , these investments changed their budgets so that they no longer planned to spend more than 50 percent of the investments' fiscal year 2013 and the 2014 budgets on development , modernization , and enhancement.final 89 investments are identified in appendix ii .

the investments selected for the review account for about 58 percent of the development , modernization , and enhancement spending on all federal agencies' major it investments for fiscal years 2013 and 2014 reported in omb's exhibit 53 for fiscal year 2014 .

to address our first objective , we reviewed omb guidance related to the use of incremental development , as well as industry guidance , and identified three key components of incremental development that agencies should include in their policies .

in order to identify these components , we reviewed omb's guidance and leading industry guidance on institutionalizing processes throughout an organization.analysis identified three key policy components that will help agencies effectively implement omb's requirement for incremental development .

require delivery of functionality every 6 months .

according to the software engineering institute's ( sei ) capability maturity model® integration for acquisition ( cmmi - acq ) , as part of institutionalizing a process , organizations should document their processes , to include defining standards and requirements .

according to omb budget guidance , projects associated with major it investments must deliver functionality every 6 months .

define functionality .

as previously stated , according to sei's cmmi - acq , as part of institutionalizing a process , organizations should document that process , to include defining standards and requirements .

define a process for enforcing compliance .

according to sei's cmmi - acq , as part of institutionalizing a process , organizations should document that process , including management review activities ( eg , taking corrective action when requirements and objectives are not being satisfied ) .

additionally , omb circular a - 130 requires agencies to ensure that investments comply with their policies .

further , according to the president's fiscal year 2014 budget , agencies are to use their techstat processes to identify investments that are not being acquired incrementally and undertake corrective actions .

at each selected agency , we then analyzed agency policies for incremental development and compared these policies to the three components identified above .

for each agency , each policy component was assessed as either being not met — the agency did not provide evidence that addressed the component or provided evidence that minimally addressed the component ; partially met — the agency provided evidence that addressed about half or a large portion of the component ; or fully met — the agency provided evidence that addressed the component .

we also interviewed officials from omb and the five selected agencies to obtain information about their current and future incremental development policies .

to address our second objective , we administered a data collection instrument to each of the selected investments about how often each investment planned to deliver functionality during fiscal years 2013 and 2014 .

we then analyzed information obtained from data collection instruments describing how often the selected investments planned to deliver functionality .

we prepopulated these instruments with data obtained from omb's fiscal year 2014 exhibit 53 , as well as each investment's exhibit 300 data , which describe the investments' projects and how often each project plans to deliver functionality .

we asked officials from each investment to verify the accuracy and completeness of the data and to make corrections where needed .

because the exhibit 300 data did not always describe the investments' plans for delivering functionality in fiscal year 2014 , we asked the officials to indicate for each project whether they planned to deliver functionality in the first half of fiscal year 2014 ( i.e. , from october 1 , 2013 , to march 31 , 2014 ) and in the second half of fiscal year 2014 ( i.e. , from april 1 , 2014 , to september 30 , 2014 ) .

we also asked the investments to provide their life - cycle cost estimates and , for each project , the development methodology used and phase of the acquisition life cycle .

using the information obtained through the data collection instruments , we determined the extent to which the selected investments and their projects planned to meet omb's guidance on incremental development .

to assess whether investments had planned to deliver functionality every 6 months , we determined whether the selected investments planned to deliver functionality in each of the following four time frames: ( 1 ) the first half of fiscal year 2013 ( i.e. , from october 1 , 2012 , to march 31 , 2013 ) , ( 2 ) the second half of fiscal year 2013 ( i.e. , from april 1 , 2013 , to september 30 , 2013 ) , ( 3 ) the first half of fiscal year 2014 , and ( 4 ) the second half of fiscal year 2014 .

to determine whether investments had planned to deliver functionality every 12 months , we analyzed whether the selected investments planned to deliver functionality in each of the following two time frames: ( 1 ) fiscal year 2013 and ( 2 ) fiscal year 2014 .

we presented our results to the five selected agencies and omb and solicited their input and explanations for the results .

to determine the reliability of the exhibit 300 data , we performed three steps .

first , as previously mentioned , we asked officials from each investment to verify the accuracy and completeness of these data and provide the correct information where needed .

second , we removed projects that were not intended to deliver functionality and activities that were inaccurately classified as resulting in the delivery of functionality .

to do so , we compared the descriptions of the projects and activities with a definition of functionality that we developed .

to develop this definition , we reviewed omb and agency guidance , as well as leading practices.we defined functionality as follows: the implementation of it requirements that is intended to either ( 1 ) ultimately be used by one or more customers in production ( actual deployment may occur at a later date than when the functionality is reported as being delivered ) or ( 2 ) be a delivery of a prototype or pilot .

we then assessed the descriptions of the activities and projects agencies reported in their exhibit 300 submissions against our definition .

we presented the activities and projects that did not meet our definition to the selected agencies and solicited their input and explanations .

third , where there was a conflict between the exhibit 300 data and agencies' answers to our questions regarding plans for delivering functionality in fiscal year 2014 , we presented the conflict to the agencies and obtained clarification .

we determined that the data were sufficiently reliable for the purpose of this report , which is to determine the extent to which the selected investments planned to deliver functionality every 6 and 12 months , during fiscal years 2013 and 2014 .

gao , information technology: critical factors underlying successful major acquisitions , gao - 12-7 ( washington , d.c.: oct. 21 , 2011 ) .

methods .

additionally , we compared the factors to omb's 25 point implementation plan to reform federal information technology management .

further , because defense guidance encourages investments to deliver functionality every 12-18 months where possible , we asked the selected investments and officials from defense's office of the cio to identify the key factors that have both enabled and inhibited their efforts to deliver functionality for their major it investments every 12-18 months .

we compared the information we received to the eight factors the five selected agencies commonly identified as enabling incremental development during a 6-month period and the seven factors commonly identified by the five agencies as inhibiting incremental development during a 6-month period .

we also performed a content analysis of the information we received in order to identify additional factors .

we conducted this performance audit from may 2013 to may 2014 , in accordance with generally accepted government auditing standards .

those standards require that we plan and perform the audit to obtain sufficient , appropriate evidence to provide a reasonable basis for our findings and conclusions based on our audit objectives .

we believe that the evidence obtained provides a reasonable basis for our findings and conclusions based on our audit objectives .

below is the list of investments that are included in this review , as well as whether each planned to deliver functionality every 6 and 12 months for fiscal years 2013 and 2014 .

in addition to the contact name above , individuals making contributions to this report included dave hinchman ( assistant director ) , deborah a. davis ( assistant director ) , rebecca eyler , kaelin kuhn , jamelyn payan , meredith raymond , kevin smith , andrew stavisky , and kevin walsh .

