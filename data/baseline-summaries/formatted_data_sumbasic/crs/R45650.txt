o ne of the core purposes of the first amendment's free speech clause is to foster "an uninhibited marketplace of ideas," testing the "truth" of various ideas "in the competition of the market. .

social media sites provide one avenue for the transmission of those ideas .

the supreme court has recognized that the internet in general , and social media sites in particular , are "important places" for people to "speak and listen," observing that "social media users employ these websites to engage in a wide array of protected first amendment activity. .

users of social media sites such as facebook , twitter , youtube , or instagram can use these platforms to post art or news , debate political issues , and document their lives .

in a study conducted in early 2018 , the pew research center found that 68% of u.s. adults use facebook , 35% use instagram , and 24% report using twitter .

these sites not only allow users to post content , they also connect users with each other , allowing users to seek out friends and content and often recommending new connections to the user .

on most social media platforms , users can then send content to specific people , or set permissions allowing only certain people to view that content .

through human curation and the use of algorithms , these platforms decide how content is displayed to other users .

in curating this content , social media sites may also edit user content , combine it , or draft their own additions to that content .

these platforms are generally free to users , and make revenue by selling targeted advertising space , among other things .

thus , social media sites engage in a wide variety of activities , at least some of which entail hosting — and creating — constitutionally protected speech .

social media companies have recognized their role in providing platforms for speech .

to take one example , in a september 2018 hearing before the senate select committee on intelligence , the founder and chief executive officer of twitter , jack dorsey , repeatedly referred to twitter as a "digital public square," emphasizing the importance of "free and open exchange" on the platform .

critically , however , social media sites also have content - moderation policies under which they may remove certain content .

further , these sites determine how content is presented: who sees it , when , and where .

as one scholar has said , social media sites "create rules and systems to curate speech out of a sense of corporate social responsibility , but also .

 .

 .

because their economic viability depends on meeting users' speech and community norms. .

speech posted on the internet "exists in an architecture of privately owned websites , servers , routers , and backbones," and its existence online is subject to the rules of those private companies .

consequently , one first amendment scholar predicted ten years ago that "the most important decisions affecting the future of freedom of speech will not occur in constitutional law ; they will be decisions about technological design , legislative and administrative regulations , the formation of new business models , and the collective activities of end - users. .

social media companies have come under increased scrutiny regarding the type of user content that they allow to be posted on their sites , and the ways in which they may promote — or deemphasize — certain content .

a wide variety of people have expressed concern that these sites do not do enough to counter harmful , offensive , or false content .

at the same time , others have argued that the platforms take down or deemphasize too much legitimate content .

in the september 2018 hearing referenced above , sheryl sandberg , the chief operating officer of facebook , expressed the difficulty of determining what types of speech would violate company standards barring hate speech .

both dorsey and facebook founder and chief executive officer mark zuckerberg have been asked to respond to allegations of political bias in their platforms' content moderation decisions at hearings before house and senate committees .

commentators and legislators alike have questioned whether social media sites' content policies are living up to the free speech ideals they have espoused .

as a result , some , including members of congress , have called for regulation of social media platforms , focused on the way those companies police content .

in light of this public policy debate , this report begins by outlining the current legal framework governing social media sites' treatment of users' content , focusing on the first amendment and section 230 of the communications decency act of 1996 ( cda ) .

as explained below , under existing law , lawsuits predicated on these sites' decisions to remove or to host content have been largely unsuccessful because of ( 1 ) doctrines that prevent the first amendment from being applied to private social media companies , and ( 2 ) section 230 of the cda , which often protects social media companies from being held liable under federal or state laws for these decisions .

the debate over whether the federal government should fill this legal vacuum has raised the question as to whether and to what extent the federal government can regulate the way social media sites present users' content , either to require these sites to take down , restrict access to , or qualify certain types of content , or , on the other hand , protect users' rights to post content on those sites .

such government regulation would constitute state action that implicates the first amendment .

while the issue largely remains an open question in the courts , the first amendment may provide some protection for social media companies when they make content presentation decisions , limiting the federal government's ability to regulate those decisions .

the extent of any free speech protections will depend on how courts view social media companies and the specific action being regulated .

accordingly , the bulk of this report explores how the first amendment applies to social media providers' content presentation decisions .

looking to three possible analogues drawn from existing first amendment law , the report explores whether social media companies could be viewed in the same way as company towns , broadcasters , or newspaper editors .

the report also explains the possible regulatory implications of each first amendment framework as congress considers the novel legal issues raised by the regulation of social media .

under current federal law , social media users may face at least two significant barriers if they attempt to sue a social media provider for its decisions about hosting or limiting access to users' content .

the first , which likely applies only to lawsuits predicated on a platform's decision to remove rather than allow content , is the state action requirement of the first amendment .

the state action doctrine provides that constitutional free speech protections generally apply only when a person is harmed by an action of the government , rather than a private party .

the second legal barrier is the cda's section 230 , which offers broad immunity to "interactive computer service" providers .

section 230 ( c ) ( 1 ) provides immunity from any lawsuit that seeks to hold a service provider liable for publishing information that was created by an "information content provider," effectively protecting social media sites from liability for hosting content .

by contrast , section 230 ( c ) ( 2 ) provides immunity for sites that take good faith action to restrict access to content that the provider or users deem "obscene , lewd , lascivious , filthy , excessively violent , harassing , or otherwise objectionable. .

thus , federal law does not currently provide a recourse for many users who would like to challenge a social media site's decision to ban or restrict content , or to host content — and may affirmatively bar liability in certain circumstances .

as discussed above , courts have often dismissed lawsuits attempting to hold social media providers liable for regulating users' content , whether because the court concludes that the first amendment does not apply to the actions of these private actors or because the court holds that section 230 ( c ) ( 2 ) of the cda bars the lawsuit .

additionally , section 230 ( c ) ( 1 ) may bar lawsuits that seek to hold these platforms liable because of their decisions to publish certain content .

particularly because of section 230 , there are few , if any , federal or state laws that expressly govern social media sites' decisions about whether and how to present users' content .

consequently , users' ability to post speech on social media platforms is governed primarily by the private moderation policies created by these companies .

in response to broader public policy concerns about how social media entities are policing user content , some commentators and legislators have proposed federal regulation both to protect users' ability to speak freely on those platforms and to require these platforms to take down , deemphasize , or clarify certain content .

while the first amendment , as discussed above , may not apply in disputes between private parties , a federal law regulating internet content decisions would likely qualify as state action sufficient to implicate the first amendment .

after all , the first amendment provides that " congress shall make no law .

 .

 .

abridging the freedom of speech. .

once state action is established , the next consideration is to what extent the first amendment protects social media platforms' content moderation decisions .

stated another way , the relevant question is when social media providers can assert that government regulation infringes on their own speech .

perhaps most obviously , if a social media site posts content that it has created itself , the site may raise first amendment objections to a law expressly regulating that speech .

social media providers may also argue that they are exercising protected speech rights when they are choosing whether to publish content that was originally created by users and when they make decisions about how to present that content .

however , the fact that a law affects speech protected by the first amendment does not necessarily mean that it is unconstitutional .

as explained below , the first amendment allows some regulation of speech and does not prohibit regulation of conduct .

the permissibility of federal regulation of social media sites will turn in large part on what activity is being regulated .

to the extent that federal regulation specifically targets communicative content — that is , speech — or social media platforms' decisions about whether and how to present that content , that regulation may raise constitutional questions .

while the supreme court has not yet weighed in on the question , lower courts have held that when search engines make decisions regarding the presentation of search results , they are exercising editorial functions protected as speech under the first amendment .

if this reasoning were to be extended to social media sites' decisions regarding the presentation of users' content , congress's ability to regulate those decisions would be relatively limited .

however , even assuming that congress were to regulate the protected speech of social media companies , this would not necessarily doom a regulation .

if , for example , the particular speech being regulated is commercial speech , such as advertisements , the regulation would likely be evaluated under a lower level of scrutiny .

in addition , the court has recognized certain , relatively limited categories of speech that can be more readily regulated: "for example , speech that is obscene or defamatory can be constitutionally proscribed because the social interest in order and morality outweighs the negligible contribution of those categories of speech to the marketplace of ideas. .

but even with respect to these categories of speech , the government may violate the first amendment if it engages in further content or viewpoint discrimination within that category .

thus , the supreme court has said as an example that while "the government may proscribe libel," "it may not make the further content discrimination of proscribing only libel critical of the government. .

in addition , if the law imposes criminal liability , the court may require a mental state requirement , so that , for example , the government has to prove that the defendant knew the speech was obscene .

courts will also apply a lower level of scrutiny to content - neutral regulations .

a content - neutral law that regulates only "the time , place , or manner of protected speech" may be constitutional if it is "narrowly tailored to serve a significant governmental interest. .

if a law is not only content - neutral but also focused primarily on regulating conduct , imposing only an incidental burden on speech , a court will uphold the regulation if "it furthers an important or substantial governmental interest ; if the governmental interest is unrelated to the suppression of free expression ; and if the incidental restriction on alleged first amendment freedoms is no greater than is essential to the furtherance of that interest. .

thus , for example , in turner broadcasting , the supreme court held that the fcc's must - carry provisions should be reviewed under an intermediate standard , rather than under strict scrutiny , because the rules were content - neutral: their application did not depend on "the content of the cable operators' programming" or the messages of the speakers carried .

and in fair , the court upheld the solomon amendment under intermediate scrutiny after concluding that the law regulated conduct that was not "inherently expressive" and only incidentally burdened speech .

the court said that the law did "not focus on the content of a school's recruiting policy," but on "the result achieved by the policy. .

additionally , if congress highlights "special characteristics" of social media to justify heightened regulation , courts may be more willing to uphold those regulations .

although the supreme court in reno rejected certain "special justifications" that the government argued should allow greater regulation of the internet at large , some have argued that special characteristics of social media might justify limited regulation to address those issues , particularly if those justifications are distinct from the ones rejected in reno , or if there is evidence that conditions have changed since that decision was issued .

to date , however , no courts have found that such special justifications exist , let alone approved of regulations addressing those issues .

finally , congress may consider how any new regulation would fit into the existing legal framework of the cda's section 230 .

section 230 creates immunity from most civil lawsuits that seek to treat service providers as the "publisher or speaker" of content created by another , and also provides that interactive service providers may not be held liable for taking good faith action to restrict access to content that the provider or users deem "obscene , lewd , lascivious , filthy , excessively violent , harassing , or otherwise objectionable. .

insofar as any new federal regulations would subject social media providers to liability for publishing content created by users , or for restricting access to that content , those regulations might conflict with section 230 , and congress may consider expressly setting out the relationship between those new regulations and section 230 .

as a general principle of law , courts are reluctant to imply that new statutes repeal prior laws unless the "two statutes are in 'irreconcilable conflict,' or .

 .

 .

the latter act covers the whole subject of the earlier one and 'is clearly intended as a substitute.' .

accordingly , if a new law does not explain how it relates to section 230 , courts will attempt to read the statutes harmoniously , giving effect to both .

if congress were to create an express exception from section 230 , one issue would be determining the proper scope of that exception , so that congress is allowing liability only for certain specific activity that it is seeking to discourage .

section 230 was enacted , in part , in response to a trial court decision ruling that an internet service provider should be considered a "publisher" of defamatory statements that a third party had posted on a bulletin board that it hosted , and could therefore be subject to suit for libel .

critical to the court's decision was the fact that the service provider had moderated its message boards , qualifying the site as a publisher for purposes of the libel claim in the view of the court .

by specifying that no provider of an interactive computer service "shall be treated as the publisher or speaker" of another's content , congress sought , among other things , to overturn this decision .

a number of representatives , including one of the bill's sponsors , said at the time that they wanted to ensure that "computer good samaritans" would not "tak[e] on liability" by regulating offensive content .

as discussed , courts subsequently interpreted this provision to bar liability for a wide variety of legal claims , not solely suits for defamation .

section 230 , enacted in 1996 , has often been described as central to the development of the modern internet .

one scholar asserted that "no other sentence in the u.s. code .

 .

 .

has been responsible for the creation of more value than that one. .

therefore , while congress may want to modify this broad immunity , it is important to first understand how that immunity currently operates , and why it was created in the first place .

