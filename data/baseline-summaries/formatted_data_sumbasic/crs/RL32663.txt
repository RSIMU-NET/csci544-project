federal government agencies and programs work to accomplish widely varying missions .

these agencies and programs use a number of public policy approaches , including federal spending,tax laws , tax expenditures , ( 1 ) and regulation .

 ( 2 ) in fy2004 , estimated federal spending was $2.3 trillion,and taxexpenditures totaled approximately $1 trillion .

estimates of the off - budget costs of federalregulations have ranged in the hundreds of billions of dollars , and corresponding estimates ofbenefits of federal regulations have ranged from the hundreds of billions to trillions of dollars .

 ( 3 ) given the scope and complexity of these various efforts , it is understandable that citizens,their elected representatives , civil servants , and the public at large would have an interest in theperformance and results of government activities .

evaluating the performance of governmentagencies and programs , however , has proven difficult and often controversial: actors in the u.s. political system ( eg , members of congress , the president,citizens , interest groups ) often disagree about the appropriate uses of public funds ; missions , goals,and objectives for public programs ; and criteria for evaluating success .

one person's key programmay be another person's key example of waste and abuse , and different people have differentconceptions of what "good performance" means .

even when consensus is reached on a program's appropriate goals andevaluation criteria , it is often difficult and sometimes almost impossible to separate the discreteinfluence that a federal program had on key outcomes from the influence of other actors ( eg , stateand local governments ) , trends ( eg , globalization , demographic changes ) , and events ( eg , naturaldisasters ) .

federal agencies and programs often have multiple purposes , and sometimesthese purposes may conflict or be in tension with one another .

finding and assessing a balanceamong priorities can be controversial and difficult .

the outcomes of some agencies and programs are viewed by many observersas inherently difficult to measure .

foreign policy and research and development programs have beencited as examples .

there is frequently a time lag between an agency's or program's actions andeventual results ( or lack thereof ) .

in the absence of this eventual outcome data , it is often difficultto know how to assess if a program is succeeding .

many observers have asserted that agencies do not adequately evaluate theperformance or results of their programs - - or integrate evaluation efforts across agency boundaries - - possibly due to lack of capacity , management attention and commitment , or resources .

 ( 4 ) in spite of these and other challenges , ( 5 ) in the last 50 years both congress and the president haveundertaken numerous efforts - - sometimes referred to as performance management , performancebudgeting , strategic planning , or program evaluation - - to analyze and manage the federalgovernment's performance .

many of those initiatives attempted in varying ways to use performanceinformation to influence budget and management decisions for agencies and programs .

 ( 6 ) the bush administration'srelease of part ratings along with the president's fy2004 and fy2005 budget proposals , and itsplans to continue doing so for fy2006 and subsequent years , represent the latest of these efforts .

previous sections of this report discussed how the part is structured , how it has been used,and how various actors have assessed its design and implementation .

this section discusses potentialcriteria for evaluating the part or other program evaluations , which might be considered bycongress during the budget process , in oversight of federal agencies and programs , and regardinglegislation that relates to program evaluation .

 ( 66 ) should congress focus on the question of criteria , the programevaluation and social science literature suggests that three standards or criteria may be helpful: theconcepts of validity , reliability , and objectivity .

validity has been defined as "the extent to which any measuring instrumentmeasures what it is intended to measure. .

 ( 67 ) for example , because the part is supposed to measure theeffectiveness of federal programs , its validity turns on the extent to which part scores reflect theactual "effectiveness" of those programs .

 ( 68 ) reliability has been described as "the relative amount of random inconsistencyor unsystematic fluctuation of individual responses on a measure" ; that is , the extent to which severalattempts at measuring something are consistent ( eg , by several human judges or several uses of thesame instrument ) .

 ( 69 ) therefore , the degree to which the part is reliable can be illustrated by the extent to which separateapplications of the instrument to the same program yield the same , or very similar,assessments .

objectivity has been defined as "whether [an] inquiry is pursued in a way thatmaximizes the chances that the conclusions reached will be true. .

 ( 70 ) definitions of the wordalso frequently suggest concepts of fairness and absence of bias .

the opposite concept is subjectivity , suggesting , in turn , concepts of bias , prejudice , or unfairness .

therefore , making ajudgment about the objectivity of the part or its implementation "involves judging a course ofinquiry , or an inquirer , against some rational standard of how an inquiry ought to have been pursuedin order to maximize the chances of producing true findings " ( emphasis in original ) .

 ( 71 ) although these three criteria can each be considered individually , in application they may prove tobe highly interrelated .

for example , a measurement tool that is subjectively applied may yield resultsthat , if repeated , are not consistent or do not seem reliable .

conversely , a lack of reliable results maysuggest that the instrument being used may not be valid , or that it is not being applied in an objectivemanner .

in these situations , further analysis is typically necessary to determine whether problemsexist and what their nature may be .

with regard to the part , the administration has made numerous assessments regardingprogram effectiveness .

but how should one validly , reliably , and objectively determine a programis effective ? .

should congress wish to explore these issues regarding the part or other evaluations,congress might assess the extent to which the assessments have been , or will be , completed validly,reliably , and objectively .

different observers will likely have different views about the validity , reliability , andobjectivity of omb's part instrument , usage , and determinations .

nonetheless , some previousassessments of the part suggest areas of particular concern .

for example , in its study of the part,gao reported that one of the two reasons why programs were designated by the administration as"results not demonstrated" ( nearly 50% of the 234 programs assessed for fy2004 ) was that omband agencies disagreed on how to assess agency program performance , as represented by "long - termand annual performance measures. .

 ( 72 ) different officials in the executive branch appeared to havedifferent conceptions of what the appropriate goals of programs , and measures to assess programs,should be - - raising questions about the validity of the instrument .

it is reasonable to conclude thatactors outside the executive branch , including members of congress , citizens , and interest groups,may have different perspectives and judgments on appropriate program goals and measures .

undergpra , stakeholder views such as these are required to be solicited by statute .

under the part,however , the role and process for stakeholder participation appears less certain .

other issues that gao identified could be interpreted as relating to the part instrument's validity in assessing program effectiveness ( eg , omb definitions of specific programs inconsistentwith agency organization and planning ) ; its reliability in making consistent assessments anddeterminations ( eg , inconsistent application of the instrument across multiple programs ) ; and its objectivity in design and usage .

to illustrate with some potential examples of objectivity issues,subjectivity could arguably be resident in a number of part questions , including , among others,when omb conducted its assessment for fy2005: ( 73 ) whether a program is "excessively" or "unnecessarily" ... "redundant orduplicative of any other federal , state , local , or private effort" [question 1.3 , p. 22] ; ( 74 ) whether a program's design is free of "major flaws" [question 1.4 , p. 23] ; ( 75 ) whether a program's performance measures "meaningfully" reflect theprogram's purpose [question 2.1 , p. 25] ; ( 76 ) and whether a program has demonstrated "adequate" progress in achievinglong - term performance goals [question 4.1 , p. 47] .

use of such terms that , in the absence of clear definitions , are subject to a variety of interpretationscan raise questions about the objectivity of the instrument and its ratings .

in one of its earliest publications on the part , omb said that "[w]hile subjectivity can beminimized , it can never be completely eliminated regardless of the method or tool .

 ( 77 ) omb went on to say,though , that the part "makes public and transparent the questions omb asks in advance of makingjudgments , and opens up any subjectivity in that process for discussion and debate. .

that said , thepart and its implementation to date nevertheless appear to place much of the process for debatingand determining program goals and measures squarely within the executive branch .

