finding and assessing a balanceamong priorities can be controversial and difficult . the outcomes of some agencies and programs are viewed by many observersas inherently difficult to measure . foreign policy and research and development programs have beencited as examples . there is frequently a time lag between an agency's or program's actions andeventual results ( or lack thereof ) . in the absence of this eventual outcome data , it is often difficultto know how to assess if a program is succeeding . many observers have asserted that agencies do not adequately evaluate theperformance or results of their programs - - or integrate evaluation efforts across agency boundaries - - possibly due to lack of capacity , management attention and commitment , or resources . ( 4 ) in spite of these and other challenges , ( 5 ) in the last 50 years both congress and the president haveundertaken numerous efforts - - sometimes referred to as performance management , performancebudgeting , strategic planning , or program evaluation - - to analyze and manage the federalgovernment's performance . many of those initiatives attempted in varying ways to use performanceinformation to influence budget and management decisions for agencies and programs . ( 6 ) the bush administration'srelease of part ratings along with the president's fy2004 and fy2005 budget proposals , and itsplans to continue doing so for fy2006 and subsequent years , represent the latest of these efforts . previous sections of this report discussed how the part is structured , how it has been used,and how various actors have assessed its design and implementation . this section discusses potentialcriteria for evaluating the part or other program evaluations , which might be considered bycongress during the budget process , in oversight of federal agencies and programs , and regardinglegislation that relates to program evaluation . ( 66 ) should congress focus on the question of criteria , the programevaluation and social science literature suggests that three standards or criteria may be helpful: theconcepts of validity , reliability , and objectivity . validity has been defined as "the extent to which any measuring instrumentmeasures what it is intended to measure. ( 67 ) for example , because the part is supposed to measure theeffectiveness of federal programs , its validity turns on the extent to which part scores reflect theactual "effectiveness" of those programs . ( 68 ) reliability has been described as "the relative amount of random inconsistencyor unsystematic fluctuation of individual responses on a measure" ; that is , the extent to which severalattempts at measuring something are consistent ( eg , by several human judges or several uses of thesame instrument ) . ( 69 ) therefore , the degree to which the part is reliable can be illustrated by the extent to which separateapplications of the instrument to the same program yield the same , or very similar,assessments . objectivity has been defined as "whether [an] inquiry is pursued in a way thatmaximizes the chances that the conclusions reached will be true. ( 70 ) definitions of the wordalso frequently suggest concepts of fairness and absence of bias . the opposite concept is subjectivity , suggesting , in turn , concepts of bias , prejudice , or unfairness .