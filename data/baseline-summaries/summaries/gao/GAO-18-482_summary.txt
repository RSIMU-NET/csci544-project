gender . we found no potential non - response bias for gender because the distribution of gender for respondents was similar to that in the population of students enrolled in the program . in addition to our non - response bias analysis , we assessed the reliability of the survey data by reviewing relevant agency documentation about the data and the system that produced them , testing data electronically , and interviewing eta officials knowledgeable about the data . we determined that the student survey data were sufficiently reliable for our purposes . for the 12 safety - related survey questions , job corps policy specified responses that the agency counted as safe or unsafe , which we followed . as noted previously , eta considers students to feel safe if they provided certain responses to each of the 12 safety - related survey questions , some of which are phrased as statements . for example , if a student provided a response of “mostly false” or “very false” to the statement “i thought about leaving job corps because of a personal safety concern,” that student would be counted as feeling safe on that survey question ( see table 3 ) . the percentages that we calculated are not comparable to prior publications , including eta reports , because , for example , eta revised ( i.e. , recoded ) students' responses in certain circumstances , as explained below in table 7 . meanwhile , we used the original responses that students provided and did not revise them . also , eta excluded responses of “don't know / does not apply” from its percentages . as a result , our percentages are not comparable with those reported by eta . we also calculated national measures of safety for the program and for particular demographic groups of students ( eg , male , female ) . our calculation was similar to eta's national safety rating in certain respects . it is unclear whether the distribution of race for respondents differs from that in the population . specifically , ignoring item non - response , about 7 percent of respondents selected “other,” and if those respondents were black / african american , the distributions between the respondents and sample would be similar since this would result in the respondent race percentage being close to 50 percent , like the population of enrollees . if respondents who selected “other” were actually distributed across the race categories , this would result in a difference between the respondent and population race / ethnicity characteristics , and to the extent that students' responses to safety questions differ by race , this could result in a potential bias of respondent survey results we analyzed . we analyzed race for purposes of potential non - response bias , and not as part of statistical tests of survey results described below . we determined that the student survey data were sufficiently reliable for our purposes . for example , as eta did , we determined how safe each individual student felt as the unit of analysis . therefore , the national measures of gao and eta may not equal the average of the 12 questions because , for example , not all students answered every safety question .