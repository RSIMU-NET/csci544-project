iv . to determine the extent to which fra used recommended practices for awarding discretionary grants , we examined office of management and budget guidance , guidance from several federal agencies , and our reports on this issue . ( see table 7. ) we identified key grant practices recommended across executive branch agencies and compared them to practices analyzed in our prior work . specifically , we identified six recommended practices relating to ( 1 ) communicating with potential applicants prior to the competition , ( 2 ) planning for administering the review of applications , ( 3 ) developing a technical review panel with certain characteristics , ( 4 ) assessing applicants' abilities to manage grant funds , ( 5 ) notifying applicants of decisions , and ( 6 ) documenting reasons for award decisions . we compared these practices to information from the 2009 funding announcement , guidance to applicant reviewers , and to statements made by fra officials regarding their implementation of their grants award program . for this effort , one analyst carried out the comparison and a second analyst verified the comparison results . where differences existed , the two analysts discussed them and reached agreement . we also discussed the extent of fra's use of several of these practices with the officials from our sample of 10 states . to determine the extent fra publicly communicated information about the results of its award competition , we compared the information it communicated to the public about its awards to the types of information communicated by a random sample of 20 other competitively awarded recovery act programs . ( see table 8. ) we selected the sample from 193 recovery act programs identified in the catalog of federal domestic assistance as competitive grant programs using recovery act funds . in addition , we compared the information communicated about fra's awards to the information communicated by the innovation grants program ( race to the top ) â€” a discretionary grant program run by the department of education . we included the race to the top program because you expressed interest in it . we first reviewed materials on fra's web site and other public releases , such as press releases and outreach presentations to determine what fra publicly communicated . we then discussed these results with fra officials to confirm our results . for each of the 21 other recovery act programs , we reviewed three public information sources: ( 1 ) the program's catalog of federal domestic assistance award announcement , ( 2 ) internet search results , and ( 3 ) grants.gov , which provides information on more than 1,000 grant programs . for each program , we searched these sources for information about final award results ( project description , why the project was selected , and award amount ) and for information that demonstrated how applications fared at different states of the process ( eligibility determination and internal reviews , such as technical review panels ) . we defined the results of any technical review as either scores or comments , and when at least one of these elements was listed in at least one of the three sources of information , we concluded that technical review information was publicly communicated about the program .