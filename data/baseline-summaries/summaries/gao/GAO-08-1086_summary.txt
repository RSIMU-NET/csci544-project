for sbinet , this has not occurred . contractor oversight was limited . project scope and complexity were underestimated . to manage sbinet , dhs established a program office within cbp . the program office is led by a program manager and deputy program managers for program operations and mission operations . according to sbinet officials , schedule changes are due largely to an immature system design , and the lack of a stable development approach is due to insufficient staff and turnover . the absence of clarity and stability in these key aspects of sbinet introduces considerable program risks , hampers dhs's ability to measure program progress , and impairs the ability of the congress to oversee the program and hold dhs accountable for program results . one key aspect of successfully managing large it programs , like sbinet , is establishing program commitments , including what capabilities are to be deployed and when and where they are to be deployed . only when such commitments are clearly established can program progress be measured and can responsible parties be held accountable . the scope and timing of planned sbinet deployments and capabilities that are to be delivered have not been clearly established , but rather have continued to change since the program began . table 2 provides a high - level description of the major reviews that are to be part of project laydown in the order that they occur . among the key processes provided for in the sbinet system life cycle management approach are processes for developing and managing requirements and for managing testing activities . with respect to requirements development and management , sbinet requirements are to consist of a hierarchy of six types of requirements , with the high - level operational requirements at the top . these high - level requirements are to be decomposed into lower - level , more detailed system , component , design , software , and project requirements . as another example , the draft test and evaluation master plan does not clearly define the roles and responsibilities of various entities that are involved in system testing . specifically , the plan identifies seven entities , but it only provides vague descriptions of their respective roles and responsibilities that are not meaningful enough to effectively guide their efforts . for example , the plan identifies two entities that are to be involved in operational testing: the dhs science and technology test and evaluation office and the u.s. army test and evaluation command . according to the plan , the dhs office is to function as the operational test authority and will be responsible for initial planning of “dedicated initial” and “follow - on” operational testing and evaluation , and the army group is to conduct operational testing . with no further clarification , it is not clear what is expected of each of these entities , including how they are to interact . table 6 lists each of the identified entities and provides their respective roles and responsibilities copied from the draft plan . besides being vague , the descriptions of roles and responsibilities are also incomplete and not consistent with other program documents .