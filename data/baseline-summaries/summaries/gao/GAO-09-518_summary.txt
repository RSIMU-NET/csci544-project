we do not agree . at the time we concluded our work , drrs requirements were not stable , as evidenced by the fact that an additional 530 requirements had been identified that the dio was still in the process of reviewing and had yet to reach a position on their inclusion , or process them through the drrs change control governance process . moreover , when we concluded our work , this change control process had yet to be approved by the drrs governance structure . as we state in our report , the introduction of such a large number of requirements provided a compelling basis for concluding that requirements had yet to progress to the point that they could be considered sufficiently complete and correct to provide a stable baseline . our recommendation also noted that the secretary should take steps to ensure that the different levels of requirements be aligned with one another . dod's comments did not address this aspect of our recommendation . the department did not agree with our recommendation for ensuring that drrs testing is effectively managed . in this regard , it stated that drrs testing is already in place and performing effectively , and stated , among other things , that ( 1 ) the dio goes through a rigorous testing regimen that includes documenting test plans with user test cases for each incremental release to include utilizing system integration , acceptance , interoperability , and operational testing ; ( 2 ) user test cases and functionality are validated by designated testers independent of the developers prior to a deployment ; and ( 3 ) for interoperability testing the dio has a designated test director and the joint interoperability test command ( jitc ) is the designated interoperability and operational test activity . we do not agree . as our report concludes , drrs testing has not been effectively managed because it has not followed a rigorous testing regimen that includes documented test plans , cases , and procedures . to support this conclusion , our report cites numerous examples of test planning and execution weaknesses , as well as the dio's repeated inability to demonstrate through requisite documentation that the testing performed on drrs has been adequate . our report shows that test events for already acquired , as well as currently deployed and operating , drrs releases and subreleases were not based on well - defined plans and dod had not filled its testing director vacancy . further , our report shows that test events were not fully executed in accordance with plans that did exist , or executed in a verifiable manner , or both . for example , although increments of drrs functionality had been put into production , the program had no documentation ( eg , test procedures , test cases , test results ) to show that the program office had performed system integration testing , system acceptance testing , or operational testing on any drrs release or subrelease , even though the dio's test strategy stated that such tests were to be performed before system capabilities became operational .