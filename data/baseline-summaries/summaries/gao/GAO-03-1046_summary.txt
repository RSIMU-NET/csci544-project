we did not independently evaluate the model's accuracy . as our report makes clear , our basic point is that the model's forecasting ability has not been documented in accordance with army guidance . we continue to believe that without adequate documentation , the army cannot show that it has taken sufficient steps to ensure the model's credibility in terms of its forecasting capability . dod also provided technical comments , which we incorporated where appropriate . we did not independently evaluate the model or the application of the steps ; rather , we reviewed the adequacy of the steps that the army program manager stated were taken to ensure the credibility of the model . to determine the adequacy of the steps the army has taken to ensure the credibility of its civilian workforce - forecasting model , we discussed civfors with the army's civfors program manager in the army g - 1 office , civilian personnel policy directorate , who has overall responsibility for the workforce analysis and the forecasting system . in addition , army contractor officials who are responsible for providing technical , analytic , and management support to operate , maintain , and enhance the planning tool and model participated in several of our discussions with the program manager . we reviewed the following civfors's documents regarding the information technology support structure: the configuration management manual , the system's specifications , the design / subsystem documentation , the operator's manual , and the user's manual . in addition , we reviewed the 1987 and draft 2002 test analysis report on the civilian forecasting system and other documentation provided by the army to obtain information on how the model operates according to model assumptions . documentation has often not been a priority for several reasons . according to the army's civfors program manager , lack of documentation is primarily due to limited funding , which was spent on implementing changes to civfors and wass rather than on the production of formal documents . further , a shortage of staff ( only one staff person — the program manager ) and loss of documents during the attack on the pentagon on september 11 , 2001 , also affected the amount of documentation the army could provide us . the forecasts for the last 2 years could then be compared to the actual historical data . the army , however , performed tests comparing patterns of forecasts against historical data ( called “in sample” tests ) , showing that forecasts reflect the same patterns as the historical data used to develop them for a sample of three army major commands . however , the draft document that was provided to us was inadequate to fully assess the sampling used by the army and the value of the tests . finally , the army could not provide adequate documentation of an independent or peer review of the model . the army's civfors program manager stated that the major commands served as peer reviewers by conducting a comparison of their workforce data to wass and civfors workforce data . we believe that such assessments by users provide important information but do not constitute a peer review as defined in army guidance .