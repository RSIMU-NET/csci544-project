we agree . we asked doe staff to compare their new performance indicators to the goals in the 2002 plan , and those are the goals that we presented for comparison in table 1 of our report . a discussion of the remainder of the hundreds of other goals was beyond the scope of our review and would not have added to an understanding of the overall problems with doe's goals . finally , we disagree with doe's comment that we mischaracterized the results of recent independent reviews . we noted instances in these reports where improvements were found . however , we also devoted appropriate attention to evidence in these reports that address whether doe's corrective actions have been effective . as our report states , these reports consistently found that these actions have not yet had their intended effect . in nrc's written comments , reproduced in appendix v , the agency agreed with our conclusions but suggested that doe be given the flexibility to choose alternative approaches to achieve and measure quality assurance program performance . we agree that alternative approaches could be used to measure performance ; however , to ensure the success of any approaches , doe must include objective measurements and time frames for reaching and sustaining desired performance and include an end point for closing out the corrective action plan . to assess the status of doe's corrective actions to resolve recurring quality problems , we reviewed audits and deficiency reports written by the program over the past 5 years that identified problems with data , models , and software . we did not independently assess the adequacy of data , models , and software , but rather relied on the results of the project's quality assurance audits . in addition , we reviewed numerous documents that nrc prepared as part of its prelicensing activities at yucca mountain , including observations of quality assurance audits , nrc on - site representative reports , and correspondence between nrc and doe on quality matters . we also observed an out - briefing of a quality assurance audit to obtain additional knowledge of how quality problems are identified and reported . we did not include the full range of performance indicators ( goals ) that have recently been developed , and continue to change , to assess the 2002 plan . instead , of the hundreds of indicators that are being developed to manage the project , we relied on those few that bechtel officials told us were connected to the goals of the 2002 plan . as table 1 shows , some improvements have been made in specifying the quantitative aspects of the goals , but weaknesses continue to exist in the new goals . in fact , table 1 shows that doe no longer has a goal in its performance tool that specifically tracks the trend in problems related to roles and responsibilities . this omission is particularly important because the area of roles and responsibilities was noted in the 2002 plan as one of the biggest sources of problems in the quality assurance process , and , as the recent assessments have found , this is an area with continuing problems . we disagree .