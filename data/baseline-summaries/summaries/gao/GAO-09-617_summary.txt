effective measures have all four attributes . agency measures often embodied one or more of these attributes ; however , the measures did not always address all key attributes . not all agencies have used predominantly quantitative measures of the 24 agencies we surveyed , most , but not all , used predominantly quantitative measures . specifically , 14 had discrete and quantitative measures comprising over 75 percent of their total number of measures , including 7 agencies for which 100 percent of their measures were quantitative . for an additional 7 agencies , 51 to 75 percent of their measures were quantitative . examples of such measures included percentage of incidents addressed according to policies and percentage of high - risk vulnerabilities mitigated in 30 days . such measures can be useful in comparing results with other information . however , for the remaining 3 agencies , less than half of their measures were quantitative . as an example of a nonquantitative measure , one agency reported its trusted internet connections implementation approach as an outcome - based performance measure — intended to determine the effectiveness or efficiency of information security policies and procedures . the measure , however , did not include a discrete unit of measure , but solely a description of the agency's plans to deploy six trusted internet connections access points and its inclusion of internet portal consolidation alternatives , justifications , and significant milestones . another agency developed a measure for continuity of operations planning by measuring the extent to which the plan enables the execution in a degraded environment or at alternate locations using qualitative indicators — such as “minor,” “some,” “significant,” and “major” deficiencies — that were not defined . examples of other agency measures that could result in ambiguous results include ensure systems have no default user ids , review it use policy document and update as necessary , and conduct reviews of it security programs at operating units . to the extent that agencies do not use quantifiable measures of their security control activities , they may limit their ability to produce accurate and useful assessments of their information security programs . agencies measures were not always clearly defined or did not always have specific performance targets while many agency measures were clearly defined , they were not consistently so in all cases and did not always set specific performance targets . of the 24 agencies , 16 had clear definitions measures for over 75 percent of their total number of measures . an example of an impact metric involves a financial institution that wanted to better understand its malware risks . to do so , the institution developed a metric that compared a compliance metric ( percentage of systems with updated antivirus software ) with a control effectiveness metric ( time to deploy new patches [from a security vendor] to all systems ) to produce a measure of the organization's overall exposure to malware because of systems not being fully up to date with security patches . the institution found that the measure could be used to gauge the overall impact of its information security program on the risk of malware infection .