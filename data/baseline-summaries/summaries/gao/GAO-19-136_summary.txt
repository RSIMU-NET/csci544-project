early engagement . the jms program office planned to involve users early in development but , in practice , did not do so . jms program documentation states that users were to be involved in user engagement sessions within the first 4 weeks of iterative development . however , the first documented user engagement session was held more than a year after development start . the contractor said it had to gain approval from the dod chief information office to employ commercial cloud - based testing for the unclassified portions of ocx but it has not gained similar approval for the classified portion . the sbirs contractor is using a software testing tool that would allow for faster automated testing but is not yet realizing the full benefit of its use . the sbirs testers did not use this tool in the way it was intended . specifically , the contractor said that when the software was deployed to the testing environment , testers deactivated the software at the end of their shifts instead of allowing it to run continuously until the tests were complete . the contractor said the testers did this because there were concerns over unauthorized access to the system if no one was present . as a result , the contractor separated the tests into 8-hour segments rather than allowing the tests to run continuously , reducing the effectiveness and value of automated testing . the defense science board , defense innovation board , and others have recommended dod use tools that enable the developers , users , and management to work together daily . as noted , dod is required to begin implementation of the recommendations made in the defense science board report . software metrics are measurements which provide insight to the status and quality of software development . metrics may not support newer development approaches . we have previously found that leading developers track software - specific metrics to gauge a program's progress , and that traditional cost and schedule metrics alone may not provide suitable awareness for managing iterative software development performance . three programs have faced challenges in identifying and collecting metrics that provide meaningful insight into software development progress: jms planned to collect traditional software development metrics to measure software size and quality , as well as agile metrics that provide insight into development speed and efficiency . however , officials from the jms government integrator managing sub - contracts said they lack regular reporting of metrics and access to data from subcontractors that would allow them to identify defects early . these officials said this was a challenge because the program has to run its own quality scans at the end of each sprint instead of being able to identify defects on a daily basis . muos program officials were able to receive agile metrics from the contractor when they transitioned to agile development , but they lacked access to the source data , which they said hindered their ability to oversee development . ocx program officials said they plan to use performance - based metrics throughout the remainder of the program . however , the metrics may not adequately track performance as intended .