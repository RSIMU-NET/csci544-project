iii. ) by law , dot&e serves as the principal adviser on operational test and evaluation in dod and bears several key responsibilities , including monitoring and reviewing all operational test and evaluation in dod , reporting to the secretary of defense and congressional committees whether the tests and evaluations of weapon systems were adequate and whether the results confirmed that the system is operationally suitable and effective for combat before a decision is made to proceed to full - rate production , and submitting to the secretary of defense and congressional decisionmakers an annual report summarizing operational test and evaluation activities during the preceding fiscal year . in 1993 , dod's advisory panel on streamlining and codifying acquisition laws concluded that dot&e was impeding the goals of acquisition reform by ( 1 ) promoting unnecessary oversight , ( 2 ) requiring excessive reporting detail , ( 3 ) inhibiting the services' discretion in testing , and ( 4 ) limiting participation of system contractors in operational tests where such involvement is deemed necessary by the services . the following year , dod proposed legislative changes that would have reduced the scope and authority of dot&e . to compile case study data , we interviewed current action officers in both dot&e and the appropriate operational test agency and reviewed documentation provided by the operational test agencies , dot&e , and ida . using structured questionnaires , we interviewed 12 dot&e and 27 operational test agency action officers responsible for the 13 selected systems as well as managers and technical support personnel in each organization . in addition , we interviewed the commanders of each of the service testing agencies and dot&e . when possible , we corroborated information obtained from interviews with documentation , including test and evaluation master plans , beyond low - rate initial production reports , defense acquisition executive summary status reports , defense acquisition memoranda , and interagency correspondence . in washington , d.c. , we obtained data from or performed work at the office of the director of operational test and evaluation , osd ; deputy under secretary of defense for acquisition reform ; directorate of navy test and evaluation and technology requirements , office of the chief of naval operations ; test and evaluation management agency , director of army staff ; air force test and evaluation directorate ; and the dod office of the inspector general . we also reviewed data and interviewed officials from the army operational test and evaluation command and the institute for defense analyses , alexandria , virginia ; the navy commander , operational test and evaluation force , norfolk , virginia ; and the air force operational test and evaluation command , kirtland air force base , new mexico . the use of a systematic case study framework enabled us to identify and categorize the types of impacts attributable to dot&e among the systems studied . in addition , this framework enabled us to identify trends among factors that correlate with dot&e effectiveness . however , we were unable to generalize to all systems subject to osd - level oversight .