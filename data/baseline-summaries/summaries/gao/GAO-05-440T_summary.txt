messrs . chairmen , this concludes our statement . we would be happy to answer any questions that you or members of the subcommittees may have at this time . if you should have any questions about this testimony , please contact randolph c. hite at ( 202 ) 512-3439 or hiter@gao.gov , or jess t. ford at ( 202 ) 512-4128 or fordj@gao.gov . other major contributors to this testimony included john brummet , barbara collier , deborah davis , jamelyn payan , and elizabeth singer . this is a work of the u.s. government and is not subject to copyright protection in the united states . it may be reproduced and distributed in its entirety without further permission from gao . however , because this work may contain copyrighted images or other material , permission from the copyright holder may be necessary if you wish to reproduce this material separately . each indicator is discussed below . whether defined system requirements are being met is one indicator of system performance . in june 2004 , we reported that performance reports showed that some , but not all , key system requirements were being measured , and that these measured requirements were being met . table 4 shows examples of key system performance requirements . however , we also reported that not all key performance requirements were being adequately measured . for example , reports used to measure system availability measured the time that the system infrastructure was successfully connected to the network . while these reports can be used to identify problems that could affect the system availability , they do not fully measure sevis availability . instead , they measure the availability of the communications software on the application servers . this means that the sevis application could still be unavailable even though the communications software is available . similarly , program officials stated that they used a central processing unit activity report to measure resource usage . however , this report focuses on the shared infrastructure environment , which supports sevis and two other applications , and does not specifically measure sevis - related central processing performance . program officials did not provide any reports that measured performance against other resource usage requirements , such as random access memory and network usage . program officials acknowledged that some key performance requirements were not formally measured and stated that they augmented these formal performance measurement reports with other , less formal measures , such as browsing the daily help desk logs to determine if there were serious performance problems requiring system changes or modifications , as well as using the system themselves on a continuous basis . according to these officials , a combination of formal performance reports and less formal performance monitoring efforts gave them a sufficient picture of how well sevis was performing . further , program officials stated that they were exploring additional tools to monitor system performance . for example , they stated that they were in the process of implementing a new tool to capture the availability of the sevis application , and that they planned to begin using it by the end of april 2004 .