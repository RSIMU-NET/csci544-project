irs does not have a timeline for this integration . in 2016 , we identified five leading practices for designing a well - developed and documented pilot program: ( 1 ) : ensuring stakeholder communication , ( 2 ) establishing objectives , ( 3 ) ensuring scalability , ( 4 ) having an assessment methodology , and ( 5 ) developing a data - analysis plan . these practices enhance the quality , credibility , and usefulness of evaluations and help ensure that time and resources are used effectively . however , we have previously reported that without well - defined , appropriate , clear , and measurable objectives , it will be difficult to ensure appropriate evaluation data are collected and available to measure performance against the objectives and goals . in short , it will be difficult for irs to know whether it achieved its objectives . without knowing this , irs will have difficulty justifying investing additional resources . ensure scalability of pilot design: the purpose of a pilot is generally to inform a decision on whether and how to implement a new approach in a broader context . identifying criteria or standards for identifying lessons about the pilot will help inform an agency's decisions about scalability and when to integrate pilot activities into overall efforts . we previously reported that the criteria and standards should be observable and measureable events , actions , or characteristics that provide evidence that the pilot objectives have been met . irs's efforts in designing the isac partially aligned with this leading practice . first , irs identified and integrated lessons learned into its pilot . for example , ahead of isac's launch , irs's contractor identified potential capabilities of the isac based on lessons learned from four isacs from other industries and a 2-day collaborative session in summer 2015 . in february 2017 , 1 month after the isac's launch , the board established the metrics subgroup to develop evaluation criteria to determine the extent to which the pilot objectives have been met . according to isac board officials , the metrics subgroup is developing and testing metrics that the isac board expects to use beginning in the 2018 filing season . the metrics are designed to measure participation in the isac , contribution of data or information to the isac , and the effectiveness of the data or information provided . irs also took steps to improve the isac pilot design , which will help it scale the pilot in the future . for example , in may 2017 , irs's contractor presented lessons learned from the 2017 filing season , including what was accomplished , what should be changed in future filing seasons , and areas for future attention to consider how well the lessons learned can be applied when the pilot is scaled up . the contractor's presentation also outlined recommendations from a may 2017 independent assessment of the isac , including the current status of each recommendation and actions needed to implement them . in addition , during the july 2017 isac board meeting , irs's contractor discussed lessons learned , and the irs isac executive official discussed takeaways thus far from standing up the isac .