portfoliostat . in order to eliminate duplication , move to shared services , and improve portfolio management processes , in march 2012 omb launched the portfoliostat initiative . specifically , portfoliostat requires agencies to conduct an annual agency - wide it portfolio review to , among other things , reduce commodity it spending and demonstrate how their it investments align with the agency's mission and business functions . portfoliostat is designed to assist agencies in assessing the current maturity of their it investment management process , making decisions on eliminating duplicative investments , and moving to shared solutions in order to maximize the return on it investments across the portfolio . omb believes that the portfoliostat effort has the potential to save the government $2.5 billion over the next 3 years by , for example , consolidating duplicative systems . over the past several years , we have highlighted omb and agency efforts to improve the transparency into and oversight of underperforming federal it investments , more effectively manage it , and address duplicative investments . notably , we issued a series of reports on: the it dashboard ; omb and agency efforts to address troubled projects through techstat reviews ; critical factors underlying successful acquisitions ; and omb and agency efforts to improve the management of it through federal data center consolidation efforts , as well as address duplication through portfoliostat . omb has taken significant steps to enhance the oversight , transparency , and accountability of federal it investments by creating its it dashboard , and by improving the accuracy of investment ratings . however , we found weaknesses with the accuracy and reliability of cost and schedule data , and we recommended steps that omb should take to improve these data . our july 2010 report found that the cost and schedule ratings on omb's dashboard were not always accurate for the investments we reviewed , because these ratings did not take into consideration current performance . as a result , the ratings were based on outdated information . we recommended that omb report on its planned changes to the dashboard to improve the accuracy of performance information and provide guidance to agencies to standardize milestone reporting . omb agreed with our recommendations and , as a result , updated the dashboard's cost and schedule calculations to include both ongoing and completed activities . similarly , in march 2011 , omb had initiated several efforts to increase the dashboard's value as an oversight tool , and had used its data to improve federal it management . however , agency practices and the dashboard's calculations contributed to inaccuracies in the reported investment performance data . these included , for instance , missing data submissions or erroneous data at each of the five agencies we reviewed , along with instances of inconsistent program baselines and unreliable source data . as a result , we recommended that the agencies take steps to improve the accuracy and reliability of their dashboard information , and that omb improve how it rates investments relative to current performance and schedule variance . most agencies generally concurred with our recommendations ; omb agreed with our recommendation for improving ratings for schedule variance .