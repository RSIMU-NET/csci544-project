most agencies have agreed with our recommendations . most recently , in june 2016 , we determined that 13 of the 15 agencies selected for in - depth review had not fully considered risks when rating their major investments on the it dashboard . specifically , our assessments of risk for 95 investments at 15 selected agencies matched the cio ratings posted on the dashboard 22 times , showed more risk 60 times , and showed less risk 13 times . figure 3 summarizes how our assessments compared to the selected investments' cio ratings . aside from the inherently judgmental nature of risk ratings , we identified three factors which contributed to differences between our assessments and the cio ratings: forty of the 95 cio ratings were not updated during the month we reviewed , which led to more differences between our assessments and the cios' ratings . this underscores the importance of frequent rating updates , which help to ensure that the information on the dashboard is timely and accurately reflects recent changes to investment status . three agencies' rating processes spanned longer than 1 month . longer processes mean that cio ratings are based on older data , and may not reflect the current level of investment risk . seven agencies' rating processes did not focus on active risks . according to omb's guidance , cio ratings should reflect the cio's assessment of the risk and the investment's ability to accomplish its goals . cio ratings that do not incorporate active risks increase the chance that ratings overstate the likelihood of investment success . as a result , we concluded that the associated risk rating processes used by the 15 agencies were generally understating the level of an investment's risk , raising the likelihood that critical federal investments in it are not receiving the appropriate levels of oversight . to better ensure that the dashboard ratings more accurately reflect risk , we recommended that the 15 agencies take actions to improve the quality and frequency of their cio ratings . twelve agencies generally agreed with or did not comment on the recommendations and three agencies disagreed , stating their cio ratings were adequate . however , we noted that weaknesses in their processes still existed and that we continued to believe our recommendations were appropriate . omb generally agreed with this recommendation . however , when we testified on this issue slightly more than 2 years later in november 2015 , we found that omb had only conducted one techstat review between march 2013 and october 2015 . in addition , we noted that omb had not listed any savings from techstats in any of its required quarterly reporting to congress since june 2012 . this issue continues to be a concern and , in january 2017 , the federal cio council issued a report titled the state of federal information technology , which noted that while early techstats saved money and turned around underperforming investments it was unclear if omb had performed any techstats in recent years .