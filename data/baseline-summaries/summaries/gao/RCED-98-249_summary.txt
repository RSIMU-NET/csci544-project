( app . ii discusses in detail how we developed each comparison. ) comparisons of ost's deployment rate with the rates of other organizations must be viewed with caution when assessing how well em is doing in deploying ost - developed technologies . we found few organizations that engage in the range of environmental research ost performs , and no organization we contacted routinely tracked deployment data on its projects . data provided by the two organizations differed widely in source and composition . finally , many individuals we contacted question whether a deployment rate is a sufficient benchmark for successful r&d . most organizations we contacted , including some private technology developers , did not track deployment data comparable to ost's . of the eight government programs and two private sector programs engaged in environmental technology research we contacted , only the site program and estcp could provide data on deployment . even these two programs needed to compile their information so that it could be expressed as deployment rates . table 2.3 shows the entities that we contacted . furthermore , we found that only one of the other government programs listed in table 2.3 engaged in nearly the full range of environmental r&d that ost performs . ost's r&d includes basic science research , applied research and engineering development , field testing and demonstration , and implementation by the end user ( commercialization ) . most of the governmental organizations we contacted performed either the early stages of r&d or the later stages , but not both . technology development efforts undertaken at the early stages have more unknowns and are likely to involve a greater risk of failure than efforts at the later stages . since we would expect performance results to differ for each stage , meaningful comparisons can only be made among projects or programs that are at similar stages of r&d maturity . two organizations provided us with very different types of data . epa's site program had accumulated survey data on the number of contracts their technology vendors had obtained over about 8 years . we agreed that a contract for use could be considered deployment of the technology . as to be expected , the survey responses were less than 100 percent , unlike the ost and estcp data , which include all of these agencies' technology projects . therefore , the data from epa's site program are incomplete , and the deployment rate for site could actually be higher . the department of defense's estcp provided a description of the transition ( deployment ) status for all of its projects from the program's first 2 years of existence . since estcp is a relatively new program , its deployment data are based on a limited number of projects and may be less representative of the program's future performance . we did not verify the accuracy of these organizations' deployment data , but we reviewed their available project summaries and believe the organizations' approaches were reasonable responses to our request . nevertheless , differences in how the programs defined deployment , and whether they counted incomplete projects , will affect computed rates . as we have previously reported , measuring the performance of r&d programs is difficult .