dod has not adequately ( 1 ) coordinated its efforts to develop and use software metrics for defense acquisition programs ; ( 2 ) made maximum use of contractors' software development processes that have been favorably assessed by credible , independent evaluation ; ( 3 ) developed team - building efforts to ensure the early and continuous involvement of users with acquisition , post - deployment support and testing personnel ; and ( 4 ) filled in the policy and oversight voids that have contributed to the shortfalls that we have addressed .
the barriers are ( 1 ) failure of the acquisition community to adequately address the critical nature of software ; ( 2 ) lack of credible cost , schedule , and performance data as the basis for decision - making ; ( 3 ) lack of specific software test and evaluation policy ; and ( 4 ) ineffective definition and management of requirements for software .
also , this relatively informal process has focused on projecting weapon systems' suitability but not effectiveness .
activity officials said that the indicators provided a clear and concise reporting structure ; improved planning and data collection for future projections ; provided a degree of standardization among projects , contractors , and in - house personnel ; fostered closer adherence to military standards ; greatly improved communication and problem and process definition ; acted as a reliable early - warning system if plans were not met ; and highlighted small problems early so that they could be resolved before they grew .